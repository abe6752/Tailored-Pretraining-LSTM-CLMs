{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language-based generative models\n",
    "\n",
    "LSTM and GPT-based vanila chemical language models have been implemented and simple usage is explained. Development history and statistics for parameter selection are reported separately in `lstm_generative_model_flow.md`.\n",
    "\n",
    "Currently, LSTM-based model is stable and should be used for language modeling. The GPT-based model is okay, but still hard to understand its behaivors in particular hyperparameters (nblocks, nheads, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning data status: filtered\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.paths import ensure_dirs, PRETRAIN_DATA, PRETRAIN_RESULTS, FINETUNE_FILTER\n",
    "ensure_dirs()\n",
    "print(f'Fine-tuning data status: {FINETUNE_FILTER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Function to run command line application on a jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importstr(module_str, from_=None):\n",
    "\t\"\"\"\n",
    "\tmodule_str: module to be loaded as string \n",
    "\t>>> importstr('os) -> <module 'os'>\n",
    "\t\"\"\"\n",
    "\tif (from_ is None) and ':' in module_str:\n",
    "\t\tmodule_str, from_ = module_str.rsplit(':')\n",
    "\tmodule = __import__(module_str)\n",
    "\tfor sub_str in module_str.split('.')[1:]:\n",
    "\t\tmodule = getattr(module, sub_str)\n",
    "\t\n",
    "\tif from_:\n",
    "\t\ttry:\n",
    "\t\t\treturn getattr(module, from_)\n",
    "\t\texcept:\n",
    "\t\t\traise ImportError(f'{module_str}.{from_}')\n",
    "\treturn module\n",
    "\n",
    "\n",
    "def run(app, *argv):\n",
    "\targv=list(argv)\n",
    "\tapp_cls=importstr(*app.rsplit('.',1))\n",
    "\tapp_cls(argv).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize SMILES\n",
    "To determine which tokens are used ('[C@H]', 'c', '8', etc.), a data set is tokenized to calculate the frequency of tokens. This process can be skipped if you already know the tokens to be used for your generative models.\n",
    "\n",
    "### 1. Calculate frequency of tokens\n",
    "To calcualte the frequency of tokens, `apps.count_token_frequency.py` is used. Example data set is provided as well in the data folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the script as a command line program. Use the following codes. If you want to run on a notebook, run the codes in the next cell.\n",
    "```\n",
    "$ python generative_models/apps/count_token.py --data data/top2000_curated_cpds_chembl31.tsv --smi-colname washed_openeye_smiles --outdir tokenize_results --heavy-atom-ratio-thres 0.95\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['pubchem_filtered_ac', \n",
    "                'pubchem_unfiltered_ac', \n",
    "                'pubchem_inac', \n",
    "                'chembl_filtered', \n",
    "                'chembl_unfiltered', \n",
    "                'zinc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logging\n",
      "Loaded molecules: 125149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannnot handeld mols in rdkit: 0\n",
      "passed mols: 125149\n",
      "maximum heavy atom number is 128\n",
      "minimum heavy atom number is 5\n",
      "the NHA percentile 0.95\n",
      "the NHA threshold 38.0\n",
      "selected molecules: 119665\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../src/model/')\n",
    "\n",
    "for dataset in dataset_list:\n",
    "\trun(\"generative_models.apps.CountTokens.CountTokenFreqApp\",\n",
    "\t\tf\"--data={PRETRAIN_DATA}/{dataset}_rdsmi3.tsv\",\n",
    "\t\t\"--smi-colname=rdkit_smiles\",\n",
    "\t\tf\"--outdir={PRETRAIN_RESULTS}/{dataset}_results/tokenize_results\",\n",
    "\t\t\"--heavy-atom-ratio-thres=0.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can find **tokenize_resluts** folder specficed as input arguments. Looking at token frequencies in `token_requency.tsv`, you can determine which tokens you are going to use for a subsequent analysis, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        frequency\n",
      "c         1342612\n",
      "C          898109\n",
      "(          510468\n",
      ")          510468\n",
      "1          352208\n",
      "O          325534\n",
      "2          260868\n",
      "=          239737\n",
      "N          201959\n",
      "n          135041\n",
      "3          118084\n",
      "S           39204\n",
      "F           34531\n",
      "-           28161\n",
      "4           27192\n",
      "Cl          23110\n",
      "s           19913\n",
      "/           18058\n",
      "o           15958\n",
      "[C@H]       15689\n",
      "[C@@H]      15345\n",
      "[nH]        11740\n",
      "[N+]         7766\n",
      "[O-]         7745\n",
      "#            7281\n",
      "Br           6537\n",
      "\\            4300\n",
      "5            2970\n",
      "I             767\n",
      "[C@@]         516\n",
      "[C@]          501\n",
      "P             342\n",
      "[n+]          306\n",
      "6             198\n",
      "7              18\n",
      "[c-]           15\n",
      "[S@@]           9\n",
      "B               6\n",
      "[S+]            6\n",
      "[P+]            3\n",
      "[s+]            3\n",
      "[As]            3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "freq = pd.read_csv(f'{PRETRAIN_RESULTS}/{dataset}_results/tokenize_results/token_frequency.tsv', sep='\\t', index_col=0)\n",
    "print(freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a gap between `[+]` and `6` as below:\n",
    "|Token| Frequency|\n",
    "|----|----|\n",
    "|[S+]|183|\n",
    "|[n+]|178|\n",
    "|6|44|\n",
    "\n",
    "Thus, tokens with more than 100 frequencies are employed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['c', 'C', '(', ')', '1', 'O', '2', '=', 'N', 'n', '3', 'S', 'F', '-',\n",
       "       '4', 'Cl', 's', '/', 'o', '[C@H]', '[C@@H]', '[nH]', '[N+]', '[O-]',\n",
       "       '#', 'Br', '\\', '5', 'I', '[C@@]', '[C@]', 'P', '[n+]', '6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok_tokens = freq[freq['frequency'] > 100].index\n",
    "ok_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select eligible molecules and vocabulary set (dictionarr) for generation\n",
    "We decided the tokens eligible for generative models. The training data set `ok_mols.tsv` were further filtered based on the selected tokens above.\n",
    "`app.select_mols_vocab.py` is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logging\n",
      "Loaded molecules: 119665\n",
      "curating tokens: frequency threshold: 50\n",
      "ok tokens: 34\n",
      "Index(['c', 'C', '(', ')', '1', 'O', '2', '=', 'N', 'n', '3', 'S', 'F', '-',\n",
      "       '4', 'Cl', 's', '/', 'o', '[C@H]', '[C@@H]', '[nH]', '[N+]', '[O-]',\n",
      "       '#', 'Br', '\\', '5', 'I', '[C@@]', '[C@]', 'P', '[n+]', '6'],\n",
      "      dtype='object')\n",
      "eligible smiles: 119623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/noteboooks/../src/model/generative_models/apps/SelectMolsVocab.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  okmols['ntokens_srt_end'] = okmols['rdkit_smiles'].apply(lambda x: count_moltokens(x, include_begin_and_end=True))\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../src/model/')\n",
    "\n",
    "for dataset in dataset_list:\n",
    "\tappname  = 'generative_models.apps.SelectMolsVocab.SelectMolsVocabApp'\n",
    "\trun(appname,\n",
    "\t\tf\"--data={PRETRAIN_RESULTS}/{dataset}_results/tokenize_results/ok_mols.tsv\",\n",
    "\t\t\"--smi-colname=rdkit_smiles\",\n",
    "\t\tf\"--tokens={PRETRAIN_RESULTS}/{dataset}_results/tokenize_results/token_frequency.tsv\",\n",
    "\t\tf\"--outdir={PRETRAIN_RESULTS}/{dataset}_results/dataset4lstm\",\n",
    "\t\t\"--token-frequency-thres=50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find `dataset4lstm` folder, where passed_molecules tsv and vocaburary dictionary pickle files are stored. These two files are used for training LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Build LSTM model\n",
    "An LSTM model is trained on the data set specified. Various options are available. You can decide appropriate one by traial, or you can read the report on benchmark testing regarding hyperparameteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm\n",
      "Start logging\n",
      "CUDA is on: 1 devices\n",
      "Model initialization is done\n",
      "Starting: GenerativeModelTrainer, Namespace(num_workers=20, outdir='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/pretrain/pubchem_inac_results/vanilalstm', case='', override_folder=1, tensorboard_prefix='tensor_board', data='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/pretrain/pubchem_inac_results/dataset4lstm/passed_mols.tsv', smi_colname='rdkit_smiles', vocab='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/pretrain/pubchem_inac_results/dataset4lstm/vocab.pickle', random_seed=42, debug=None, sampling_epoch=1000, model='lstm', write_xlsx=None, save_snapshot_models='10,20,30', batch_size=128, epochs=30, validation_ratio=0.1, lr=0.0005, exclude_pad_loss=1, early_stopping_patience=4, load_model=None, model_structure=None, model_state=None, standardize_smiles=None, embed_dim=256, dropout_ratio=0.2, nlayers=4, hidden_dim=512, layernorm=True)\n",
      "Epoch 1/30, training: 842 and validation:94 batches, size 128*1\n",
      "Epoch 1 -- Batch 1/ 842, training loss 3.8531744480133057\n",
      "Epoch 1 -- Batch 2/ 842, training loss 3.047800064086914\n",
      "Epoch 1 -- Batch 3/ 842, training loss 2.6955626010894775\n",
      "Epoch 1 -- Batch 4/ 842, training loss 2.655299663543701\n",
      "Epoch 1 -- Batch 5/ 842, training loss 2.5407824516296387\n",
      "Epoch 1 -- Batch 6/ 842, training loss 2.556136131286621\n",
      "Epoch 1 -- Batch 7/ 842, training loss 2.493978261947632\n",
      "Epoch 1 -- Batch 8/ 842, training loss 2.541430950164795\n",
      "Epoch 1 -- Batch 9/ 842, training loss 2.497735023498535\n",
      "Epoch 1 -- Batch 10/ 842, training loss 2.4752273559570312\n",
      "Epoch 1 -- Batch 11/ 842, training loss 2.4679317474365234\n",
      "Epoch 1 -- Batch 12/ 842, training loss 2.4573683738708496\n",
      "Epoch 1 -- Batch 13/ 842, training loss 2.4469971656799316\n",
      "Epoch 1 -- Batch 14/ 842, training loss 2.416463613510132\n",
      "Epoch 1 -- Batch 15/ 842, training loss 2.4035565853118896\n",
      "Epoch 1 -- Batch 16/ 842, training loss 2.398279905319214\n",
      "Epoch 1 -- Batch 17/ 842, training loss 2.397644519805908\n",
      "Epoch 1 -- Batch 18/ 842, training loss 2.365234136581421\n",
      "Epoch 1 -- Batch 19/ 842, training loss 2.360945463180542\n",
      "Epoch 1 -- Batch 20/ 842, training loss 2.3617708683013916\n",
      "Epoch 1 -- Batch 21/ 842, training loss 2.3508152961730957\n",
      "Epoch 1 -- Batch 22/ 842, training loss 2.3436648845672607\n",
      "Epoch 1 -- Batch 23/ 842, training loss 2.300675868988037\n",
      "Epoch 1 -- Batch 24/ 842, training loss 2.3127293586730957\n",
      "Epoch 1 -- Batch 25/ 842, training loss 2.2981820106506348\n",
      "Epoch 1 -- Batch 26/ 842, training loss 2.2766551971435547\n",
      "Epoch 1 -- Batch 27/ 842, training loss 2.2511985301971436\n",
      "Epoch 1 -- Batch 28/ 842, training loss 2.237640857696533\n",
      "Epoch 1 -- Batch 29/ 842, training loss 2.2399098873138428\n",
      "Epoch 1 -- Batch 30/ 842, training loss 2.256568193435669\n",
      "Epoch 1 -- Batch 31/ 842, training loss 2.21903395652771\n",
      "Epoch 1 -- Batch 32/ 842, training loss 2.2142322063446045\n",
      "Epoch 1 -- Batch 33/ 842, training loss 2.1949455738067627\n",
      "Epoch 1 -- Batch 34/ 842, training loss 2.189408540725708\n",
      "Epoch 1 -- Batch 35/ 842, training loss 2.183382034301758\n",
      "Epoch 1 -- Batch 36/ 842, training loss 2.1213507652282715\n",
      "Epoch 1 -- Batch 37/ 842, training loss 2.129833221435547\n",
      "Epoch 1 -- Batch 38/ 842, training loss 2.126695156097412\n",
      "Epoch 1 -- Batch 39/ 842, training loss 2.0738775730133057\n",
      "Epoch 1 -- Batch 40/ 842, training loss 2.0958669185638428\n",
      "Epoch 1 -- Batch 41/ 842, training loss 2.0860321521759033\n",
      "Epoch 1 -- Batch 42/ 842, training loss 2.03798770904541\n",
      "Epoch 1 -- Batch 43/ 842, training loss 2.035299777984619\n",
      "Epoch 1 -- Batch 44/ 842, training loss 2.0024800300598145\n",
      "Epoch 1 -- Batch 45/ 842, training loss 1.9781328439712524\n",
      "Epoch 1 -- Batch 46/ 842, training loss 1.9954065084457397\n",
      "Epoch 1 -- Batch 47/ 842, training loss 1.9724600315093994\n",
      "Epoch 1 -- Batch 48/ 842, training loss 1.9251360893249512\n",
      "Epoch 1 -- Batch 49/ 842, training loss 1.922834873199463\n",
      "Epoch 1 -- Batch 50/ 842, training loss 1.9284422397613525\n",
      "Epoch 1 -- Batch 51/ 842, training loss 1.901545524597168\n",
      "Epoch 1 -- Batch 52/ 842, training loss 1.9913119077682495\n",
      "Epoch 1 -- Batch 53/ 842, training loss 1.9659521579742432\n",
      "Epoch 1 -- Batch 54/ 842, training loss 1.9389069080352783\n",
      "Epoch 1 -- Batch 55/ 842, training loss 1.8818118572235107\n",
      "Epoch 1 -- Batch 56/ 842, training loss 1.8710418939590454\n",
      "Epoch 1 -- Batch 57/ 842, training loss 1.888631820678711\n",
      "Epoch 1 -- Batch 58/ 842, training loss 1.855806589126587\n",
      "Epoch 1 -- Batch 59/ 842, training loss 1.8220276832580566\n",
      "Epoch 1 -- Batch 60/ 842, training loss 1.8564480543136597\n",
      "Epoch 1 -- Batch 61/ 842, training loss 1.8344075679779053\n",
      "Epoch 1 -- Batch 62/ 842, training loss 1.7997641563415527\n",
      "Epoch 1 -- Batch 63/ 842, training loss 1.7990117073059082\n",
      "Epoch 1 -- Batch 64/ 842, training loss 1.7452975511550903\n",
      "Epoch 1 -- Batch 65/ 842, training loss 1.76669180393219\n",
      "Epoch 1 -- Batch 66/ 842, training loss 1.7791743278503418\n",
      "Epoch 1 -- Batch 67/ 842, training loss 1.7165979146957397\n",
      "Epoch 1 -- Batch 68/ 842, training loss 1.7008448839187622\n",
      "Epoch 1 -- Batch 69/ 842, training loss 1.690462350845337\n",
      "Epoch 1 -- Batch 70/ 842, training loss 1.6867785453796387\n",
      "Epoch 1 -- Batch 71/ 842, training loss 1.6747359037399292\n",
      "Epoch 1 -- Batch 72/ 842, training loss 1.6554713249206543\n",
      "Epoch 1 -- Batch 73/ 842, training loss 1.6374984979629517\n",
      "Epoch 1 -- Batch 74/ 842, training loss 1.6652867794036865\n",
      "Epoch 1 -- Batch 75/ 842, training loss 1.6643871068954468\n",
      "Epoch 1 -- Batch 76/ 842, training loss 1.6139227151870728\n",
      "Epoch 1 -- Batch 77/ 842, training loss 1.6274718046188354\n",
      "Epoch 1 -- Batch 78/ 842, training loss 1.6445280313491821\n",
      "Epoch 1 -- Batch 79/ 842, training loss 1.595831274986267\n",
      "Epoch 1 -- Batch 80/ 842, training loss 1.5824644565582275\n",
      "Epoch 1 -- Batch 81/ 842, training loss 1.6183923482894897\n",
      "Epoch 1 -- Batch 82/ 842, training loss 1.5810132026672363\n",
      "Epoch 1 -- Batch 83/ 842, training loss 1.5685973167419434\n",
      "Epoch 1 -- Batch 84/ 842, training loss 1.5511891841888428\n",
      "Epoch 1 -- Batch 85/ 842, training loss 1.5722450017929077\n",
      "Epoch 1 -- Batch 86/ 842, training loss 1.5236064195632935\n",
      "Epoch 1 -- Batch 87/ 842, training loss 1.514626383781433\n",
      "Epoch 1 -- Batch 88/ 842, training loss 1.4962276220321655\n",
      "Epoch 1 -- Batch 89/ 842, training loss 1.5295295715332031\n",
      "Epoch 1 -- Batch 90/ 842, training loss 1.5229129791259766\n",
      "Epoch 1 -- Batch 91/ 842, training loss 1.5174931287765503\n",
      "Epoch 1 -- Batch 92/ 842, training loss 1.499566674232483\n",
      "Epoch 1 -- Batch 93/ 842, training loss 1.4884119033813477\n",
      "Epoch 1 -- Batch 94/ 842, training loss 1.5083202123641968\n",
      "Epoch 1 -- Batch 95/ 842, training loss 1.5058021545410156\n",
      "Epoch 1 -- Batch 96/ 842, training loss 1.4641085863113403\n",
      "Epoch 1 -- Batch 97/ 842, training loss 1.488114833831787\n",
      "Epoch 1 -- Batch 98/ 842, training loss 1.4558614492416382\n",
      "Epoch 1 -- Batch 99/ 842, training loss 1.4754098653793335\n",
      "Epoch 1 -- Batch 100/ 842, training loss 1.4805291891098022\n",
      "Epoch 1 -- Batch 101/ 842, training loss 1.455409288406372\n",
      "Epoch 1 -- Batch 102/ 842, training loss 1.472519040107727\n",
      "Epoch 1 -- Batch 103/ 842, training loss 1.4726001024246216\n",
      "Epoch 1 -- Batch 104/ 842, training loss 1.472219705581665\n",
      "Epoch 1 -- Batch 105/ 842, training loss 1.4453099966049194\n",
      "Epoch 1 -- Batch 106/ 842, training loss 1.4200427532196045\n",
      "Epoch 1 -- Batch 107/ 842, training loss 1.4064655303955078\n",
      "Epoch 1 -- Batch 108/ 842, training loss 1.3832331895828247\n",
      "Epoch 1 -- Batch 109/ 842, training loss 1.415136456489563\n",
      "Epoch 1 -- Batch 110/ 842, training loss 1.409119963645935\n",
      "Epoch 1 -- Batch 111/ 842, training loss 1.4361144304275513\n",
      "Epoch 1 -- Batch 112/ 842, training loss 1.4228293895721436\n",
      "Epoch 1 -- Batch 113/ 842, training loss 1.4323883056640625\n",
      "Epoch 1 -- Batch 114/ 842, training loss 1.4229373931884766\n",
      "Epoch 1 -- Batch 115/ 842, training loss 1.387548804283142\n",
      "Epoch 1 -- Batch 116/ 842, training loss 1.4155346155166626\n",
      "Epoch 1 -- Batch 117/ 842, training loss 1.3718527555465698\n",
      "Epoch 1 -- Batch 118/ 842, training loss 1.412764072418213\n",
      "Epoch 1 -- Batch 119/ 842, training loss 1.3854024410247803\n",
      "Epoch 1 -- Batch 120/ 842, training loss 1.3567572832107544\n",
      "Epoch 1 -- Batch 121/ 842, training loss 1.3986986875534058\n",
      "Epoch 1 -- Batch 122/ 842, training loss 1.3890960216522217\n",
      "Epoch 1 -- Batch 123/ 842, training loss 1.308623194694519\n",
      "Epoch 1 -- Batch 124/ 842, training loss 1.3801829814910889\n",
      "Epoch 1 -- Batch 125/ 842, training loss 1.3453900814056396\n",
      "Epoch 1 -- Batch 126/ 842, training loss 1.397861123085022\n",
      "Epoch 1 -- Batch 127/ 842, training loss 1.389207124710083\n",
      "Epoch 1 -- Batch 128/ 842, training loss 1.3354058265686035\n",
      "Epoch 1 -- Batch 129/ 842, training loss 1.395947813987732\n",
      "Epoch 1 -- Batch 130/ 842, training loss 1.394919991493225\n",
      "Epoch 1 -- Batch 131/ 842, training loss 1.359679937362671\n",
      "Epoch 1 -- Batch 132/ 842, training loss 1.3624111413955688\n",
      "Epoch 1 -- Batch 133/ 842, training loss 1.3735655546188354\n",
      "Epoch 1 -- Batch 134/ 842, training loss 1.3397477865219116\n",
      "Epoch 1 -- Batch 135/ 842, training loss 1.2869808673858643\n",
      "Epoch 1 -- Batch 136/ 842, training loss 1.36514151096344\n",
      "Epoch 1 -- Batch 137/ 842, training loss 1.3044253587722778\n",
      "Epoch 1 -- Batch 138/ 842, training loss 1.3238933086395264\n",
      "Epoch 1 -- Batch 139/ 842, training loss 1.348669171333313\n",
      "Epoch 1 -- Batch 140/ 842, training loss 1.3215327262878418\n",
      "Epoch 1 -- Batch 141/ 842, training loss 1.284318208694458\n",
      "Epoch 1 -- Batch 142/ 842, training loss 1.322674036026001\n",
      "Epoch 1 -- Batch 143/ 842, training loss 1.2803795337677002\n",
      "Epoch 1 -- Batch 144/ 842, training loss 1.3093904256820679\n",
      "Epoch 1 -- Batch 145/ 842, training loss 1.3362994194030762\n",
      "Epoch 1 -- Batch 146/ 842, training loss 1.2422860860824585\n",
      "Epoch 1 -- Batch 147/ 842, training loss 1.3139004707336426\n",
      "Epoch 1 -- Batch 148/ 842, training loss 1.358626365661621\n",
      "Epoch 1 -- Batch 149/ 842, training loss 1.2911317348480225\n",
      "Epoch 1 -- Batch 150/ 842, training loss 1.2882521152496338\n",
      "Epoch 1 -- Batch 151/ 842, training loss 1.2622852325439453\n",
      "Epoch 1 -- Batch 152/ 842, training loss 1.3352898359298706\n",
      "Epoch 1 -- Batch 153/ 842, training loss 1.295654535293579\n",
      "Epoch 1 -- Batch 154/ 842, training loss 1.2724841833114624\n",
      "Epoch 1 -- Batch 155/ 842, training loss 1.2847553491592407\n",
      "Epoch 1 -- Batch 156/ 842, training loss 1.2770997285842896\n",
      "Epoch 1 -- Batch 157/ 842, training loss 1.2321645021438599\n",
      "Epoch 1 -- Batch 158/ 842, training loss 1.2511905431747437\n",
      "Epoch 1 -- Batch 159/ 842, training loss 1.2448341846466064\n",
      "Epoch 1 -- Batch 160/ 842, training loss 1.313064694404602\n",
      "Epoch 1 -- Batch 161/ 842, training loss 1.2899755239486694\n",
      "Epoch 1 -- Batch 162/ 842, training loss 1.230137825012207\n",
      "Epoch 1 -- Batch 163/ 842, training loss 1.2650853395462036\n",
      "Epoch 1 -- Batch 164/ 842, training loss 1.2344847917556763\n",
      "Epoch 1 -- Batch 165/ 842, training loss 1.226925253868103\n",
      "Epoch 1 -- Batch 166/ 842, training loss 1.2073956727981567\n",
      "Epoch 1 -- Batch 167/ 842, training loss 1.247131109237671\n",
      "Epoch 1 -- Batch 168/ 842, training loss 1.2423673868179321\n",
      "Epoch 1 -- Batch 169/ 842, training loss 1.2154161930084229\n",
      "Epoch 1 -- Batch 170/ 842, training loss 1.2068455219268799\n",
      "Epoch 1 -- Batch 171/ 842, training loss 1.207261085510254\n",
      "Epoch 1 -- Batch 172/ 842, training loss 1.1969927549362183\n",
      "Epoch 1 -- Batch 173/ 842, training loss 1.182600498199463\n",
      "Epoch 1 -- Batch 174/ 842, training loss 1.241685390472412\n",
      "Epoch 1 -- Batch 175/ 842, training loss 1.1996337175369263\n",
      "Epoch 1 -- Batch 176/ 842, training loss 1.1855649948120117\n",
      "Epoch 1 -- Batch 177/ 842, training loss 1.2080481052398682\n",
      "Epoch 1 -- Batch 178/ 842, training loss 1.174067497253418\n",
      "Epoch 1 -- Batch 179/ 842, training loss 1.1835156679153442\n",
      "Epoch 1 -- Batch 180/ 842, training loss 1.2343634366989136\n",
      "Epoch 1 -- Batch 181/ 842, training loss 1.2236100435256958\n",
      "Epoch 1 -- Batch 182/ 842, training loss 1.225214958190918\n",
      "Epoch 1 -- Batch 183/ 842, training loss 1.2006909847259521\n",
      "Epoch 1 -- Batch 184/ 842, training loss 1.2348127365112305\n",
      "Epoch 1 -- Batch 185/ 842, training loss 1.223734974861145\n",
      "Epoch 1 -- Batch 186/ 842, training loss 1.2042615413665771\n",
      "Epoch 1 -- Batch 187/ 842, training loss 1.1929891109466553\n",
      "Epoch 1 -- Batch 188/ 842, training loss 1.1792694330215454\n",
      "Epoch 1 -- Batch 189/ 842, training loss 1.1511653661727905\n",
      "Epoch 1 -- Batch 190/ 842, training loss 1.167608380317688\n",
      "Epoch 1 -- Batch 191/ 842, training loss 1.19874906539917\n",
      "Epoch 1 -- Batch 192/ 842, training loss 1.1493620872497559\n",
      "Epoch 1 -- Batch 193/ 842, training loss 1.1405818462371826\n",
      "Epoch 1 -- Batch 194/ 842, training loss 1.1867607831954956\n",
      "Epoch 1 -- Batch 195/ 842, training loss 1.1866344213485718\n",
      "Epoch 1 -- Batch 196/ 842, training loss 1.171847939491272\n",
      "Epoch 1 -- Batch 197/ 842, training loss 1.1983288526535034\n",
      "Epoch 1 -- Batch 198/ 842, training loss 1.1816811561584473\n",
      "Epoch 1 -- Batch 199/ 842, training loss 1.191249132156372\n",
      "Epoch 1 -- Batch 200/ 842, training loss 1.17481529712677\n",
      "Epoch 1 -- Batch 201/ 842, training loss 1.146034836769104\n",
      "Epoch 1 -- Batch 202/ 842, training loss 1.133041501045227\n",
      "Epoch 1 -- Batch 203/ 842, training loss 1.121756911277771\n",
      "Epoch 1 -- Batch 204/ 842, training loss 1.1174349784851074\n",
      "Epoch 1 -- Batch 205/ 842, training loss 1.132077693939209\n",
      "Epoch 1 -- Batch 206/ 842, training loss 1.1415340900421143\n",
      "Epoch 1 -- Batch 207/ 842, training loss 1.081947684288025\n",
      "Epoch 1 -- Batch 208/ 842, training loss 1.1324750185012817\n",
      "Epoch 1 -- Batch 209/ 842, training loss 1.1171514987945557\n",
      "Epoch 1 -- Batch 210/ 842, training loss 1.1560386419296265\n",
      "Epoch 1 -- Batch 211/ 842, training loss 1.1517590284347534\n",
      "Epoch 1 -- Batch 212/ 842, training loss 1.132381558418274\n",
      "Epoch 1 -- Batch 213/ 842, training loss 1.1490187644958496\n",
      "Epoch 1 -- Batch 214/ 842, training loss 1.1124476194381714\n",
      "Epoch 1 -- Batch 215/ 842, training loss 1.0848429203033447\n",
      "Epoch 1 -- Batch 216/ 842, training loss 1.0874629020690918\n",
      "Epoch 1 -- Batch 217/ 842, training loss 1.1060551404953003\n",
      "Epoch 1 -- Batch 218/ 842, training loss 1.1386651992797852\n",
      "Epoch 1 -- Batch 219/ 842, training loss 1.1069024801254272\n",
      "Epoch 1 -- Batch 220/ 842, training loss 1.0842432975769043\n",
      "Epoch 1 -- Batch 221/ 842, training loss 1.1315406560897827\n",
      "Epoch 1 -- Batch 222/ 842, training loss 1.136196494102478\n",
      "Epoch 1 -- Batch 223/ 842, training loss 1.073534607887268\n",
      "Epoch 1 -- Batch 224/ 842, training loss 1.08113694190979\n",
      "Epoch 1 -- Batch 225/ 842, training loss 1.1380057334899902\n",
      "Epoch 1 -- Batch 226/ 842, training loss 1.104440689086914\n",
      "Epoch 1 -- Batch 227/ 842, training loss 1.060361385345459\n",
      "Epoch 1 -- Batch 228/ 842, training loss 1.0722157955169678\n",
      "Epoch 1 -- Batch 229/ 842, training loss 1.075156331062317\n",
      "Epoch 1 -- Batch 230/ 842, training loss 1.0729060173034668\n",
      "Epoch 1 -- Batch 231/ 842, training loss 1.0992683172225952\n",
      "Epoch 1 -- Batch 232/ 842, training loss 1.0494937896728516\n",
      "Epoch 1 -- Batch 233/ 842, training loss 1.1605470180511475\n",
      "Epoch 1 -- Batch 234/ 842, training loss 1.1086655855178833\n",
      "Epoch 1 -- Batch 235/ 842, training loss 1.0852665901184082\n",
      "Epoch 1 -- Batch 236/ 842, training loss 1.0618313550949097\n",
      "Epoch 1 -- Batch 237/ 842, training loss 1.1242979764938354\n",
      "Epoch 1 -- Batch 238/ 842, training loss 1.049509048461914\n",
      "Epoch 1 -- Batch 239/ 842, training loss 1.09652841091156\n",
      "Epoch 1 -- Batch 240/ 842, training loss 1.0849387645721436\n",
      "Epoch 1 -- Batch 241/ 842, training loss 1.0531986951828003\n",
      "Epoch 1 -- Batch 242/ 842, training loss 1.0490561723709106\n",
      "Epoch 1 -- Batch 243/ 842, training loss 1.0974870920181274\n",
      "Epoch 1 -- Batch 244/ 842, training loss 1.0602304935455322\n",
      "Epoch 1 -- Batch 245/ 842, training loss 1.0556010007858276\n",
      "Epoch 1 -- Batch 246/ 842, training loss 1.074341058731079\n",
      "Epoch 1 -- Batch 247/ 842, training loss 1.0448925495147705\n",
      "Epoch 1 -- Batch 248/ 842, training loss 1.0936384201049805\n",
      "Epoch 1 -- Batch 249/ 842, training loss 1.0330561399459839\n",
      "Epoch 1 -- Batch 250/ 842, training loss 1.043988585472107\n",
      "Epoch 1 -- Batch 251/ 842, training loss 1.0843875408172607\n",
      "Epoch 1 -- Batch 252/ 842, training loss 1.074846863746643\n",
      "Epoch 1 -- Batch 253/ 842, training loss 1.0333038568496704\n",
      "Epoch 1 -- Batch 254/ 842, training loss 1.0332814455032349\n",
      "Epoch 1 -- Batch 255/ 842, training loss 1.0550795793533325\n",
      "Epoch 1 -- Batch 256/ 842, training loss 1.039853572845459\n",
      "Epoch 1 -- Batch 257/ 842, training loss 1.0257729291915894\n",
      "Epoch 1 -- Batch 258/ 842, training loss 1.0312950611114502\n",
      "Epoch 1 -- Batch 259/ 842, training loss 1.0437395572662354\n",
      "Epoch 1 -- Batch 260/ 842, training loss 1.032447099685669\n",
      "Epoch 1 -- Batch 261/ 842, training loss 1.0545293092727661\n",
      "Epoch 1 -- Batch 262/ 842, training loss 1.05570650100708\n",
      "Epoch 1 -- Batch 263/ 842, training loss 1.0129719972610474\n",
      "Epoch 1 -- Batch 264/ 842, training loss 1.006089687347412\n",
      "Epoch 1 -- Batch 265/ 842, training loss 1.0169621706008911\n",
      "Epoch 1 -- Batch 266/ 842, training loss 1.02788507938385\n",
      "Epoch 1 -- Batch 267/ 842, training loss 1.0594794750213623\n",
      "Epoch 1 -- Batch 268/ 842, training loss 1.0678653717041016\n",
      "Epoch 1 -- Batch 269/ 842, training loss 1.0195826292037964\n",
      "Epoch 1 -- Batch 270/ 842, training loss 1.0373438596725464\n",
      "Epoch 1 -- Batch 271/ 842, training loss 1.0422554016113281\n",
      "Epoch 1 -- Batch 272/ 842, training loss 1.0250506401062012\n",
      "Epoch 1 -- Batch 273/ 842, training loss 1.0069206953048706\n",
      "Epoch 1 -- Batch 274/ 842, training loss 1.0082848072052002\n",
      "Epoch 1 -- Batch 275/ 842, training loss 1.037319302558899\n",
      "Epoch 1 -- Batch 276/ 842, training loss 0.9982219338417053\n",
      "Epoch 1 -- Batch 277/ 842, training loss 0.9973495602607727\n",
      "Epoch 1 -- Batch 278/ 842, training loss 0.9839858412742615\n",
      "Epoch 1 -- Batch 279/ 842, training loss 1.0371344089508057\n",
      "Epoch 1 -- Batch 280/ 842, training loss 1.0062607526779175\n",
      "Epoch 1 -- Batch 281/ 842, training loss 1.049400806427002\n",
      "Epoch 1 -- Batch 282/ 842, training loss 1.0216286182403564\n",
      "Epoch 1 -- Batch 283/ 842, training loss 0.9922358393669128\n",
      "Epoch 1 -- Batch 284/ 842, training loss 1.0165860652923584\n",
      "Epoch 1 -- Batch 285/ 842, training loss 1.0157654285430908\n",
      "Epoch 1 -- Batch 286/ 842, training loss 1.032699465751648\n",
      "Epoch 1 -- Batch 287/ 842, training loss 0.9685953855514526\n",
      "Epoch 1 -- Batch 288/ 842, training loss 0.987716019153595\n",
      "Epoch 1 -- Batch 289/ 842, training loss 1.007878065109253\n",
      "Epoch 1 -- Batch 290/ 842, training loss 0.9694539308547974\n",
      "Epoch 1 -- Batch 291/ 842, training loss 0.9961000680923462\n",
      "Epoch 1 -- Batch 292/ 842, training loss 0.9716240167617798\n",
      "Epoch 1 -- Batch 293/ 842, training loss 0.972568154335022\n",
      "Epoch 1 -- Batch 294/ 842, training loss 0.9657648801803589\n",
      "Epoch 1 -- Batch 295/ 842, training loss 0.9625951647758484\n",
      "Epoch 1 -- Batch 296/ 842, training loss 0.9630196690559387\n",
      "Epoch 1 -- Batch 297/ 842, training loss 1.026691198348999\n",
      "Epoch 1 -- Batch 298/ 842, training loss 0.9794580936431885\n",
      "Epoch 1 -- Batch 299/ 842, training loss 1.0561472177505493\n",
      "Epoch 1 -- Batch 300/ 842, training loss 0.955308735370636\n",
      "Epoch 1 -- Batch 301/ 842, training loss 0.9201570153236389\n",
      "Epoch 1 -- Batch 302/ 842, training loss 0.9816664457321167\n",
      "Epoch 1 -- Batch 303/ 842, training loss 0.9312308430671692\n",
      "Epoch 1 -- Batch 304/ 842, training loss 0.9431972503662109\n",
      "Epoch 1 -- Batch 305/ 842, training loss 0.9872433543205261\n",
      "Epoch 1 -- Batch 306/ 842, training loss 0.93955397605896\n",
      "Epoch 1 -- Batch 307/ 842, training loss 0.9430227875709534\n",
      "Epoch 1 -- Batch 308/ 842, training loss 0.9318268895149231\n",
      "Epoch 1 -- Batch 309/ 842, training loss 0.9472988247871399\n",
      "Epoch 1 -- Batch 310/ 842, training loss 0.9423914551734924\n",
      "Epoch 1 -- Batch 311/ 842, training loss 0.9696857929229736\n",
      "Epoch 1 -- Batch 312/ 842, training loss 0.9713621735572815\n",
      "Epoch 1 -- Batch 313/ 842, training loss 0.9500836730003357\n",
      "Epoch 1 -- Batch 314/ 842, training loss 0.963410496711731\n",
      "Epoch 1 -- Batch 315/ 842, training loss 0.9610412120819092\n",
      "Epoch 1 -- Batch 316/ 842, training loss 0.9419121146202087\n",
      "Epoch 1 -- Batch 317/ 842, training loss 0.9127109050750732\n",
      "Epoch 1 -- Batch 318/ 842, training loss 0.935967206954956\n",
      "Epoch 1 -- Batch 319/ 842, training loss 0.9824745059013367\n",
      "Epoch 1 -- Batch 320/ 842, training loss 0.935979425907135\n",
      "Epoch 1 -- Batch 321/ 842, training loss 0.9303054809570312\n",
      "Epoch 1 -- Batch 322/ 842, training loss 0.9582892656326294\n",
      "Epoch 1 -- Batch 323/ 842, training loss 0.9765728116035461\n",
      "Epoch 1 -- Batch 324/ 842, training loss 0.9857956171035767\n",
      "Epoch 1 -- Batch 325/ 842, training loss 0.9254420399665833\n",
      "Epoch 1 -- Batch 326/ 842, training loss 0.8808455467224121\n",
      "Epoch 1 -- Batch 327/ 842, training loss 0.8996924757957458\n",
      "Epoch 1 -- Batch 328/ 842, training loss 0.9342353343963623\n",
      "Epoch 1 -- Batch 329/ 842, training loss 0.924716591835022\n",
      "Epoch 1 -- Batch 330/ 842, training loss 0.9796018600463867\n",
      "Epoch 1 -- Batch 331/ 842, training loss 0.9081547260284424\n",
      "Epoch 1 -- Batch 332/ 842, training loss 0.9394670128822327\n",
      "Epoch 1 -- Batch 333/ 842, training loss 0.8976759910583496\n",
      "Epoch 1 -- Batch 334/ 842, training loss 0.8813340067863464\n",
      "Epoch 1 -- Batch 335/ 842, training loss 0.885934054851532\n",
      "Epoch 1 -- Batch 336/ 842, training loss 0.9246321320533752\n",
      "Epoch 1 -- Batch 337/ 842, training loss 0.9221257567405701\n",
      "Epoch 1 -- Batch 338/ 842, training loss 0.9123082160949707\n",
      "Epoch 1 -- Batch 339/ 842, training loss 0.8841254711151123\n",
      "Epoch 1 -- Batch 340/ 842, training loss 0.9404474496841431\n",
      "Epoch 1 -- Batch 341/ 842, training loss 0.9225177764892578\n",
      "Epoch 1 -- Batch 342/ 842, training loss 0.9200707674026489\n",
      "Epoch 1 -- Batch 343/ 842, training loss 0.8750401735305786\n",
      "Epoch 1 -- Batch 344/ 842, training loss 0.9408519864082336\n",
      "Epoch 1 -- Batch 345/ 842, training loss 0.8815819025039673\n",
      "Epoch 1 -- Batch 346/ 842, training loss 0.9369534850120544\n",
      "Epoch 1 -- Batch 347/ 842, training loss 0.9532326459884644\n",
      "Epoch 1 -- Batch 348/ 842, training loss 0.9544427394866943\n",
      "Epoch 1 -- Batch 349/ 842, training loss 0.9052211046218872\n",
      "Epoch 1 -- Batch 350/ 842, training loss 0.9432087540626526\n",
      "Epoch 1 -- Batch 351/ 842, training loss 0.9165734648704529\n",
      "Epoch 1 -- Batch 352/ 842, training loss 0.9192953109741211\n",
      "Epoch 1 -- Batch 353/ 842, training loss 0.9496067762374878\n",
      "Epoch 1 -- Batch 354/ 842, training loss 0.8919909596443176\n",
      "Epoch 1 -- Batch 355/ 842, training loss 0.9181823134422302\n",
      "Epoch 1 -- Batch 356/ 842, training loss 0.9486091732978821\n",
      "Epoch 1 -- Batch 357/ 842, training loss 0.9119793176651001\n",
      "Epoch 1 -- Batch 358/ 842, training loss 0.907505452632904\n",
      "Epoch 1 -- Batch 359/ 842, training loss 0.9147143959999084\n",
      "Epoch 1 -- Batch 360/ 842, training loss 0.8977813124656677\n",
      "Epoch 1 -- Batch 361/ 842, training loss 0.9068940281867981\n",
      "Epoch 1 -- Batch 362/ 842, training loss 0.8947456479072571\n",
      "Epoch 1 -- Batch 363/ 842, training loss 0.9008930921554565\n",
      "Epoch 1 -- Batch 364/ 842, training loss 0.9279236793518066\n",
      "Epoch 1 -- Batch 365/ 842, training loss 0.9059066772460938\n",
      "Epoch 1 -- Batch 366/ 842, training loss 0.9055163264274597\n",
      "Epoch 1 -- Batch 367/ 842, training loss 0.8909409642219543\n",
      "Epoch 1 -- Batch 368/ 842, training loss 0.8953582644462585\n",
      "Epoch 1 -- Batch 369/ 842, training loss 0.8963011503219604\n",
      "Epoch 1 -- Batch 370/ 842, training loss 0.9131476283073425\n",
      "Epoch 1 -- Batch 371/ 842, training loss 0.902908444404602\n",
      "Epoch 1 -- Batch 372/ 842, training loss 0.9031791090965271\n",
      "Epoch 1 -- Batch 373/ 842, training loss 0.8798067569732666\n",
      "Epoch 1 -- Batch 374/ 842, training loss 0.9131044745445251\n",
      "Epoch 1 -- Batch 375/ 842, training loss 0.8587934970855713\n",
      "Epoch 1 -- Batch 376/ 842, training loss 0.8708475828170776\n",
      "Epoch 1 -- Batch 377/ 842, training loss 0.9053928256034851\n",
      "Epoch 1 -- Batch 378/ 842, training loss 0.911952555179596\n",
      "Epoch 1 -- Batch 379/ 842, training loss 0.9131067395210266\n",
      "Epoch 1 -- Batch 380/ 842, training loss 0.8741642236709595\n",
      "Epoch 1 -- Batch 381/ 842, training loss 0.8706406354904175\n",
      "Epoch 1 -- Batch 382/ 842, training loss 0.8703227639198303\n",
      "Epoch 1 -- Batch 383/ 842, training loss 0.8564654588699341\n",
      "Epoch 1 -- Batch 384/ 842, training loss 0.8834115266799927\n",
      "Epoch 1 -- Batch 385/ 842, training loss 0.8965626358985901\n",
      "Epoch 1 -- Batch 386/ 842, training loss 0.8642919659614563\n",
      "Epoch 1 -- Batch 387/ 842, training loss 0.8957363367080688\n",
      "Epoch 1 -- Batch 388/ 842, training loss 0.9072050452232361\n",
      "Epoch 1 -- Batch 389/ 842, training loss 0.8842483758926392\n",
      "Epoch 1 -- Batch 390/ 842, training loss 0.8759899735450745\n",
      "Epoch 1 -- Batch 391/ 842, training loss 0.892375111579895\n",
      "Epoch 1 -- Batch 392/ 842, training loss 0.8710872530937195\n",
      "Epoch 1 -- Batch 393/ 842, training loss 0.8854901194572449\n",
      "Epoch 1 -- Batch 394/ 842, training loss 0.8604824542999268\n",
      "Epoch 1 -- Batch 395/ 842, training loss 0.9068519473075867\n",
      "Epoch 1 -- Batch 396/ 842, training loss 0.8699748516082764\n",
      "Epoch 1 -- Batch 397/ 842, training loss 0.9198721647262573\n",
      "Epoch 1 -- Batch 398/ 842, training loss 0.9156187176704407\n",
      "Epoch 1 -- Batch 399/ 842, training loss 0.8823725581169128\n",
      "Epoch 1 -- Batch 400/ 842, training loss 0.8924106359481812\n",
      "Epoch 1 -- Batch 401/ 842, training loss 0.9014524817466736\n",
      "Epoch 1 -- Batch 402/ 842, training loss 0.869375467300415\n",
      "Epoch 1 -- Batch 403/ 842, training loss 0.8663175106048584\n",
      "Epoch 1 -- Batch 404/ 842, training loss 0.8621004819869995\n",
      "Epoch 1 -- Batch 405/ 842, training loss 0.8843511939048767\n",
      "Epoch 1 -- Batch 406/ 842, training loss 0.8847057819366455\n",
      "Epoch 1 -- Batch 407/ 842, training loss 0.8914353847503662\n",
      "Epoch 1 -- Batch 408/ 842, training loss 0.8426349759101868\n",
      "Epoch 1 -- Batch 409/ 842, training loss 0.8498368263244629\n",
      "Epoch 1 -- Batch 410/ 842, training loss 0.8499515056610107\n",
      "Epoch 1 -- Batch 411/ 842, training loss 0.8756726384162903\n",
      "Epoch 1 -- Batch 412/ 842, training loss 0.9074758291244507\n",
      "Epoch 1 -- Batch 413/ 842, training loss 0.8590196371078491\n",
      "Epoch 1 -- Batch 414/ 842, training loss 0.8443007469177246\n",
      "Epoch 1 -- Batch 415/ 842, training loss 0.8400166630744934\n",
      "Epoch 1 -- Batch 416/ 842, training loss 0.8735140562057495\n",
      "Epoch 1 -- Batch 417/ 842, training loss 0.8560211062431335\n",
      "Epoch 1 -- Batch 418/ 842, training loss 0.841002345085144\n",
      "Epoch 1 -- Batch 419/ 842, training loss 0.8941638469696045\n",
      "Epoch 1 -- Batch 420/ 842, training loss 0.861052393913269\n",
      "Epoch 1 -- Batch 421/ 842, training loss 0.8429691791534424\n",
      "Epoch 1 -- Batch 422/ 842, training loss 0.8799730539321899\n",
      "Epoch 1 -- Batch 423/ 842, training loss 0.8573629856109619\n",
      "Epoch 1 -- Batch 424/ 842, training loss 0.873975396156311\n",
      "Epoch 1 -- Batch 425/ 842, training loss 0.8350356817245483\n",
      "Epoch 1 -- Batch 426/ 842, training loss 0.8710615634918213\n",
      "Epoch 1 -- Batch 427/ 842, training loss 0.8664669394493103\n",
      "Epoch 1 -- Batch 428/ 842, training loss 0.8888611793518066\n",
      "Epoch 1 -- Batch 429/ 842, training loss 0.8738792538642883\n",
      "Epoch 1 -- Batch 430/ 842, training loss 0.8476229906082153\n",
      "Epoch 1 -- Batch 431/ 842, training loss 0.8595547080039978\n",
      "Epoch 1 -- Batch 432/ 842, training loss 0.8301104307174683\n",
      "Epoch 1 -- Batch 433/ 842, training loss 0.8452420234680176\n",
      "Epoch 1 -- Batch 434/ 842, training loss 0.8450015187263489\n",
      "Epoch 1 -- Batch 435/ 842, training loss 0.815527081489563\n",
      "Epoch 1 -- Batch 436/ 842, training loss 0.854428231716156\n",
      "Epoch 1 -- Batch 437/ 842, training loss 0.8737502098083496\n",
      "Epoch 1 -- Batch 438/ 842, training loss 0.842529296875\n",
      "Epoch 1 -- Batch 439/ 842, training loss 0.8901571035385132\n",
      "Epoch 1 -- Batch 440/ 842, training loss 0.8452913165092468\n",
      "Epoch 1 -- Batch 441/ 842, training loss 0.8629471063613892\n",
      "Epoch 1 -- Batch 442/ 842, training loss 0.846538782119751\n",
      "Epoch 1 -- Batch 443/ 842, training loss 0.8101373314857483\n",
      "Epoch 1 -- Batch 444/ 842, training loss 0.8498930931091309\n",
      "Epoch 1 -- Batch 445/ 842, training loss 0.8366714119911194\n",
      "Epoch 1 -- Batch 446/ 842, training loss 0.8545528054237366\n",
      "Epoch 1 -- Batch 447/ 842, training loss 0.8368353247642517\n",
      "Epoch 1 -- Batch 448/ 842, training loss 0.836821973323822\n",
      "Epoch 1 -- Batch 449/ 842, training loss 0.8383981585502625\n",
      "Epoch 1 -- Batch 450/ 842, training loss 0.8196572661399841\n",
      "Epoch 1 -- Batch 451/ 842, training loss 0.855404257774353\n",
      "Epoch 1 -- Batch 452/ 842, training loss 0.8612871766090393\n",
      "Epoch 1 -- Batch 453/ 842, training loss 0.8351840376853943\n",
      "Epoch 1 -- Batch 454/ 842, training loss 0.8625591397285461\n",
      "Epoch 1 -- Batch 455/ 842, training loss 0.8136736750602722\n",
      "Epoch 1 -- Batch 456/ 842, training loss 0.8089489936828613\n",
      "Epoch 1 -- Batch 457/ 842, training loss 0.835198700428009\n",
      "Epoch 1 -- Batch 458/ 842, training loss 0.8422339558601379\n",
      "Epoch 1 -- Batch 459/ 842, training loss 0.8476406335830688\n",
      "Epoch 1 -- Batch 460/ 842, training loss 0.8176770210266113\n",
      "Epoch 1 -- Batch 461/ 842, training loss 0.8433565497398376\n",
      "Epoch 1 -- Batch 462/ 842, training loss 0.8430517911911011\n",
      "Epoch 1 -- Batch 463/ 842, training loss 0.8287097215652466\n",
      "Epoch 1 -- Batch 464/ 842, training loss 0.8233701586723328\n",
      "Epoch 1 -- Batch 465/ 842, training loss 0.842252790927887\n",
      "Epoch 1 -- Batch 466/ 842, training loss 0.7888981103897095\n",
      "Epoch 1 -- Batch 467/ 842, training loss 0.8293129801750183\n",
      "Epoch 1 -- Batch 468/ 842, training loss 0.8226533532142639\n",
      "Epoch 1 -- Batch 469/ 842, training loss 0.8535874485969543\n",
      "Epoch 1 -- Batch 470/ 842, training loss 0.8289564847946167\n",
      "Epoch 1 -- Batch 471/ 842, training loss 0.8066938519477844\n",
      "Epoch 1 -- Batch 472/ 842, training loss 0.810874879360199\n",
      "Epoch 1 -- Batch 473/ 842, training loss 0.8069499135017395\n",
      "Epoch 1 -- Batch 474/ 842, training loss 0.8380351662635803\n",
      "Epoch 1 -- Batch 475/ 842, training loss 0.8271153569221497\n",
      "Epoch 1 -- Batch 476/ 842, training loss 0.8229203224182129\n",
      "Epoch 1 -- Batch 477/ 842, training loss 0.8390068411827087\n",
      "Epoch 1 -- Batch 478/ 842, training loss 0.8278477787971497\n",
      "Epoch 1 -- Batch 479/ 842, training loss 0.8048758506774902\n",
      "Epoch 1 -- Batch 480/ 842, training loss 0.8115369081497192\n",
      "Epoch 1 -- Batch 481/ 842, training loss 0.8211053013801575\n",
      "Epoch 1 -- Batch 482/ 842, training loss 0.8282350897789001\n",
      "Epoch 1 -- Batch 483/ 842, training loss 0.8343241214752197\n",
      "Epoch 1 -- Batch 484/ 842, training loss 0.8296473026275635\n",
      "Epoch 1 -- Batch 485/ 842, training loss 0.7767269611358643\n",
      "Epoch 1 -- Batch 486/ 842, training loss 0.8176524639129639\n",
      "Epoch 1 -- Batch 487/ 842, training loss 0.8152406811714172\n",
      "Epoch 1 -- Batch 488/ 842, training loss 0.800128698348999\n",
      "Epoch 1 -- Batch 489/ 842, training loss 0.8078305721282959\n",
      "Epoch 1 -- Batch 490/ 842, training loss 0.8398146033287048\n",
      "Epoch 1 -- Batch 491/ 842, training loss 0.7946900725364685\n",
      "Epoch 1 -- Batch 492/ 842, training loss 0.8155795335769653\n",
      "Epoch 1 -- Batch 493/ 842, training loss 0.7959610819816589\n",
      "Epoch 1 -- Batch 494/ 842, training loss 0.8034815788269043\n",
      "Epoch 1 -- Batch 495/ 842, training loss 0.8432043194770813\n",
      "Epoch 1 -- Batch 496/ 842, training loss 0.8025131821632385\n",
      "Epoch 1 -- Batch 497/ 842, training loss 0.809823751449585\n",
      "Epoch 1 -- Batch 498/ 842, training loss 0.839586079120636\n",
      "Epoch 1 -- Batch 499/ 842, training loss 0.8161823749542236\n",
      "Epoch 1 -- Batch 500/ 842, training loss 0.8390093445777893\n",
      "Epoch 1 -- Batch 501/ 842, training loss 0.8360801339149475\n",
      "Epoch 1 -- Batch 502/ 842, training loss 0.8054105043411255\n",
      "Epoch 1 -- Batch 503/ 842, training loss 0.8567941784858704\n",
      "Epoch 1 -- Batch 504/ 842, training loss 0.8347176909446716\n",
      "Epoch 1 -- Batch 505/ 842, training loss 0.8395769596099854\n",
      "Epoch 1 -- Batch 506/ 842, training loss 0.8236449956893921\n",
      "Epoch 1 -- Batch 507/ 842, training loss 0.7985959649085999\n",
      "Epoch 1 -- Batch 508/ 842, training loss 0.7916383147239685\n",
      "Epoch 1 -- Batch 509/ 842, training loss 0.8147704005241394\n",
      "Epoch 1 -- Batch 510/ 842, training loss 0.8070777654647827\n",
      "Epoch 1 -- Batch 511/ 842, training loss 0.8166976571083069\n",
      "Epoch 1 -- Batch 512/ 842, training loss 0.7789731621742249\n",
      "Epoch 1 -- Batch 513/ 842, training loss 0.7864196300506592\n",
      "Epoch 1 -- Batch 514/ 842, training loss 0.806613564491272\n",
      "Epoch 1 -- Batch 515/ 842, training loss 0.8157222270965576\n",
      "Epoch 1 -- Batch 516/ 842, training loss 0.7924432158470154\n",
      "Epoch 1 -- Batch 517/ 842, training loss 0.8122100830078125\n",
      "Epoch 1 -- Batch 518/ 842, training loss 0.8002097010612488\n",
      "Epoch 1 -- Batch 519/ 842, training loss 0.8333739042282104\n",
      "Epoch 1 -- Batch 520/ 842, training loss 0.8062907457351685\n",
      "Epoch 1 -- Batch 521/ 842, training loss 0.8059511780738831\n",
      "Epoch 1 -- Batch 522/ 842, training loss 0.782760500907898\n",
      "Epoch 1 -- Batch 523/ 842, training loss 0.7979065775871277\n",
      "Epoch 1 -- Batch 524/ 842, training loss 0.8052309155464172\n",
      "Epoch 1 -- Batch 525/ 842, training loss 0.8155795931816101\n",
      "Epoch 1 -- Batch 526/ 842, training loss 0.8198181986808777\n",
      "Epoch 1 -- Batch 527/ 842, training loss 0.7885827422142029\n",
      "Epoch 1 -- Batch 528/ 842, training loss 0.7922330498695374\n",
      "Epoch 1 -- Batch 529/ 842, training loss 0.8171719908714294\n",
      "Epoch 1 -- Batch 530/ 842, training loss 0.7778380513191223\n",
      "Epoch 1 -- Batch 531/ 842, training loss 0.8117959499359131\n",
      "Epoch 1 -- Batch 532/ 842, training loss 0.78475421667099\n",
      "Epoch 1 -- Batch 533/ 842, training loss 0.8038374781608582\n",
      "Epoch 1 -- Batch 534/ 842, training loss 0.7829799056053162\n",
      "Epoch 1 -- Batch 535/ 842, training loss 0.7998347878456116\n",
      "Epoch 1 -- Batch 536/ 842, training loss 0.8070089817047119\n",
      "Epoch 1 -- Batch 537/ 842, training loss 0.800493061542511\n",
      "Epoch 1 -- Batch 538/ 842, training loss 0.8090494275093079\n",
      "Epoch 1 -- Batch 539/ 842, training loss 0.8049964308738708\n",
      "Epoch 1 -- Batch 540/ 842, training loss 0.8023801445960999\n",
      "Epoch 1 -- Batch 541/ 842, training loss 0.8069923520088196\n",
      "Epoch 1 -- Batch 542/ 842, training loss 0.8164859414100647\n",
      "Epoch 1 -- Batch 543/ 842, training loss 0.7970431447029114\n",
      "Epoch 1 -- Batch 544/ 842, training loss 0.7880479693412781\n",
      "Epoch 1 -- Batch 545/ 842, training loss 0.7818538546562195\n",
      "Epoch 1 -- Batch 546/ 842, training loss 0.7688695788383484\n",
      "Epoch 1 -- Batch 547/ 842, training loss 0.8359960913658142\n",
      "Epoch 1 -- Batch 548/ 842, training loss 0.8010686635971069\n",
      "Epoch 1 -- Batch 549/ 842, training loss 0.7663111090660095\n",
      "Epoch 1 -- Batch 550/ 842, training loss 0.778209924697876\n",
      "Epoch 1 -- Batch 551/ 842, training loss 0.7929143309593201\n",
      "Epoch 1 -- Batch 552/ 842, training loss 0.7488410472869873\n",
      "Epoch 1 -- Batch 553/ 842, training loss 0.7826374173164368\n",
      "Epoch 1 -- Batch 554/ 842, training loss 0.7851136922836304\n",
      "Epoch 1 -- Batch 555/ 842, training loss 0.7951108813285828\n",
      "Epoch 1 -- Batch 556/ 842, training loss 0.8026012182235718\n",
      "Epoch 1 -- Batch 557/ 842, training loss 0.782334566116333\n",
      "Epoch 1 -- Batch 558/ 842, training loss 0.7727000713348389\n",
      "Epoch 1 -- Batch 559/ 842, training loss 0.8026127219200134\n",
      "Epoch 1 -- Batch 560/ 842, training loss 0.7906301617622375\n",
      "Epoch 1 -- Batch 561/ 842, training loss 0.7802971005439758\n",
      "Epoch 1 -- Batch 562/ 842, training loss 0.791257917881012\n",
      "Epoch 1 -- Batch 563/ 842, training loss 0.7901970744132996\n",
      "Epoch 1 -- Batch 564/ 842, training loss 0.7764137387275696\n",
      "Epoch 1 -- Batch 565/ 842, training loss 0.7788635492324829\n",
      "Epoch 1 -- Batch 566/ 842, training loss 0.7621157169342041\n",
      "Epoch 1 -- Batch 567/ 842, training loss 0.8097175359725952\n",
      "Epoch 1 -- Batch 568/ 842, training loss 0.7999906539916992\n",
      "Epoch 1 -- Batch 569/ 842, training loss 0.8087242841720581\n",
      "Epoch 1 -- Batch 570/ 842, training loss 0.8051984906196594\n",
      "Epoch 1 -- Batch 571/ 842, training loss 0.7685178518295288\n",
      "Epoch 1 -- Batch 572/ 842, training loss 0.7711060643196106\n",
      "Epoch 1 -- Batch 573/ 842, training loss 0.7868304252624512\n",
      "Epoch 1 -- Batch 574/ 842, training loss 0.7626889944076538\n",
      "Epoch 1 -- Batch 575/ 842, training loss 0.7772658467292786\n",
      "Epoch 1 -- Batch 576/ 842, training loss 0.7664275169372559\n",
      "Epoch 1 -- Batch 577/ 842, training loss 0.7789492011070251\n",
      "Epoch 1 -- Batch 578/ 842, training loss 0.7982267141342163\n",
      "Epoch 1 -- Batch 579/ 842, training loss 0.8149798512458801\n",
      "Epoch 1 -- Batch 580/ 842, training loss 0.7842810153961182\n",
      "Epoch 1 -- Batch 581/ 842, training loss 0.7428940534591675\n",
      "Epoch 1 -- Batch 582/ 842, training loss 0.7771124839782715\n",
      "Epoch 1 -- Batch 583/ 842, training loss 0.7811887860298157\n",
      "Epoch 1 -- Batch 584/ 842, training loss 0.7793924808502197\n",
      "Epoch 1 -- Batch 585/ 842, training loss 0.7710723280906677\n",
      "Epoch 1 -- Batch 586/ 842, training loss 0.7719812393188477\n",
      "Epoch 1 -- Batch 587/ 842, training loss 0.7791911363601685\n",
      "Epoch 1 -- Batch 588/ 842, training loss 0.7727911472320557\n",
      "Epoch 1 -- Batch 589/ 842, training loss 0.7670729756355286\n",
      "Epoch 1 -- Batch 590/ 842, training loss 0.7914665937423706\n",
      "Epoch 1 -- Batch 591/ 842, training loss 0.76363605260849\n",
      "Epoch 1 -- Batch 592/ 842, training loss 0.8101664185523987\n",
      "Epoch 1 -- Batch 593/ 842, training loss 0.777576744556427\n",
      "Epoch 1 -- Batch 594/ 842, training loss 0.7708021402359009\n",
      "Epoch 1 -- Batch 595/ 842, training loss 0.7581953406333923\n",
      "Epoch 1 -- Batch 596/ 842, training loss 0.7723737955093384\n",
      "Epoch 1 -- Batch 597/ 842, training loss 0.735793948173523\n",
      "Epoch 1 -- Batch 598/ 842, training loss 0.7824945449829102\n",
      "Epoch 1 -- Batch 599/ 842, training loss 0.774909496307373\n",
      "Epoch 1 -- Batch 600/ 842, training loss 0.7609233260154724\n",
      "Epoch 1 -- Batch 601/ 842, training loss 0.7780589461326599\n",
      "Epoch 1 -- Batch 602/ 842, training loss 0.7705259323120117\n",
      "Epoch 1 -- Batch 603/ 842, training loss 0.7313373684883118\n",
      "Epoch 1 -- Batch 604/ 842, training loss 0.7907092571258545\n",
      "Epoch 1 -- Batch 605/ 842, training loss 0.8012191653251648\n",
      "Epoch 1 -- Batch 606/ 842, training loss 0.7967799305915833\n",
      "Epoch 1 -- Batch 607/ 842, training loss 0.7747230529785156\n",
      "Epoch 1 -- Batch 608/ 842, training loss 0.743156373500824\n",
      "Epoch 1 -- Batch 609/ 842, training loss 0.7728906273841858\n",
      "Epoch 1 -- Batch 610/ 842, training loss 0.766508936882019\n",
      "Epoch 1 -- Batch 611/ 842, training loss 0.7961792945861816\n",
      "Epoch 1 -- Batch 612/ 842, training loss 0.7814649343490601\n",
      "Epoch 1 -- Batch 613/ 842, training loss 0.816896915435791\n",
      "Epoch 1 -- Batch 614/ 842, training loss 0.7698925137519836\n",
      "Epoch 1 -- Batch 615/ 842, training loss 0.8062146902084351\n",
      "Epoch 1 -- Batch 616/ 842, training loss 0.789743185043335\n",
      "Epoch 1 -- Batch 617/ 842, training loss 0.7741971015930176\n",
      "Epoch 1 -- Batch 618/ 842, training loss 0.7864781022071838\n",
      "Epoch 1 -- Batch 619/ 842, training loss 0.7400951981544495\n",
      "Epoch 1 -- Batch 620/ 842, training loss 0.7528246641159058\n",
      "Epoch 1 -- Batch 621/ 842, training loss 0.7639182806015015\n",
      "Epoch 1 -- Batch 622/ 842, training loss 0.7375532984733582\n",
      "Epoch 1 -- Batch 623/ 842, training loss 0.78352290391922\n",
      "Epoch 1 -- Batch 624/ 842, training loss 0.7622447609901428\n",
      "Epoch 1 -- Batch 625/ 842, training loss 0.7980503439903259\n",
      "Epoch 1 -- Batch 626/ 842, training loss 0.7571067214012146\n",
      "Epoch 1 -- Batch 627/ 842, training loss 0.7466049194335938\n",
      "Epoch 1 -- Batch 628/ 842, training loss 0.7577725052833557\n",
      "Epoch 1 -- Batch 629/ 842, training loss 0.7185793519020081\n",
      "Epoch 1 -- Batch 630/ 842, training loss 0.7471032738685608\n",
      "Epoch 1 -- Batch 631/ 842, training loss 0.7735995650291443\n",
      "Epoch 1 -- Batch 632/ 842, training loss 0.73800128698349\n",
      "Epoch 1 -- Batch 633/ 842, training loss 0.757606029510498\n",
      "Epoch 1 -- Batch 634/ 842, training loss 0.796079695224762\n",
      "Epoch 1 -- Batch 635/ 842, training loss 0.749925434589386\n",
      "Epoch 1 -- Batch 636/ 842, training loss 0.7489458322525024\n",
      "Epoch 1 -- Batch 637/ 842, training loss 0.7643322348594666\n",
      "Epoch 1 -- Batch 638/ 842, training loss 0.7480067610740662\n",
      "Epoch 1 -- Batch 639/ 842, training loss 0.7904467582702637\n",
      "Epoch 1 -- Batch 640/ 842, training loss 0.7878620028495789\n",
      "Epoch 1 -- Batch 641/ 842, training loss 0.7591564655303955\n",
      "Epoch 1 -- Batch 642/ 842, training loss 0.768701434135437\n",
      "Epoch 1 -- Batch 643/ 842, training loss 0.7448426485061646\n",
      "Epoch 1 -- Batch 644/ 842, training loss 0.726120114326477\n",
      "Epoch 1 -- Batch 645/ 842, training loss 0.7499252557754517\n",
      "Epoch 1 -- Batch 646/ 842, training loss 0.7587211728096008\n",
      "Epoch 1 -- Batch 647/ 842, training loss 0.775244951248169\n",
      "Epoch 1 -- Batch 648/ 842, training loss 0.7395098209381104\n",
      "Epoch 1 -- Batch 649/ 842, training loss 0.7875323295593262\n",
      "Epoch 1 -- Batch 650/ 842, training loss 0.7436728477478027\n",
      "Epoch 1 -- Batch 651/ 842, training loss 0.7573518753051758\n",
      "Epoch 1 -- Batch 652/ 842, training loss 0.7323753237724304\n",
      "Epoch 1 -- Batch 653/ 842, training loss 0.7757499814033508\n",
      "Epoch 1 -- Batch 654/ 842, training loss 0.7514704465866089\n",
      "Epoch 1 -- Batch 655/ 842, training loss 0.7646369338035583\n",
      "Epoch 1 -- Batch 656/ 842, training loss 0.7601540088653564\n",
      "Epoch 1 -- Batch 657/ 842, training loss 0.7665337324142456\n",
      "Epoch 1 -- Batch 658/ 842, training loss 0.7447523474693298\n",
      "Epoch 1 -- Batch 659/ 842, training loss 0.7735554575920105\n",
      "Epoch 1 -- Batch 660/ 842, training loss 0.7397884130477905\n",
      "Epoch 1 -- Batch 661/ 842, training loss 0.7491486668586731\n",
      "Epoch 1 -- Batch 662/ 842, training loss 0.7606021165847778\n",
      "Epoch 1 -- Batch 663/ 842, training loss 0.755011796951294\n",
      "Epoch 1 -- Batch 664/ 842, training loss 0.7351861000061035\n",
      "Epoch 1 -- Batch 665/ 842, training loss 0.7374958395957947\n",
      "Epoch 1 -- Batch 666/ 842, training loss 0.7798550724983215\n",
      "Epoch 1 -- Batch 667/ 842, training loss 0.7696169018745422\n",
      "Epoch 1 -- Batch 668/ 842, training loss 0.734503448009491\n",
      "Epoch 1 -- Batch 669/ 842, training loss 0.7667807936668396\n",
      "Epoch 1 -- Batch 670/ 842, training loss 0.7270732522010803\n",
      "Epoch 1 -- Batch 671/ 842, training loss 0.7276344895362854\n",
      "Epoch 1 -- Batch 672/ 842, training loss 0.7563460469245911\n",
      "Epoch 1 -- Batch 673/ 842, training loss 0.7383654713630676\n",
      "Epoch 1 -- Batch 674/ 842, training loss 0.7516587972640991\n",
      "Epoch 1 -- Batch 675/ 842, training loss 0.746755063533783\n",
      "Epoch 1 -- Batch 676/ 842, training loss 0.7768259048461914\n",
      "Epoch 1 -- Batch 677/ 842, training loss 0.7657322287559509\n",
      "Epoch 1 -- Batch 678/ 842, training loss 0.7497124075889587\n",
      "Epoch 1 -- Batch 679/ 842, training loss 0.7590751647949219\n",
      "Epoch 1 -- Batch 680/ 842, training loss 0.7795467972755432\n",
      "Epoch 1 -- Batch 681/ 842, training loss 0.735482394695282\n",
      "Epoch 1 -- Batch 682/ 842, training loss 0.7605503797531128\n",
      "Epoch 1 -- Batch 683/ 842, training loss 0.7398133277893066\n",
      "Epoch 1 -- Batch 684/ 842, training loss 0.7532978653907776\n",
      "Epoch 1 -- Batch 685/ 842, training loss 0.7338200807571411\n",
      "Epoch 1 -- Batch 686/ 842, training loss 0.7235179543495178\n",
      "Epoch 1 -- Batch 687/ 842, training loss 0.710164487361908\n",
      "Epoch 1 -- Batch 688/ 842, training loss 0.7280026078224182\n",
      "Epoch 1 -- Batch 689/ 842, training loss 0.74093097448349\n",
      "Epoch 1 -- Batch 690/ 842, training loss 0.7558721899986267\n",
      "Epoch 1 -- Batch 691/ 842, training loss 0.7642524242401123\n",
      "Epoch 1 -- Batch 692/ 842, training loss 0.7137734889984131\n",
      "Epoch 1 -- Batch 693/ 842, training loss 0.7353173494338989\n",
      "Epoch 1 -- Batch 694/ 842, training loss 0.7452419400215149\n",
      "Epoch 1 -- Batch 695/ 842, training loss 0.7719696164131165\n",
      "Epoch 1 -- Batch 696/ 842, training loss 0.7286381721496582\n",
      "Epoch 1 -- Batch 697/ 842, training loss 0.721979558467865\n",
      "Epoch 1 -- Batch 698/ 842, training loss 0.7319355607032776\n",
      "Epoch 1 -- Batch 699/ 842, training loss 0.7129916548728943\n",
      "Epoch 1 -- Batch 700/ 842, training loss 0.7225127816200256\n",
      "Epoch 1 -- Batch 701/ 842, training loss 0.7509315013885498\n",
      "Epoch 1 -- Batch 702/ 842, training loss 0.7371529340744019\n",
      "Epoch 1 -- Batch 703/ 842, training loss 0.7642473578453064\n",
      "Epoch 1 -- Batch 704/ 842, training loss 0.7624203562736511\n",
      "Epoch 1 -- Batch 705/ 842, training loss 0.7038012742996216\n",
      "Epoch 1 -- Batch 706/ 842, training loss 0.7433125376701355\n",
      "Epoch 1 -- Batch 707/ 842, training loss 0.7193448543548584\n",
      "Epoch 1 -- Batch 708/ 842, training loss 0.7213928699493408\n",
      "Epoch 1 -- Batch 709/ 842, training loss 0.7574990391731262\n",
      "Epoch 1 -- Batch 710/ 842, training loss 0.7210198044776917\n",
      "Epoch 1 -- Batch 711/ 842, training loss 0.7165524959564209\n",
      "Epoch 1 -- Batch 712/ 842, training loss 0.7484226226806641\n",
      "Epoch 1 -- Batch 713/ 842, training loss 0.7459642887115479\n",
      "Epoch 1 -- Batch 714/ 842, training loss 0.7200824022293091\n",
      "Epoch 1 -- Batch 715/ 842, training loss 0.7498201131820679\n",
      "Epoch 1 -- Batch 716/ 842, training loss 0.7426939606666565\n",
      "Epoch 1 -- Batch 717/ 842, training loss 0.7196384072303772\n",
      "Epoch 1 -- Batch 718/ 842, training loss 0.7317627668380737\n",
      "Epoch 1 -- Batch 719/ 842, training loss 0.7515766024589539\n",
      "Epoch 1 -- Batch 720/ 842, training loss 0.7315930128097534\n",
      "Epoch 1 -- Batch 721/ 842, training loss 0.7292978763580322\n",
      "Epoch 1 -- Batch 722/ 842, training loss 0.7304384708404541\n",
      "Epoch 1 -- Batch 723/ 842, training loss 0.7165714502334595\n",
      "Epoch 1 -- Batch 724/ 842, training loss 0.7463989853858948\n",
      "Epoch 1 -- Batch 725/ 842, training loss 0.6980623006820679\n",
      "Epoch 1 -- Batch 726/ 842, training loss 0.7747904062271118\n",
      "Epoch 1 -- Batch 727/ 842, training loss 0.7694963216781616\n",
      "Epoch 1 -- Batch 728/ 842, training loss 0.766086220741272\n",
      "Epoch 1 -- Batch 729/ 842, training loss 0.7476078867912292\n",
      "Epoch 1 -- Batch 730/ 842, training loss 0.7437839508056641\n",
      "Epoch 1 -- Batch 731/ 842, training loss 0.7455843687057495\n",
      "Epoch 1 -- Batch 732/ 842, training loss 0.7564186453819275\n",
      "Epoch 1 -- Batch 733/ 842, training loss 0.7251577377319336\n",
      "Epoch 1 -- Batch 734/ 842, training loss 0.740901529788971\n",
      "Epoch 1 -- Batch 735/ 842, training loss 0.7544674873352051\n",
      "Epoch 1 -- Batch 736/ 842, training loss 0.717166543006897\n",
      "Epoch 1 -- Batch 737/ 842, training loss 0.712914228439331\n",
      "Epoch 1 -- Batch 738/ 842, training loss 0.7430906891822815\n",
      "Epoch 1 -- Batch 739/ 842, training loss 0.6867219805717468\n",
      "Epoch 1 -- Batch 740/ 842, training loss 0.7309576869010925\n",
      "Epoch 1 -- Batch 741/ 842, training loss 0.7157227396965027\n",
      "Epoch 1 -- Batch 742/ 842, training loss 0.7407349944114685\n",
      "Epoch 1 -- Batch 743/ 842, training loss 0.7313405871391296\n",
      "Epoch 1 -- Batch 744/ 842, training loss 0.717165470123291\n",
      "Epoch 1 -- Batch 745/ 842, training loss 0.7208559513092041\n",
      "Epoch 1 -- Batch 746/ 842, training loss 0.7349185347557068\n",
      "Epoch 1 -- Batch 747/ 842, training loss 0.7040762901306152\n",
      "Epoch 1 -- Batch 748/ 842, training loss 0.7262915372848511\n",
      "Epoch 1 -- Batch 749/ 842, training loss 0.7242445349693298\n",
      "Epoch 1 -- Batch 750/ 842, training loss 0.7213717103004456\n",
      "Epoch 1 -- Batch 751/ 842, training loss 0.7473413348197937\n",
      "Epoch 1 -- Batch 752/ 842, training loss 0.7500461339950562\n",
      "Epoch 1 -- Batch 753/ 842, training loss 0.730474591255188\n",
      "Epoch 1 -- Batch 754/ 842, training loss 0.7331140637397766\n",
      "Epoch 1 -- Batch 755/ 842, training loss 0.729247510433197\n",
      "Epoch 1 -- Batch 756/ 842, training loss 0.6658645272254944\n",
      "Epoch 1 -- Batch 757/ 842, training loss 0.7408135533332825\n",
      "Epoch 1 -- Batch 758/ 842, training loss 0.7259998917579651\n",
      "Epoch 1 -- Batch 759/ 842, training loss 0.718465268611908\n",
      "Epoch 1 -- Batch 760/ 842, training loss 0.7185249924659729\n",
      "Epoch 1 -- Batch 761/ 842, training loss 0.7107529640197754\n",
      "Epoch 1 -- Batch 762/ 842, training loss 0.7406924962997437\n",
      "Epoch 1 -- Batch 763/ 842, training loss 0.7129355072975159\n",
      "Epoch 1 -- Batch 764/ 842, training loss 0.7092922925949097\n",
      "Epoch 1 -- Batch 765/ 842, training loss 0.7131449580192566\n",
      "Epoch 1 -- Batch 766/ 842, training loss 0.7319498658180237\n",
      "Epoch 1 -- Batch 767/ 842, training loss 0.706939160823822\n",
      "Epoch 1 -- Batch 768/ 842, training loss 0.7453176975250244\n",
      "Epoch 1 -- Batch 769/ 842, training loss 0.7076834440231323\n",
      "Epoch 1 -- Batch 770/ 842, training loss 0.692307710647583\n",
      "Epoch 1 -- Batch 771/ 842, training loss 0.7255570292472839\n",
      "Epoch 1 -- Batch 772/ 842, training loss 0.7210735082626343\n",
      "Epoch 1 -- Batch 773/ 842, training loss 0.7470434904098511\n",
      "Epoch 1 -- Batch 774/ 842, training loss 0.7129689455032349\n",
      "Epoch 1 -- Batch 775/ 842, training loss 0.7445557713508606\n",
      "Epoch 1 -- Batch 776/ 842, training loss 0.7070325016975403\n",
      "Epoch 1 -- Batch 777/ 842, training loss 0.724822461605072\n",
      "Epoch 1 -- Batch 778/ 842, training loss 0.7251554727554321\n",
      "Epoch 1 -- Batch 779/ 842, training loss 0.7212738990783691\n",
      "Epoch 1 -- Batch 780/ 842, training loss 0.7175377607345581\n",
      "Epoch 1 -- Batch 781/ 842, training loss 0.7103699445724487\n",
      "Epoch 1 -- Batch 782/ 842, training loss 0.722893238067627\n",
      "Epoch 1 -- Batch 783/ 842, training loss 0.691952109336853\n",
      "Epoch 1 -- Batch 784/ 842, training loss 0.7010759115219116\n",
      "Epoch 1 -- Batch 785/ 842, training loss 0.7243145108222961\n",
      "Epoch 1 -- Batch 786/ 842, training loss 0.6986722350120544\n",
      "Epoch 1 -- Batch 787/ 842, training loss 0.7030426263809204\n",
      "Epoch 1 -- Batch 788/ 842, training loss 0.7297210693359375\n",
      "Epoch 1 -- Batch 789/ 842, training loss 0.731853723526001\n",
      "Epoch 1 -- Batch 790/ 842, training loss 0.7209417819976807\n",
      "Epoch 1 -- Batch 791/ 842, training loss 0.7122098207473755\n",
      "Epoch 1 -- Batch 792/ 842, training loss 0.7140151858329773\n",
      "Epoch 1 -- Batch 793/ 842, training loss 0.723240315914154\n",
      "Epoch 1 -- Batch 794/ 842, training loss 0.7385525107383728\n",
      "Epoch 1 -- Batch 795/ 842, training loss 0.7332271933555603\n",
      "Epoch 1 -- Batch 796/ 842, training loss 0.7346266508102417\n",
      "Epoch 1 -- Batch 797/ 842, training loss 0.7194958329200745\n",
      "Epoch 1 -- Batch 798/ 842, training loss 0.7157232761383057\n",
      "Epoch 1 -- Batch 799/ 842, training loss 0.7386189699172974\n",
      "Epoch 1 -- Batch 800/ 842, training loss 0.7287363409996033\n",
      "Epoch 1 -- Batch 801/ 842, training loss 0.7022500038146973\n",
      "Epoch 1 -- Batch 802/ 842, training loss 0.7147515416145325\n",
      "Epoch 1 -- Batch 803/ 842, training loss 0.7245744466781616\n",
      "Epoch 1 -- Batch 804/ 842, training loss 0.706708550453186\n",
      "Epoch 1 -- Batch 805/ 842, training loss 0.7073889970779419\n",
      "Epoch 1 -- Batch 806/ 842, training loss 0.7138877511024475\n",
      "Epoch 1 -- Batch 807/ 842, training loss 0.7111074924468994\n",
      "Epoch 1 -- Batch 808/ 842, training loss 0.7221058011054993\n",
      "Epoch 1 -- Batch 809/ 842, training loss 0.7246441841125488\n",
      "Epoch 1 -- Batch 810/ 842, training loss 0.7089544534683228\n",
      "Epoch 1 -- Batch 811/ 842, training loss 0.7024605870246887\n",
      "Epoch 1 -- Batch 812/ 842, training loss 0.7163859605789185\n",
      "Epoch 1 -- Batch 813/ 842, training loss 0.6952371597290039\n",
      "Epoch 1 -- Batch 814/ 842, training loss 0.7258763313293457\n",
      "Epoch 1 -- Batch 815/ 842, training loss 0.727327823638916\n",
      "Epoch 1 -- Batch 816/ 842, training loss 0.7178701162338257\n",
      "Epoch 1 -- Batch 817/ 842, training loss 0.7152204513549805\n",
      "Epoch 1 -- Batch 818/ 842, training loss 0.7369027137756348\n",
      "Epoch 1 -- Batch 819/ 842, training loss 0.7382321953773499\n",
      "Epoch 1 -- Batch 820/ 842, training loss 0.7008467316627502\n",
      "Epoch 1 -- Batch 821/ 842, training loss 0.7232751846313477\n",
      "Epoch 1 -- Batch 822/ 842, training loss 0.6998561024665833\n",
      "Epoch 1 -- Batch 823/ 842, training loss 0.7149536609649658\n",
      "Epoch 1 -- Batch 824/ 842, training loss 0.6913912296295166\n",
      "Epoch 1 -- Batch 825/ 842, training loss 0.7083802223205566\n",
      "Epoch 1 -- Batch 826/ 842, training loss 0.6972036957740784\n",
      "Epoch 1 -- Batch 827/ 842, training loss 0.723323404788971\n",
      "Epoch 1 -- Batch 828/ 842, training loss 0.6913089156150818\n",
      "Epoch 1 -- Batch 829/ 842, training loss 0.6871433258056641\n",
      "Epoch 1 -- Batch 830/ 842, training loss 0.6941906809806824\n",
      "Epoch 1 -- Batch 831/ 842, training loss 0.7172227501869202\n",
      "Epoch 1 -- Batch 832/ 842, training loss 0.7584533095359802\n",
      "Epoch 1 -- Batch 833/ 842, training loss 0.7017185091972351\n",
      "Epoch 1 -- Batch 834/ 842, training loss 0.6948850154876709\n",
      "Epoch 1 -- Batch 835/ 842, training loss 0.7113261818885803\n",
      "Epoch 1 -- Batch 836/ 842, training loss 0.6988588571548462\n",
      "Epoch 1 -- Batch 837/ 842, training loss 0.692767858505249\n",
      "Epoch 1 -- Batch 838/ 842, training loss 0.6814400553703308\n",
      "Epoch 1 -- Batch 839/ 842, training loss 0.6982045769691467\n",
      "Epoch 1 -- Batch 840/ 842, training loss 0.7228618860244751\n",
      "Epoch 1 -- Batch 841/ 842, training loss 0.7017273306846619\n",
      "Epoch 1 -- Batch 842/ 842, training loss 0.655218780040741\n",
      "----------------------------------------------------------------------\n",
      "Epoch 1 -- Batch 1/ 94, validation loss 0.6804775595664978\n",
      "Epoch 1 -- Batch 2/ 94, validation loss 0.6933826208114624\n",
      "Epoch 1 -- Batch 3/ 94, validation loss 0.6910951733589172\n",
      "Epoch 1 -- Batch 4/ 94, validation loss 0.6913628578186035\n",
      "Epoch 1 -- Batch 5/ 94, validation loss 0.6698461174964905\n",
      "Epoch 1 -- Batch 6/ 94, validation loss 0.6778905391693115\n",
      "Epoch 1 -- Batch 7/ 94, validation loss 0.6581210494041443\n",
      "Epoch 1 -- Batch 8/ 94, validation loss 0.7034772038459778\n",
      "Epoch 1 -- Batch 9/ 94, validation loss 0.6510263085365295\n",
      "Epoch 1 -- Batch 10/ 94, validation loss 0.6704269051551819\n",
      "Epoch 1 -- Batch 11/ 94, validation loss 0.7035161852836609\n",
      "Epoch 1 -- Batch 12/ 94, validation loss 0.6779568791389465\n",
      "Epoch 1 -- Batch 13/ 94, validation loss 0.6905947923660278\n",
      "Epoch 1 -- Batch 14/ 94, validation loss 0.6886858344078064\n",
      "Epoch 1 -- Batch 15/ 94, validation loss 0.7185940742492676\n",
      "Epoch 1 -- Batch 16/ 94, validation loss 0.6773529648780823\n",
      "Epoch 1 -- Batch 17/ 94, validation loss 0.6805100440979004\n",
      "Epoch 1 -- Batch 18/ 94, validation loss 0.6560958623886108\n",
      "Epoch 1 -- Batch 19/ 94, validation loss 0.6762996315956116\n",
      "Epoch 1 -- Batch 20/ 94, validation loss 0.6817664504051208\n",
      "Epoch 1 -- Batch 21/ 94, validation loss 0.682775616645813\n",
      "Epoch 1 -- Batch 22/ 94, validation loss 0.6649525165557861\n",
      "Epoch 1 -- Batch 23/ 94, validation loss 0.6656966209411621\n",
      "Epoch 1 -- Batch 24/ 94, validation loss 0.6864382028579712\n",
      "Epoch 1 -- Batch 25/ 94, validation loss 0.7190359830856323\n",
      "Epoch 1 -- Batch 26/ 94, validation loss 0.6894286870956421\n",
      "Epoch 1 -- Batch 27/ 94, validation loss 0.682472825050354\n",
      "Epoch 1 -- Batch 28/ 94, validation loss 0.6889105439186096\n",
      "Epoch 1 -- Batch 29/ 94, validation loss 0.6908175349235535\n",
      "Epoch 1 -- Batch 30/ 94, validation loss 0.6893277168273926\n",
      "Epoch 1 -- Batch 31/ 94, validation loss 0.6833860278129578\n",
      "Epoch 1 -- Batch 32/ 94, validation loss 0.6863015294075012\n",
      "Epoch 1 -- Batch 33/ 94, validation loss 0.7105854749679565\n",
      "Epoch 1 -- Batch 34/ 94, validation loss 0.6672804951667786\n",
      "Epoch 1 -- Batch 35/ 94, validation loss 0.6866488456726074\n",
      "Epoch 1 -- Batch 36/ 94, validation loss 0.6664239764213562\n",
      "Epoch 1 -- Batch 37/ 94, validation loss 0.6628384590148926\n",
      "Epoch 1 -- Batch 38/ 94, validation loss 0.6724305152893066\n",
      "Epoch 1 -- Batch 39/ 94, validation loss 0.6802381277084351\n",
      "Epoch 1 -- Batch 40/ 94, validation loss 0.683471143245697\n",
      "Epoch 1 -- Batch 41/ 94, validation loss 0.6823075413703918\n",
      "Epoch 1 -- Batch 42/ 94, validation loss 0.6979515552520752\n",
      "Epoch 1 -- Batch 43/ 94, validation loss 0.6937559247016907\n",
      "Epoch 1 -- Batch 44/ 94, validation loss 0.6931914687156677\n",
      "Epoch 1 -- Batch 45/ 94, validation loss 0.6786823272705078\n",
      "Epoch 1 -- Batch 46/ 94, validation loss 0.6991059184074402\n",
      "Epoch 1 -- Batch 47/ 94, validation loss 0.6734524965286255\n",
      "Epoch 1 -- Batch 48/ 94, validation loss 0.6735745072364807\n",
      "Epoch 1 -- Batch 49/ 94, validation loss 0.7178976535797119\n",
      "Epoch 1 -- Batch 50/ 94, validation loss 0.6756504774093628\n",
      "Epoch 1 -- Batch 51/ 94, validation loss 0.6772751808166504\n",
      "Epoch 1 -- Batch 52/ 94, validation loss 0.6604562997817993\n",
      "Epoch 1 -- Batch 53/ 94, validation loss 0.6795802116394043\n",
      "Epoch 1 -- Batch 54/ 94, validation loss 0.7072193622589111\n",
      "Epoch 1 -- Batch 55/ 94, validation loss 0.6975374817848206\n",
      "Epoch 1 -- Batch 56/ 94, validation loss 0.6659603714942932\n",
      "Epoch 1 -- Batch 57/ 94, validation loss 0.6853426694869995\n",
      "Epoch 1 -- Batch 58/ 94, validation loss 0.6815770864486694\n",
      "Epoch 1 -- Batch 59/ 94, validation loss 0.693423867225647\n",
      "Epoch 1 -- Batch 60/ 94, validation loss 0.6930336952209473\n",
      "Epoch 1 -- Batch 61/ 94, validation loss 0.6823846101760864\n",
      "Epoch 1 -- Batch 62/ 94, validation loss 0.6898705363273621\n",
      "Epoch 1 -- Batch 63/ 94, validation loss 0.691178023815155\n",
      "Epoch 1 -- Batch 64/ 94, validation loss 0.6522872447967529\n",
      "Epoch 1 -- Batch 65/ 94, validation loss 0.6791911721229553\n",
      "Epoch 1 -- Batch 66/ 94, validation loss 0.6735827326774597\n",
      "Epoch 1 -- Batch 67/ 94, validation loss 0.6793895959854126\n",
      "Epoch 1 -- Batch 68/ 94, validation loss 0.684284508228302\n",
      "Epoch 1 -- Batch 69/ 94, validation loss 0.6732199788093567\n",
      "Epoch 1 -- Batch 70/ 94, validation loss 0.6628385782241821\n",
      "Epoch 1 -- Batch 71/ 94, validation loss 0.6895080804824829\n",
      "Epoch 1 -- Batch 72/ 94, validation loss 0.6937965154647827\n",
      "Epoch 1 -- Batch 73/ 94, validation loss 0.6951081156730652\n",
      "Epoch 1 -- Batch 74/ 94, validation loss 0.7007991671562195\n",
      "Epoch 1 -- Batch 75/ 94, validation loss 0.6825163960456848\n",
      "Epoch 1 -- Batch 76/ 94, validation loss 0.684004545211792\n",
      "Epoch 1 -- Batch 77/ 94, validation loss 0.6614083051681519\n",
      "Epoch 1 -- Batch 78/ 94, validation loss 0.6967300772666931\n",
      "Epoch 1 -- Batch 79/ 94, validation loss 0.676898717880249\n",
      "Epoch 1 -- Batch 80/ 94, validation loss 0.7201351523399353\n",
      "Epoch 1 -- Batch 81/ 94, validation loss 0.6923434734344482\n",
      "Epoch 1 -- Batch 82/ 94, validation loss 0.6783255934715271\n",
      "Epoch 1 -- Batch 83/ 94, validation loss 0.6844721436500549\n",
      "Epoch 1 -- Batch 84/ 94, validation loss 0.7017139196395874\n",
      "Epoch 1 -- Batch 85/ 94, validation loss 0.6789239645004272\n",
      "Epoch 1 -- Batch 86/ 94, validation loss 0.7042070031166077\n",
      "Epoch 1 -- Batch 87/ 94, validation loss 0.6962811350822449\n",
      "Epoch 1 -- Batch 88/ 94, validation loss 0.6782096028327942\n",
      "Epoch 1 -- Batch 89/ 94, validation loss 0.6876701712608337\n",
      "Epoch 1 -- Batch 90/ 94, validation loss 0.6783854961395264\n",
      "Epoch 1 -- Batch 91/ 94, validation loss 0.6846461296081543\n",
      "Epoch 1 -- Batch 92/ 94, validation loss 0.6654298305511475\n",
      "Epoch 1 -- Batch 93/ 94, validation loss 0.6805793642997742\n",
      "Epoch 1 -- Batch 94/ 94, validation loss 0.7143859267234802\n",
      "----------------------------------------------------------------------\n",
      "Epoch 1 loss: Training 1.0304776430130005, Validation 0.7143859267234802\n",
      "----------------------------------------------------------------------\n",
      "Epoch 2/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 18\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(NC1Cn2cc(NC(=O)Nc3ccc(OC(F)(F)F)cc3F)nc3nc2s1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1(C)C(C)C(C)N2CCCCN(C(=O)NCc3ccccc3)C1'\n",
      "[19:03:55] Explicit valence for atom # 9 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1/C(=N/NC2=O)c(=O)oc1N'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 20 21 22 23 24 25 26\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCCCC(=O)Nc1cccc(-n2nc(=S)n(C3CC(=O)N(C)C)(C(=O)N(C)C)C2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cn1c(S(=O)(=O)Nc2ccc(F)cc2)nc2c1ccccc12'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)Nc2ccc([N+](=O)[O-])c3nccn22)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cc(-c2cc(C(=O)NCc3ccccc3)sn2c2CNC(=O)c2ccccc2C)cc1Cl'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1n[nH]c(=O)n1C[C@H](C)OC[C@H]1CCCN(CC2CC3)C[C@@H]1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cn1c(=O)c(OCC(=O)N2CCCC2)c(=O)oc2cc1Br'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 16 17 19 21\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 23 24\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'C=CCn1ccc(=O)c2[nH]c(N3CCN(c4nc(-c4ccccc4)nnn3)CC3)n1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C=CC(=O)C2=C(c3ccccc3C)C2=CC2=N)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 17\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 17\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 17 18 19 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(Cn3ccc(C(=O)N4CCOc4ccccc4[nH]3)nn2)cc1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(C(=O)NCCCN2[C@@H](CO)C[C@@H]2C[C@H](CCN(C(=O)N34CCC(C)CC3)[C@H](CO)O2)N1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(CN2c3c(scc4nc(C)c(C)nc4C)c2=O)cc1OC'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CC(C)NC(=O)C2C1CC(=O)N(C)CCOCC3'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Cc1cccc(-n2c(NSc3nc(-c4ccc(Br)cc4)nnc3N3CCOCC3)nc1-c1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC2(S)CCC(=O)N3CC(=O)Nc2nccn2C)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 11 12 26 27\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1c(C#N)c(C(=O)OCC)c(=O)[nH]c2c1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: COCC1ONC(=O)C2(C)N(c3cccc(C)c3)[C@H](CO)N31)N1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COCC1ONC(=O)C2(C)N(c3cccc(C)c3)[C@H](CO)N31)N1' for input: 'COCC1ONC(=O)C2(C)N(c3cccc(C)c3)[C@H](CO)N31)N1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 12 14\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-n2nc3c(-c4cccc(N)cc4)n[nH]2)o1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 23\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(OC(=O)Nc1ccccc1)Cc1ccc2c(c1)OCC(=O)N(CC)CC3'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 16 17 18 19\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 9\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCCc1nnc(C)c1C(=O)N(Cc1ccco1)C2CC1CCCCCC2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 16 17 20 30 31\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 18\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCC(C(=O)Nc1ccc2nc(-c3ccccc4)nc2n1)N1CCOCC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1nc2c(cc2c1C#N)CCCC1)OCCO2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C1N2C(=O)N[C@@H](CCN(C)Cc2cncn3)C(c2ccccc2)C2C1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: CC(=O)Cn1c(SCC(=O)Nc2ccc(F)cc2)nc2c(-c4cccc5c(C)ccc4c3)nnc23)c1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CC(=O)Cn1c(SCC(=O)Nc2ccc(F)cc2)nc2c(-c4cccc5c(C)ccc4c3)nnc23)c1' for input: 'CC(=O)Cn1c(SCC(=O)Nc2ccc(F)cc2)nc2c(-c4cccc5c(C)ccc4c3)nnc23)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C(=O)N(CCCN2CCCCN2S(=O)(=O)c2ccc(F)cc2)CS(=O)(=O)C2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 8 9\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 26 27 35\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 16\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Oc1ccc(-c2cc3nCc3ccc(Cl)cc3c2=O)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCc1ccc(S(=O)(=O)N(Cc2ccc(F)c(F)c2)C(C(=O)OCC)C(C)N2CCO)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(S(=O)(=O)NCCCSC3c2ccc(F)c(Cl)c2)sc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 13 14 15 17 19 22\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 23 24 25 26\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 22\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'N#Cc1c(Nc2nc(-c3cccs3)c(O)[nH]c2c2C)c(C)oc1C'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 22 23\n",
      "[19:03:55] Conflicting single bond directions around double bond at index 1.\n",
      "[19:03:55]   BondStereo set to STEREONONE and single bond directions set to NONE.\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Nc2cnc(N3CCCCC3)nc2N2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CCN1CCCC1)C1(CNC(c2ccccc2)(C)C2=O)CCCC'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 13 25\n",
      "[19:03:55] Explicit valence for atom # 8 Br, 2, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COCN1Cc2c([nH]c3ccccc3c2=O)/C(=C/c2ccco2)C(C)(C)C2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CCc1ccccc1)N1CCNC(c2nnc3sc4ccccc4c2)C2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 11 12 22 29 30\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 15\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1cc2ccccc1[nH]1)NNC1CCCC1'\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 13 and atom 15 for input: 'O=C(NCc1cccs1)N1CCN(S(=O)2c2ccc(F)cc2)CCC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CNC(=O)C1(C)CC(C)C1)N1CC1CCC2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccnc2ccccc1c1=O)NCc1ccccc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1CCC(C)NC(=O)[C@@H]2CCC[C@H](O)C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 21 22\n",
      "[19:03:55] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'OCCNC(=O)/N=C/c1ccc(C2(C)C3)cc1'\n",
      "[19:03:55] Explicit valence for atom # 16 Cl, 2, is greater than permitted\n",
      "[19:03:55] Explicit valence for atom # 7 O, 3, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(-c2nc(Nc3ccccc3C)nc(Cc3cc(Cl)c([N+](=O)[O-])[nH]3)n12'\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 9 for input: 'C(C)CC(=O)n1c(C=C2c2ccccc2C(=O)Oc2ccccc2)sc2ccccc12'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN(C)c1ccc(-c2ccn(Nc3nc(Nc4ccccc4F)nc3Nc4ccccc4o3)cc2)c(C)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 18 19 20\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(N2C(=O)NCC2)c(N3CCOCC2)cc1)[C@@H](C)C[C@H]1CC'\n",
      "[19:03:55] SMILES Parse Error: ring closure 3 duplicates bond between atom 10 and atom 11 for input: 'COc1ccc(N(c2cnc3c3ncsc3c3C(C#N)NC(C)(C)C)CC2)nn1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 16\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 22\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C1C(c2ccccc2)C1c1cccc(-n2nnc3c2CCN(C)C2=O)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cc(C2NC(=O)CN(Cc3cccnc3)CCCCN22)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC12CCCCN1C1C(=O)NCc1c2ccccc2[nH]1'\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: COc1ccccc1NC(=O)c1cc(6CCN2CCC(=O)N3CCCC3)ccc2C(=O)N2CCOCC2)cc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1NC(=O)c1cc(6CCN2CCC(=O)N3CCCC3)ccc2C(=O)N2CCOCC2)cc1' for input: 'COc1ccccc1NC(=O)c1cc(6CCN2CCC(=O)N3CCCC3)ccc2C(=O)N2CCOCC2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(C(=O)Nc2nc3c(c2c(=O)o1)CCCC3'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 15 16\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 19\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'CCOc1cccc(-c2cc(-c3cccnc3)c(N/N=C\\c3ccccc3)nc21'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1CCC3CCN1C(=O)c1c(NC(=O)c2ccc(C)o2)nn(C)c1=O'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCCCCCCCCCOCC(=O)NCCCCC)CC\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCCCCCOCC(=O)NCCCCC)CC' for input: 'CCCCCCCCCCCCCCCOCC(=O)NCCCCC)CC'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1ccc2c(c1)OC(=O)C(C)(C)N2C(=O)c1cc(F)ccc1Cl'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 17\n",
      "[19:03:55] Explicit valence for atom # 10 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: CC1=C(C)N/C(=C/\\C=C/c2cccs2)NC(=O)CCC2=O\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CC1=C(C)N/C(=C/\\C=C/c2cccs2)NC(=O)CCC2=O' for input: 'CC1=C(C)N/C(=C/\\C=C/c2cccs2)NC(=O)CCC2=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 20\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(OCC(=O)Nc2ccc3c(c2)n(CO)c(=O)n(-c3ccccc3)cc2c1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(C#N)N(Cc2ccc(C(=O)N3CCCCC3)c(=O)[nH]2)C2CC3)o1'\n",
      "[19:03:55] Explicit valence for atom # 16 C, 5, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 23 25 26\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)nc2n1Cc1ccccc1O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CCN(S(=O)(=O)c2ccc(Nc3nnc(-c4cccnc4)n4)cn2)CC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1CC[C@@H](CC(=O)N2CCCC2)C[C@H]1CN(C)C(=O)N[C@H]12CCCC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 13\n",
      "[19:03:55] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Nc1cccc(/C=C\\\\C(=O)CSc2nnc(-c3ccccc3)n3nc2c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC/CCC(NC(=O)CC1CCCN1C)S2C(=O)c1ccccc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 17\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1ccccc1-c1ccc2c(c1)OCC(=O)NCc1ccccc1C2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1NC(=O)N[C@H]1C2CC[C@@H](NC(=O)[C@H](O)CO3)[C@@H]2C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 27\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2cc(-c3cc[nH]c3c3)cn[nH]2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 20\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1ccccc1)c1ccccc1C(=O)N(CC(=O)Nc1ccccc1Cl)C1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc3c(C)c(C(=O)N4CCOc4ccccc4[nH]3)cs2)c(C)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)CNC(=O)c1ccc2c(c1)OC[C@H](C)N(Cc1cccc(Cl)c1)OC3'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(NC(=O)C(CC)n1-c1ccc(OC)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 23\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C(Nc1nnc(-c2cccc(N3CCN(CCO)CCOc4ccsc4)c2)C2)s1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(COc1ccccc1CNCCC1CN(Cc2ccccc2)C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(NCCCCNCC1CCCO1)N(Cc1ccccn1)C(=O)N2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cc(Oc2ccccc2)on1)C1CC1C1CCN(C(=O)c1cccc(C(=O)OCC)c1)C2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(O)CNC(=O)c1ccc2cc(C(C)(C)C)ccc3c1=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 7 8 9 10 12 13\n",
      "[19:03:55] Explicit valence for atom # 0 F, 2, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1cccc(NC(=S)NCCc2nonc22)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 18 29 30\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(/C(O)=C(N2CC(=O)NC(Cc3ccccc3)Cc3ccc4ccccc4n4)C2=O)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2(C(C)(C)C#N)nc3n(C5NCc5ccccc5)CN(c4ccc(F)cc4)C(=O)N2)cc1OC'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[19:03:55] Explicit valence for atom # 24 O, 4, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: CCC(O)C/C=C//N=N/Nc1ccccc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CCC(O)C/C=C//N=N/Nc1ccccc1' for input: 'CCC(O)C/C=C//N=N/Nc1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCCCOCCCNC(=O)c1cc2c(c1)n(C(C)C)c(=O)n(C)c1=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 20\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 19 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(O)CSc1nc(Cl)sc1N2CCCCCC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 27\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 10 11 12 13 14 15\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Cc1cccc(N2C[C@@H](NCCNC(=O)c3cccnc3)OC[C@H]2C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'C=CCN1C(=O)Nc2ccc([N+](=O)[O-])c(N3CCc4ccccc45)c2C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cn1Oc(=O)c2c(Cc3cc(C)nc4ccccc44)c(C)oc2n1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1[nH]c(NC(=O)c2ccn2cc(-c3cccc(F)c3)cc2)oc1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1c[nH]c2nc3c(c(=O)n2CCC4)c1=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 32\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN(C)C[C@@H]1Oc2nc(N(C)C)nc(C)c2C(C)CC(=O)O[C@@H]2C(=O)OC[C@H](C)N(C(=O)c2ccccc2)C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 20 21 22 23 24 25 26 27 28\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 5 21 23 24\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 24 25 26\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=P(NC12CCC1CCC1)c1cc(=O)c2ccccc2o1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cccc2oc(=O)c(Nc3ccccc3C3CCCN3C(=O)C2COCC2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 18\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=c1c2c(cccc2c(=O)n(-c2ccc(Cl)cc2)c(=O)[nH]1)C2CCCS1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'N#C[C@@H]1[C@@H]2[C@@H]3COCC[C@@]3(CCN(CC(=O)c5ccccc5)C[C@H]34)[C@H]1N2Cc1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: CCOCCCn1c2ccc(S(=O)(=O)N3c4ccccc4CC)cc2C)cc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CCOCCCn1c2ccc(S(=O)(=O)N3c4ccccc4CC)cc2C)cc1' for input: 'CCOCCCn1c2ccc(S(=O)(=O)N3c4ccccc4CC)cc2C)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCC(C)N=Cc1c(C(=O)N2CCCCC2)c(Cl)cc2s1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(CNc1nc(NCc2ccccc2F)nc2c1ccccc1n1CCC(=O)NCCC(C)C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 18\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'NS(=O)(=O)c1cc(Cl)c(C2C3CCN(S(=O)(=O)c5cccc(F)c4)C3CCN22)cc1C'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 16 17\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 11\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 18\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 24\n",
      "[19:03:55] Explicit valence for atom # 11 O, 3, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 24 26\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2nc(NC(=O)c3cccnc3)c(-c3ccccc3)n2)c1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2nc(NC(=O)c3cccnc3)c(-c3ccccc3)n2)c1' for input: 'COc1ccc2nc(NC(=O)c3cccnc3)c(-c3ccccc3)n2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 15 19 20 21\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 13 15 18 21 24\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2CC(=N)N(Cc3ccccc3)NCSc2nccs2)cc1OC'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: COC(=O)CSC(=O)OCCCN(C)C)C1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COC(=O)CSC(=O)OCCCN(C)C)C1' for input: 'COC(=O)CSC(=O)OCCCN(C)C)C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)C(=O)N(C)[C@H](O)[C@H](OC)CN3C(=O)CNC(C)=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 10 11 12\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1nc(CNC(=O)c2ccc(Br)cc2)sc1C(=O)NC2CCN(C3CCCCC3)C(=O)C12'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 10\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'CN(C)C(=O)c1ccc(-c2csc(Nc3ccc(-c4onnc4-c4ccccc3)cn2)s1'\n",
      "[19:03:55] Explicit valence for atom # 15 O, 3, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(c1)C(CN(C)Cc1ccncc1)CC3CCN4Cc2nnnn2CC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc2n(CC(C(=O)NCCN3CCC4(C5CCN(C5)CCO5)C(C)C4)CC3)nc(N)cc12'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(F)cc1)C1(CCCNC2)C(=O)N(N)CCCN1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CCN[C@H](C)[C@H]2C[C@H](C)N(CCN2CCCC2)C1=O'\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: COc1ccc(NC(=O)CNC(=O)Nc2ccc()c3ccccc23)cc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(NC(=O)CNC(=O)Nc2ccc()c3ccccc23)cc1' for input: 'COc1ccc(NC(=O)CNC(=O)Nc2ccc()c3ccccc23)cc1'\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 10 and atom 11 for input: 'O=C(COC(=O)CC[C@@H]1N=C2c2ccc(Br)cc21)C1=C(COCC2CC1)NC2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 10\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(CCNC(SC)C(=O)c2cc(S(=O)(=O)N2CCCC2)ccc2Br'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 23\n",
      "[19:03:55] Explicit valence for atom # 7 C, 5, is greater than permitted\n",
      "[19:03:55] Explicit valence for atom # 15 O, 3, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCc1cc(C(=O)N2CCN(c3ncnc3sc4c(=O)[nH]cc43)CC2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 13 14 16\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 22\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 20 22 24\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(=O)Nc2nnc3ccccc3c23)cc1OC'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 7 15 16 17\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 29\n",
      "[19:03:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 2 for input: 'O=S1c1ccccc1-c1cccc(C(=O)N2CCOCC2)c1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: O=C1C3C3C=CC(C4)C(N4)C2C(=O)NC2c2cccn(O)c2)cc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'O=C1C3C3C=CC(C4)C(N4)C2C(=O)NC2c2cccn(O)c2)cc1' for input: 'O=C1C3C3C=CC(C4)C(N4)C2C(=O)NC2c2cccn(O)c2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 19 20 23 24\n",
      "[19:03:55] Explicit valence for atom # 5 Cl, 2, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(O)C(c1ccc2c(c1)O[C@@H](CO)N(C(=O)CCc1ccccc1)[C@H](C)[C@@H](C)CN3CCCN(C)C2=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 13 14\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'c1cnc(C(c2cccs2)N(C)C2CC4)cc(Cl)c12'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc(NCCN3C(=O)CCC4C=C)nc3ncccn32)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(O)[C@@H]1CC(=O)N(C)C(=O)C2C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 14\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 18\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1C[C@@H]2C[C@@H](CC(=O)N2CCN(c4ccc([N+]n6)cc3)[C@H]3C2)[C@H]1CO'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)NC12CCNC(Cc2cccn(F)c2)C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCNC(=O)c1ccc(-c2nn3ccc(-c4ccccc4)c[nH]2)c(C)n1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 17 18\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 8\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(-n2c(N)nc(CN3CCCC(C(C)=O)N3)nc2c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1cc(NC(=O)Nc2c(F)cccc2F)sc2n1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 25 27\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cn1ncc(NC(=O)CN(Cc2ccccc2)C2(C)C(C)C)CC1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'CCOC(=O)c1ccc(-c2soc2c(=O)n(-c2ccccc2)n1-c1nncc1-c1ccc(F)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CSc1cc(C)c2ncc(-c3cnc(N)cc3)n1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'NOc1ccc(-n2nnc3sc(-c4ccc(Br)cc4)c(=O)o2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CC(=O)NCc2cccc3c(=O)c2cc2c(=O)n(-c4ccc(Cl)cc4)sc13)c1ccccc1F'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 17 18\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: CCOC(O)CC#Cc1nc2c(=O)n(C)c(=O)n2C)cc1[N+](=O)[O-]\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CCOC(O)CC#Cc1nc2c(=O)n(C)c(=O)n2C)cc1[N+](=O)[O-]' for input: 'CCOC(O)CC#Cc1nc2c(=O)n(C)c(=O)n2C)cc1[N+](=O)[O-]'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)N[C@@H]1CCCCCCC(=O)N1CCCC1'\n",
      "[19:03:55] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C1C[C@@H]2S[C@@H](OCC[C@H](O)Cn2CN1Cc2n[nH]cc2)[C@@H]1CO'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 22\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 27 28\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 28\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)N1CCN(Cc2c(=O)c(=O)c(C)nc3c2c(=O)o2)CCC(C)(C)OC3'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: C[C@H]1OCCCN1C(=O)O[C@@H]1CN(C)C(=O)Nc2ccc3c(c2)CCN3CCc2ccco2)C1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'C[C@H]1OCCCN1C(=O)O[C@@H]1CN(C)C(=O)Nc2ccc3c(c2)CCN3CCc2ccco2)C1' for input: 'C[C@H]1OCCCN1C(=O)O[C@@H]1CN(C)C(=O)Nc2ccc3c(c2)CCN3CCc2ccco2)C1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: C[C@H]1CN(C)C(=O)c2c(C#C)n(C)c(=O)n(C)c22)c1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'C[C@H]1CN(C)C(=O)c2c(C#C)n(C)c(=O)n(C)c22)c1' for input: 'C[C@H]1CN(C)C(=O)c2c(C#C)n(C)c(=O)n(C)c22)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)c(=O)[nH]c(=S)n(CC(=O)NCCCc3ccccc3)c12'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)C1=C(C)OC(N)=C(N)O2'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Cc1cc(C)nc(SCc2ccc(NC(=O)c3c(sn4-c5cccc(C4C5CC5CC4)cc3)cs2)C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 11 12 13 20 21\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(-c2noc(CNC3CCS(=O)(=O)N(CC(=O)N4CCOCC4)[C@H](C)CO3)n12'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)CN2C(=O)NC3(C(C)(C)N)C(=O)N3c2ccc(C)cc2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C1N=C(NC(=O)CN2C(=O)CCC2(C)NC2=O)ccc1Br'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nsc3c2Nc2ccccc2C(=O)O)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'C#CCC1C2CC(=O)N(c3ccc(F)cc3)[C@H]2CCN2C(=O)NCc2ccccc21'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 18 19 23 26\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(NCC1CC1)C1CCN(C(=O)C(=O)N(Cc2ccco2C)CC2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 12 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CNC(=O)COC(=O)c1c(O)c2ccccc2[nH]c1Nc1nc(-c2ccccc2)sc1-1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1[C@@H](CN(Cc2cccnn2)Cc2ccccc2N(C)C)CCCCCOC'\n",
      "[19:03:55] Explicit valence for atom # 9 O, 4, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 18\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 19 20 21 22 23 24\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])c1cc(-c2ccc3c(c2)OCc3n[nH]c(=O)[nH]3)cnon2c1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1CCN(C(=O)CCCCN(CCCCNC2CCCCC2)C2)CC1'\n",
      "[19:03:55] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CC2CC(CC(=O)Nc2ccc(Br)c([N+](=O)[O-])c2)c1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CC2CCC1C1)CCCCCCCCC2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 37 38\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCCn1c(=O)c2c(n3nc(CN4CCOCC4)c(-c3ccco4)nsc23)nc1-c1cccc(C)c1'\n",
      "[19:03:55] Explicit valence for atom # 10 Br, 2, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cc(OCC(=O)Nc2ccc2c3c(c2)CCC[C@]34)cc2ccccc12'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 23\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'C=C(C)c1cccc(/C=C(/C#N)C#C\\C(=O)N(C)C2=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(/C=C/c1cccoc1)N1CCNC2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 15 27\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(CSC1c1cccc2c(-c1ccc(F)cc1)n1nnc(Cc2ccc(F)cc2)n1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)N[C@H]2C[C@@H]31[C@@][C@@H]1(C)Cn(OSC)n1CO'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)cc1N1CC2(CNC(=S)=C(C(C)(C)C3)C1=O)NC2c1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCN(Cc1nccn1C1CCN(C(=O)N(C)CC2)CC1)C(C)C'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14 15 16\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 15 24\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 18 19 20\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(CC1C=CN(c2ccc(OCC#O)cc2)/C(=O)N=C/N1CCOCC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1CCCN1Nc1nnc(SCC(=O)N/C=C/c2c[nH]c(=O)n3C)cc1=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'CC(C)N1CCc2c([nH]c3cc(-c4ccccc4)ccn3[C@@H]1CN(C)S2(=O)=O'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COC(=O)c1cccc(Nc2ncc(Nc3ncc(F)cc3)nc2-c1ccnc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OCc2ccc(S(=O)(=O)N(Cc3cccs3)n(C(C)C)n2)cc2C(=O)NCCN2CCOCC2)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COCCCNC1CCNC12'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 21 23\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'NNC(=O)CCn1c(=O)[nH]c2ccc(OCc3ccccc3F)cc1=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:03:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 35 and atom 36 for input: 'CCn1c(=O)c2c(C(=O)Nc3ccc(F)cc3)sc(OC)c2nc2c1n1CCN(Cc1ccccc1C1CC2)C2'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COC(c1cccc1C(=O)N/C(=N\\Nc1cccc(F)c1)C(Cl)c1ccco1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 10 20\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-n1c(=O)c(/C=C\\c2ccc3c(c2)OC(=O)Cc3cccc([N+](=O)[O-])n3C)c(=O)n2C1c1ccc(OC)cc1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(Cn2nc3nc4c(cc2c2)CCN(C[C@@H](C)CO)CC3C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1ccc2c(c1)OCO2)NC[C@@H]1CC[C@H](CC[C@@]2(C(=O)NCCCN3CCOCC3)O[C@@H]2CO2)O1'\n",
      "[19:03:55] Explicit valence for atom # 26 O, 4, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9 11 12 13 14\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(/NS(=O)(=O)c1cc(-c2ccc(F)cc2)on1)c1ccncc1c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 25 and atom 26 for input: 'COc1ccc(C2C(=O)NC(O)=C(C(C(=O)Oc3ccccc4)C2C)C2c2cc(C)ccc2C)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 15 17\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C(=O)NCC2CC(C)(C)Oc3ccccc22)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 13 14 15 16 17 18\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CCC2(CNC(=O)c2ccc(Cl)cn2)C1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CNc2cc(=O)n(C)c3nc(-c4cccc(F)c4)c(N3CCCO4)nc22)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CC2CCC1)N(c1ccccc1)c1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CNC(=O)CSCc1nnc2c3c(c(Cl)c1Cl)CCCC3'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2ccc(Br)c(S(=O)(=O)N3CC(C(F)(F)F)FC2)s2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 19 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COC(=O)COC(=O)CN1CCOC(C(=O)NC3CCC3)CC2C1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(CC1CC2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1C(=O)Nc1ccccc1NC2CCOCC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 9 10 11 12 13\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(-n2c(-c3ccccc3F)nc3c3ccccc3cc21'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 9 28 29\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14\n",
      "[19:03:55] Explicit valence for atom # 12 N, 4, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)/C1CN(c2ccc(OC)c(OC)c2)CC[C@H]2CN1CC[C@H](C)Oc1ccccc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 5 25 26\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 11 16 17\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1nocc1C(N=NCC1NC(=O)CCC1CC2CCCC1)N2CC(=O)Nc1ccccc1C'\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: CN1CCN(CCCOc2cccc(C(=O)O3CCC(C)=)c2)CC1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CN1CCN(CCCOc2cccc(C(=O)O3CCC(C)=)c2)CC1' for input: 'CN1CCN(CCCOc2cccc(C(=O)O3CCC(C)=)c2)CC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: COCCCCNC(=O)[C@@H]1[C@@H](CO)[C@H]2CN(C)C2=O)c1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COCCCCNC(=O)[C@@H]1[C@@H](CO)[C@H]2CN(C)C2=O)c1' for input: 'COCCCCNC(=O)[C@@H]1[C@@H](CO)[C@H]2CN(C)C2=O)c1'\n",
      "[19:03:55] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1nc2c(oc(=O)n(Cc2ccco2)n3ccccc22)n1'\n",
      "[19:03:55] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 13 and atom 14 for input: 'COc1ccc(Nc2nccnc2N2c2ccc(Br)cc2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1(C)C2N=C([n+]3ccc([N+](=O)[O-])cc3)C2CCCC3C2C1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COCc1nc2sc3c(c(=O)n2-c2ccc(OC)cc1)CCC3'\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: Cc1ccc(CN2CCN3c4ccccc(-4)C#CC2)cc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(CN2CCN3c4ccccc(-4)C#CC2)cc1' for input: 'Cc1ccc(CN2CCN3c4ccccc(-4)C#CC2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1CCN(C(=O)c2cc([C@@H]3C[C@H](O)COC(=O)[C@@H]4C(O)(F)F)C(=O)[C@@H]3C2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 18 19\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 31\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 8 13\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: CCOC(=O)C1=C(C)NC2=C(N3CCCC3)Cc3cc(O5C)c(OC)cc3C3=O)c1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CCOC(=O)C1=C(C)NC2=C(N3CCCC3)Cc3cc(O5C)c(OC)cc3C3=O)c1' for input: 'CCOC(=O)C1=C(C)NC2=C(N3CCCC3)Cc3cc(O5C)c(OC)cc3C3=O)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 26 27 30\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(C2CC(C)OCC23CC(=O)NC2CCCN2C(=O)CCC2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CSCC(=O)N2CCN(c2cc3ccccc3[nH]2)C1)CCC1CCCO1'\n",
      "[19:03:55] Explicit valence for atom # 7 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: CC12CC3=CCCN(C(=O)c3ccc(-c4c(F)cc(F)cc4)n3)C2)C1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CC12CC3=CCCN(C(=O)c3ccc(-c4c(F)cc(F)cc4)n3)C2)C1' for input: 'CC12CC3=CCCN(C(=O)c3ccc(-c4c(F)cc(F)cc4)n3)C2)C1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 17 18 24\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 14 25 26 27\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 10 11 12 14 25 28 29\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cc(C2C(C#C)=C(CC(=O)NCc3ccccc3)=C(O)C(C(=O)Nc2ccc(C)cc2)(C)OC)cn1-c1cccc(Cl)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'NS(=O)(=O)c1nc(CN2CCC2(C)CC(=O)N3C2CCCCC2)cc1'\n",
      "[19:03:55] Explicit valence for atom # 16 O, 3, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cc(N2CCCC2)c(CN2CCC(CN(C)C(=O)C[C@H]3CCCC3)[C@H](CO)Oc2ccccc2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)C(NS(=O)(=O)c1cccs1)C1(CCCCC1)C(=O)N2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CSC1=NN4CCOC2)CN1CCOC'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 15 17\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(-c2cn4ccccc4c3nn2)cc1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: O=C(c1ccccn1)N1CCN2CCN(C(=O)Nc3ncc(-c4ccss4)c3C)CC2)c1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'O=C(c1ccccn1)N1CCN2CCN(C(=O)Nc3ncc(-c4ccss4)c3C)CC2)c1' for input: 'O=C(c1ccccn1)N1CCN2CCN(C(=O)Nc3ncc(-c4ccss4)c3C)CC2)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(F)c(F)c1)[C@@H]1C[C@@H]2[C@@H]3CCN[C@H]2C(=O)N(C)C[C@H]12'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 25\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOCCOC(=O)c1cc(C(=O)N2CCCC3C(C)C)cc(=O)[nH]c2c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccnc(N2CCN(C(=O)C3CCC3)C[C@H]2CN2C(=O)CCC(=O)NCCc2ccccc2)cc1OC'\n",
      "[19:03:55] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCNC(=O)Cc1nnc2c1c(=O)[nH]c(=O)n1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(NC(=O)Cc2nn3ccc(N4CCN(C)CC5)cc3c2)cn1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 18 19\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Nc2nnc(CC3CCCCC3)c2ncn3C3CC2)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17 18 19 20 21\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(CCCOC(=O)c1cn(SC)n(-c2cccs2)o1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CC2(CCNC3CC(c2ccc(Cl)cc2)S3)S(=O)(=O)c2ccccc21'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)C(=O)N([C@@H](C)CO)C[C@H](C)N(C(=O)c1cccs1)CC3'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 21 22 23 24 33 34 35\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 18\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 13 14 21\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C=C2C(=O)[C@@H](c3nc4ccco4)Cc3ccc(OC)cc32)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'NC(=O)C1CCN(c2nc(Nc3cccn4ncnn3c3=O)[C@H]2CO)cc1'\n",
      "[19:03:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 2 for input: 'O=C1C1CCN(CCNc2cc(C)c(C)c(C)n2)CC1'\n",
      "[19:03:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 8 and atom 9 for input: 'CSc1nnc2c1CC1c1c(C(c3ccccc2)n[nH]c1-c1cccc(OC)c1)C1CCN(C1CC2)C2'\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: Cc1c(C/=C(O)C(=O)Nc2ccccc2OC)nc2sccc2c1=O\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'Cc1c(C/=C(O)C(=O)Nc2ccccc2OC)nc2sccc2c1=O' for input: 'Cc1c(C/=C(O)C(=O)Nc2ccccc2OC)nc2sccc2c1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(C2SC2c3ccccc3C2)n1)c1ccc2ccncc2c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N(C[C@H]2CN(C)C(=O)C3CC[C@H]3CCN(Cc5ccccn5)C[C@@H]34)c2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 9 10 18\n",
      "[19:03:55] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 24 25 26 27 28\n",
      "[19:03:55] Explicit valence for atom # 19 N, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc3ncccc3c(=O)c2-c2nn2c(=O)cc(-c3ccc(F)cc3)cc(Br)[nH]2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 28 29 31 32 33\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN(Cc1cc3c(cc1Cl)C(=O)CCC(=O)NC)CCN3CCOCC2=O'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(CCN1CCN(S(=O)(=O)c2cc(F)ccc2C2=O)CC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-n1c(SCC(=O)Nc2nccs2)sc2c1nc2ccccc21'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 15 16 17 18\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(C[C@@H]1CC[C@@H]2[C@H]CCN[C@@H]2C(=O)NC1CCN(C)CC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[19:03:55] SMILES Parse Error: ring closure 3 duplicates bond between atom 13 and atom 14 for input: 'CCOc1ccc(/C=C2/N=C(/C#N)S3c3cc(S(=O)(=O)N4CCCCC4)c(OC)c(OC)n3C2=O)cc1C'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 10\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)CCn1c(=O)c(C=C(C)C)[nH]c2c1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)C2CCN(Cc3ccc(O)cc3)CN1C1=O'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'CN(C)C(=O)c1ccc(C2([C@@H]3CCNC(=O)N(C)C[C@H]3O[C@H]2O)c2ccccc21'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 15\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 13 14\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 20\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(NC(=O)C(C(=O)NCc2cc3ccccc3s2)c(=O)n1-c1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=C(NCCC(=O)N1CCCC1)n(n1cnc2ccccc13'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 31\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CSc1ccc2c(=O)occc2/C=C/C=C\\N(CCC)C(=O)NC12CC3CC(CCC2)C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)C(=O)NC1CCSn2c1NC(=O)Cc1nnc(-c2ccno2)n1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2CCN(Cc3nc(=CC(C)n[nH]c3=O)=C(C)NC3=NS(C)(=O)=O)C2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CC[C@@H]2[C@H]3C[C@@H]3COCN2C2c1ccc(-c2ccc(F)cc2)n1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCC1CCCN1C(=O)CN1CCN(c2nc(C(C)C)c3[nH]cnc3c(=O)n2-2)CC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cccc(-c2nc(C(C)(C)CC3=N)c(N3CCCC3=O)s2)c1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(-c2nc3ccccc3[nH]c2nc2ccccc12'\n",
      "[19:03:55] SMILES Parse Error: ring closure 4 duplicates bond between atom 15 and atom 16 for input: 'COc1ccc(-c2c(C)n3c(=O)[nH]c(=Sn4c4ccccc4O)[nH]c23)cc1C'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 19\n",
      "[19:03:55] SMILES Parse Error: duplicated ring closure 2 bonds atom 7 to itself for input: 'CN1C(=O)/C(=C/Cc22c3cccnc3[nH]2)[nH]c2c1CCC2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 40\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 22 23\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C(=O)/C(=C\\C(=O)C(C)NC(=O)c2ccccc2)s2)cc([N+](=O)[O-])c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[19:03:55] Explicit valence for atom # 25 C, 5, is greater than permitted\n",
      "[19:03:55] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 20 and atom 21 for input: 'COc1ccc(Nc2nc(N3CCCCC3)nc3[nH]c2c2-c2ccccc2)s1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN1CC2(CC1)Cc1cn2c(=O)n(C)c(=O)n(C)c21'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN(Cc2ccc(N3C(C)=O)ccc2N(C)C)CC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 15\n",
      "[19:03:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 22 and atom 23 for input: 'COc1cccc(/C=C2\\c3ccccc3C2=CCC33CCCN2C2=O)cc1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 34\n",
      "[19:03:55] Explicit valence for atom # 1 O, 3, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CCOCC1)N1CCC1(Cc2cccc(Cl)c2)CC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'NC(=O)N1CCC(CN(C)CC(=O)Nc2c(C(=O)N3CCO4CCCC3)CC[C@@H](C)O3)CC1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(CN(CCc2ccc(Nc3ccc(F)cc3)o2)C(=OOCOC(=O)c2cccs2)cc1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)N1CC2CCCC2CCC2C1(C)C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C[C@]N[C@H]1C[C@H]2CC[C@H](C3)[C@@H]1C(=O)NCCc1ccc(C)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)cc(C(=O)NCC2(C)NC(=O)c3cc(C4C)ccc3OC)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 19 20 21 22\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 10 25\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 23 24\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 23 24\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN(CC)C(=O)CSc1nc(C2CCCCN2CCCN2)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1nnnc2c1-c1ccccc1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCN(C)S(=O)(=O)c1c(C(C)C)n(-c2ccc2c(c1)[C@H](CC(N)=O)N24CCN(C(=O)c4cccc(F)c4)O[C@H]22)o1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C1Nc2ccccc2C(=O)NC1CCC1CCC'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(COc1cccc(Br)c1)N1CCC(c2nc4ccc(F)cc4c3)CC1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Cc1cccc(-c2ccc(-c3c(Nn4ccccc4Cl)sc3)oc2c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 15 16 18\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CN(C12CCCCC1)C(=O)CSC1(C)CC(C)C1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COCCOC(=O)Cn1cccc1C(NC(=O)c1ccccc1C(F)(F)F'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CN(C(=O)c2c3c(cc4ccccc34)CCN3C)CC1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 10 11 12 26 28 29\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(Br)cc1NC(=S)c1cccs1)Nc1ccccc1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(Br)cc1NC(=S)c1cccs1)Nc1ccccc1' for input: 'COc1ccc(Br)cc1NC(=S)c1cccs1)Nc1ccccc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(C)(=O)=O)cc1OC(=O)C1CC1CCN1C(=O)c1c(NC(C)=O)cc(C)o1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 14 15 16 17 18 19 22\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 26\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(N2CCOCC2CN1CC(O)(Oc2ccccc2)C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C=C2/C(=O)N=C(N)C(N)(C)C22C)c(C)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 12 13 14\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc2nc(CC(C)C(C)C)c(=O)n3c2c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cc(Br)ccc1NC(=O)CCCn1ccc2c(c1=O)OCC'\n",
      "[19:03:55] Explicit valence for atom # 2 O, 4, is greater than permitted\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(Oc2cc(C(=O)Nc3ccc(-c4ccccc4)cn3)oc2cnn[nH]1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Cn1c(NC(=O)CN2CCCC2)sc2c1CCN(C)Cc1ccccc1O3'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 17 18 19\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 14 15 16\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1CCN(C2C2CC(C)C2)CCN(CC)C2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc2c(c1)nc1cc3ncccc3n1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)[C@@H](CCSc2noc(C)n2C)n2C(C)C)cc1'\n",
      "[19:03:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 13 and atom 14 for input: 'COCCNC(=O)C1CCN(CC1)C1c1ccccc1[N+](=O)[O-]'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1nc2sc3ccc([nH](=O)[O-])cc2[nH]c2c1C#N)CCCC'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14\n",
      "[19:03:55] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCc1csc2cc1C(C)C(=O)Nc1ccccc1C'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'Nc1ccccc1-c1cccc(-c1ccccc1)c1cccs1'\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'O=S(=O)(c1cccc(CN2CCN(c3ccccc3)cn2)n1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C1c2occc2n2c(c1=O)C[C@H](CCCN3CCC(C)CCCN12)NC1CC1'\n",
      "[19:03:55] SMILES Parse Error: syntax error while parsing: CN1CCN(CCNC(=O)CN2CCN(S==O)c3nc(C)ccc3C2)CC1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'CN1CCN(CCNC(=O)CN2CCN(S==O)c3nc(C)ccc3C2)CC1' for input: 'CN1CCN(CCNC(=O)CN2CCN(S==O)c3nc(C)ccc3C2)CC1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)Cn1c(C)c(CN2CCC(=O)Nc2ncc(C)n2)c(=O)[nH]c1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC1C(c2ccccc2)OC1=C(C#N)C(N)=C(N)N2'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C2=NC(O)=C(C#N)CN(c3ccncc3)C[C@@H]23)c1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(C1(C(=O)CCC(=O)NCc2cccnc2)CC2)cc1C(N)=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(S(=O)(=O)N2C[C@@H]3[C@H](CO)N4CC[C@H]4CC(=O)N(C)C[C@@H]5S[C@H]([C@]4)[C@H]3[C@]23)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccccc1-n1cnc2ccc3c(cc2o(=O)Nc2ccc(CN(C)C)cc1)C2'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 3 13\n",
      "[19:03:55] Explicit valence for atom # 7 O, 3, is greater than permitted\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCC(C)NC(=O)Nc1c(C)oc2c1CCN(C(=O)c1ccc(C)nn1)C3'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19 20 21\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1ncc2cc(-c3csoc3)cn1)c1ccco1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(CCC1(c2ccccc2)CC1=O)NCC1CC3'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2ccc(-c3cnoc3-c4nc(-ccccc6)n3)c(C)[nH]3)cc2cc1OC'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: O=C1NC(=O)N(Cc2cccc([N+](=O)[O-])c2))C1\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'O=C1NC(=O)N(Cc2cccc([N+](=O)[O-])c2))C1' for input: 'O=C1NC(=O)N(Cc2cccc([N+](=O)[O-])c2))C1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)c1nn[nH]1-c1cccc(C(=O)N2CCC(NCCOCC3CCO)CC2)c1'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 30\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(F)cc1)N(Cc1ccco1)C1'\n",
      "[19:03:55] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(-c2ccc3c(c2)O[C@H](CO)N(Cc4ccccc4)[C@H](CO)[C@H]3O3)cc2)cc1N\n",
      "[19:03:55] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(-c2ccc3c(c2)O[C@H](CO)N(Cc4ccccc4)[C@H](CO)[C@H]3O3)cc2)cc1N' for input: 'Cc1cc(-c2ccc3c(c2)O[C@H](CO)N(Cc4ccccc4)[C@H](CO)[C@H]3O3)cc2)cc1N'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 15\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 17\n",
      "[19:03:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(-c2cc(-c3ccccc3)c(Cl)c(S(=O)(=O)N3CCCCC3)c(N2CCCCC2)c(=O)[nH]c1=O'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-n2cc(C)c(N3CCN(c4nccc(F)c5)n3CC2)c2F)cc1'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'COCCOCCNC(=O)CN1CCc2c(C)cccc2O[C@@H]1CN(C)CC1CC'\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCCCn1c(SCC2(C)CCNC(=O)C2)nc2ccccc2[C@H]1C(=O)N2CCCC1=O'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 1 3 4\n",
      "[19:03:55] SMILES Parse Error: unclosed ring for input: 'CCNc1ccc2c(c1)CN(CCCOc1nccc1C3)CC3'\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 20 21 22 23\n",
      "[19:03:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 13 14 28 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 -- Batch 1/ 842, training loss 0.6875993013381958\n",
      "Epoch 2 -- Batch 2/ 842, training loss 0.7156004309654236\n",
      "Epoch 2 -- Batch 3/ 842, training loss 0.6980999112129211\n",
      "Epoch 2 -- Batch 4/ 842, training loss 0.7139374017715454\n",
      "Epoch 2 -- Batch 5/ 842, training loss 0.6910369992256165\n",
      "Epoch 2 -- Batch 6/ 842, training loss 0.7170813083648682\n",
      "Epoch 2 -- Batch 7/ 842, training loss 0.7075843214988708\n",
      "Epoch 2 -- Batch 8/ 842, training loss 0.7003893852233887\n",
      "Epoch 2 -- Batch 9/ 842, training loss 0.7086516618728638\n",
      "Epoch 2 -- Batch 10/ 842, training loss 0.7113326787948608\n",
      "Epoch 2 -- Batch 11/ 842, training loss 0.6944671869277954\n",
      "Epoch 2 -- Batch 12/ 842, training loss 0.727820634841919\n",
      "Epoch 2 -- Batch 13/ 842, training loss 0.6805467009544373\n",
      "Epoch 2 -- Batch 14/ 842, training loss 0.7049128413200378\n",
      "Epoch 2 -- Batch 15/ 842, training loss 0.6942419409751892\n",
      "Epoch 2 -- Batch 16/ 842, training loss 0.7016972303390503\n",
      "Epoch 2 -- Batch 17/ 842, training loss 0.6909859776496887\n",
      "Epoch 2 -- Batch 18/ 842, training loss 0.6859922409057617\n",
      "Epoch 2 -- Batch 19/ 842, training loss 0.7156960964202881\n",
      "Epoch 2 -- Batch 20/ 842, training loss 0.7259957790374756\n",
      "Epoch 2 -- Batch 21/ 842, training loss 0.6971284747123718\n",
      "Epoch 2 -- Batch 22/ 842, training loss 0.693824827671051\n",
      "Epoch 2 -- Batch 23/ 842, training loss 0.7092857956886292\n",
      "Epoch 2 -- Batch 24/ 842, training loss 0.7044201493263245\n",
      "Epoch 2 -- Batch 25/ 842, training loss 0.7146473526954651\n",
      "Epoch 2 -- Batch 26/ 842, training loss 0.6939874887466431\n",
      "Epoch 2 -- Batch 27/ 842, training loss 0.6972777843475342\n",
      "Epoch 2 -- Batch 28/ 842, training loss 0.6966094374656677\n",
      "Epoch 2 -- Batch 29/ 842, training loss 0.6858588457107544\n",
      "Epoch 2 -- Batch 30/ 842, training loss 0.7063834071159363\n",
      "Epoch 2 -- Batch 31/ 842, training loss 0.696331799030304\n",
      "Epoch 2 -- Batch 32/ 842, training loss 0.6676054000854492\n",
      "Epoch 2 -- Batch 33/ 842, training loss 0.7015002369880676\n",
      "Epoch 2 -- Batch 34/ 842, training loss 0.6892783641815186\n",
      "Epoch 2 -- Batch 35/ 842, training loss 0.689644455909729\n",
      "Epoch 2 -- Batch 36/ 842, training loss 0.6863088607788086\n",
      "Epoch 2 -- Batch 37/ 842, training loss 0.7033366560935974\n",
      "Epoch 2 -- Batch 38/ 842, training loss 0.7199753522872925\n",
      "Epoch 2 -- Batch 39/ 842, training loss 0.6794714331626892\n",
      "Epoch 2 -- Batch 40/ 842, training loss 0.6804958581924438\n",
      "Epoch 2 -- Batch 41/ 842, training loss 0.7296215295791626\n",
      "Epoch 2 -- Batch 42/ 842, training loss 0.6620686650276184\n",
      "Epoch 2 -- Batch 43/ 842, training loss 0.6871704459190369\n",
      "Epoch 2 -- Batch 44/ 842, training loss 0.6943257451057434\n",
      "Epoch 2 -- Batch 45/ 842, training loss 0.7009158730506897\n",
      "Epoch 2 -- Batch 46/ 842, training loss 0.688637375831604\n",
      "Epoch 2 -- Batch 47/ 842, training loss 0.6749985218048096\n",
      "Epoch 2 -- Batch 48/ 842, training loss 0.7084526419639587\n",
      "Epoch 2 -- Batch 49/ 842, training loss 0.6981592178344727\n",
      "Epoch 2 -- Batch 50/ 842, training loss 0.6785221099853516\n",
      "Epoch 2 -- Batch 51/ 842, training loss 0.6862871050834656\n",
      "Epoch 2 -- Batch 52/ 842, training loss 0.6890366673469543\n",
      "Epoch 2 -- Batch 53/ 842, training loss 0.6995041966438293\n",
      "Epoch 2 -- Batch 54/ 842, training loss 0.667056143283844\n",
      "Epoch 2 -- Batch 55/ 842, training loss 0.7261245846748352\n",
      "Epoch 2 -- Batch 56/ 842, training loss 0.6971566677093506\n",
      "Epoch 2 -- Batch 57/ 842, training loss 0.690028190612793\n",
      "Epoch 2 -- Batch 58/ 842, training loss 0.6993023753166199\n",
      "Epoch 2 -- Batch 59/ 842, training loss 0.6808476448059082\n",
      "Epoch 2 -- Batch 60/ 842, training loss 0.692064642906189\n",
      "Epoch 2 -- Batch 61/ 842, training loss 0.691180408000946\n",
      "Epoch 2 -- Batch 62/ 842, training loss 0.7036555409431458\n",
      "Epoch 2 -- Batch 63/ 842, training loss 0.7017922401428223\n",
      "Epoch 2 -- Batch 64/ 842, training loss 0.6785683631896973\n",
      "Epoch 2 -- Batch 65/ 842, training loss 0.6730949878692627\n",
      "Epoch 2 -- Batch 66/ 842, training loss 0.6810400485992432\n",
      "Epoch 2 -- Batch 67/ 842, training loss 0.727788507938385\n",
      "Epoch 2 -- Batch 68/ 842, training loss 0.6802322864532471\n",
      "Epoch 2 -- Batch 69/ 842, training loss 0.7068325877189636\n",
      "Epoch 2 -- Batch 70/ 842, training loss 0.6990726590156555\n",
      "Epoch 2 -- Batch 71/ 842, training loss 0.6647095084190369\n",
      "Epoch 2 -- Batch 72/ 842, training loss 0.6639155149459839\n",
      "Epoch 2 -- Batch 73/ 842, training loss 0.7017754912376404\n",
      "Epoch 2 -- Batch 74/ 842, training loss 0.6866602301597595\n",
      "Epoch 2 -- Batch 75/ 842, training loss 0.6699839234352112\n",
      "Epoch 2 -- Batch 76/ 842, training loss 0.7115024328231812\n",
      "Epoch 2 -- Batch 77/ 842, training loss 0.6540187001228333\n",
      "Epoch 2 -- Batch 78/ 842, training loss 0.6864281296730042\n",
      "Epoch 2 -- Batch 79/ 842, training loss 0.698988139629364\n",
      "Epoch 2 -- Batch 80/ 842, training loss 0.6876975893974304\n",
      "Epoch 2 -- Batch 81/ 842, training loss 0.6957702040672302\n",
      "Epoch 2 -- Batch 82/ 842, training loss 0.6990162134170532\n",
      "Epoch 2 -- Batch 83/ 842, training loss 0.6979128122329712\n",
      "Epoch 2 -- Batch 84/ 842, training loss 0.6718668937683105\n",
      "Epoch 2 -- Batch 85/ 842, training loss 0.6794806718826294\n",
      "Epoch 2 -- Batch 86/ 842, training loss 0.669411838054657\n",
      "Epoch 2 -- Batch 87/ 842, training loss 0.6742222309112549\n",
      "Epoch 2 -- Batch 88/ 842, training loss 0.6723722815513611\n",
      "Epoch 2 -- Batch 89/ 842, training loss 0.7306368350982666\n",
      "Epoch 2 -- Batch 90/ 842, training loss 0.6887809634208679\n",
      "Epoch 2 -- Batch 91/ 842, training loss 0.6663344502449036\n",
      "Epoch 2 -- Batch 92/ 842, training loss 0.6748236417770386\n",
      "Epoch 2 -- Batch 93/ 842, training loss 0.7187585830688477\n",
      "Epoch 2 -- Batch 94/ 842, training loss 0.6646870374679565\n",
      "Epoch 2 -- Batch 95/ 842, training loss 0.6890656352043152\n",
      "Epoch 2 -- Batch 96/ 842, training loss 0.6994247436523438\n",
      "Epoch 2 -- Batch 97/ 842, training loss 0.688169002532959\n",
      "Epoch 2 -- Batch 98/ 842, training loss 0.6881335973739624\n",
      "Epoch 2 -- Batch 99/ 842, training loss 0.6754452586174011\n",
      "Epoch 2 -- Batch 100/ 842, training loss 0.6901566982269287\n",
      "Epoch 2 -- Batch 101/ 842, training loss 0.6732017397880554\n",
      "Epoch 2 -- Batch 102/ 842, training loss 0.6964712738990784\n",
      "Epoch 2 -- Batch 103/ 842, training loss 0.6906859874725342\n",
      "Epoch 2 -- Batch 104/ 842, training loss 0.694635808467865\n",
      "Epoch 2 -- Batch 105/ 842, training loss 0.6688211560249329\n",
      "Epoch 2 -- Batch 106/ 842, training loss 0.6831472516059875\n",
      "Epoch 2 -- Batch 107/ 842, training loss 0.6752638816833496\n",
      "Epoch 2 -- Batch 108/ 842, training loss 0.7245535850524902\n",
      "Epoch 2 -- Batch 109/ 842, training loss 0.671773374080658\n",
      "Epoch 2 -- Batch 110/ 842, training loss 0.6802659630775452\n",
      "Epoch 2 -- Batch 111/ 842, training loss 0.7084060907363892\n",
      "Epoch 2 -- Batch 112/ 842, training loss 0.6763896942138672\n",
      "Epoch 2 -- Batch 113/ 842, training loss 0.683239221572876\n",
      "Epoch 2 -- Batch 114/ 842, training loss 0.6603732109069824\n",
      "Epoch 2 -- Batch 115/ 842, training loss 0.667305052280426\n",
      "Epoch 2 -- Batch 116/ 842, training loss 0.6550419926643372\n",
      "Epoch 2 -- Batch 117/ 842, training loss 0.7008105516433716\n",
      "Epoch 2 -- Batch 118/ 842, training loss 0.6536304354667664\n",
      "Epoch 2 -- Batch 119/ 842, training loss 0.6816660761833191\n",
      "Epoch 2 -- Batch 120/ 842, training loss 0.6690702438354492\n",
      "Epoch 2 -- Batch 121/ 842, training loss 0.6821003556251526\n",
      "Epoch 2 -- Batch 122/ 842, training loss 0.6796138286590576\n",
      "Epoch 2 -- Batch 123/ 842, training loss 0.6827768683433533\n",
      "Epoch 2 -- Batch 124/ 842, training loss 0.6565890908241272\n",
      "Epoch 2 -- Batch 125/ 842, training loss 0.6570547223091125\n",
      "Epoch 2 -- Batch 126/ 842, training loss 0.6545560956001282\n",
      "Epoch 2 -- Batch 127/ 842, training loss 0.6995714902877808\n",
      "Epoch 2 -- Batch 128/ 842, training loss 0.69630366563797\n",
      "Epoch 2 -- Batch 129/ 842, training loss 0.7010355591773987\n",
      "Epoch 2 -- Batch 130/ 842, training loss 0.6494277119636536\n",
      "Epoch 2 -- Batch 131/ 842, training loss 0.6926873326301575\n",
      "Epoch 2 -- Batch 132/ 842, training loss 0.6754044890403748\n",
      "Epoch 2 -- Batch 133/ 842, training loss 0.66926109790802\n",
      "Epoch 2 -- Batch 134/ 842, training loss 0.6974388957023621\n",
      "Epoch 2 -- Batch 135/ 842, training loss 0.6537889242172241\n",
      "Epoch 2 -- Batch 136/ 842, training loss 0.715172529220581\n",
      "Epoch 2 -- Batch 137/ 842, training loss 0.6759426593780518\n",
      "Epoch 2 -- Batch 138/ 842, training loss 0.6753462553024292\n",
      "Epoch 2 -- Batch 139/ 842, training loss 0.6723887324333191\n",
      "Epoch 2 -- Batch 140/ 842, training loss 0.6799887418746948\n",
      "Epoch 2 -- Batch 141/ 842, training loss 0.6939612627029419\n",
      "Epoch 2 -- Batch 142/ 842, training loss 0.6550948023796082\n",
      "Epoch 2 -- Batch 143/ 842, training loss 0.700018584728241\n",
      "Epoch 2 -- Batch 144/ 842, training loss 0.6911033987998962\n",
      "Epoch 2 -- Batch 145/ 842, training loss 0.6858398914337158\n",
      "Epoch 2 -- Batch 146/ 842, training loss 0.6883041858673096\n",
      "Epoch 2 -- Batch 147/ 842, training loss 0.6850512027740479\n",
      "Epoch 2 -- Batch 148/ 842, training loss 0.7055487036705017\n",
      "Epoch 2 -- Batch 149/ 842, training loss 0.6772491931915283\n",
      "Epoch 2 -- Batch 150/ 842, training loss 0.6849007606506348\n",
      "Epoch 2 -- Batch 151/ 842, training loss 0.6839410066604614\n",
      "Epoch 2 -- Batch 152/ 842, training loss 0.6829432845115662\n",
      "Epoch 2 -- Batch 153/ 842, training loss 0.6785656213760376\n",
      "Epoch 2 -- Batch 154/ 842, training loss 0.6557878255844116\n",
      "Epoch 2 -- Batch 155/ 842, training loss 0.684141218662262\n",
      "Epoch 2 -- Batch 156/ 842, training loss 0.6724352836608887\n",
      "Epoch 2 -- Batch 157/ 842, training loss 0.6594989895820618\n",
      "Epoch 2 -- Batch 158/ 842, training loss 0.6598029732704163\n",
      "Epoch 2 -- Batch 159/ 842, training loss 0.6612139940261841\n",
      "Epoch 2 -- Batch 160/ 842, training loss 0.6857991814613342\n",
      "Epoch 2 -- Batch 161/ 842, training loss 0.6440267562866211\n",
      "Epoch 2 -- Batch 162/ 842, training loss 0.6822139024734497\n",
      "Epoch 2 -- Batch 163/ 842, training loss 0.6639752388000488\n",
      "Epoch 2 -- Batch 164/ 842, training loss 0.6969030499458313\n",
      "Epoch 2 -- Batch 165/ 842, training loss 0.6976977586746216\n",
      "Epoch 2 -- Batch 166/ 842, training loss 0.654000997543335\n",
      "Epoch 2 -- Batch 167/ 842, training loss 0.657091498374939\n",
      "Epoch 2 -- Batch 168/ 842, training loss 0.6751416921615601\n",
      "Epoch 2 -- Batch 169/ 842, training loss 0.6875137686729431\n",
      "Epoch 2 -- Batch 170/ 842, training loss 0.6552811861038208\n",
      "Epoch 2 -- Batch 171/ 842, training loss 0.661994993686676\n",
      "Epoch 2 -- Batch 172/ 842, training loss 0.678717851638794\n",
      "Epoch 2 -- Batch 173/ 842, training loss 0.690357506275177\n",
      "Epoch 2 -- Batch 174/ 842, training loss 0.6584975123405457\n",
      "Epoch 2 -- Batch 175/ 842, training loss 0.6701030135154724\n",
      "Epoch 2 -- Batch 176/ 842, training loss 0.6626279354095459\n",
      "Epoch 2 -- Batch 177/ 842, training loss 0.663149893283844\n",
      "Epoch 2 -- Batch 178/ 842, training loss 0.6793795824050903\n",
      "Epoch 2 -- Batch 179/ 842, training loss 0.6736202239990234\n",
      "Epoch 2 -- Batch 180/ 842, training loss 0.6827353835105896\n",
      "Epoch 2 -- Batch 181/ 842, training loss 0.6429356336593628\n",
      "Epoch 2 -- Batch 182/ 842, training loss 0.6992273330688477\n",
      "Epoch 2 -- Batch 183/ 842, training loss 0.6880501508712769\n",
      "Epoch 2 -- Batch 184/ 842, training loss 0.6831591725349426\n",
      "Epoch 2 -- Batch 185/ 842, training loss 0.6503486633300781\n",
      "Epoch 2 -- Batch 186/ 842, training loss 0.6505081653594971\n",
      "Epoch 2 -- Batch 187/ 842, training loss 0.6562616229057312\n",
      "Epoch 2 -- Batch 188/ 842, training loss 0.6672425866127014\n",
      "Epoch 2 -- Batch 189/ 842, training loss 0.6975277662277222\n",
      "Epoch 2 -- Batch 190/ 842, training loss 0.6635702252388\n",
      "Epoch 2 -- Batch 191/ 842, training loss 0.6686458587646484\n",
      "Epoch 2 -- Batch 192/ 842, training loss 0.6985171437263489\n",
      "Epoch 2 -- Batch 193/ 842, training loss 0.6487429738044739\n",
      "Epoch 2 -- Batch 194/ 842, training loss 0.6521140336990356\n",
      "Epoch 2 -- Batch 195/ 842, training loss 0.6619089841842651\n",
      "Epoch 2 -- Batch 196/ 842, training loss 0.6656733155250549\n",
      "Epoch 2 -- Batch 197/ 842, training loss 0.6610840559005737\n",
      "Epoch 2 -- Batch 198/ 842, training loss 0.6828576326370239\n",
      "Epoch 2 -- Batch 199/ 842, training loss 0.6874563694000244\n",
      "Epoch 2 -- Batch 200/ 842, training loss 0.6523038744926453\n",
      "Epoch 2 -- Batch 201/ 842, training loss 0.6836697459220886\n",
      "Epoch 2 -- Batch 202/ 842, training loss 0.6601417064666748\n",
      "Epoch 2 -- Batch 203/ 842, training loss 0.6570135354995728\n",
      "Epoch 2 -- Batch 204/ 842, training loss 0.6578530669212341\n",
      "Epoch 2 -- Batch 205/ 842, training loss 0.653067946434021\n",
      "Epoch 2 -- Batch 206/ 842, training loss 0.6705123782157898\n",
      "Epoch 2 -- Batch 207/ 842, training loss 0.696825385093689\n",
      "Epoch 2 -- Batch 208/ 842, training loss 0.6978369951248169\n",
      "Epoch 2 -- Batch 209/ 842, training loss 0.6846707463264465\n",
      "Epoch 2 -- Batch 210/ 842, training loss 0.6857086420059204\n",
      "Epoch 2 -- Batch 211/ 842, training loss 0.667457640171051\n",
      "Epoch 2 -- Batch 212/ 842, training loss 0.6642118692398071\n",
      "Epoch 2 -- Batch 213/ 842, training loss 0.6585335731506348\n",
      "Epoch 2 -- Batch 214/ 842, training loss 0.6585498452186584\n",
      "Epoch 2 -- Batch 215/ 842, training loss 0.6630135178565979\n",
      "Epoch 2 -- Batch 216/ 842, training loss 0.681503415107727\n",
      "Epoch 2 -- Batch 217/ 842, training loss 0.6444339156150818\n",
      "Epoch 2 -- Batch 218/ 842, training loss 0.6580221652984619\n",
      "Epoch 2 -- Batch 219/ 842, training loss 0.6827333569526672\n",
      "Epoch 2 -- Batch 220/ 842, training loss 0.6861991882324219\n",
      "Epoch 2 -- Batch 221/ 842, training loss 0.6598003506660461\n",
      "Epoch 2 -- Batch 222/ 842, training loss 0.680745005607605\n",
      "Epoch 2 -- Batch 223/ 842, training loss 0.6651408672332764\n",
      "Epoch 2 -- Batch 224/ 842, training loss 0.6657724380493164\n",
      "Epoch 2 -- Batch 225/ 842, training loss 0.6325383186340332\n",
      "Epoch 2 -- Batch 226/ 842, training loss 0.6732649207115173\n",
      "Epoch 2 -- Batch 227/ 842, training loss 0.6685328483581543\n",
      "Epoch 2 -- Batch 228/ 842, training loss 0.6719821095466614\n",
      "Epoch 2 -- Batch 229/ 842, training loss 0.6325656771659851\n",
      "Epoch 2 -- Batch 230/ 842, training loss 0.6521869897842407\n",
      "Epoch 2 -- Batch 231/ 842, training loss 0.6688111424446106\n",
      "Epoch 2 -- Batch 232/ 842, training loss 0.6447747945785522\n",
      "Epoch 2 -- Batch 233/ 842, training loss 0.6596566438674927\n",
      "Epoch 2 -- Batch 234/ 842, training loss 0.6765605211257935\n",
      "Epoch 2 -- Batch 235/ 842, training loss 0.6964946985244751\n",
      "Epoch 2 -- Batch 236/ 842, training loss 0.6602241396903992\n",
      "Epoch 2 -- Batch 237/ 842, training loss 0.6999926567077637\n",
      "Epoch 2 -- Batch 238/ 842, training loss 0.6915621757507324\n",
      "Epoch 2 -- Batch 239/ 842, training loss 0.6485314965248108\n",
      "Epoch 2 -- Batch 240/ 842, training loss 0.6477611064910889\n",
      "Epoch 2 -- Batch 241/ 842, training loss 0.6617883443832397\n",
      "Epoch 2 -- Batch 242/ 842, training loss 0.6263137459754944\n",
      "Epoch 2 -- Batch 243/ 842, training loss 0.6588662266731262\n",
      "Epoch 2 -- Batch 244/ 842, training loss 0.6538022756576538\n",
      "Epoch 2 -- Batch 245/ 842, training loss 0.6693912744522095\n",
      "Epoch 2 -- Batch 246/ 842, training loss 0.693493664264679\n",
      "Epoch 2 -- Batch 247/ 842, training loss 0.6398490071296692\n",
      "Epoch 2 -- Batch 248/ 842, training loss 0.6579535007476807\n",
      "Epoch 2 -- Batch 249/ 842, training loss 0.6881573796272278\n",
      "Epoch 2 -- Batch 250/ 842, training loss 0.6540939807891846\n",
      "Epoch 2 -- Batch 251/ 842, training loss 0.6747674942016602\n",
      "Epoch 2 -- Batch 252/ 842, training loss 0.64175945520401\n",
      "Epoch 2 -- Batch 253/ 842, training loss 0.6384894847869873\n",
      "Epoch 2 -- Batch 254/ 842, training loss 0.6964511871337891\n",
      "Epoch 2 -- Batch 255/ 842, training loss 0.6694021821022034\n",
      "Epoch 2 -- Batch 256/ 842, training loss 0.6628001928329468\n",
      "Epoch 2 -- Batch 257/ 842, training loss 0.6997632384300232\n",
      "Epoch 2 -- Batch 258/ 842, training loss 0.6706482172012329\n",
      "Epoch 2 -- Batch 259/ 842, training loss 0.6559439301490784\n",
      "Epoch 2 -- Batch 260/ 842, training loss 0.6525090932846069\n",
      "Epoch 2 -- Batch 261/ 842, training loss 0.6541163921356201\n",
      "Epoch 2 -- Batch 262/ 842, training loss 0.6646526455879211\n",
      "Epoch 2 -- Batch 263/ 842, training loss 0.6564635634422302\n",
      "Epoch 2 -- Batch 264/ 842, training loss 0.6791020035743713\n",
      "Epoch 2 -- Batch 265/ 842, training loss 0.624847412109375\n",
      "Epoch 2 -- Batch 266/ 842, training loss 0.6398270726203918\n",
      "Epoch 2 -- Batch 267/ 842, training loss 0.6760345101356506\n",
      "Epoch 2 -- Batch 268/ 842, training loss 0.6654760241508484\n",
      "Epoch 2 -- Batch 269/ 842, training loss 0.6462790966033936\n",
      "Epoch 2 -- Batch 270/ 842, training loss 0.6538580656051636\n",
      "Epoch 2 -- Batch 271/ 842, training loss 0.6355645656585693\n",
      "Epoch 2 -- Batch 272/ 842, training loss 0.6420526504516602\n",
      "Epoch 2 -- Batch 273/ 842, training loss 0.6451221108436584\n",
      "Epoch 2 -- Batch 274/ 842, training loss 0.6862878799438477\n",
      "Epoch 2 -- Batch 275/ 842, training loss 0.675807535648346\n",
      "Epoch 2 -- Batch 276/ 842, training loss 0.6510252356529236\n",
      "Epoch 2 -- Batch 277/ 842, training loss 0.6514864563941956\n",
      "Epoch 2 -- Batch 278/ 842, training loss 0.6742841005325317\n",
      "Epoch 2 -- Batch 279/ 842, training loss 0.6572398543357849\n",
      "Epoch 2 -- Batch 280/ 842, training loss 0.637008786201477\n",
      "Epoch 2 -- Batch 281/ 842, training loss 0.6588643789291382\n",
      "Epoch 2 -- Batch 282/ 842, training loss 0.6635513305664062\n",
      "Epoch 2 -- Batch 283/ 842, training loss 0.6809722781181335\n",
      "Epoch 2 -- Batch 284/ 842, training loss 0.6606827974319458\n",
      "Epoch 2 -- Batch 285/ 842, training loss 0.6855270862579346\n",
      "Epoch 2 -- Batch 286/ 842, training loss 0.6660073399543762\n",
      "Epoch 2 -- Batch 287/ 842, training loss 0.6279599666595459\n",
      "Epoch 2 -- Batch 288/ 842, training loss 0.6734316349029541\n",
      "Epoch 2 -- Batch 289/ 842, training loss 0.6666538715362549\n",
      "Epoch 2 -- Batch 290/ 842, training loss 0.6704680919647217\n",
      "Epoch 2 -- Batch 291/ 842, training loss 0.6622444987297058\n",
      "Epoch 2 -- Batch 292/ 842, training loss 0.6727789044380188\n",
      "Epoch 2 -- Batch 293/ 842, training loss 0.660568118095398\n",
      "Epoch 2 -- Batch 294/ 842, training loss 0.6421881318092346\n",
      "Epoch 2 -- Batch 295/ 842, training loss 0.6610738635063171\n",
      "Epoch 2 -- Batch 296/ 842, training loss 0.6426593065261841\n",
      "Epoch 2 -- Batch 297/ 842, training loss 0.648940920829773\n",
      "Epoch 2 -- Batch 298/ 842, training loss 0.6476591229438782\n",
      "Epoch 2 -- Batch 299/ 842, training loss 0.6430846452713013\n",
      "Epoch 2 -- Batch 300/ 842, training loss 0.6746965050697327\n",
      "Epoch 2 -- Batch 301/ 842, training loss 0.632205069065094\n",
      "Epoch 2 -- Batch 302/ 842, training loss 0.6650205850601196\n",
      "Epoch 2 -- Batch 303/ 842, training loss 0.6708435416221619\n",
      "Epoch 2 -- Batch 304/ 842, training loss 0.6499348282814026\n",
      "Epoch 2 -- Batch 305/ 842, training loss 0.6705489158630371\n",
      "Epoch 2 -- Batch 306/ 842, training loss 0.6490080952644348\n",
      "Epoch 2 -- Batch 307/ 842, training loss 0.659180223941803\n",
      "Epoch 2 -- Batch 308/ 842, training loss 0.6585628390312195\n",
      "Epoch 2 -- Batch 309/ 842, training loss 0.7085771560668945\n",
      "Epoch 2 -- Batch 310/ 842, training loss 0.6695988774299622\n",
      "Epoch 2 -- Batch 311/ 842, training loss 0.6513290405273438\n",
      "Epoch 2 -- Batch 312/ 842, training loss 0.6642653346061707\n",
      "Epoch 2 -- Batch 313/ 842, training loss 0.6433447003364563\n",
      "Epoch 2 -- Batch 314/ 842, training loss 0.6577135324478149\n",
      "Epoch 2 -- Batch 315/ 842, training loss 0.6755666732788086\n",
      "Epoch 2 -- Batch 316/ 842, training loss 0.6553743481636047\n",
      "Epoch 2 -- Batch 317/ 842, training loss 0.6456359624862671\n",
      "Epoch 2 -- Batch 318/ 842, training loss 0.6679831147193909\n",
      "Epoch 2 -- Batch 319/ 842, training loss 0.6367248892784119\n",
      "Epoch 2 -- Batch 320/ 842, training loss 0.6707696318626404\n",
      "Epoch 2 -- Batch 321/ 842, training loss 0.6716541051864624\n",
      "Epoch 2 -- Batch 322/ 842, training loss 0.6890610456466675\n",
      "Epoch 2 -- Batch 323/ 842, training loss 0.640761137008667\n",
      "Epoch 2 -- Batch 324/ 842, training loss 0.6582827568054199\n",
      "Epoch 2 -- Batch 325/ 842, training loss 0.638168454170227\n",
      "Epoch 2 -- Batch 326/ 842, training loss 0.6271506547927856\n",
      "Epoch 2 -- Batch 327/ 842, training loss 0.6437473893165588\n",
      "Epoch 2 -- Batch 328/ 842, training loss 0.6493489146232605\n",
      "Epoch 2 -- Batch 329/ 842, training loss 0.6309781670570374\n",
      "Epoch 2 -- Batch 330/ 842, training loss 0.6195114254951477\n",
      "Epoch 2 -- Batch 331/ 842, training loss 0.6412744522094727\n",
      "Epoch 2 -- Batch 332/ 842, training loss 0.6516762375831604\n",
      "Epoch 2 -- Batch 333/ 842, training loss 0.6287705302238464\n",
      "Epoch 2 -- Batch 334/ 842, training loss 0.6579514741897583\n",
      "Epoch 2 -- Batch 335/ 842, training loss 0.6582735776901245\n",
      "Epoch 2 -- Batch 336/ 842, training loss 0.6626603007316589\n",
      "Epoch 2 -- Batch 337/ 842, training loss 0.6401218771934509\n",
      "Epoch 2 -- Batch 338/ 842, training loss 0.6642841696739197\n",
      "Epoch 2 -- Batch 339/ 842, training loss 0.6775468587875366\n",
      "Epoch 2 -- Batch 340/ 842, training loss 0.6678925156593323\n",
      "Epoch 2 -- Batch 341/ 842, training loss 0.6541070342063904\n",
      "Epoch 2 -- Batch 342/ 842, training loss 0.6421411633491516\n",
      "Epoch 2 -- Batch 343/ 842, training loss 0.6427851915359497\n",
      "Epoch 2 -- Batch 344/ 842, training loss 0.6646765470504761\n",
      "Epoch 2 -- Batch 345/ 842, training loss 0.6325962543487549\n",
      "Epoch 2 -- Batch 346/ 842, training loss 0.6673538684844971\n",
      "Epoch 2 -- Batch 347/ 842, training loss 0.6435136795043945\n",
      "Epoch 2 -- Batch 348/ 842, training loss 0.6185950040817261\n",
      "Epoch 2 -- Batch 349/ 842, training loss 0.6352031230926514\n",
      "Epoch 2 -- Batch 350/ 842, training loss 0.6453791856765747\n",
      "Epoch 2 -- Batch 351/ 842, training loss 0.6565854549407959\n",
      "Epoch 2 -- Batch 352/ 842, training loss 0.6288079023361206\n",
      "Epoch 2 -- Batch 353/ 842, training loss 0.6630434989929199\n",
      "Epoch 2 -- Batch 354/ 842, training loss 0.6578116416931152\n",
      "Epoch 2 -- Batch 355/ 842, training loss 0.6453074216842651\n",
      "Epoch 2 -- Batch 356/ 842, training loss 0.6410396695137024\n",
      "Epoch 2 -- Batch 357/ 842, training loss 0.6528947353363037\n",
      "Epoch 2 -- Batch 358/ 842, training loss 0.6284177303314209\n",
      "Epoch 2 -- Batch 359/ 842, training loss 0.6761924624443054\n",
      "Epoch 2 -- Batch 360/ 842, training loss 0.6553372144699097\n",
      "Epoch 2 -- Batch 361/ 842, training loss 0.6325312256813049\n",
      "Epoch 2 -- Batch 362/ 842, training loss 0.6423202157020569\n",
      "Epoch 2 -- Batch 363/ 842, training loss 0.6361802220344543\n",
      "Epoch 2 -- Batch 364/ 842, training loss 0.6293497681617737\n",
      "Epoch 2 -- Batch 365/ 842, training loss 0.6448603868484497\n",
      "Epoch 2 -- Batch 366/ 842, training loss 0.6514162421226501\n",
      "Epoch 2 -- Batch 367/ 842, training loss 0.6352128386497498\n",
      "Epoch 2 -- Batch 368/ 842, training loss 0.6469805836677551\n",
      "Epoch 2 -- Batch 369/ 842, training loss 0.6719337105751038\n",
      "Epoch 2 -- Batch 370/ 842, training loss 0.6548805236816406\n",
      "Epoch 2 -- Batch 371/ 842, training loss 0.6398535370826721\n",
      "Epoch 2 -- Batch 372/ 842, training loss 0.5995187163352966\n",
      "Epoch 2 -- Batch 373/ 842, training loss 0.632845938205719\n",
      "Epoch 2 -- Batch 374/ 842, training loss 0.6283591985702515\n",
      "Epoch 2 -- Batch 375/ 842, training loss 0.6521888375282288\n",
      "Epoch 2 -- Batch 376/ 842, training loss 0.6338144540786743\n",
      "Epoch 2 -- Batch 377/ 842, training loss 0.6620054244995117\n",
      "Epoch 2 -- Batch 378/ 842, training loss 0.6545464992523193\n",
      "Epoch 2 -- Batch 379/ 842, training loss 0.6343583464622498\n",
      "Epoch 2 -- Batch 380/ 842, training loss 0.6812280416488647\n",
      "Epoch 2 -- Batch 381/ 842, training loss 0.627577543258667\n",
      "Epoch 2 -- Batch 382/ 842, training loss 0.6555021405220032\n",
      "Epoch 2 -- Batch 383/ 842, training loss 0.6233345866203308\n",
      "Epoch 2 -- Batch 384/ 842, training loss 0.6411625146865845\n",
      "Epoch 2 -- Batch 385/ 842, training loss 0.6482747197151184\n",
      "Epoch 2 -- Batch 386/ 842, training loss 0.6375010013580322\n",
      "Epoch 2 -- Batch 387/ 842, training loss 0.6230171918869019\n",
      "Epoch 2 -- Batch 388/ 842, training loss 0.6657992601394653\n",
      "Epoch 2 -- Batch 389/ 842, training loss 0.6443665027618408\n",
      "Epoch 2 -- Batch 390/ 842, training loss 0.633263349533081\n",
      "Epoch 2 -- Batch 391/ 842, training loss 0.6218785047531128\n",
      "Epoch 2 -- Batch 392/ 842, training loss 0.6572492718696594\n",
      "Epoch 2 -- Batch 393/ 842, training loss 0.6579384803771973\n",
      "Epoch 2 -- Batch 394/ 842, training loss 0.638885498046875\n",
      "Epoch 2 -- Batch 395/ 842, training loss 0.6268255114555359\n",
      "Epoch 2 -- Batch 396/ 842, training loss 0.6282876133918762\n",
      "Epoch 2 -- Batch 397/ 842, training loss 0.6067562103271484\n",
      "Epoch 2 -- Batch 398/ 842, training loss 0.6617336869239807\n",
      "Epoch 2 -- Batch 399/ 842, training loss 0.6130459904670715\n",
      "Epoch 2 -- Batch 400/ 842, training loss 0.6546046137809753\n",
      "Epoch 2 -- Batch 401/ 842, training loss 0.6229271292686462\n",
      "Epoch 2 -- Batch 402/ 842, training loss 0.6604657769203186\n",
      "Epoch 2 -- Batch 403/ 842, training loss 0.6548295617103577\n",
      "Epoch 2 -- Batch 404/ 842, training loss 0.6385363936424255\n",
      "Epoch 2 -- Batch 405/ 842, training loss 0.6541268229484558\n",
      "Epoch 2 -- Batch 406/ 842, training loss 0.6188837289810181\n",
      "Epoch 2 -- Batch 407/ 842, training loss 0.6385096311569214\n",
      "Epoch 2 -- Batch 408/ 842, training loss 0.6203629374504089\n",
      "Epoch 2 -- Batch 409/ 842, training loss 0.6425920128822327\n",
      "Epoch 2 -- Batch 410/ 842, training loss 0.660414457321167\n",
      "Epoch 2 -- Batch 411/ 842, training loss 0.6435312032699585\n",
      "Epoch 2 -- Batch 412/ 842, training loss 0.6661331653594971\n",
      "Epoch 2 -- Batch 413/ 842, training loss 0.639476478099823\n",
      "Epoch 2 -- Batch 414/ 842, training loss 0.662356436252594\n",
      "Epoch 2 -- Batch 415/ 842, training loss 0.6439754366874695\n",
      "Epoch 2 -- Batch 416/ 842, training loss 0.641713559627533\n",
      "Epoch 2 -- Batch 417/ 842, training loss 0.6519713401794434\n",
      "Epoch 2 -- Batch 418/ 842, training loss 0.676169216632843\n",
      "Epoch 2 -- Batch 419/ 842, training loss 0.6413646936416626\n",
      "Epoch 2 -- Batch 420/ 842, training loss 0.6345271468162537\n",
      "Epoch 2 -- Batch 421/ 842, training loss 0.6618589162826538\n",
      "Epoch 2 -- Batch 422/ 842, training loss 0.6721978783607483\n",
      "Epoch 2 -- Batch 423/ 842, training loss 0.6415836811065674\n",
      "Epoch 2 -- Batch 424/ 842, training loss 0.6447931528091431\n",
      "Epoch 2 -- Batch 425/ 842, training loss 0.6310957074165344\n",
      "Epoch 2 -- Batch 426/ 842, training loss 0.6486754417419434\n",
      "Epoch 2 -- Batch 427/ 842, training loss 0.6776862144470215\n",
      "Epoch 2 -- Batch 428/ 842, training loss 0.6316320896148682\n",
      "Epoch 2 -- Batch 429/ 842, training loss 0.6552650332450867\n",
      "Epoch 2 -- Batch 430/ 842, training loss 0.6290048956871033\n",
      "Epoch 2 -- Batch 431/ 842, training loss 0.6439287066459656\n",
      "Epoch 2 -- Batch 432/ 842, training loss 0.6468423008918762\n",
      "Epoch 2 -- Batch 433/ 842, training loss 0.655263364315033\n",
      "Epoch 2 -- Batch 434/ 842, training loss 0.6326438188552856\n",
      "Epoch 2 -- Batch 435/ 842, training loss 0.6617687940597534\n",
      "Epoch 2 -- Batch 436/ 842, training loss 0.6748063564300537\n",
      "Epoch 2 -- Batch 437/ 842, training loss 0.6254169940948486\n",
      "Epoch 2 -- Batch 438/ 842, training loss 0.6509116291999817\n",
      "Epoch 2 -- Batch 439/ 842, training loss 0.6288637518882751\n",
      "Epoch 2 -- Batch 440/ 842, training loss 0.6656315922737122\n",
      "Epoch 2 -- Batch 441/ 842, training loss 0.6559075117111206\n",
      "Epoch 2 -- Batch 442/ 842, training loss 0.6591143012046814\n",
      "Epoch 2 -- Batch 443/ 842, training loss 0.6611346006393433\n",
      "Epoch 2 -- Batch 444/ 842, training loss 0.6347237825393677\n",
      "Epoch 2 -- Batch 445/ 842, training loss 0.6750649213790894\n",
      "Epoch 2 -- Batch 446/ 842, training loss 0.6592050790786743\n",
      "Epoch 2 -- Batch 447/ 842, training loss 0.6578434109687805\n",
      "Epoch 2 -- Batch 448/ 842, training loss 0.602677047252655\n",
      "Epoch 2 -- Batch 449/ 842, training loss 0.6312470436096191\n",
      "Epoch 2 -- Batch 450/ 842, training loss 0.6494559049606323\n",
      "Epoch 2 -- Batch 451/ 842, training loss 0.6278074979782104\n",
      "Epoch 2 -- Batch 452/ 842, training loss 0.6264700889587402\n",
      "Epoch 2 -- Batch 453/ 842, training loss 0.6171804070472717\n",
      "Epoch 2 -- Batch 454/ 842, training loss 0.6521326303482056\n",
      "Epoch 2 -- Batch 455/ 842, training loss 0.6094732284545898\n",
      "Epoch 2 -- Batch 456/ 842, training loss 0.6453377604484558\n",
      "Epoch 2 -- Batch 457/ 842, training loss 0.6457142233848572\n",
      "Epoch 2 -- Batch 458/ 842, training loss 0.6424365043640137\n",
      "Epoch 2 -- Batch 459/ 842, training loss 0.627541720867157\n",
      "Epoch 2 -- Batch 460/ 842, training loss 0.641406774520874\n",
      "Epoch 2 -- Batch 461/ 842, training loss 0.661117672920227\n",
      "Epoch 2 -- Batch 462/ 842, training loss 0.6243716478347778\n",
      "Epoch 2 -- Batch 463/ 842, training loss 0.6704422831535339\n",
      "Epoch 2 -- Batch 464/ 842, training loss 0.6406723260879517\n",
      "Epoch 2 -- Batch 465/ 842, training loss 0.6495180726051331\n",
      "Epoch 2 -- Batch 466/ 842, training loss 0.6633086204528809\n",
      "Epoch 2 -- Batch 467/ 842, training loss 0.6417256593704224\n",
      "Epoch 2 -- Batch 468/ 842, training loss 0.616969108581543\n",
      "Epoch 2 -- Batch 469/ 842, training loss 0.6287476420402527\n",
      "Epoch 2 -- Batch 470/ 842, training loss 0.6550032496452332\n",
      "Epoch 2 -- Batch 471/ 842, training loss 0.6702821850776672\n",
      "Epoch 2 -- Batch 472/ 842, training loss 0.6571237444877625\n",
      "Epoch 2 -- Batch 473/ 842, training loss 0.628653347492218\n",
      "Epoch 2 -- Batch 474/ 842, training loss 0.628380537033081\n",
      "Epoch 2 -- Batch 475/ 842, training loss 0.643433690071106\n",
      "Epoch 2 -- Batch 476/ 842, training loss 0.6636815667152405\n",
      "Epoch 2 -- Batch 477/ 842, training loss 0.653840184211731\n",
      "Epoch 2 -- Batch 478/ 842, training loss 0.646287202835083\n",
      "Epoch 2 -- Batch 479/ 842, training loss 0.6116942763328552\n",
      "Epoch 2 -- Batch 480/ 842, training loss 0.6388356685638428\n",
      "Epoch 2 -- Batch 481/ 842, training loss 0.6269257664680481\n",
      "Epoch 2 -- Batch 482/ 842, training loss 0.6670957803726196\n",
      "Epoch 2 -- Batch 483/ 842, training loss 0.6293084025382996\n",
      "Epoch 2 -- Batch 484/ 842, training loss 0.6316110491752625\n",
      "Epoch 2 -- Batch 485/ 842, training loss 0.6376385688781738\n",
      "Epoch 2 -- Batch 486/ 842, training loss 0.6324935555458069\n",
      "Epoch 2 -- Batch 487/ 842, training loss 0.5957257151603699\n",
      "Epoch 2 -- Batch 488/ 842, training loss 0.6213549971580505\n",
      "Epoch 2 -- Batch 489/ 842, training loss 0.6227930784225464\n",
      "Epoch 2 -- Batch 490/ 842, training loss 0.6550018191337585\n",
      "Epoch 2 -- Batch 491/ 842, training loss 0.6406877040863037\n",
      "Epoch 2 -- Batch 492/ 842, training loss 0.623308539390564\n",
      "Epoch 2 -- Batch 493/ 842, training loss 0.6618466973304749\n",
      "Epoch 2 -- Batch 494/ 842, training loss 0.6196898221969604\n",
      "Epoch 2 -- Batch 495/ 842, training loss 0.6397406458854675\n",
      "Epoch 2 -- Batch 496/ 842, training loss 0.6393616199493408\n",
      "Epoch 2 -- Batch 497/ 842, training loss 0.6501536965370178\n",
      "Epoch 2 -- Batch 498/ 842, training loss 0.6306026577949524\n",
      "Epoch 2 -- Batch 499/ 842, training loss 0.6022564172744751\n",
      "Epoch 2 -- Batch 500/ 842, training loss 0.6628509759902954\n",
      "Epoch 2 -- Batch 501/ 842, training loss 0.6394079923629761\n",
      "Epoch 2 -- Batch 502/ 842, training loss 0.6485377550125122\n",
      "Epoch 2 -- Batch 503/ 842, training loss 0.6445724964141846\n",
      "Epoch 2 -- Batch 504/ 842, training loss 0.6180853843688965\n",
      "Epoch 2 -- Batch 505/ 842, training loss 0.6363043189048767\n",
      "Epoch 2 -- Batch 506/ 842, training loss 0.6170982122421265\n",
      "Epoch 2 -- Batch 507/ 842, training loss 0.6107966899871826\n",
      "Epoch 2 -- Batch 508/ 842, training loss 0.6283169984817505\n",
      "Epoch 2 -- Batch 509/ 842, training loss 0.6065770983695984\n",
      "Epoch 2 -- Batch 510/ 842, training loss 0.6303349137306213\n",
      "Epoch 2 -- Batch 511/ 842, training loss 0.6279527544975281\n",
      "Epoch 2 -- Batch 512/ 842, training loss 0.6286429166793823\n",
      "Epoch 2 -- Batch 513/ 842, training loss 0.6484193801879883\n",
      "Epoch 2 -- Batch 514/ 842, training loss 0.6163821816444397\n",
      "Epoch 2 -- Batch 515/ 842, training loss 0.6258020997047424\n",
      "Epoch 2 -- Batch 516/ 842, training loss 0.6358678340911865\n",
      "Epoch 2 -- Batch 517/ 842, training loss 0.6293366551399231\n",
      "Epoch 2 -- Batch 518/ 842, training loss 0.6655482053756714\n",
      "Epoch 2 -- Batch 519/ 842, training loss 0.6385580897331238\n",
      "Epoch 2 -- Batch 520/ 842, training loss 0.6045624017715454\n",
      "Epoch 2 -- Batch 521/ 842, training loss 0.6282938122749329\n",
      "Epoch 2 -- Batch 522/ 842, training loss 0.6216939687728882\n",
      "Epoch 2 -- Batch 523/ 842, training loss 0.6171311736106873\n",
      "Epoch 2 -- Batch 524/ 842, training loss 0.6202983856201172\n",
      "Epoch 2 -- Batch 525/ 842, training loss 0.628136157989502\n",
      "Epoch 2 -- Batch 526/ 842, training loss 0.6652128100395203\n",
      "Epoch 2 -- Batch 527/ 842, training loss 0.6339551210403442\n",
      "Epoch 2 -- Batch 528/ 842, training loss 0.6219903230667114\n",
      "Epoch 2 -- Batch 529/ 842, training loss 0.6620383858680725\n",
      "Epoch 2 -- Batch 530/ 842, training loss 0.6340116858482361\n",
      "Epoch 2 -- Batch 531/ 842, training loss 0.6225890517234802\n",
      "Epoch 2 -- Batch 532/ 842, training loss 0.6244596838951111\n",
      "Epoch 2 -- Batch 533/ 842, training loss 0.62432461977005\n",
      "Epoch 2 -- Batch 534/ 842, training loss 0.5798372626304626\n",
      "Epoch 2 -- Batch 535/ 842, training loss 0.6311726570129395\n",
      "Epoch 2 -- Batch 536/ 842, training loss 0.6464741826057434\n",
      "Epoch 2 -- Batch 537/ 842, training loss 0.6227750778198242\n",
      "Epoch 2 -- Batch 538/ 842, training loss 0.6243160367012024\n",
      "Epoch 2 -- Batch 539/ 842, training loss 0.6225967407226562\n",
      "Epoch 2 -- Batch 540/ 842, training loss 0.633154034614563\n",
      "Epoch 2 -- Batch 541/ 842, training loss 0.6176908016204834\n",
      "Epoch 2 -- Batch 542/ 842, training loss 0.6305590867996216\n",
      "Epoch 2 -- Batch 543/ 842, training loss 0.6023673415184021\n",
      "Epoch 2 -- Batch 544/ 842, training loss 0.6192048192024231\n",
      "Epoch 2 -- Batch 545/ 842, training loss 0.6350405812263489\n",
      "Epoch 2 -- Batch 546/ 842, training loss 0.6125540137290955\n",
      "Epoch 2 -- Batch 547/ 842, training loss 0.6444870829582214\n",
      "Epoch 2 -- Batch 548/ 842, training loss 0.6410939693450928\n",
      "Epoch 2 -- Batch 549/ 842, training loss 0.6238977909088135\n",
      "Epoch 2 -- Batch 550/ 842, training loss 0.6171686053276062\n",
      "Epoch 2 -- Batch 551/ 842, training loss 0.6304473280906677\n",
      "Epoch 2 -- Batch 552/ 842, training loss 0.6678687930107117\n",
      "Epoch 2 -- Batch 553/ 842, training loss 0.6165193319320679\n",
      "Epoch 2 -- Batch 554/ 842, training loss 0.6265615820884705\n",
      "Epoch 2 -- Batch 555/ 842, training loss 0.6335970759391785\n",
      "Epoch 2 -- Batch 556/ 842, training loss 0.6472763419151306\n",
      "Epoch 2 -- Batch 557/ 842, training loss 0.631817102432251\n",
      "Epoch 2 -- Batch 558/ 842, training loss 0.6380428075790405\n",
      "Epoch 2 -- Batch 559/ 842, training loss 0.6410882472991943\n",
      "Epoch 2 -- Batch 560/ 842, training loss 0.6290701627731323\n",
      "Epoch 2 -- Batch 561/ 842, training loss 0.6487037539482117\n",
      "Epoch 2 -- Batch 562/ 842, training loss 0.6146222352981567\n",
      "Epoch 2 -- Batch 563/ 842, training loss 0.6244884133338928\n",
      "Epoch 2 -- Batch 564/ 842, training loss 0.6463951468467712\n",
      "Epoch 2 -- Batch 565/ 842, training loss 0.623735249042511\n",
      "Epoch 2 -- Batch 566/ 842, training loss 0.6190440058708191\n",
      "Epoch 2 -- Batch 567/ 842, training loss 0.6332815289497375\n",
      "Epoch 2 -- Batch 568/ 842, training loss 0.6232028007507324\n",
      "Epoch 2 -- Batch 569/ 842, training loss 0.6187360882759094\n",
      "Epoch 2 -- Batch 570/ 842, training loss 0.6078628301620483\n",
      "Epoch 2 -- Batch 571/ 842, training loss 0.6222628951072693\n",
      "Epoch 2 -- Batch 572/ 842, training loss 0.6398457288742065\n",
      "Epoch 2 -- Batch 573/ 842, training loss 0.6151196956634521\n",
      "Epoch 2 -- Batch 574/ 842, training loss 0.6557273268699646\n",
      "Epoch 2 -- Batch 575/ 842, training loss 0.6178816556930542\n",
      "Epoch 2 -- Batch 576/ 842, training loss 0.6236668825149536\n",
      "Epoch 2 -- Batch 577/ 842, training loss 0.62479567527771\n",
      "Epoch 2 -- Batch 578/ 842, training loss 0.6296841502189636\n",
      "Epoch 2 -- Batch 579/ 842, training loss 0.6370720863342285\n",
      "Epoch 2 -- Batch 580/ 842, training loss 0.6089022159576416\n",
      "Epoch 2 -- Batch 581/ 842, training loss 0.6125228404998779\n",
      "Epoch 2 -- Batch 582/ 842, training loss 0.6207907795906067\n",
      "Epoch 2 -- Batch 583/ 842, training loss 0.6292012929916382\n",
      "Epoch 2 -- Batch 584/ 842, training loss 0.6351299285888672\n",
      "Epoch 2 -- Batch 585/ 842, training loss 0.6251580119132996\n",
      "Epoch 2 -- Batch 586/ 842, training loss 0.6220419406890869\n",
      "Epoch 2 -- Batch 587/ 842, training loss 0.6268064975738525\n",
      "Epoch 2 -- Batch 588/ 842, training loss 0.6498894691467285\n",
      "Epoch 2 -- Batch 589/ 842, training loss 0.6531172394752502\n",
      "Epoch 2 -- Batch 590/ 842, training loss 0.6099866628646851\n",
      "Epoch 2 -- Batch 591/ 842, training loss 0.6142353415489197\n",
      "Epoch 2 -- Batch 592/ 842, training loss 0.6300782561302185\n",
      "Epoch 2 -- Batch 593/ 842, training loss 0.6079146862030029\n",
      "Epoch 2 -- Batch 594/ 842, training loss 0.6309789419174194\n",
      "Epoch 2 -- Batch 595/ 842, training loss 0.6058146953582764\n",
      "Epoch 2 -- Batch 596/ 842, training loss 0.6430172324180603\n",
      "Epoch 2 -- Batch 597/ 842, training loss 0.6064887642860413\n",
      "Epoch 2 -- Batch 598/ 842, training loss 0.6390306353569031\n",
      "Epoch 2 -- Batch 599/ 842, training loss 0.5786228775978088\n",
      "Epoch 2 -- Batch 600/ 842, training loss 0.6472339630126953\n",
      "Epoch 2 -- Batch 601/ 842, training loss 0.6389383673667908\n",
      "Epoch 2 -- Batch 602/ 842, training loss 0.6093400716781616\n",
      "Epoch 2 -- Batch 603/ 842, training loss 0.6432788968086243\n",
      "Epoch 2 -- Batch 604/ 842, training loss 0.6471690535545349\n",
      "Epoch 2 -- Batch 605/ 842, training loss 0.640359103679657\n",
      "Epoch 2 -- Batch 606/ 842, training loss 0.6311942934989929\n",
      "Epoch 2 -- Batch 607/ 842, training loss 0.6103608012199402\n",
      "Epoch 2 -- Batch 608/ 842, training loss 0.6457810997962952\n",
      "Epoch 2 -- Batch 609/ 842, training loss 0.6418255567550659\n",
      "Epoch 2 -- Batch 610/ 842, training loss 0.6339301466941833\n",
      "Epoch 2 -- Batch 611/ 842, training loss 0.6323893666267395\n",
      "Epoch 2 -- Batch 612/ 842, training loss 0.622935950756073\n",
      "Epoch 2 -- Batch 613/ 842, training loss 0.6335931420326233\n",
      "Epoch 2 -- Batch 614/ 842, training loss 0.6234802007675171\n",
      "Epoch 2 -- Batch 615/ 842, training loss 0.6489802598953247\n",
      "Epoch 2 -- Batch 616/ 842, training loss 0.6570032238960266\n",
      "Epoch 2 -- Batch 617/ 842, training loss 0.6385133862495422\n",
      "Epoch 2 -- Batch 618/ 842, training loss 0.627216100692749\n",
      "Epoch 2 -- Batch 619/ 842, training loss 0.6257635354995728\n",
      "Epoch 2 -- Batch 620/ 842, training loss 0.594752848148346\n",
      "Epoch 2 -- Batch 621/ 842, training loss 0.594498336315155\n",
      "Epoch 2 -- Batch 622/ 842, training loss 0.6031779050827026\n",
      "Epoch 2 -- Batch 623/ 842, training loss 0.610154926776886\n",
      "Epoch 2 -- Batch 624/ 842, training loss 0.5981360077857971\n",
      "Epoch 2 -- Batch 625/ 842, training loss 0.6364337801933289\n",
      "Epoch 2 -- Batch 626/ 842, training loss 0.6334095597267151\n",
      "Epoch 2 -- Batch 627/ 842, training loss 0.6309673190116882\n",
      "Epoch 2 -- Batch 628/ 842, training loss 0.631324827671051\n",
      "Epoch 2 -- Batch 629/ 842, training loss 0.6351461410522461\n",
      "Epoch 2 -- Batch 630/ 842, training loss 0.6224510073661804\n",
      "Epoch 2 -- Batch 631/ 842, training loss 0.5824185609817505\n",
      "Epoch 2 -- Batch 632/ 842, training loss 0.6234294176101685\n",
      "Epoch 2 -- Batch 633/ 842, training loss 0.6170719861984253\n",
      "Epoch 2 -- Batch 634/ 842, training loss 0.6132891178131104\n",
      "Epoch 2 -- Batch 635/ 842, training loss 0.6435309052467346\n",
      "Epoch 2 -- Batch 636/ 842, training loss 0.618066132068634\n",
      "Epoch 2 -- Batch 637/ 842, training loss 0.6555585265159607\n",
      "Epoch 2 -- Batch 638/ 842, training loss 0.6097292900085449\n",
      "Epoch 2 -- Batch 639/ 842, training loss 0.6225724816322327\n",
      "Epoch 2 -- Batch 640/ 842, training loss 0.6272155046463013\n",
      "Epoch 2 -- Batch 641/ 842, training loss 0.6204718947410583\n",
      "Epoch 2 -- Batch 642/ 842, training loss 0.5804511904716492\n",
      "Epoch 2 -- Batch 643/ 842, training loss 0.6161277890205383\n",
      "Epoch 2 -- Batch 644/ 842, training loss 0.6324096322059631\n",
      "Epoch 2 -- Batch 645/ 842, training loss 0.6050230860710144\n",
      "Epoch 2 -- Batch 646/ 842, training loss 0.6019161343574524\n",
      "Epoch 2 -- Batch 647/ 842, training loss 0.6179022789001465\n",
      "Epoch 2 -- Batch 648/ 842, training loss 0.6131325960159302\n",
      "Epoch 2 -- Batch 649/ 842, training loss 0.6041966676712036\n",
      "Epoch 2 -- Batch 650/ 842, training loss 0.6493709087371826\n",
      "Epoch 2 -- Batch 651/ 842, training loss 0.5991854667663574\n",
      "Epoch 2 -- Batch 652/ 842, training loss 0.6083536744117737\n",
      "Epoch 2 -- Batch 653/ 842, training loss 0.6132239103317261\n",
      "Epoch 2 -- Batch 654/ 842, training loss 0.624534547328949\n",
      "Epoch 2 -- Batch 655/ 842, training loss 0.6080731153488159\n",
      "Epoch 2 -- Batch 656/ 842, training loss 0.6293884515762329\n",
      "Epoch 2 -- Batch 657/ 842, training loss 0.6179786324501038\n",
      "Epoch 2 -- Batch 658/ 842, training loss 0.6087164282798767\n",
      "Epoch 2 -- Batch 659/ 842, training loss 0.6356005072593689\n",
      "Epoch 2 -- Batch 660/ 842, training loss 0.6171683669090271\n",
      "Epoch 2 -- Batch 661/ 842, training loss 0.6277377605438232\n",
      "Epoch 2 -- Batch 662/ 842, training loss 0.6190111041069031\n",
      "Epoch 2 -- Batch 663/ 842, training loss 0.607545793056488\n",
      "Epoch 2 -- Batch 664/ 842, training loss 0.6370691657066345\n",
      "Epoch 2 -- Batch 665/ 842, training loss 0.6457477807998657\n",
      "Epoch 2 -- Batch 666/ 842, training loss 0.6387848258018494\n",
      "Epoch 2 -- Batch 667/ 842, training loss 0.5947500467300415\n",
      "Epoch 2 -- Batch 668/ 842, training loss 0.610426664352417\n",
      "Epoch 2 -- Batch 669/ 842, training loss 0.6384028196334839\n",
      "Epoch 2 -- Batch 670/ 842, training loss 0.6317741274833679\n",
      "Epoch 2 -- Batch 671/ 842, training loss 0.6257623434066772\n",
      "Epoch 2 -- Batch 672/ 842, training loss 0.5945569276809692\n",
      "Epoch 2 -- Batch 673/ 842, training loss 0.6263896822929382\n",
      "Epoch 2 -- Batch 674/ 842, training loss 0.6288704872131348\n",
      "Epoch 2 -- Batch 675/ 842, training loss 0.5904558300971985\n",
      "Epoch 2 -- Batch 676/ 842, training loss 0.6524153351783752\n",
      "Epoch 2 -- Batch 677/ 842, training loss 0.5995725393295288\n",
      "Epoch 2 -- Batch 678/ 842, training loss 0.599237322807312\n",
      "Epoch 2 -- Batch 679/ 842, training loss 0.6141584515571594\n",
      "Epoch 2 -- Batch 680/ 842, training loss 0.5992274880409241\n",
      "Epoch 2 -- Batch 681/ 842, training loss 0.6123473048210144\n",
      "Epoch 2 -- Batch 682/ 842, training loss 0.6301447153091431\n",
      "Epoch 2 -- Batch 683/ 842, training loss 0.6021959781646729\n",
      "Epoch 2 -- Batch 684/ 842, training loss 0.6374661326408386\n",
      "Epoch 2 -- Batch 685/ 842, training loss 0.6427358984947205\n",
      "Epoch 2 -- Batch 686/ 842, training loss 0.6046788692474365\n",
      "Epoch 2 -- Batch 687/ 842, training loss 0.6119735240936279\n",
      "Epoch 2 -- Batch 688/ 842, training loss 0.6173356771469116\n",
      "Epoch 2 -- Batch 689/ 842, training loss 0.6392960548400879\n",
      "Epoch 2 -- Batch 690/ 842, training loss 0.5993553996086121\n",
      "Epoch 2 -- Batch 691/ 842, training loss 0.6092596650123596\n",
      "Epoch 2 -- Batch 692/ 842, training loss 0.6260541081428528\n",
      "Epoch 2 -- Batch 693/ 842, training loss 0.6053248643875122\n",
      "Epoch 2 -- Batch 694/ 842, training loss 0.5986630320549011\n",
      "Epoch 2 -- Batch 695/ 842, training loss 0.6146149635314941\n",
      "Epoch 2 -- Batch 696/ 842, training loss 0.6101604700088501\n",
      "Epoch 2 -- Batch 697/ 842, training loss 0.6080044507980347\n",
      "Epoch 2 -- Batch 698/ 842, training loss 0.6086634993553162\n",
      "Epoch 2 -- Batch 699/ 842, training loss 0.6446560621261597\n",
      "Epoch 2 -- Batch 700/ 842, training loss 0.6274457573890686\n",
      "Epoch 2 -- Batch 701/ 842, training loss 0.6126708984375\n",
      "Epoch 2 -- Batch 702/ 842, training loss 0.6515827178955078\n",
      "Epoch 2 -- Batch 703/ 842, training loss 0.6244593858718872\n",
      "Epoch 2 -- Batch 704/ 842, training loss 0.6176273226737976\n",
      "Epoch 2 -- Batch 705/ 842, training loss 0.6091699004173279\n",
      "Epoch 2 -- Batch 706/ 842, training loss 0.6149852275848389\n",
      "Epoch 2 -- Batch 707/ 842, training loss 0.6269145607948303\n",
      "Epoch 2 -- Batch 708/ 842, training loss 0.614891529083252\n",
      "Epoch 2 -- Batch 709/ 842, training loss 0.6187934279441833\n",
      "Epoch 2 -- Batch 710/ 842, training loss 0.6160488128662109\n",
      "Epoch 2 -- Batch 711/ 842, training loss 0.6011147499084473\n",
      "Epoch 2 -- Batch 712/ 842, training loss 0.6002145409584045\n",
      "Epoch 2 -- Batch 713/ 842, training loss 0.6356013417243958\n",
      "Epoch 2 -- Batch 714/ 842, training loss 0.6312010288238525\n",
      "Epoch 2 -- Batch 715/ 842, training loss 0.6365478038787842\n",
      "Epoch 2 -- Batch 716/ 842, training loss 0.6160213947296143\n",
      "Epoch 2 -- Batch 717/ 842, training loss 0.6319913268089294\n",
      "Epoch 2 -- Batch 718/ 842, training loss 0.6073190569877625\n",
      "Epoch 2 -- Batch 719/ 842, training loss 0.6078308820724487\n",
      "Epoch 2 -- Batch 720/ 842, training loss 0.6070477962493896\n",
      "Epoch 2 -- Batch 721/ 842, training loss 0.614639937877655\n",
      "Epoch 2 -- Batch 722/ 842, training loss 0.6380470991134644\n",
      "Epoch 2 -- Batch 723/ 842, training loss 0.6384940147399902\n",
      "Epoch 2 -- Batch 724/ 842, training loss 0.6278243660926819\n",
      "Epoch 2 -- Batch 725/ 842, training loss 0.6204606890678406\n",
      "Epoch 2 -- Batch 726/ 842, training loss 0.5861781239509583\n",
      "Epoch 2 -- Batch 727/ 842, training loss 0.6021125912666321\n",
      "Epoch 2 -- Batch 728/ 842, training loss 0.6055757403373718\n",
      "Epoch 2 -- Batch 729/ 842, training loss 0.618022620677948\n",
      "Epoch 2 -- Batch 730/ 842, training loss 0.5752847790718079\n",
      "Epoch 2 -- Batch 731/ 842, training loss 0.597166121006012\n",
      "Epoch 2 -- Batch 732/ 842, training loss 0.6186891198158264\n",
      "Epoch 2 -- Batch 733/ 842, training loss 0.5999525785446167\n",
      "Epoch 2 -- Batch 734/ 842, training loss 0.6083470582962036\n",
      "Epoch 2 -- Batch 735/ 842, training loss 0.5930588841438293\n",
      "Epoch 2 -- Batch 736/ 842, training loss 0.6267312169075012\n",
      "Epoch 2 -- Batch 737/ 842, training loss 0.6103578209877014\n",
      "Epoch 2 -- Batch 738/ 842, training loss 0.6004108786582947\n",
      "Epoch 2 -- Batch 739/ 842, training loss 0.6174412369728088\n",
      "Epoch 2 -- Batch 740/ 842, training loss 0.6095682978630066\n",
      "Epoch 2 -- Batch 741/ 842, training loss 0.6243627071380615\n",
      "Epoch 2 -- Batch 742/ 842, training loss 0.6100019812583923\n",
      "Epoch 2 -- Batch 743/ 842, training loss 0.5914187431335449\n",
      "Epoch 2 -- Batch 744/ 842, training loss 0.6154754757881165\n",
      "Epoch 2 -- Batch 745/ 842, training loss 0.6180617213249207\n",
      "Epoch 2 -- Batch 746/ 842, training loss 0.6130012273788452\n",
      "Epoch 2 -- Batch 747/ 842, training loss 0.6138640642166138\n",
      "Epoch 2 -- Batch 748/ 842, training loss 0.6019620299339294\n",
      "Epoch 2 -- Batch 749/ 842, training loss 0.6027236580848694\n",
      "Epoch 2 -- Batch 750/ 842, training loss 0.6350087523460388\n",
      "Epoch 2 -- Batch 751/ 842, training loss 0.600159764289856\n",
      "Epoch 2 -- Batch 752/ 842, training loss 0.6220769882202148\n",
      "Epoch 2 -- Batch 753/ 842, training loss 0.6111799478530884\n",
      "Epoch 2 -- Batch 754/ 842, training loss 0.6184443831443787\n",
      "Epoch 2 -- Batch 755/ 842, training loss 0.6135138273239136\n",
      "Epoch 2 -- Batch 756/ 842, training loss 0.6168439388275146\n",
      "Epoch 2 -- Batch 757/ 842, training loss 0.6174585819244385\n",
      "Epoch 2 -- Batch 758/ 842, training loss 0.6217798590660095\n",
      "Epoch 2 -- Batch 759/ 842, training loss 0.5965835452079773\n",
      "Epoch 2 -- Batch 760/ 842, training loss 0.6103540658950806\n",
      "Epoch 2 -- Batch 761/ 842, training loss 0.6116778254508972\n",
      "Epoch 2 -- Batch 762/ 842, training loss 0.6389139890670776\n",
      "Epoch 2 -- Batch 763/ 842, training loss 0.6216797232627869\n",
      "Epoch 2 -- Batch 764/ 842, training loss 0.5972044467926025\n",
      "Epoch 2 -- Batch 765/ 842, training loss 0.6225693821907043\n",
      "Epoch 2 -- Batch 766/ 842, training loss 0.5943509340286255\n",
      "Epoch 2 -- Batch 767/ 842, training loss 0.6078251004219055\n",
      "Epoch 2 -- Batch 768/ 842, training loss 0.6240614056587219\n",
      "Epoch 2 -- Batch 769/ 842, training loss 0.5875759124755859\n",
      "Epoch 2 -- Batch 770/ 842, training loss 0.6022229194641113\n",
      "Epoch 2 -- Batch 771/ 842, training loss 0.6143761873245239\n",
      "Epoch 2 -- Batch 772/ 842, training loss 0.6003231406211853\n",
      "Epoch 2 -- Batch 773/ 842, training loss 0.6192845106124878\n",
      "Epoch 2 -- Batch 774/ 842, training loss 0.6090725660324097\n",
      "Epoch 2 -- Batch 775/ 842, training loss 0.5829948782920837\n",
      "Epoch 2 -- Batch 776/ 842, training loss 0.6025223731994629\n",
      "Epoch 2 -- Batch 777/ 842, training loss 0.5864855647087097\n",
      "Epoch 2 -- Batch 778/ 842, training loss 0.6053807735443115\n",
      "Epoch 2 -- Batch 779/ 842, training loss 0.6019139885902405\n",
      "Epoch 2 -- Batch 780/ 842, training loss 0.5951997637748718\n",
      "Epoch 2 -- Batch 781/ 842, training loss 0.6063687801361084\n",
      "Epoch 2 -- Batch 782/ 842, training loss 0.602024495601654\n",
      "Epoch 2 -- Batch 783/ 842, training loss 0.6029463410377502\n",
      "Epoch 2 -- Batch 784/ 842, training loss 0.5822969079017639\n",
      "Epoch 2 -- Batch 785/ 842, training loss 0.6284616589546204\n",
      "Epoch 2 -- Batch 786/ 842, training loss 0.6227546334266663\n",
      "Epoch 2 -- Batch 787/ 842, training loss 0.5820956826210022\n",
      "Epoch 2 -- Batch 788/ 842, training loss 0.6259735822677612\n",
      "Epoch 2 -- Batch 789/ 842, training loss 0.5923481583595276\n",
      "Epoch 2 -- Batch 790/ 842, training loss 0.6060404181480408\n",
      "Epoch 2 -- Batch 791/ 842, training loss 0.6167212724685669\n",
      "Epoch 2 -- Batch 792/ 842, training loss 0.6364880800247192\n",
      "Epoch 2 -- Batch 793/ 842, training loss 0.6522567868232727\n",
      "Epoch 2 -- Batch 794/ 842, training loss 0.6110439300537109\n",
      "Epoch 2 -- Batch 795/ 842, training loss 0.6239989399909973\n",
      "Epoch 2 -- Batch 796/ 842, training loss 0.5824379920959473\n",
      "Epoch 2 -- Batch 797/ 842, training loss 0.6265636086463928\n",
      "Epoch 2 -- Batch 798/ 842, training loss 0.5899680256843567\n",
      "Epoch 2 -- Batch 799/ 842, training loss 0.5682167410850525\n",
      "Epoch 2 -- Batch 800/ 842, training loss 0.6208540201187134\n",
      "Epoch 2 -- Batch 801/ 842, training loss 0.6060963273048401\n",
      "Epoch 2 -- Batch 802/ 842, training loss 0.5830094218254089\n",
      "Epoch 2 -- Batch 803/ 842, training loss 0.6233806014060974\n",
      "Epoch 2 -- Batch 804/ 842, training loss 0.6194060444831848\n",
      "Epoch 2 -- Batch 805/ 842, training loss 0.6071438789367676\n",
      "Epoch 2 -- Batch 806/ 842, training loss 0.5962170958518982\n",
      "Epoch 2 -- Batch 807/ 842, training loss 0.5979924201965332\n",
      "Epoch 2 -- Batch 808/ 842, training loss 0.6277992129325867\n",
      "Epoch 2 -- Batch 809/ 842, training loss 0.5932916402816772\n",
      "Epoch 2 -- Batch 810/ 842, training loss 0.6272353529930115\n",
      "Epoch 2 -- Batch 811/ 842, training loss 0.5788006782531738\n",
      "Epoch 2 -- Batch 812/ 842, training loss 0.5997441411018372\n",
      "Epoch 2 -- Batch 813/ 842, training loss 0.6056435108184814\n",
      "Epoch 2 -- Batch 814/ 842, training loss 0.6422941088676453\n",
      "Epoch 2 -- Batch 815/ 842, training loss 0.5907134413719177\n",
      "Epoch 2 -- Batch 816/ 842, training loss 0.6061896681785583\n",
      "Epoch 2 -- Batch 817/ 842, training loss 0.6103944778442383\n",
      "Epoch 2 -- Batch 818/ 842, training loss 0.6080436110496521\n",
      "Epoch 2 -- Batch 819/ 842, training loss 0.6131200790405273\n",
      "Epoch 2 -- Batch 820/ 842, training loss 0.6072361469268799\n",
      "Epoch 2 -- Batch 821/ 842, training loss 0.6260161399841309\n",
      "Epoch 2 -- Batch 822/ 842, training loss 0.598697304725647\n",
      "Epoch 2 -- Batch 823/ 842, training loss 0.6073740124702454\n",
      "Epoch 2 -- Batch 824/ 842, training loss 0.6161901950836182\n",
      "Epoch 2 -- Batch 825/ 842, training loss 0.5980492234230042\n",
      "Epoch 2 -- Batch 826/ 842, training loss 0.574631929397583\n",
      "Epoch 2 -- Batch 827/ 842, training loss 0.5911605954170227\n",
      "Epoch 2 -- Batch 828/ 842, training loss 0.6107170581817627\n",
      "Epoch 2 -- Batch 829/ 842, training loss 0.6234506368637085\n",
      "Epoch 2 -- Batch 830/ 842, training loss 0.596743106842041\n",
      "Epoch 2 -- Batch 831/ 842, training loss 0.6407440900802612\n",
      "Epoch 2 -- Batch 832/ 842, training loss 0.5886935591697693\n",
      "Epoch 2 -- Batch 833/ 842, training loss 0.5832685828208923\n",
      "Epoch 2 -- Batch 834/ 842, training loss 0.6148964762687683\n",
      "Epoch 2 -- Batch 835/ 842, training loss 0.5779908299446106\n",
      "Epoch 2 -- Batch 836/ 842, training loss 0.6118212342262268\n",
      "Epoch 2 -- Batch 837/ 842, training loss 0.635679304599762\n",
      "Epoch 2 -- Batch 838/ 842, training loss 0.6132243275642395\n",
      "Epoch 2 -- Batch 839/ 842, training loss 0.5898422598838806\n",
      "Epoch 2 -- Batch 840/ 842, training loss 0.5957829356193542\n",
      "Epoch 2 -- Batch 841/ 842, training loss 0.581581711769104\n",
      "Epoch 2 -- Batch 842/ 842, training loss 0.6070127487182617\n",
      "----------------------------------------------------------------------\n",
      "Epoch 2 -- Batch 1/ 94, validation loss 0.5974158048629761\n",
      "Epoch 2 -- Batch 2/ 94, validation loss 0.5868167877197266\n",
      "Epoch 2 -- Batch 3/ 94, validation loss 0.5929883718490601\n",
      "Epoch 2 -- Batch 4/ 94, validation loss 0.6374058723449707\n",
      "Epoch 2 -- Batch 5/ 94, validation loss 0.6220283508300781\n",
      "Epoch 2 -- Batch 6/ 94, validation loss 0.5784939527511597\n",
      "Epoch 2 -- Batch 7/ 94, validation loss 0.583358108997345\n",
      "Epoch 2 -- Batch 8/ 94, validation loss 0.5730326771736145\n",
      "Epoch 2 -- Batch 9/ 94, validation loss 0.5934672355651855\n",
      "Epoch 2 -- Batch 10/ 94, validation loss 0.5965125560760498\n",
      "Epoch 2 -- Batch 11/ 94, validation loss 0.5876830220222473\n",
      "Epoch 2 -- Batch 12/ 94, validation loss 0.5827890634536743\n",
      "Epoch 2 -- Batch 13/ 94, validation loss 0.5896530151367188\n",
      "Epoch 2 -- Batch 14/ 94, validation loss 0.5972750186920166\n",
      "Epoch 2 -- Batch 15/ 94, validation loss 0.5712115168571472\n",
      "Epoch 2 -- Batch 16/ 94, validation loss 0.6151629090309143\n",
      "Epoch 2 -- Batch 17/ 94, validation loss 0.5934391021728516\n",
      "Epoch 2 -- Batch 18/ 94, validation loss 0.564551591873169\n",
      "Epoch 2 -- Batch 19/ 94, validation loss 0.5699132680892944\n",
      "Epoch 2 -- Batch 20/ 94, validation loss 0.5984220504760742\n",
      "Epoch 2 -- Batch 21/ 94, validation loss 0.5995235443115234\n",
      "Epoch 2 -- Batch 22/ 94, validation loss 0.5711272358894348\n",
      "Epoch 2 -- Batch 23/ 94, validation loss 0.6151764988899231\n",
      "Epoch 2 -- Batch 24/ 94, validation loss 0.5708281397819519\n",
      "Epoch 2 -- Batch 25/ 94, validation loss 0.5978679656982422\n",
      "Epoch 2 -- Batch 26/ 94, validation loss 0.6244785785675049\n",
      "Epoch 2 -- Batch 27/ 94, validation loss 0.5567450523376465\n",
      "Epoch 2 -- Batch 28/ 94, validation loss 0.5944285988807678\n",
      "Epoch 2 -- Batch 29/ 94, validation loss 0.598059892654419\n",
      "Epoch 2 -- Batch 30/ 94, validation loss 0.575861394405365\n",
      "Epoch 2 -- Batch 31/ 94, validation loss 0.5915744304656982\n",
      "Epoch 2 -- Batch 32/ 94, validation loss 0.5690063834190369\n",
      "Epoch 2 -- Batch 33/ 94, validation loss 0.5895241498947144\n",
      "Epoch 2 -- Batch 34/ 94, validation loss 0.5998938679695129\n",
      "Epoch 2 -- Batch 35/ 94, validation loss 0.5747214555740356\n",
      "Epoch 2 -- Batch 36/ 94, validation loss 0.576575756072998\n",
      "Epoch 2 -- Batch 37/ 94, validation loss 0.6065154671669006\n",
      "Epoch 2 -- Batch 38/ 94, validation loss 0.61664879322052\n",
      "Epoch 2 -- Batch 39/ 94, validation loss 0.5895200967788696\n",
      "Epoch 2 -- Batch 40/ 94, validation loss 0.5777068138122559\n",
      "Epoch 2 -- Batch 41/ 94, validation loss 0.6028856039047241\n",
      "Epoch 2 -- Batch 42/ 94, validation loss 0.5911733508110046\n",
      "Epoch 2 -- Batch 43/ 94, validation loss 0.5785035490989685\n",
      "Epoch 2 -- Batch 44/ 94, validation loss 0.5790077447891235\n",
      "Epoch 2 -- Batch 45/ 94, validation loss 0.6071105599403381\n",
      "Epoch 2 -- Batch 46/ 94, validation loss 0.5921338200569153\n",
      "Epoch 2 -- Batch 47/ 94, validation loss 0.5898454785346985\n",
      "Epoch 2 -- Batch 48/ 94, validation loss 0.5939331650733948\n",
      "Epoch 2 -- Batch 49/ 94, validation loss 0.5683630108833313\n",
      "Epoch 2 -- Batch 50/ 94, validation loss 0.5678711533546448\n",
      "Epoch 2 -- Batch 51/ 94, validation loss 0.6265408992767334\n",
      "Epoch 2 -- Batch 52/ 94, validation loss 0.5711963176727295\n",
      "Epoch 2 -- Batch 53/ 94, validation loss 0.5770054459571838\n",
      "Epoch 2 -- Batch 54/ 94, validation loss 0.5863953232765198\n",
      "Epoch 2 -- Batch 55/ 94, validation loss 0.5916198492050171\n",
      "Epoch 2 -- Batch 56/ 94, validation loss 0.574047327041626\n",
      "Epoch 2 -- Batch 57/ 94, validation loss 0.5743375420570374\n",
      "Epoch 2 -- Batch 58/ 94, validation loss 0.6014003157615662\n",
      "Epoch 2 -- Batch 59/ 94, validation loss 0.5901117920875549\n",
      "Epoch 2 -- Batch 60/ 94, validation loss 0.6167135834693909\n",
      "Epoch 2 -- Batch 61/ 94, validation loss 0.5943508148193359\n",
      "Epoch 2 -- Batch 62/ 94, validation loss 0.5919660925865173\n",
      "Epoch 2 -- Batch 63/ 94, validation loss 0.5903099179267883\n",
      "Epoch 2 -- Batch 64/ 94, validation loss 0.5530616044998169\n",
      "Epoch 2 -- Batch 65/ 94, validation loss 0.6201604008674622\n",
      "Epoch 2 -- Batch 66/ 94, validation loss 0.5827510952949524\n",
      "Epoch 2 -- Batch 67/ 94, validation loss 0.5945598483085632\n",
      "Epoch 2 -- Batch 68/ 94, validation loss 0.6000702381134033\n",
      "Epoch 2 -- Batch 69/ 94, validation loss 0.5587021112442017\n",
      "Epoch 2 -- Batch 70/ 94, validation loss 0.5876298546791077\n",
      "Epoch 2 -- Batch 71/ 94, validation loss 0.6015690565109253\n",
      "Epoch 2 -- Batch 72/ 94, validation loss 0.5887793302536011\n",
      "Epoch 2 -- Batch 73/ 94, validation loss 0.6073054075241089\n",
      "Epoch 2 -- Batch 74/ 94, validation loss 0.5797359943389893\n",
      "Epoch 2 -- Batch 75/ 94, validation loss 0.5741924047470093\n",
      "Epoch 2 -- Batch 76/ 94, validation loss 0.6243562698364258\n",
      "Epoch 2 -- Batch 77/ 94, validation loss 0.5957739949226379\n",
      "Epoch 2 -- Batch 78/ 94, validation loss 0.6070101857185364\n",
      "Epoch 2 -- Batch 79/ 94, validation loss 0.5666456818580627\n",
      "Epoch 2 -- Batch 80/ 94, validation loss 0.5660909414291382\n",
      "Epoch 2 -- Batch 81/ 94, validation loss 0.5851352214813232\n",
      "Epoch 2 -- Batch 82/ 94, validation loss 0.5909270644187927\n",
      "Epoch 2 -- Batch 83/ 94, validation loss 0.5879374146461487\n",
      "Epoch 2 -- Batch 84/ 94, validation loss 0.5927606225013733\n",
      "Epoch 2 -- Batch 85/ 94, validation loss 0.5868929028511047\n",
      "Epoch 2 -- Batch 86/ 94, validation loss 0.5901867151260376\n",
      "Epoch 2 -- Batch 87/ 94, validation loss 0.5797187685966492\n",
      "Epoch 2 -- Batch 88/ 94, validation loss 0.5941020250320435\n",
      "Epoch 2 -- Batch 89/ 94, validation loss 0.5970587134361267\n",
      "Epoch 2 -- Batch 90/ 94, validation loss 0.5972282290458679\n",
      "Epoch 2 -- Batch 91/ 94, validation loss 0.5919710993766785\n",
      "Epoch 2 -- Batch 92/ 94, validation loss 0.5790532231330872\n",
      "Epoch 2 -- Batch 93/ 94, validation loss 0.5733346939086914\n",
      "Epoch 2 -- Batch 94/ 94, validation loss 0.6276645064353943\n",
      "----------------------------------------------------------------------\n",
      "Epoch 2 loss: Training 0.6458790898323059, Validation 0.6276645064353943\n",
      "----------------------------------------------------------------------\n",
      "Epoch 3/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:04:05] SMILES Parse Error: syntax error while parsing: Cc1ccc(NC(=O)COc2c(C)[nH]c(=O)c3cc(-c4ccc(Cl)cc4)nc(4)c23)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(NC(=O)COc2c(C)[nH]c(=O)c3cc(-c4ccc(Cl)cc4)nc(4)c23)cc1' for input: 'Cc1ccc(NC(=O)COc2c(C)[nH]c(=O)c3cc(-c4ccc(Cl)cc4)nc(4)c23)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 19 20\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 10 12 13 14 15 16\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(C)=CCn1c(=S)sc2nc3c(cc22)CCCC3'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c(c1)ccc(C(=O)NC(=O)C2CC2)c1C'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2[nH]c(=O)c(SCC(=O)N3CCc4ccccc43)n2)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2[nH]c(=O)c(SCC(=O)N3CCc4ccccc43)n2)cc1' for input: 'Cc1ccc2[nH]c(=O)c(SCC(=O)N3CCc4ccccc43)n2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCOc1cc(C2N3C(C)=C(C)NC3=C2C(=O)CC(C)(C)C3)cc(OC)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1(CCCS(=O)(=O)CN2C(=O)c2ccccc2)C=C(C(=O)OCC)C1c1ccccc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2cc(-c3ccc(NCc4cccnc4)nc3)on2c2cccc1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CC1=CC(=O)C2C3C4C=CC(O)(C(=O)[C@@H]5C)[C@]3(F)C(C)[C@@H]2S2(=O)=O)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CC1=CC(=O)C2C3C4C=CC(O)(C(=O)[C@@H]5C)[C@]3(F)C(C)[C@@H]2S2(=O)=O)c1' for input: 'CC1=CC(=O)C2C3C4C=CC(O)(C(=O)[C@@H]5C)[C@]3(F)C(C)[C@@H]2S2(=O)=O)c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 5 6 20 21\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 11 13 14\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 14 16 17\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C1c2c(c(=O)[nH]c(=S)n2Cc2ccccc2)c(=O)n2C1c1ccc(Cl)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC1CCCc2c(sc3nc2CNC(=O)Cn2cn(C)c3ccccc23)C1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(Nc2cc(NCc3cccnc3)nc(N4CCc5ccccc53)CC3)nc2n1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 28\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'C/C(C(=O)c1ccco1)N1C(=O)C2C3Oc4ccccc4C(c4ccccc43)C2C2'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 37 38 39\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CCCCn1c(=O)c2cc(-c3ccc(F)cc3)nc3c2c(=O)n(C)c(=O)n3C)cc1=O\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CCCCn1c(=O)c2cc(-c3ccc(F)cc3)nc3c2c(=O)n(C)c(=O)n3C)cc1=O' for input: 'CCCCn1c(=O)c2cc(-c3ccc(F)cc3)nc3c2c(=O)n(C)c(=O)n3C)cc1=O'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1nc2ncc(C(=O)NCCC(=O)NCc3ccc(Cl)cc3)n2n1)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1nc2ncc(C(=O)NCCC(=O)NCc3ccc(Cl)cc3)n2n1)cc1' for input: 'Cc1nc2ncc(C(=O)NCCC(=O)NCc3ccc(Cl)cc3)n2n1)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(N2CC3(CN(C(=O)C4CCCC4)CC3)O3)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2=CC(C3(C)CC(C)(C)O)OC3=C2CCCO3)cc1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: N#C[C@H]1NC(=O)[C@H]2CC(=O)N(Cc3ccco3)C2=O)c1C\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'N#C[C@H]1NC(=O)[C@H]2CC(=O)N(Cc3ccco3)C2=O)c1C' for input: 'N#C[C@H]1NC(=O)[C@H]2CC(=O)N(Cc3ccco3)C2=O)c1C'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2[nH]c(C(=O)N(Cc3cccnc3)Cc3ccc4c(c3)OCO4)nn2)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2[nH]c(C(=O)N(Cc3cccnc3)Cc3ccc4c(c3)OCO4)nn2)c1' for input: 'Cc1ccc2[nH]c(C(=O)N(Cc3cccnc3)Cc3ccc4c(c3)OCO4)nn2)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC=C1N=C(N)OC2N=C(C3CC2)SC1c1ccc(OC)cc1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CC1(C)Sc2cccc(C(=O)Nc3cccnc3C(=O)c3ccc(C#N)cc3)C2)cc1OC\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CC1(C)Sc2cccc(C(=O)Nc3cccnc3C(=O)c3ccc(C#N)cc3)C2)cc1OC' for input: 'CC1(C)Sc2cccc(C(=O)Nc3cccnc3C(=O)c3ccc(C#N)cc3)C2)cc1OC'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 18 19 20 25 26 27\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(cc1CNC(=O)CN(C)C)c2ncnc2-1'\n",
      "[19:04:05] SMILES Parse Error: ring closure 2 duplicates bond between atom 27 and atom 28 for input: 'Cc1cc(/C=C2/C(=O)N3C(=O)N(Cc5ccccc4)CC3=O)c2ccccc2c2ccccc21'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCCc1cc2c(c1CCC(=O)c1ccc(F)cc1)NC(=O)c2ccccc2O1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COCCNC(=O)C(Nc1nc2ccccc2s2)c(=O)n(-c2cccc(Cl)c2)c1=O'\n",
      "[19:04:05] SMILES Parse Error: ring closure 3 duplicates bond between atom 15 and atom 16 for input: 'COc1ccc(S(=O)(=O)N2c3ccccc3-3c3ccccc3)cc2c2ccccc21'\n",
      "[19:04:05] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 10 for input: 'C[C@@H]1CN([C@@H](C)CO)C2(=O)c2cccc2Nc2ccc(F)cc2C(=O)N([C@H](C)CO)C[C@@H]1C'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: O=c1c2ccccc2nc(Cn2ccc(-c3ccccc3)n2)nn1)NCC(c1ccco1)N1CCCCCC1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'O=c1c2ccccc2nc(Cn2ccc(-c3ccccc3)n2)nn1)NCC(c1ccco1)N1CCCCCC1' for input: 'O=c1c2ccccc2nc(Cn2ccc(-c3ccccc3)n2)nn1)NCC(c1ccco1)N1CCCCCC1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2cnc3c(c2)CN(C(=O)c2ccccc2)C(=O)O)c(C)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1snc(C(=O)NCC23CC4CC(CC(C5)C2)C3)n1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(C(=O)Nc2ccccc2)ccc1NC(=O)CSc1nnc2n1CCN(C(=O)c1ccccc1)C2)C1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C(=O)Nc2ccccc2)ccc1NC(=O)CSc1nnc2n1CCN(C(=O)c1ccccc1)C2)C1' for input: 'Cc1cc(C(=O)Nc2ccccc2)ccc1NC(=O)CSc1nnc2n1CCN(C(=O)c1ccccc1)C2)C1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CCc1ccc2c(C)c3(C=CC(=O)Nc4ccccc4Cl)sc3n2)cc1F\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CCc1ccc2c(C)c3(C=CC(=O)Nc4ccccc4Cl)sc3n2)cc1F' for input: 'CCc1ccc2c(C)c3(C=CC(=O)Nc4ccccc4Cl)sc3n2)cc1F'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1CNCCn1c2ccc(Cl)cc2C(=O)N(C)C)c1C\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1CNCCn1c2ccc(Cl)cc2C(=O)N(C)C)c1C' for input: 'COc1ccccc1CNCCn1c2ccc(Cl)cc2C(=O)N(C)C)c1C'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccc(C2=NN3C(=S)N(C(C)=C6C(=O)C5(C)CC5)C3C(=O)N2CCc2ccccc2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 17 18 19 20\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 16\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccc(-n2cnc3c4c(c(=O)n24)CCCC4)cc1'\n",
      "[19:04:05] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[19:04:05] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 20 21 22 24 25 26 27\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CCOC(=O)COc1ccc2nc(CSc3noc(/C4CCOCC4)c3oc3=O)s2)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CCOC(=O)COc1ccc2nc(CSc3noc(/C4CCOCC4)c3oc3=O)s2)c1' for input: 'CCOC(=O)COc1ccc2nc(CSc3noc(/C4CCOCC4)c3oc3=O)s2)c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 9 23 24 25 26 27\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 26\n",
      "[19:04:05] non-ring atom 4 marked aromatic\n",
      "[19:04:05] SMILES Parse Error: extra open parentheses for input: 'O=C(CN1CCCc2ccccc21'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 8\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CSc1cccnc1C(=O)N(CCc1ccnc(Cc1ccccc1F)c1ccccc1)CC1CCCO1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCCCN1C[C@H]2CC[C@H]2CC[C@@H]2/C=C/C1=O'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 23 24 25\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)nn(CC(=O)N1CCOCC1)c1nccn1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1occc1C(=O)NCCSc1nc2ccc(Br)cn2c1NCC1CC2'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 27\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8 20 21\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccc(F)cc1)[C@@H]1CCN(CCCN2Cc3ccc(Cl)cc3C2(O)CC2)CC1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'N#Cc1cccc(C(=O)Nc2c3c(cnc3n1)CC3)C1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(c2nnnn2CCC#N)C2=NS(=O)(=O)c2cccc(C(F)(F)F)c2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 8 9 10 11 12 13\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1nc2c3c(sc2c1)C(c1ccc(F)cc1)F)CCCCC3\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1nc2c3c(sc2c1)C(c1ccc(F)cc1)F)CCCCC3' for input: 'Cc1nc2c3c(sc2c1)C(c1ccc(F)cc1)F)CCCCC3'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 4 15 16 19\n",
      "[19:04:05] SMILES Parse Error: ring closure 3 duplicates bond between atom 18 and atom 19 for input: 'COC(=O)N1CCN(c2cc(Nc3nccn3)cn3c3ncnn3CCCC2)CC1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: O=C1[C@@H](CO)N2Cc3ccccc3C2(Oc2ccccc2)C1=N/C(C#N)=O)C2\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'O=C1[C@@H](CO)N2Cc3ccccc3C2(Oc2ccccc2)C1=N/C(C#N)=O)C2' for input: 'O=C1[C@@H](CO)N2Cc3ccccc3C2(Oc2ccccc2)C1=N/C(C#N)=O)C2'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 8 9 11\n",
      "[19:04:05] SMILES Parse Error: ring closure 3 duplicates bond between atom 11 and atom 12 for input: 'Cc1cc(C)n2c(CNC(=O)C3c3ccc4ccccc33)n[nH]c2n1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1ccc2c(=O)n(Cc3csc4ccccc33)nc2c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 3 10 11 12 13 14 15\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 8 17 18 19\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 10 13\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 16 29 30 31 32\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CS(=O)(=O)N(CC(=O)N1CCc2sccc2c1)c1c(F)cccc1F)c1ccccc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CS(=O)(=O)N(CC(=O)N1CCc2sccc2c1)c1c(F)cccc1F)c1ccccc1' for input: 'CS(=O)(=O)N(CC(=O)N1CCc2sccc2c1)c1c(F)cccc1F)c1ccccc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccccc1Sc1nnc(-c2ccco1)o1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2cnn3c(N4CCC(C(C)C)OC44)[nH]c3c2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1cccc2c1N1SC(c2cccc(OC)c2)N1C(=O)c1cccnc1'\n",
      "[19:04:05] SMILES Parse Error: extra open parentheses for input: 'CN(CCn1c(=O)c2ccnn2c2nc(-c2cccnc2)n1N'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(C)n2c3c(=O)c(-c4cccs4)nn3c3nnn33)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C)n2c3c(=O)c(-c4cccs4)nn3c3nnn33)c1' for input: 'Cc1cc(C)n2c3c(=O)c(-c4cccs4)nn3c3nnn33)c1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(O)c(O)c(Sc2nc(C(c)c3ccccc3)no2)N2CCOCC2)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(O)c(O)c(Sc2nc(C(c)c3ccccc3)no2)N2CCOCC2)c1' for input: 'Cc1cc(O)c(O)c(Sc2nc(C(c)c3ccccc3)no2)N2CCOCC2)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(CNc3nnc(-c4ccc(Cl)cc4)cs3)nc2cc1c(=O)n2CCN1CCOCC1'\n",
      "[19:04:05] SMILES Parse Error: syntax error while parsing: O=C1OC(/3=C(/O)c3ccccc3C2)c2ccccc21\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'O=C1OC(/3=C(/O)c3ccccc3C2)c2ccccc21' for input: 'O=C1OC(/3=C(/O)c3ccccc3C2)c2ccccc21'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 19 21 22\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 20\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: C[C@H]1CN(C)C(=O)c2[nH]c3ccc(NC(C)=O)cc3C2=O)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'C[C@H]1CN(C)C(=O)c2[nH]c3ccc(NC(C)=O)cc3C2=O)cc1' for input: 'C[C@H]1CN(C)C(=O)c2[nH]c3ccc(NC(C)=O)cc3C2=O)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 18 19\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 12 13 14 25 26 27\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 6 7 18 19 21 22 23 24\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=c1c2c(nn1-c1ccc(F)cc1)C(=O)Nc2ccccc21'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 17 18 19 29 30\n",
      "[19:04:05] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1cccc2c(COc3ccccc3)nc3ccccc32)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1cccc2c(COc3ccccc3)nc3ccccc32)c1' for input: 'Cc1cccc2c(COc3ccccc3)nc3ccccc32)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC1(C)C(=O)C1=C(C)Nc1sc2c(c1-CC1)CSC2'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OCC(=O)NCc2nc3c(s(C)n2Cc2ccc(F)cc2)c2ccccc2=O)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cn(c(O)c1C=NCC1CC1)c1ccccc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 16 17 18 21 22 24 25\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8 15 16 17 18 19 20\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 19 20 21 22 23 24\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 8 9\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 17 18\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1cccc(-c2ccc(/C=C3/SC(=Nc4ccccc4)SC4=NS4)cc2)c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(C)Cc1ccc2c(c1)S(=O)(=O)N(CC(=O)c1ccco1)C(=O)N3'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CCC(C)OCC1OC(c2ccccc2C)C2C(=O)N(c3ccc(C)c(Cl)c3)C(c3ccc(F)cc3)N2)C1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CCC(C)OCC1OC(c2ccccc2C)C2C(=O)N(c3ccc(C)c(Cl)c3)C(c3ccc(F)cc3)N2)C1' for input: 'CCC(C)OCC1OC(c2ccccc2C)C2C(=O)N(c3ccc(C)c(Cl)c3)C(c3ccc(F)cc3)N2)C1'\n",
      "[19:04:05] Explicit valence for atom # 26 F, 3, is greater than permitted\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1cccc(NC(=O)c2ccc(-c3nc5sccc4[nH]3)c(Br)c2)c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 13 14 15\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 17 19\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'NC(=O)C12CCCC=C1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(C2CC(=O)N3C(=C(CO)O)SC3=S)cc2)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(C2CC(=O)N3C(=C(CO)O)SC3=S)cc2)cc1' for input: 'COc1ccc(C2CC(=O)N3C(=C(CO)O)SC3=S)cc2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1=C(O)C(=O)N(C)C1=C1C(F)(Cl)CC(=O)OC2c1ccc(Cl)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 19 21 22\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(C2=N/C(=C\\c3cc([N+](=O)[O-])ccc3OC)C3=O)cc2[N+](=O)[O-])c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(C2=N/C(=C\\c3cc([N+](=O)[O-])ccc3OC)C3=O)cc2[N+](=O)[O-])c1' for input: 'COc1ccc(C2=N/C(=C\\c3cc([N+](=O)[O-])ccc3OC)C3=O)cc2[N+](=O)[O-])c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc2c(c1C)C(=O)c1ccc(C(C)=O)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCc1ccc(Oc2ccc(NS(=O)(=O)c3ccc4nc4cc(C)ccc5s3)cc2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 30\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)c2ccc(-c3cnc4ccccc33)nn2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 7 25 26 27 28\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 19 20 21 22 23\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24 25 26 27 28\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 10 12 21 22 23 24\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(CN1C(=O)NCCn1c(=O)[nH]c(=O)c2ccccc21)Nc1ccc(S(=O)(=O)N2CCCC2)cc1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CC1Sc2ccc(Br)c2NC2=C(C(=O)CC(c3ccccc3)C2)C1)c1ccccc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CC1Sc2ccc(Br)c2NC2=C(C(=O)CC(c3ccccc3)C2)C1)c1ccccc1' for input: 'CC1Sc2ccc(Br)c2NC2=C(C(=O)CC(c3ccccc3)C2)C1)c1ccccc1'\n",
      "[19:04:05] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[19:04:05] SMILES Parse Error: syntax error while parsing: CN(C)c1cccc(C(=O)N2c3ccccc3S//==S(=O)=O)c2c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CN(C)c1cccc(C(=O)N2c3ccccc3S//==S(=O)=O)c2c1' for input: 'CN(C)c1cccc(C(=O)N2c3ccccc3S//==S(=O)=O)c2c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1c(C(C)(C)C)nc2ccc3c(c1)CN1CCC(C(N)=O)CC2'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C1c2ccccc2NC2(c3cc(F)ccc3Cl)/C1=C/c1ccc(Cl)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 15 16\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2(C)CC(C(=O)O)(C(C(F)(F)F)C3)c2ccccc2)c(OC)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2COC(C(F)(F)F)C2CC2C(=O)Nc2ccc(N3CCOCC3)cc2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2c(c1)[C@H]1C[C@H](CC(=O)N3CCc4ccccc43)CC1)NCCc1ccccc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 9 20 21 23\n",
      "[19:04:05] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 25\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(c(OC)ccc22)C(=O)NC1=N'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 8 21 22 23\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 7 12\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 8 16 17 18 19 20 21 22\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7\n",
      "[19:04:05] non-ring atom 7 marked aromatic\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@@H]1CCCOC[C@@H]1c2ccccc2Oc2ccc3c(c2)[C@@H]1C[C@H](Cc1ccccc1)NC(=O)N[C@@H](C)CO)c2ccccc2'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CN1CCCn2cc(C(N)=O)c(C#CC2CC3)c21'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cn1ccc2c(=O)c3c(cc1SCCO)nnn3c1=O'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1c2c(nc1CS(=O)(=O)CC)NC(=O)c1nnc(-c3ccccc3F)c(=O)[nH]1)OCOC2=O\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1c2c(nc1CS(=O)(=O)CC)NC(=O)c1nnc(-c3ccccc3F)c(=O)[nH]1)OCOC2=O' for input: 'Cc1c2c(nc1CS(=O)(=O)CC)NC(=O)c1nnc(-c3ccccc3F)c(=O)[nH]1)OCOC2=O'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13 15 16\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 9\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC1=C(C)Nc2nncn2C(C2=CCN(CC3CCO4)C2)C1=O'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(C(F)(F)F)cc1)C1CC(C2)N1CC=C1Cc1ccccc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 17 22 23\n",
      "[19:04:05] Explicit valence for atom # 19 O, 3, is greater than permitted\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1=C(O)c2c(C)[nH]c(=O)c2ccc2c(c11OCCC2)N1CCOCC1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1cc2nc(CN(C)C(=O)c3ccco3)c(C(=O)c3ccc(Cl)cc3)c(=O)[nH]2)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1cc2nc(CN(C)C(=O)c3ccco3)c(C(=O)c3ccc(Cl)cc3)c(=O)[nH]2)cc1' for input: 'Cc1cc2nc(CN(C)C(=O)c3ccco3)c(C(=O)c3ccc(Cl)cc3)c(=O)[nH]2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(C)CCc1c(C(=O)NCCCN2CCOC(C)C2)nc2c3c(s1)CCC3'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 6 8 17 18 20 21 22 24\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 5 7 8 9 10 11 12 13\n",
      "[19:04:05] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 18 19\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 9 10 11 26 27 28\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COCCNC(=O)Cn1c(C)ccc2c1=O'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 30 31 32 33 34\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(C)CCn1c2c(cnc2c(=O)n(C)c(=O)n2C)n1Cc1cccs1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 11\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1nc2sc3n(c(=O)c2c2c1)CCOC3'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: C=CC[C@@H]1CC[C@@H]2[C@@H](COc3ccc(NC(=O)Cn4cncn4)c3=O)[C@H](CO)O1)NCC#Cc1ccccc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'C=CC[C@@H]1CC[C@@H]2[C@@H](COc3ccc(NC(=O)Cn4cncn4)c3=O)[C@H](CO)O1)NCC#Cc1ccccc1' for input: 'C=CC[C@@H]1CC[C@@H]2[C@@H](COc3ccc(NC(=O)Cn4cncn4)c3=O)[C@H](CO)O1)NCC#Cc1ccccc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccc(Cl)cc1)N1CC2CC1C2CC3'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CCc1[nH]c2cc3c(ccc22)CSC3=NS(=O)(=O)c2ccccc2C)c1O\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CCc1[nH]c2cc3c(ccc22)CSC3=NS(=O)(=O)c2ccccc2C)c1O' for input: 'CCc1[nH]c2cc3c(ccc22)CSC3=NS(=O)(=O)c2ccccc2C)c1O'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c(C2Cc3nc4nc5ccccc5n33)ccc2c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 15 16 22\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CN(C)C(=O)c1ccc2c3c([nH]c3c1)COC(C)(C)C4'\n",
      "[19:04:05] SMILES Parse Error: ring closure 1 duplicates bond between atom 11 and atom 12 for input: 'CCOC(=O)C1(C)C=CCC1C1c1cc(OC)cc(OC)c1NC(=O)C1(C)OCCl'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7 8 11\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 7 8 10 22 23\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 19 21\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'c1ccccc1N1CCN(C(=O)C2CC23CCN2C(=O)NC1CCCC2)CC1'\n",
      "[19:04:05] SMILES Parse Error: ring closure 2 duplicates bond between atom 23 and atom 24 for input: 'O=C(CSc1nncc2c1nnn2-c1cccc(Cl)c1)N1CCC2c2ccc(Cc3ccccc3)cc21'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 27\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1nc(-c2ccc(Br)cc2)oc1=O)c1ccc2c(c1)N(C1CCCCC1)CC3'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'N#Cc1c(C(=O)O)nn(-c2ccc(N(C)C)cc2)c2c1ccc(Cl)cc1Cl'\n",
      "[19:04:05] Explicit valence for atom # 6 C, 5, is greater than permitted\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccc(NC(=O)COC(=O)C[C@@H]2CC[C@@H](NC(=O)C(C)(C)O)[C@@H](CO)O3)cc1'\n",
      "[19:04:05] non-ring atom 1 marked aromatic\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 9 11 12 13 14\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 21 22 23 24 25 31 32\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C1C(=O)N(c2cccs2)C2C(F)(F)F(Cl)Cl1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CNC(=O)c1ccc(COc2ccc(/C=c3\\\\C(=N)N4CCC(C5)C4C3=O)cc2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1cc2c(cc1S(=O)(=O)NCc1ccccc1)C1CCCCC1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCC(=O)Nc1ccc2c(c1)C(=O)N(C)C[C@H](OC)[C@@H](C)CN(C)C(=O)NC1=O'\n",
      "[19:04:05] SMILES Parse Error: syntax error while parsing: Cc1cccc(NC2=NN(C(=O)CN3CC=(NOC(=O)c4ccc(C)cc4)CC3)CC2=O)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1cccc(NC2=NN(C(=O)CN3CC=(NOC(=O)c4ccc(C)cc4)CC3)CC2=O)c1' for input: 'Cc1cccc(NC2=NN(C(=O)CN3CC=(NOC(=O)c4ccc(C)cc4)CC3)CC2=O)c1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 19 20 22 23\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CCOc1cccc2c(=O)c3c(cccc23)C(=O)N2CCN(c3ccccc3F)CC2)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CCOc1cccc2c(=O)c3c(cccc23)C(=O)N2CCN(c3ccccc3F)CC2)c1' for input: 'CCOc1cccc2c(=O)c3c(cccc23)C(=O)N2CCN(c3ccccc3F)CC2)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCN(CC3CCCO2)Cc1nnc(CSc2ccc(Cl)cc2)o1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 21\n",
      "[19:04:05] SMILES Parse Error: ring closure 2 duplicates bond between atom 16 and atom 17 for input: 'O=C(c1nc2sccc2c(=O)[nH]1)N1CC(=O)N2c2ccccc2Cc2ccccc21'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Cn2c(-c3ccccc3)cnc2SCC(=O)NC2CC2)cc11C'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 22 23 24 25 26\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2cc3c(cccc23)C(c2cccc(Cl)c2)n2cccc2F)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2cc3c(cccc23)C(c2cccc(Cl)c2)n2cccc2F)c1' for input: 'Cc1ccc2cc3c(cccc23)C(c2cccc(Cl)c2)n2cccc2F)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@]12CC=CC3C1CC(C1)O[C@H](CN(C)Cc1ccco1)C2=O'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])c1cccc(Nc2ccc3n(c3c2)CCCN3)c1'\n",
      "[19:04:05] SMILES Parse Error: ring closure 1 duplicates bond between atom 19 and atom 20 for input: 'O=C(Nc1nc(Cl)nc2c1ccnc1)c1ccccc1N1CCCCC1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 20\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2c(C(=O)N3CCOCC3)cc(-c3ccc(F)cc3)n2C)c1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2c(C(=O)N3CCOCC3)cc(-c3ccc(F)cc3)n2C)c1' for input: 'Cc1ccc2c(C(=O)N3CCOCC3)cc(-c3ccc(F)cc3)n2C)c1'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CC1CC12CC3CC4CC4CC(=O)C=CC(=O)C(=NC(=Cc/ccc4ccccc5)N=C(C)C4)c31)N2\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CC1CC12CC3CC4CC4CC(=O)C=CC(=O)C(=NC(=Cc/ccc4ccccc5)N=C(C)C4)c31)N2' for input: 'CC1CC12CC3CC4CC4CC(=O)C=CC(=O)C(=NC(=Cc/ccc4ccccc5)N=C(C)C4)c31)N2'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'C[C@@H](CO)N1C[C@H](C)[C@H](CN(C)C(=O)CNc2cccc(C(F)(F)F)c2)OCCOCC[C@@]2(C)OCC[C@@H]1C'\n",
      "[19:04:05] non-ring atom 6 marked aromatic\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: Cn1c(=O)c2c(nc3n2-c(cnc3ccccc33)C(C#N)=C#N)sn1)NC(C)(C)C\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'Cn1c(=O)c2c(nc3n2-c(cnc3ccccc33)C(C#N)=C#N)sn1)NC(C)(C)C' for input: 'Cn1c(=O)c2c(nc3n2-c(cnc3ccccc33)C(C#N)=C#N)sn1)NC(C)(C)C'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: COc1cc2c(nc(N3CCN(C(=O)CCCC4=O)C4)n3)c(=O)[nH]c2cc1Br)F\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'COc1cc2c(nc(N3CCN(C(=O)CCCC4=O)C4)n3)c(=O)[nH]c2cc1Br)F' for input: 'COc1cc2c(nc(N3CCN(C(=O)CCCC4=O)C4)n3)c(=O)[nH]c2cc1Br)F'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 15 16 24\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 6 7 17 21\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 7 8 9 10 20 21\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 6 7 8 9 10 11\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C1CC(c2cccc(Cl)c2)N1CCCN(C(=O)c1ccccc1Cl)C1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 10 11 12 13 14 15 16\n",
      "[19:04:05] SMILES Parse Error: ring closure 3 duplicates bond between atom 13 and atom 14 for input: 'CCOc1ccc(C2C(C#N)=C(N)n3c3ccccc3O2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C2=NNC(=S)Nc2ccc(SC)cc2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 12 29 30 31 32 33 34\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 8 21 22\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 17\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: CC1=C(C(=O)OCC)SC(C(=O)O)CC(=O)N1C)c1cccs1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'CC1=C(C(=O)OCC)SC(C(=O)O)CC(=O)N1C)c1cccs1' for input: 'CC1=C(C(=O)OCC)SC(C(=O)O)CC(=O)N1C)c1cccs1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)c2cccc(N3C(=O)C4C5=CC(C5)C5C5C)C3C2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10 11 12\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2cc(C(F)(F)F)c3c(nc3cccnc34)O2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 8 12 22 23\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 18 19 21\n",
      "[19:04:05] SMILES Parse Error: extra open parentheses for input: 'C/C(=N\\OC(=O)c1ccccc1=O)N(CCC'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 11 12 13 15 16 17 18 19 20 21 22 23\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 11 12 17\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cn1c(=O)c2c(nc3n(CC(C)(C)C)n2CCC#C)n(C)c2c1=O'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2=NOC[N+]3C(=O)NC2(n2cnc3ccccc23)C2CCCCC2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=S(=O)(Nc1ccc(Nc2cccc3[nH]cc2)cc1)c1cccc(O)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC1=NN2C(=O)C(c3ccccc3)OC2=C1C(=O)CC(C)(C)C2'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'Cc1nnc(NC(=O)COc2ccc3c(c2)CCC(=O)O)nn2c1C'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(Cl)cc1F)c1ccc2c(c1)N(C=CC1=S)CC2'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 13\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 16 17\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2c(c1)c1ccccc1Oc1ccco1)n1cccc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 7 8\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)C(=O)NC1CCS2(=O)=O'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: C[C@@H]1CN([C@H](C)CO)C(=O)c2ccc(NS(=O)(=O)c3ccccc3F)nc2)O[C@H]1CO\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'C[C@@H]1CN([C@H](C)CO)C(=O)c2ccc(NS(=O)(=O)c3ccccc3F)nc2)O[C@H]1CO' for input: 'C[C@@H]1CN([C@H](C)CO)C(=O)c2ccc(NS(=O)(=O)c3ccccc3F)nc2)O[C@H]1CO'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 8 12 29 30 31\n",
      "[19:04:05] Explicit valence for atom # 5 Cl, 2, is greater than permitted\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)CN(C)C[C@H]2OC)c(Cl)c1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C(NCCc1cccnc1)c1cc3ccccc2oc1=O'\n",
      "[19:04:05] SMILES Parse Error: syntax error while parsing: S=C(Nc1ccccc1)NCCn1c(-)cccc1=O\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'S=C(Nc1ccccc1)NCCn1c(-)cccc1=O' for input: 'S=C(Nc1ccccc1)NCCn1c(-)cccc1=O'\n",
      "[19:04:05] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2cc(OCC(C(=O)NCC(C)(C)C)N3C(=O)COC(=O)c4ccco4)nc3c2)cc1\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2cc(OCC(C(=O)NCC(C)(C)C)N3C(=O)COC(=O)c4ccco4)nc3c2)cc1' for input: 'COc1ccc2cc(OCC(C(=O)NCC(C)(C)C)N3C(=O)COC(=O)c4ccco4)nc3c2)cc1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 15 23 24 25 26 27 28\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCc1nc2c(Oc3ccc(F)cc2)c(C)nn2C1CCCCC1'\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'O=C1c2cc(Cl)ccc2C(=O)N1c1ccc(S(=O)(=O)N2C3CCCC2CC2)cc1'\n",
      "[19:04:05] SMILES Parse Error: unclosed ring for input: 'CCCCC(=O)Nc1ccc(-c2ccc(C(=O)NC4CCCC3)cc2)cc1'\n",
      "[19:04:05] SMILES Parse Error: syntax error while parsing: O=C(Cn1cnc2cc(N3CCCC3)ncc21)Nc1ccc(F)c(\n",
      "[19:04:05] SMILES Parse Error: Failed parsing SMILES 'O=C(Cn1cnc2cc(N3CCCC3)ncc21)Nc1ccc(F)c(' for input: 'O=C(Cn1cnc2cc(N3CCCC3)ncc21)Nc1ccc(F)c('\n",
      "[19:04:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8 18 19 20 21 22 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 -- Batch 1/ 842, training loss 0.5862991213798523\n",
      "Epoch 3 -- Batch 2/ 842, training loss 0.6026047468185425\n",
      "Epoch 3 -- Batch 3/ 842, training loss 0.6008363366127014\n",
      "Epoch 3 -- Batch 4/ 842, training loss 0.6324578523635864\n",
      "Epoch 3 -- Batch 5/ 842, training loss 0.5863271355628967\n",
      "Epoch 3 -- Batch 6/ 842, training loss 0.6186871528625488\n",
      "Epoch 3 -- Batch 7/ 842, training loss 0.6066606640815735\n",
      "Epoch 3 -- Batch 8/ 842, training loss 0.6082261800765991\n",
      "Epoch 3 -- Batch 9/ 842, training loss 0.6186850070953369\n",
      "Epoch 3 -- Batch 10/ 842, training loss 0.6124225854873657\n",
      "Epoch 3 -- Batch 11/ 842, training loss 0.6059475541114807\n",
      "Epoch 3 -- Batch 12/ 842, training loss 0.5809412598609924\n",
      "Epoch 3 -- Batch 13/ 842, training loss 0.5831202864646912\n",
      "Epoch 3 -- Batch 14/ 842, training loss 0.6151729822158813\n",
      "Epoch 3 -- Batch 15/ 842, training loss 0.5801334977149963\n",
      "Epoch 3 -- Batch 16/ 842, training loss 0.6074866056442261\n",
      "Epoch 3 -- Batch 17/ 842, training loss 0.5938974618911743\n",
      "Epoch 3 -- Batch 18/ 842, training loss 0.6122013330459595\n",
      "Epoch 3 -- Batch 19/ 842, training loss 0.600267767906189\n",
      "Epoch 3 -- Batch 20/ 842, training loss 0.6215549111366272\n",
      "Epoch 3 -- Batch 21/ 842, training loss 0.6161168217658997\n",
      "Epoch 3 -- Batch 22/ 842, training loss 0.606941282749176\n",
      "Epoch 3 -- Batch 23/ 842, training loss 0.5933483242988586\n",
      "Epoch 3 -- Batch 24/ 842, training loss 0.6099992394447327\n",
      "Epoch 3 -- Batch 25/ 842, training loss 0.5838514566421509\n",
      "Epoch 3 -- Batch 26/ 842, training loss 0.5870512127876282\n",
      "Epoch 3 -- Batch 27/ 842, training loss 0.6061617732048035\n",
      "Epoch 3 -- Batch 28/ 842, training loss 0.6210626363754272\n",
      "Epoch 3 -- Batch 29/ 842, training loss 0.6263226866722107\n",
      "Epoch 3 -- Batch 30/ 842, training loss 0.5965897440910339\n",
      "Epoch 3 -- Batch 31/ 842, training loss 0.5855149030685425\n",
      "Epoch 3 -- Batch 32/ 842, training loss 0.5856472849845886\n",
      "Epoch 3 -- Batch 33/ 842, training loss 0.5899994373321533\n",
      "Epoch 3 -- Batch 34/ 842, training loss 0.5821742415428162\n",
      "Epoch 3 -- Batch 35/ 842, training loss 0.5676259398460388\n",
      "Epoch 3 -- Batch 36/ 842, training loss 0.5811085104942322\n",
      "Epoch 3 -- Batch 37/ 842, training loss 0.5939076542854309\n",
      "Epoch 3 -- Batch 38/ 842, training loss 0.6054807901382446\n",
      "Epoch 3 -- Batch 39/ 842, training loss 0.5872681736946106\n",
      "Epoch 3 -- Batch 40/ 842, training loss 0.5926654934883118\n",
      "Epoch 3 -- Batch 41/ 842, training loss 0.5884166955947876\n",
      "Epoch 3 -- Batch 42/ 842, training loss 0.5688036680221558\n",
      "Epoch 3 -- Batch 43/ 842, training loss 0.6241393685340881\n",
      "Epoch 3 -- Batch 44/ 842, training loss 0.5740668177604675\n",
      "Epoch 3 -- Batch 45/ 842, training loss 0.5616389513015747\n",
      "Epoch 3 -- Batch 46/ 842, training loss 0.6074908375740051\n",
      "Epoch 3 -- Batch 47/ 842, training loss 0.5736870169639587\n",
      "Epoch 3 -- Batch 48/ 842, training loss 0.5547909736633301\n",
      "Epoch 3 -- Batch 49/ 842, training loss 0.6225321292877197\n",
      "Epoch 3 -- Batch 50/ 842, training loss 0.5841237306594849\n",
      "Epoch 3 -- Batch 51/ 842, training loss 0.5902597904205322\n",
      "Epoch 3 -- Batch 52/ 842, training loss 0.571029782295227\n",
      "Epoch 3 -- Batch 53/ 842, training loss 0.5911245346069336\n",
      "Epoch 3 -- Batch 54/ 842, training loss 0.6077362895011902\n",
      "Epoch 3 -- Batch 55/ 842, training loss 0.5817571878433228\n",
      "Epoch 3 -- Batch 56/ 842, training loss 0.5983113646507263\n",
      "Epoch 3 -- Batch 57/ 842, training loss 0.5839232206344604\n",
      "Epoch 3 -- Batch 58/ 842, training loss 0.5943213105201721\n",
      "Epoch 3 -- Batch 59/ 842, training loss 0.5740625262260437\n",
      "Epoch 3 -- Batch 60/ 842, training loss 0.6095262765884399\n",
      "Epoch 3 -- Batch 61/ 842, training loss 0.5869386196136475\n",
      "Epoch 3 -- Batch 62/ 842, training loss 0.5911425352096558\n",
      "Epoch 3 -- Batch 63/ 842, training loss 0.5952856540679932\n",
      "Epoch 3 -- Batch 64/ 842, training loss 0.5793009400367737\n",
      "Epoch 3 -- Batch 65/ 842, training loss 0.6094070672988892\n",
      "Epoch 3 -- Batch 66/ 842, training loss 0.602049708366394\n",
      "Epoch 3 -- Batch 67/ 842, training loss 0.5812612771987915\n",
      "Epoch 3 -- Batch 68/ 842, training loss 0.6030064821243286\n",
      "Epoch 3 -- Batch 69/ 842, training loss 0.5958215594291687\n",
      "Epoch 3 -- Batch 70/ 842, training loss 0.606109082698822\n",
      "Epoch 3 -- Batch 71/ 842, training loss 0.6113668084144592\n",
      "Epoch 3 -- Batch 72/ 842, training loss 0.599940299987793\n",
      "Epoch 3 -- Batch 73/ 842, training loss 0.6092402338981628\n",
      "Epoch 3 -- Batch 74/ 842, training loss 0.5989741086959839\n",
      "Epoch 3 -- Batch 75/ 842, training loss 0.5761556029319763\n",
      "Epoch 3 -- Batch 76/ 842, training loss 0.5920153260231018\n",
      "Epoch 3 -- Batch 77/ 842, training loss 0.607620120048523\n",
      "Epoch 3 -- Batch 78/ 842, training loss 0.5923616290092468\n",
      "Epoch 3 -- Batch 79/ 842, training loss 0.5898304581642151\n",
      "Epoch 3 -- Batch 80/ 842, training loss 0.6293923258781433\n",
      "Epoch 3 -- Batch 81/ 842, training loss 0.5933566689491272\n",
      "Epoch 3 -- Batch 82/ 842, training loss 0.5851337909698486\n",
      "Epoch 3 -- Batch 83/ 842, training loss 0.5833036303520203\n",
      "Epoch 3 -- Batch 84/ 842, training loss 0.611103355884552\n",
      "Epoch 3 -- Batch 85/ 842, training loss 0.5828872323036194\n",
      "Epoch 3 -- Batch 86/ 842, training loss 0.5997608304023743\n",
      "Epoch 3 -- Batch 87/ 842, training loss 0.6069015264511108\n",
      "Epoch 3 -- Batch 88/ 842, training loss 0.5933161377906799\n",
      "Epoch 3 -- Batch 89/ 842, training loss 0.599648118019104\n",
      "Epoch 3 -- Batch 90/ 842, training loss 0.6244726777076721\n",
      "Epoch 3 -- Batch 91/ 842, training loss 0.580493688583374\n",
      "Epoch 3 -- Batch 92/ 842, training loss 0.6265156865119934\n",
      "Epoch 3 -- Batch 93/ 842, training loss 0.5857186913490295\n",
      "Epoch 3 -- Batch 94/ 842, training loss 0.5818077325820923\n",
      "Epoch 3 -- Batch 95/ 842, training loss 0.5731983780860901\n",
      "Epoch 3 -- Batch 96/ 842, training loss 0.5586509108543396\n",
      "Epoch 3 -- Batch 97/ 842, training loss 0.5861713886260986\n",
      "Epoch 3 -- Batch 98/ 842, training loss 0.5926916599273682\n",
      "Epoch 3 -- Batch 99/ 842, training loss 0.6052297353744507\n",
      "Epoch 3 -- Batch 100/ 842, training loss 0.5853221416473389\n",
      "Epoch 3 -- Batch 101/ 842, training loss 0.6055324077606201\n",
      "Epoch 3 -- Batch 102/ 842, training loss 0.5785311460494995\n",
      "Epoch 3 -- Batch 103/ 842, training loss 0.603543758392334\n",
      "Epoch 3 -- Batch 104/ 842, training loss 0.5952035784721375\n",
      "Epoch 3 -- Batch 105/ 842, training loss 0.585091233253479\n",
      "Epoch 3 -- Batch 106/ 842, training loss 0.600946843624115\n",
      "Epoch 3 -- Batch 107/ 842, training loss 0.5840820074081421\n",
      "Epoch 3 -- Batch 108/ 842, training loss 0.5930309295654297\n",
      "Epoch 3 -- Batch 109/ 842, training loss 0.6094017028808594\n",
      "Epoch 3 -- Batch 110/ 842, training loss 0.5973489284515381\n",
      "Epoch 3 -- Batch 111/ 842, training loss 0.5902615785598755\n",
      "Epoch 3 -- Batch 112/ 842, training loss 0.6097145080566406\n",
      "Epoch 3 -- Batch 113/ 842, training loss 0.5744802951812744\n",
      "Epoch 3 -- Batch 114/ 842, training loss 0.6151588559150696\n",
      "Epoch 3 -- Batch 115/ 842, training loss 0.5943452715873718\n",
      "Epoch 3 -- Batch 116/ 842, training loss 0.6048550009727478\n",
      "Epoch 3 -- Batch 117/ 842, training loss 0.5860012769699097\n",
      "Epoch 3 -- Batch 118/ 842, training loss 0.6040200591087341\n",
      "Epoch 3 -- Batch 119/ 842, training loss 0.578378438949585\n",
      "Epoch 3 -- Batch 120/ 842, training loss 0.6190174221992493\n",
      "Epoch 3 -- Batch 121/ 842, training loss 0.6132000684738159\n",
      "Epoch 3 -- Batch 122/ 842, training loss 0.587524950504303\n",
      "Epoch 3 -- Batch 123/ 842, training loss 0.5974321365356445\n",
      "Epoch 3 -- Batch 124/ 842, training loss 0.5951306819915771\n",
      "Epoch 3 -- Batch 125/ 842, training loss 0.5838937759399414\n",
      "Epoch 3 -- Batch 126/ 842, training loss 0.5760467052459717\n",
      "Epoch 3 -- Batch 127/ 842, training loss 0.6139828562736511\n",
      "Epoch 3 -- Batch 128/ 842, training loss 0.5713716745376587\n",
      "Epoch 3 -- Batch 129/ 842, training loss 0.5999122262001038\n",
      "Epoch 3 -- Batch 130/ 842, training loss 0.6150620579719543\n",
      "Epoch 3 -- Batch 131/ 842, training loss 0.585696280002594\n",
      "Epoch 3 -- Batch 132/ 842, training loss 0.5598470568656921\n",
      "Epoch 3 -- Batch 133/ 842, training loss 0.5874181985855103\n",
      "Epoch 3 -- Batch 134/ 842, training loss 0.5916396975517273\n",
      "Epoch 3 -- Batch 135/ 842, training loss 0.6087479591369629\n",
      "Epoch 3 -- Batch 136/ 842, training loss 0.5930553078651428\n",
      "Epoch 3 -- Batch 137/ 842, training loss 0.5825111269950867\n",
      "Epoch 3 -- Batch 138/ 842, training loss 0.585273265838623\n",
      "Epoch 3 -- Batch 139/ 842, training loss 0.5850686430931091\n",
      "Epoch 3 -- Batch 140/ 842, training loss 0.5866724848747253\n",
      "Epoch 3 -- Batch 141/ 842, training loss 0.6095702648162842\n",
      "Epoch 3 -- Batch 142/ 842, training loss 0.597696840763092\n",
      "Epoch 3 -- Batch 143/ 842, training loss 0.6057044267654419\n",
      "Epoch 3 -- Batch 144/ 842, training loss 0.5574837327003479\n",
      "Epoch 3 -- Batch 145/ 842, training loss 0.5794357061386108\n",
      "Epoch 3 -- Batch 146/ 842, training loss 0.5790759921073914\n",
      "Epoch 3 -- Batch 147/ 842, training loss 0.5681014657020569\n",
      "Epoch 3 -- Batch 148/ 842, training loss 0.5918990969657898\n",
      "Epoch 3 -- Batch 149/ 842, training loss 0.5849682092666626\n",
      "Epoch 3 -- Batch 150/ 842, training loss 0.5987390279769897\n",
      "Epoch 3 -- Batch 151/ 842, training loss 0.585354745388031\n",
      "Epoch 3 -- Batch 152/ 842, training loss 0.5602262616157532\n",
      "Epoch 3 -- Batch 153/ 842, training loss 0.5760229229927063\n",
      "Epoch 3 -- Batch 154/ 842, training loss 0.581449031829834\n",
      "Epoch 3 -- Batch 155/ 842, training loss 0.5935505032539368\n",
      "Epoch 3 -- Batch 156/ 842, training loss 0.5754172205924988\n",
      "Epoch 3 -- Batch 157/ 842, training loss 0.5703099966049194\n",
      "Epoch 3 -- Batch 158/ 842, training loss 0.5781999826431274\n",
      "Epoch 3 -- Batch 159/ 842, training loss 0.6094123125076294\n",
      "Epoch 3 -- Batch 160/ 842, training loss 0.6009097099304199\n",
      "Epoch 3 -- Batch 161/ 842, training loss 0.5946827530860901\n",
      "Epoch 3 -- Batch 162/ 842, training loss 0.5968421697616577\n",
      "Epoch 3 -- Batch 163/ 842, training loss 0.6040989756584167\n",
      "Epoch 3 -- Batch 164/ 842, training loss 0.5790736675262451\n",
      "Epoch 3 -- Batch 165/ 842, training loss 0.5737478733062744\n",
      "Epoch 3 -- Batch 166/ 842, training loss 0.6304630041122437\n",
      "Epoch 3 -- Batch 167/ 842, training loss 0.5907295942306519\n",
      "Epoch 3 -- Batch 168/ 842, training loss 0.5800575613975525\n",
      "Epoch 3 -- Batch 169/ 842, training loss 0.5658794045448303\n",
      "Epoch 3 -- Batch 170/ 842, training loss 0.5687397122383118\n",
      "Epoch 3 -- Batch 171/ 842, training loss 0.5920233726501465\n",
      "Epoch 3 -- Batch 172/ 842, training loss 0.6180949211120605\n",
      "Epoch 3 -- Batch 173/ 842, training loss 0.5995123386383057\n",
      "Epoch 3 -- Batch 174/ 842, training loss 0.6079295873641968\n",
      "Epoch 3 -- Batch 175/ 842, training loss 0.5799756050109863\n",
      "Epoch 3 -- Batch 176/ 842, training loss 0.5850128531455994\n",
      "Epoch 3 -- Batch 177/ 842, training loss 0.5907015800476074\n",
      "Epoch 3 -- Batch 178/ 842, training loss 0.5640841126441956\n",
      "Epoch 3 -- Batch 179/ 842, training loss 0.5887219309806824\n",
      "Epoch 3 -- Batch 180/ 842, training loss 0.5975217223167419\n",
      "Epoch 3 -- Batch 181/ 842, training loss 0.573845386505127\n",
      "Epoch 3 -- Batch 182/ 842, training loss 0.569920539855957\n",
      "Epoch 3 -- Batch 183/ 842, training loss 0.6067174077033997\n",
      "Epoch 3 -- Batch 184/ 842, training loss 0.577292799949646\n",
      "Epoch 3 -- Batch 185/ 842, training loss 0.5849844217300415\n",
      "Epoch 3 -- Batch 186/ 842, training loss 0.5791882276535034\n",
      "Epoch 3 -- Batch 187/ 842, training loss 0.5789391994476318\n",
      "Epoch 3 -- Batch 188/ 842, training loss 0.5849847793579102\n",
      "Epoch 3 -- Batch 189/ 842, training loss 0.592976450920105\n",
      "Epoch 3 -- Batch 190/ 842, training loss 0.564850926399231\n",
      "Epoch 3 -- Batch 191/ 842, training loss 0.5779882073402405\n",
      "Epoch 3 -- Batch 192/ 842, training loss 0.607900083065033\n",
      "Epoch 3 -- Batch 193/ 842, training loss 0.5857709646224976\n",
      "Epoch 3 -- Batch 194/ 842, training loss 0.5820143222808838\n",
      "Epoch 3 -- Batch 195/ 842, training loss 0.5841743350028992\n",
      "Epoch 3 -- Batch 196/ 842, training loss 0.5945465564727783\n",
      "Epoch 3 -- Batch 197/ 842, training loss 0.6002911329269409\n",
      "Epoch 3 -- Batch 198/ 842, training loss 0.5890560746192932\n",
      "Epoch 3 -- Batch 199/ 842, training loss 0.6105819344520569\n",
      "Epoch 3 -- Batch 200/ 842, training loss 0.5849395990371704\n",
      "Epoch 3 -- Batch 201/ 842, training loss 0.6153622269630432\n",
      "Epoch 3 -- Batch 202/ 842, training loss 0.5983882546424866\n",
      "Epoch 3 -- Batch 203/ 842, training loss 0.6060875058174133\n",
      "Epoch 3 -- Batch 204/ 842, training loss 0.5544564723968506\n",
      "Epoch 3 -- Batch 205/ 842, training loss 0.5949208736419678\n",
      "Epoch 3 -- Batch 206/ 842, training loss 0.5893086791038513\n",
      "Epoch 3 -- Batch 207/ 842, training loss 0.5801960825920105\n",
      "Epoch 3 -- Batch 208/ 842, training loss 0.5954355001449585\n",
      "Epoch 3 -- Batch 209/ 842, training loss 0.5735288858413696\n",
      "Epoch 3 -- Batch 210/ 842, training loss 0.6129817962646484\n",
      "Epoch 3 -- Batch 211/ 842, training loss 0.5770962238311768\n",
      "Epoch 3 -- Batch 212/ 842, training loss 0.6281173229217529\n",
      "Epoch 3 -- Batch 213/ 842, training loss 0.5843170881271362\n",
      "Epoch 3 -- Batch 214/ 842, training loss 0.5765175819396973\n",
      "Epoch 3 -- Batch 215/ 842, training loss 0.5845395922660828\n",
      "Epoch 3 -- Batch 216/ 842, training loss 0.5736075043678284\n",
      "Epoch 3 -- Batch 217/ 842, training loss 0.578139066696167\n",
      "Epoch 3 -- Batch 218/ 842, training loss 0.5821735262870789\n",
      "Epoch 3 -- Batch 219/ 842, training loss 0.5912178754806519\n",
      "Epoch 3 -- Batch 220/ 842, training loss 0.5832725763320923\n",
      "Epoch 3 -- Batch 221/ 842, training loss 0.593255341053009\n",
      "Epoch 3 -- Batch 222/ 842, training loss 0.589238703250885\n",
      "Epoch 3 -- Batch 223/ 842, training loss 0.5791773200035095\n",
      "Epoch 3 -- Batch 224/ 842, training loss 0.6156742572784424\n",
      "Epoch 3 -- Batch 225/ 842, training loss 0.5688962936401367\n",
      "Epoch 3 -- Batch 226/ 842, training loss 0.5848110318183899\n",
      "Epoch 3 -- Batch 227/ 842, training loss 0.6030088663101196\n",
      "Epoch 3 -- Batch 228/ 842, training loss 0.577394425868988\n",
      "Epoch 3 -- Batch 229/ 842, training loss 0.572090208530426\n",
      "Epoch 3 -- Batch 230/ 842, training loss 0.5872692465782166\n",
      "Epoch 3 -- Batch 231/ 842, training loss 0.6001585721969604\n",
      "Epoch 3 -- Batch 232/ 842, training loss 0.6129581332206726\n",
      "Epoch 3 -- Batch 233/ 842, training loss 0.5696396231651306\n",
      "Epoch 3 -- Batch 234/ 842, training loss 0.576224684715271\n",
      "Epoch 3 -- Batch 235/ 842, training loss 0.5806001424789429\n",
      "Epoch 3 -- Batch 236/ 842, training loss 0.5837878584861755\n",
      "Epoch 3 -- Batch 237/ 842, training loss 0.5785588622093201\n",
      "Epoch 3 -- Batch 238/ 842, training loss 0.5889732837677002\n",
      "Epoch 3 -- Batch 239/ 842, training loss 0.6196348071098328\n",
      "Epoch 3 -- Batch 240/ 842, training loss 0.61265629529953\n",
      "Epoch 3 -- Batch 241/ 842, training loss 0.591341495513916\n",
      "Epoch 3 -- Batch 242/ 842, training loss 0.5750433802604675\n",
      "Epoch 3 -- Batch 243/ 842, training loss 0.6074919700622559\n",
      "Epoch 3 -- Batch 244/ 842, training loss 0.5807526111602783\n",
      "Epoch 3 -- Batch 245/ 842, training loss 0.5943968296051025\n",
      "Epoch 3 -- Batch 246/ 842, training loss 0.5596322417259216\n",
      "Epoch 3 -- Batch 247/ 842, training loss 0.5867955088615417\n",
      "Epoch 3 -- Batch 248/ 842, training loss 0.5798767805099487\n",
      "Epoch 3 -- Batch 249/ 842, training loss 0.5781117081642151\n",
      "Epoch 3 -- Batch 250/ 842, training loss 0.5672106146812439\n",
      "Epoch 3 -- Batch 251/ 842, training loss 0.6021551489830017\n",
      "Epoch 3 -- Batch 252/ 842, training loss 0.6091280579566956\n",
      "Epoch 3 -- Batch 253/ 842, training loss 0.5534394383430481\n",
      "Epoch 3 -- Batch 254/ 842, training loss 0.5784009695053101\n",
      "Epoch 3 -- Batch 255/ 842, training loss 0.5639533400535583\n",
      "Epoch 3 -- Batch 256/ 842, training loss 0.5648007392883301\n",
      "Epoch 3 -- Batch 257/ 842, training loss 0.591245174407959\n",
      "Epoch 3 -- Batch 258/ 842, training loss 0.5951505899429321\n",
      "Epoch 3 -- Batch 259/ 842, training loss 0.5817839503288269\n",
      "Epoch 3 -- Batch 260/ 842, training loss 0.5753263831138611\n",
      "Epoch 3 -- Batch 261/ 842, training loss 0.5997032523155212\n",
      "Epoch 3 -- Batch 262/ 842, training loss 0.594754159450531\n",
      "Epoch 3 -- Batch 263/ 842, training loss 0.543281078338623\n",
      "Epoch 3 -- Batch 264/ 842, training loss 0.5828093886375427\n",
      "Epoch 3 -- Batch 265/ 842, training loss 0.5827195644378662\n",
      "Epoch 3 -- Batch 266/ 842, training loss 0.5739102363586426\n",
      "Epoch 3 -- Batch 267/ 842, training loss 0.5863040685653687\n",
      "Epoch 3 -- Batch 268/ 842, training loss 0.6199038028717041\n",
      "Epoch 3 -- Batch 269/ 842, training loss 0.5727458000183105\n",
      "Epoch 3 -- Batch 270/ 842, training loss 0.584630012512207\n",
      "Epoch 3 -- Batch 271/ 842, training loss 0.5937256813049316\n",
      "Epoch 3 -- Batch 272/ 842, training loss 0.5760028958320618\n",
      "Epoch 3 -- Batch 273/ 842, training loss 0.5687176585197449\n",
      "Epoch 3 -- Batch 274/ 842, training loss 0.573998749256134\n",
      "Epoch 3 -- Batch 275/ 842, training loss 0.5966858267784119\n",
      "Epoch 3 -- Batch 276/ 842, training loss 0.5880926251411438\n",
      "Epoch 3 -- Batch 277/ 842, training loss 0.5788139700889587\n",
      "Epoch 3 -- Batch 278/ 842, training loss 0.5847728252410889\n",
      "Epoch 3 -- Batch 279/ 842, training loss 0.5823250412940979\n",
      "Epoch 3 -- Batch 280/ 842, training loss 0.6050737500190735\n",
      "Epoch 3 -- Batch 281/ 842, training loss 0.5991626381874084\n",
      "Epoch 3 -- Batch 282/ 842, training loss 0.6077913045883179\n",
      "Epoch 3 -- Batch 283/ 842, training loss 0.5580540299415588\n",
      "Epoch 3 -- Batch 284/ 842, training loss 0.5849348902702332\n",
      "Epoch 3 -- Batch 285/ 842, training loss 0.5754510760307312\n",
      "Epoch 3 -- Batch 286/ 842, training loss 0.5885645151138306\n",
      "Epoch 3 -- Batch 287/ 842, training loss 0.5833966732025146\n",
      "Epoch 3 -- Batch 288/ 842, training loss 0.595160186290741\n",
      "Epoch 3 -- Batch 289/ 842, training loss 0.5900164842605591\n",
      "Epoch 3 -- Batch 290/ 842, training loss 0.5701180100440979\n",
      "Epoch 3 -- Batch 291/ 842, training loss 0.5940360426902771\n",
      "Epoch 3 -- Batch 292/ 842, training loss 0.5704162120819092\n",
      "Epoch 3 -- Batch 293/ 842, training loss 0.570932149887085\n",
      "Epoch 3 -- Batch 294/ 842, training loss 0.5652899742126465\n",
      "Epoch 3 -- Batch 295/ 842, training loss 0.5936789512634277\n",
      "Epoch 3 -- Batch 296/ 842, training loss 0.6105722784996033\n",
      "Epoch 3 -- Batch 297/ 842, training loss 0.5888258218765259\n",
      "Epoch 3 -- Batch 298/ 842, training loss 0.5748109221458435\n",
      "Epoch 3 -- Batch 299/ 842, training loss 0.5839947462081909\n",
      "Epoch 3 -- Batch 300/ 842, training loss 0.6083554625511169\n",
      "Epoch 3 -- Batch 301/ 842, training loss 0.5720729827880859\n",
      "Epoch 3 -- Batch 302/ 842, training loss 0.5740742683410645\n",
      "Epoch 3 -- Batch 303/ 842, training loss 0.6098450422286987\n",
      "Epoch 3 -- Batch 304/ 842, training loss 0.5852594375610352\n",
      "Epoch 3 -- Batch 305/ 842, training loss 0.5968878269195557\n",
      "Epoch 3 -- Batch 306/ 842, training loss 0.5606035590171814\n",
      "Epoch 3 -- Batch 307/ 842, training loss 0.5636235475540161\n",
      "Epoch 3 -- Batch 308/ 842, training loss 0.5746071338653564\n",
      "Epoch 3 -- Batch 309/ 842, training loss 0.6026365756988525\n",
      "Epoch 3 -- Batch 310/ 842, training loss 0.5912523865699768\n",
      "Epoch 3 -- Batch 311/ 842, training loss 0.5888974666595459\n",
      "Epoch 3 -- Batch 312/ 842, training loss 0.5645833015441895\n",
      "Epoch 3 -- Batch 313/ 842, training loss 0.5827043056488037\n",
      "Epoch 3 -- Batch 314/ 842, training loss 0.559270441532135\n",
      "Epoch 3 -- Batch 315/ 842, training loss 0.5775492191314697\n",
      "Epoch 3 -- Batch 316/ 842, training loss 0.5783513784408569\n",
      "Epoch 3 -- Batch 317/ 842, training loss 0.5836542844772339\n",
      "Epoch 3 -- Batch 318/ 842, training loss 0.5990058183670044\n",
      "Epoch 3 -- Batch 319/ 842, training loss 0.5964294075965881\n",
      "Epoch 3 -- Batch 320/ 842, training loss 0.58815598487854\n",
      "Epoch 3 -- Batch 321/ 842, training loss 0.5867336988449097\n",
      "Epoch 3 -- Batch 322/ 842, training loss 0.5876165628433228\n",
      "Epoch 3 -- Batch 323/ 842, training loss 0.5568475127220154\n",
      "Epoch 3 -- Batch 324/ 842, training loss 0.5777652263641357\n",
      "Epoch 3 -- Batch 325/ 842, training loss 0.5822051763534546\n",
      "Epoch 3 -- Batch 326/ 842, training loss 0.5510955452919006\n",
      "Epoch 3 -- Batch 327/ 842, training loss 0.580293595790863\n",
      "Epoch 3 -- Batch 328/ 842, training loss 0.5967789888381958\n",
      "Epoch 3 -- Batch 329/ 842, training loss 0.5801617503166199\n",
      "Epoch 3 -- Batch 330/ 842, training loss 0.5611177682876587\n",
      "Epoch 3 -- Batch 331/ 842, training loss 0.5738579034805298\n",
      "Epoch 3 -- Batch 332/ 842, training loss 0.59222012758255\n",
      "Epoch 3 -- Batch 333/ 842, training loss 0.5687099695205688\n",
      "Epoch 3 -- Batch 334/ 842, training loss 0.5894842147827148\n",
      "Epoch 3 -- Batch 335/ 842, training loss 0.5919629335403442\n",
      "Epoch 3 -- Batch 336/ 842, training loss 0.5754272937774658\n",
      "Epoch 3 -- Batch 337/ 842, training loss 0.578698992729187\n",
      "Epoch 3 -- Batch 338/ 842, training loss 0.5889232158660889\n",
      "Epoch 3 -- Batch 339/ 842, training loss 0.5789644122123718\n",
      "Epoch 3 -- Batch 340/ 842, training loss 0.5643128752708435\n",
      "Epoch 3 -- Batch 341/ 842, training loss 0.5711867213249207\n",
      "Epoch 3 -- Batch 342/ 842, training loss 0.5732977390289307\n",
      "Epoch 3 -- Batch 343/ 842, training loss 0.5676608085632324\n",
      "Epoch 3 -- Batch 344/ 842, training loss 0.5675752758979797\n",
      "Epoch 3 -- Batch 345/ 842, training loss 0.5682534575462341\n",
      "Epoch 3 -- Batch 346/ 842, training loss 0.5737495422363281\n",
      "Epoch 3 -- Batch 347/ 842, training loss 0.5614792108535767\n",
      "Epoch 3 -- Batch 348/ 842, training loss 0.567036509513855\n",
      "Epoch 3 -- Batch 349/ 842, training loss 0.6041096448898315\n",
      "Epoch 3 -- Batch 350/ 842, training loss 0.5764798521995544\n",
      "Epoch 3 -- Batch 351/ 842, training loss 0.5690323114395142\n",
      "Epoch 3 -- Batch 352/ 842, training loss 0.5764603018760681\n",
      "Epoch 3 -- Batch 353/ 842, training loss 0.5988030433654785\n",
      "Epoch 3 -- Batch 354/ 842, training loss 0.5801907181739807\n",
      "Epoch 3 -- Batch 355/ 842, training loss 0.5737400054931641\n",
      "Epoch 3 -- Batch 356/ 842, training loss 0.5668507218360901\n",
      "Epoch 3 -- Batch 357/ 842, training loss 0.5722330808639526\n",
      "Epoch 3 -- Batch 358/ 842, training loss 0.566278874874115\n",
      "Epoch 3 -- Batch 359/ 842, training loss 0.594455361366272\n",
      "Epoch 3 -- Batch 360/ 842, training loss 0.5666581988334656\n",
      "Epoch 3 -- Batch 361/ 842, training loss 0.5833888649940491\n",
      "Epoch 3 -- Batch 362/ 842, training loss 0.5789642930030823\n",
      "Epoch 3 -- Batch 363/ 842, training loss 0.5771298408508301\n",
      "Epoch 3 -- Batch 364/ 842, training loss 0.5767378211021423\n",
      "Epoch 3 -- Batch 365/ 842, training loss 0.5582161545753479\n",
      "Epoch 3 -- Batch 366/ 842, training loss 0.5788522958755493\n",
      "Epoch 3 -- Batch 367/ 842, training loss 0.5797803997993469\n",
      "Epoch 3 -- Batch 368/ 842, training loss 0.5936259031295776\n",
      "Epoch 3 -- Batch 369/ 842, training loss 0.5735032558441162\n",
      "Epoch 3 -- Batch 370/ 842, training loss 0.5845179557800293\n",
      "Epoch 3 -- Batch 371/ 842, training loss 0.5537039637565613\n",
      "Epoch 3 -- Batch 372/ 842, training loss 0.5933527946472168\n",
      "Epoch 3 -- Batch 373/ 842, training loss 0.5856748819351196\n",
      "Epoch 3 -- Batch 374/ 842, training loss 0.5966500639915466\n",
      "Epoch 3 -- Batch 375/ 842, training loss 0.6050838232040405\n",
      "Epoch 3 -- Batch 376/ 842, training loss 0.5837902426719666\n",
      "Epoch 3 -- Batch 377/ 842, training loss 0.5751612186431885\n",
      "Epoch 3 -- Batch 378/ 842, training loss 0.5650702714920044\n",
      "Epoch 3 -- Batch 379/ 842, training loss 0.5809600949287415\n",
      "Epoch 3 -- Batch 380/ 842, training loss 0.5673817992210388\n",
      "Epoch 3 -- Batch 381/ 842, training loss 0.5719388723373413\n",
      "Epoch 3 -- Batch 382/ 842, training loss 0.5799890756607056\n",
      "Epoch 3 -- Batch 383/ 842, training loss 0.5637292861938477\n",
      "Epoch 3 -- Batch 384/ 842, training loss 0.5927297472953796\n",
      "Epoch 3 -- Batch 385/ 842, training loss 0.565162718296051\n",
      "Epoch 3 -- Batch 386/ 842, training loss 0.5422462224960327\n",
      "Epoch 3 -- Batch 387/ 842, training loss 0.5750417113304138\n",
      "Epoch 3 -- Batch 388/ 842, training loss 0.5538889169692993\n",
      "Epoch 3 -- Batch 389/ 842, training loss 0.5761052966117859\n",
      "Epoch 3 -- Batch 390/ 842, training loss 0.588438093662262\n",
      "Epoch 3 -- Batch 391/ 842, training loss 0.5657971501350403\n",
      "Epoch 3 -- Batch 392/ 842, training loss 0.5606444478034973\n",
      "Epoch 3 -- Batch 393/ 842, training loss 0.5467814803123474\n",
      "Epoch 3 -- Batch 394/ 842, training loss 0.554617166519165\n",
      "Epoch 3 -- Batch 395/ 842, training loss 0.5859613418579102\n",
      "Epoch 3 -- Batch 396/ 842, training loss 0.5623143911361694\n",
      "Epoch 3 -- Batch 397/ 842, training loss 0.5700016617774963\n",
      "Epoch 3 -- Batch 398/ 842, training loss 0.6004983186721802\n",
      "Epoch 3 -- Batch 399/ 842, training loss 0.5870518088340759\n",
      "Epoch 3 -- Batch 400/ 842, training loss 0.5616710782051086\n",
      "Epoch 3 -- Batch 401/ 842, training loss 0.5911927819252014\n",
      "Epoch 3 -- Batch 402/ 842, training loss 0.5594452619552612\n",
      "Epoch 3 -- Batch 403/ 842, training loss 0.5619056224822998\n",
      "Epoch 3 -- Batch 404/ 842, training loss 0.5731800198554993\n",
      "Epoch 3 -- Batch 405/ 842, training loss 0.5784734487533569\n",
      "Epoch 3 -- Batch 406/ 842, training loss 0.5936214327812195\n",
      "Epoch 3 -- Batch 407/ 842, training loss 0.5511143803596497\n",
      "Epoch 3 -- Batch 408/ 842, training loss 0.5840264558792114\n",
      "Epoch 3 -- Batch 409/ 842, training loss 0.5771452188491821\n",
      "Epoch 3 -- Batch 410/ 842, training loss 0.60999995470047\n",
      "Epoch 3 -- Batch 411/ 842, training loss 0.5481060743331909\n",
      "Epoch 3 -- Batch 412/ 842, training loss 0.5943939685821533\n",
      "Epoch 3 -- Batch 413/ 842, training loss 0.6252058148384094\n",
      "Epoch 3 -- Batch 414/ 842, training loss 0.5514832139015198\n",
      "Epoch 3 -- Batch 415/ 842, training loss 0.5738422870635986\n",
      "Epoch 3 -- Batch 416/ 842, training loss 0.5719099640846252\n",
      "Epoch 3 -- Batch 417/ 842, training loss 0.5500066876411438\n",
      "Epoch 3 -- Batch 418/ 842, training loss 0.5687176585197449\n",
      "Epoch 3 -- Batch 419/ 842, training loss 0.5852051377296448\n",
      "Epoch 3 -- Batch 420/ 842, training loss 0.5814887881278992\n",
      "Epoch 3 -- Batch 421/ 842, training loss 0.6020622253417969\n",
      "Epoch 3 -- Batch 422/ 842, training loss 0.5893909931182861\n",
      "Epoch 3 -- Batch 423/ 842, training loss 0.5821792483329773\n",
      "Epoch 3 -- Batch 424/ 842, training loss 0.5791059136390686\n",
      "Epoch 3 -- Batch 425/ 842, training loss 0.5946664214134216\n",
      "Epoch 3 -- Batch 426/ 842, training loss 0.563278079032898\n",
      "Epoch 3 -- Batch 427/ 842, training loss 0.553178608417511\n",
      "Epoch 3 -- Batch 428/ 842, training loss 0.5868622064590454\n",
      "Epoch 3 -- Batch 429/ 842, training loss 0.6151632070541382\n",
      "Epoch 3 -- Batch 430/ 842, training loss 0.5967991948127747\n",
      "Epoch 3 -- Batch 431/ 842, training loss 0.598716139793396\n",
      "Epoch 3 -- Batch 432/ 842, training loss 0.575799286365509\n",
      "Epoch 3 -- Batch 433/ 842, training loss 0.5605201125144958\n",
      "Epoch 3 -- Batch 434/ 842, training loss 0.5878911018371582\n",
      "Epoch 3 -- Batch 435/ 842, training loss 0.5760577917098999\n",
      "Epoch 3 -- Batch 436/ 842, training loss 0.5812250971794128\n",
      "Epoch 3 -- Batch 437/ 842, training loss 0.5750781893730164\n",
      "Epoch 3 -- Batch 438/ 842, training loss 0.5960859060287476\n",
      "Epoch 3 -- Batch 439/ 842, training loss 0.5636202096939087\n",
      "Epoch 3 -- Batch 440/ 842, training loss 0.5859061479568481\n",
      "Epoch 3 -- Batch 441/ 842, training loss 0.5688679814338684\n",
      "Epoch 3 -- Batch 442/ 842, training loss 0.5780174136161804\n",
      "Epoch 3 -- Batch 443/ 842, training loss 0.5619717836380005\n",
      "Epoch 3 -- Batch 444/ 842, training loss 0.5660194158554077\n",
      "Epoch 3 -- Batch 445/ 842, training loss 0.5803590416908264\n",
      "Epoch 3 -- Batch 446/ 842, training loss 0.5728403925895691\n",
      "Epoch 3 -- Batch 447/ 842, training loss 0.6029071807861328\n",
      "Epoch 3 -- Batch 448/ 842, training loss 0.5773284435272217\n",
      "Epoch 3 -- Batch 449/ 842, training loss 0.5703900456428528\n",
      "Epoch 3 -- Batch 450/ 842, training loss 0.5718590617179871\n",
      "Epoch 3 -- Batch 451/ 842, training loss 0.5557535886764526\n",
      "Epoch 3 -- Batch 452/ 842, training loss 0.5826253294944763\n",
      "Epoch 3 -- Batch 453/ 842, training loss 0.5702255368232727\n",
      "Epoch 3 -- Batch 454/ 842, training loss 0.5740991234779358\n",
      "Epoch 3 -- Batch 455/ 842, training loss 0.5695256590843201\n",
      "Epoch 3 -- Batch 456/ 842, training loss 0.6048359870910645\n",
      "Epoch 3 -- Batch 457/ 842, training loss 0.5765501856803894\n",
      "Epoch 3 -- Batch 458/ 842, training loss 0.5560171604156494\n",
      "Epoch 3 -- Batch 459/ 842, training loss 0.5739387273788452\n",
      "Epoch 3 -- Batch 460/ 842, training loss 0.5812235474586487\n",
      "Epoch 3 -- Batch 461/ 842, training loss 0.5798247456550598\n",
      "Epoch 3 -- Batch 462/ 842, training loss 0.5804183483123779\n",
      "Epoch 3 -- Batch 463/ 842, training loss 0.5631443858146667\n",
      "Epoch 3 -- Batch 464/ 842, training loss 0.5813717842102051\n",
      "Epoch 3 -- Batch 465/ 842, training loss 0.5710975527763367\n",
      "Epoch 3 -- Batch 466/ 842, training loss 0.5818563103675842\n",
      "Epoch 3 -- Batch 467/ 842, training loss 0.563859224319458\n",
      "Epoch 3 -- Batch 468/ 842, training loss 0.5510146617889404\n",
      "Epoch 3 -- Batch 469/ 842, training loss 0.5755850076675415\n",
      "Epoch 3 -- Batch 470/ 842, training loss 0.5722436904907227\n",
      "Epoch 3 -- Batch 471/ 842, training loss 0.5615821480751038\n",
      "Epoch 3 -- Batch 472/ 842, training loss 0.5797807574272156\n",
      "Epoch 3 -- Batch 473/ 842, training loss 0.5813195109367371\n",
      "Epoch 3 -- Batch 474/ 842, training loss 0.5620990991592407\n",
      "Epoch 3 -- Batch 475/ 842, training loss 0.5533708333969116\n",
      "Epoch 3 -- Batch 476/ 842, training loss 0.542008101940155\n",
      "Epoch 3 -- Batch 477/ 842, training loss 0.5963348150253296\n",
      "Epoch 3 -- Batch 478/ 842, training loss 0.5856874585151672\n",
      "Epoch 3 -- Batch 479/ 842, training loss 0.5757461786270142\n",
      "Epoch 3 -- Batch 480/ 842, training loss 0.5894066691398621\n",
      "Epoch 3 -- Batch 481/ 842, training loss 0.5826408863067627\n",
      "Epoch 3 -- Batch 482/ 842, training loss 0.576935350894928\n",
      "Epoch 3 -- Batch 483/ 842, training loss 0.5873169898986816\n",
      "Epoch 3 -- Batch 484/ 842, training loss 0.5830956697463989\n",
      "Epoch 3 -- Batch 485/ 842, training loss 0.5486497282981873\n",
      "Epoch 3 -- Batch 486/ 842, training loss 0.5733172297477722\n",
      "Epoch 3 -- Batch 487/ 842, training loss 0.5584200024604797\n",
      "Epoch 3 -- Batch 488/ 842, training loss 0.5631689429283142\n",
      "Epoch 3 -- Batch 489/ 842, training loss 0.5832000970840454\n",
      "Epoch 3 -- Batch 490/ 842, training loss 0.5947344303131104\n",
      "Epoch 3 -- Batch 491/ 842, training loss 0.5771160125732422\n",
      "Epoch 3 -- Batch 492/ 842, training loss 0.5815485715866089\n",
      "Epoch 3 -- Batch 493/ 842, training loss 0.582399308681488\n",
      "Epoch 3 -- Batch 494/ 842, training loss 0.5803526639938354\n",
      "Epoch 3 -- Batch 495/ 842, training loss 0.5952721834182739\n",
      "Epoch 3 -- Batch 496/ 842, training loss 0.5542557835578918\n",
      "Epoch 3 -- Batch 497/ 842, training loss 0.581634521484375\n",
      "Epoch 3 -- Batch 498/ 842, training loss 0.5661319494247437\n",
      "Epoch 3 -- Batch 499/ 842, training loss 0.5863447785377502\n",
      "Epoch 3 -- Batch 500/ 842, training loss 0.5771490335464478\n",
      "Epoch 3 -- Batch 501/ 842, training loss 0.5621294379234314\n",
      "Epoch 3 -- Batch 502/ 842, training loss 0.551038384437561\n",
      "Epoch 3 -- Batch 503/ 842, training loss 0.5459222793579102\n",
      "Epoch 3 -- Batch 504/ 842, training loss 0.5557777285575867\n",
      "Epoch 3 -- Batch 505/ 842, training loss 0.5757859945297241\n",
      "Epoch 3 -- Batch 506/ 842, training loss 0.5486014485359192\n",
      "Epoch 3 -- Batch 507/ 842, training loss 0.5787521600723267\n",
      "Epoch 3 -- Batch 508/ 842, training loss 0.566034734249115\n",
      "Epoch 3 -- Batch 509/ 842, training loss 0.579953134059906\n",
      "Epoch 3 -- Batch 510/ 842, training loss 0.5928831696510315\n",
      "Epoch 3 -- Batch 511/ 842, training loss 0.5720742344856262\n",
      "Epoch 3 -- Batch 512/ 842, training loss 0.5598044991493225\n",
      "Epoch 3 -- Batch 513/ 842, training loss 0.5471808314323425\n",
      "Epoch 3 -- Batch 514/ 842, training loss 0.5719195008277893\n",
      "Epoch 3 -- Batch 515/ 842, training loss 0.5416659712791443\n",
      "Epoch 3 -- Batch 516/ 842, training loss 0.5953563451766968\n",
      "Epoch 3 -- Batch 517/ 842, training loss 0.5530807971954346\n",
      "Epoch 3 -- Batch 518/ 842, training loss 0.5805842876434326\n",
      "Epoch 3 -- Batch 519/ 842, training loss 0.5284165143966675\n",
      "Epoch 3 -- Batch 520/ 842, training loss 0.5690732598304749\n",
      "Epoch 3 -- Batch 521/ 842, training loss 0.5569536089897156\n",
      "Epoch 3 -- Batch 522/ 842, training loss 0.5800870060920715\n",
      "Epoch 3 -- Batch 523/ 842, training loss 0.5531779527664185\n",
      "Epoch 3 -- Batch 524/ 842, training loss 0.5582892298698425\n",
      "Epoch 3 -- Batch 525/ 842, training loss 0.5481806993484497\n",
      "Epoch 3 -- Batch 526/ 842, training loss 0.5957558155059814\n",
      "Epoch 3 -- Batch 527/ 842, training loss 0.5766728520393372\n",
      "Epoch 3 -- Batch 528/ 842, training loss 0.5679228901863098\n",
      "Epoch 3 -- Batch 529/ 842, training loss 0.5866333246231079\n",
      "Epoch 3 -- Batch 530/ 842, training loss 0.5662168860435486\n",
      "Epoch 3 -- Batch 531/ 842, training loss 0.548835813999176\n",
      "Epoch 3 -- Batch 532/ 842, training loss 0.5768629312515259\n",
      "Epoch 3 -- Batch 533/ 842, training loss 0.5672500133514404\n",
      "Epoch 3 -- Batch 534/ 842, training loss 0.5602860450744629\n",
      "Epoch 3 -- Batch 535/ 842, training loss 0.5729578733444214\n",
      "Epoch 3 -- Batch 536/ 842, training loss 0.5625202655792236\n",
      "Epoch 3 -- Batch 537/ 842, training loss 0.566207230091095\n",
      "Epoch 3 -- Batch 538/ 842, training loss 0.5611960887908936\n",
      "Epoch 3 -- Batch 539/ 842, training loss 0.5706935524940491\n",
      "Epoch 3 -- Batch 540/ 842, training loss 0.5715137124061584\n",
      "Epoch 3 -- Batch 541/ 842, training loss 0.5753348469734192\n",
      "Epoch 3 -- Batch 542/ 842, training loss 0.5576982498168945\n",
      "Epoch 3 -- Batch 543/ 842, training loss 0.5497832894325256\n",
      "Epoch 3 -- Batch 544/ 842, training loss 0.573745846748352\n",
      "Epoch 3 -- Batch 545/ 842, training loss 0.5622350573539734\n",
      "Epoch 3 -- Batch 546/ 842, training loss 0.5720000267028809\n",
      "Epoch 3 -- Batch 547/ 842, training loss 0.5468273162841797\n",
      "Epoch 3 -- Batch 548/ 842, training loss 0.5709431767463684\n",
      "Epoch 3 -- Batch 549/ 842, training loss 0.5657584071159363\n",
      "Epoch 3 -- Batch 550/ 842, training loss 0.5609778761863708\n",
      "Epoch 3 -- Batch 551/ 842, training loss 0.5639709234237671\n",
      "Epoch 3 -- Batch 552/ 842, training loss 0.5643079876899719\n",
      "Epoch 3 -- Batch 553/ 842, training loss 0.573204755783081\n",
      "Epoch 3 -- Batch 554/ 842, training loss 0.5806193351745605\n",
      "Epoch 3 -- Batch 555/ 842, training loss 0.57431560754776\n",
      "Epoch 3 -- Batch 556/ 842, training loss 0.5737584829330444\n",
      "Epoch 3 -- Batch 557/ 842, training loss 0.5540791153907776\n",
      "Epoch 3 -- Batch 558/ 842, training loss 0.5886204242706299\n",
      "Epoch 3 -- Batch 559/ 842, training loss 0.5678550004959106\n",
      "Epoch 3 -- Batch 560/ 842, training loss 0.5771293640136719\n",
      "Epoch 3 -- Batch 561/ 842, training loss 0.5704158544540405\n",
      "Epoch 3 -- Batch 562/ 842, training loss 0.5593621730804443\n",
      "Epoch 3 -- Batch 563/ 842, training loss 0.5562494993209839\n",
      "Epoch 3 -- Batch 564/ 842, training loss 0.589130699634552\n",
      "Epoch 3 -- Batch 565/ 842, training loss 0.5818729400634766\n",
      "Epoch 3 -- Batch 566/ 842, training loss 0.5770543217658997\n",
      "Epoch 3 -- Batch 567/ 842, training loss 0.5670820474624634\n",
      "Epoch 3 -- Batch 568/ 842, training loss 0.5654203295707703\n",
      "Epoch 3 -- Batch 569/ 842, training loss 0.5462199449539185\n",
      "Epoch 3 -- Batch 570/ 842, training loss 0.5822505354881287\n",
      "Epoch 3 -- Batch 571/ 842, training loss 0.5623294711112976\n",
      "Epoch 3 -- Batch 572/ 842, training loss 0.5536550283432007\n",
      "Epoch 3 -- Batch 573/ 842, training loss 0.5631463527679443\n",
      "Epoch 3 -- Batch 574/ 842, training loss 0.5982541441917419\n",
      "Epoch 3 -- Batch 575/ 842, training loss 0.5779036283493042\n",
      "Epoch 3 -- Batch 576/ 842, training loss 0.5596825480461121\n",
      "Epoch 3 -- Batch 577/ 842, training loss 0.5602995157241821\n",
      "Epoch 3 -- Batch 578/ 842, training loss 0.5941649675369263\n",
      "Epoch 3 -- Batch 579/ 842, training loss 0.5322389006614685\n",
      "Epoch 3 -- Batch 580/ 842, training loss 0.5540070533752441\n",
      "Epoch 3 -- Batch 581/ 842, training loss 0.5755305290222168\n",
      "Epoch 3 -- Batch 582/ 842, training loss 0.5556546449661255\n",
      "Epoch 3 -- Batch 583/ 842, training loss 0.5660659670829773\n",
      "Epoch 3 -- Batch 584/ 842, training loss 0.5462984442710876\n",
      "Epoch 3 -- Batch 585/ 842, training loss 0.5483725666999817\n",
      "Epoch 3 -- Batch 586/ 842, training loss 0.5712719559669495\n",
      "Epoch 3 -- Batch 587/ 842, training loss 0.5660867691040039\n",
      "Epoch 3 -- Batch 588/ 842, training loss 0.5508183240890503\n",
      "Epoch 3 -- Batch 589/ 842, training loss 0.5681890845298767\n",
      "Epoch 3 -- Batch 590/ 842, training loss 0.5807934403419495\n",
      "Epoch 3 -- Batch 591/ 842, training loss 0.5544828176498413\n",
      "Epoch 3 -- Batch 592/ 842, training loss 0.568575918674469\n",
      "Epoch 3 -- Batch 593/ 842, training loss 0.5548747181892395\n",
      "Epoch 3 -- Batch 594/ 842, training loss 0.5795836448669434\n",
      "Epoch 3 -- Batch 595/ 842, training loss 0.5616772770881653\n",
      "Epoch 3 -- Batch 596/ 842, training loss 0.585722029209137\n",
      "Epoch 3 -- Batch 597/ 842, training loss 0.5964699983596802\n",
      "Epoch 3 -- Batch 598/ 842, training loss 0.5558798909187317\n",
      "Epoch 3 -- Batch 599/ 842, training loss 0.5502002239227295\n",
      "Epoch 3 -- Batch 600/ 842, training loss 0.5656136274337769\n",
      "Epoch 3 -- Batch 601/ 842, training loss 0.5799046158790588\n",
      "Epoch 3 -- Batch 602/ 842, training loss 0.559221625328064\n",
      "Epoch 3 -- Batch 603/ 842, training loss 0.5615042448043823\n",
      "Epoch 3 -- Batch 604/ 842, training loss 0.5684732794761658\n",
      "Epoch 3 -- Batch 605/ 842, training loss 0.5380620956420898\n",
      "Epoch 3 -- Batch 606/ 842, training loss 0.5536804795265198\n",
      "Epoch 3 -- Batch 607/ 842, training loss 0.5669761896133423\n",
      "Epoch 3 -- Batch 608/ 842, training loss 0.5646782517433167\n",
      "Epoch 3 -- Batch 609/ 842, training loss 0.5800321698188782\n",
      "Epoch 3 -- Batch 610/ 842, training loss 0.5798123478889465\n",
      "Epoch 3 -- Batch 611/ 842, training loss 0.5763668417930603\n",
      "Epoch 3 -- Batch 612/ 842, training loss 0.5694822669029236\n",
      "Epoch 3 -- Batch 613/ 842, training loss 0.5449687242507935\n",
      "Epoch 3 -- Batch 614/ 842, training loss 0.5414279103279114\n",
      "Epoch 3 -- Batch 615/ 842, training loss 0.5585542917251587\n",
      "Epoch 3 -- Batch 616/ 842, training loss 0.5691207647323608\n",
      "Epoch 3 -- Batch 617/ 842, training loss 0.5612658262252808\n",
      "Epoch 3 -- Batch 618/ 842, training loss 0.5956992506980896\n",
      "Epoch 3 -- Batch 619/ 842, training loss 0.5601566433906555\n",
      "Epoch 3 -- Batch 620/ 842, training loss 0.5633804202079773\n",
      "Epoch 3 -- Batch 621/ 842, training loss 0.5546472668647766\n",
      "Epoch 3 -- Batch 622/ 842, training loss 0.5808018445968628\n",
      "Epoch 3 -- Batch 623/ 842, training loss 0.5804315209388733\n",
      "Epoch 3 -- Batch 624/ 842, training loss 0.5457301735877991\n",
      "Epoch 3 -- Batch 625/ 842, training loss 0.5678120255470276\n",
      "Epoch 3 -- Batch 626/ 842, training loss 0.5673828721046448\n",
      "Epoch 3 -- Batch 627/ 842, training loss 0.5804165601730347\n",
      "Epoch 3 -- Batch 628/ 842, training loss 0.5674773454666138\n",
      "Epoch 3 -- Batch 629/ 842, training loss 0.5828124284744263\n",
      "Epoch 3 -- Batch 630/ 842, training loss 0.5675405263900757\n",
      "Epoch 3 -- Batch 631/ 842, training loss 0.5621634721755981\n",
      "Epoch 3 -- Batch 632/ 842, training loss 0.5457503199577332\n",
      "Epoch 3 -- Batch 633/ 842, training loss 0.5737985968589783\n",
      "Epoch 3 -- Batch 634/ 842, training loss 0.5709834694862366\n",
      "Epoch 3 -- Batch 635/ 842, training loss 0.5607452392578125\n",
      "Epoch 3 -- Batch 636/ 842, training loss 0.5593796968460083\n",
      "Epoch 3 -- Batch 637/ 842, training loss 0.5783793926239014\n",
      "Epoch 3 -- Batch 638/ 842, training loss 0.5650816559791565\n",
      "Epoch 3 -- Batch 639/ 842, training loss 0.5564925670623779\n",
      "Epoch 3 -- Batch 640/ 842, training loss 0.5905073285102844\n",
      "Epoch 3 -- Batch 641/ 842, training loss 0.5434435606002808\n",
      "Epoch 3 -- Batch 642/ 842, training loss 0.560615599155426\n",
      "Epoch 3 -- Batch 643/ 842, training loss 0.5492870807647705\n",
      "Epoch 3 -- Batch 644/ 842, training loss 0.5600908398628235\n",
      "Epoch 3 -- Batch 645/ 842, training loss 0.6019220948219299\n",
      "Epoch 3 -- Batch 646/ 842, training loss 0.5600490570068359\n",
      "Epoch 3 -- Batch 647/ 842, training loss 0.5576933026313782\n",
      "Epoch 3 -- Batch 648/ 842, training loss 0.5685304403305054\n",
      "Epoch 3 -- Batch 649/ 842, training loss 0.5768956542015076\n",
      "Epoch 3 -- Batch 650/ 842, training loss 0.5653924942016602\n",
      "Epoch 3 -- Batch 651/ 842, training loss 0.5502890348434448\n",
      "Epoch 3 -- Batch 652/ 842, training loss 0.5607980489730835\n",
      "Epoch 3 -- Batch 653/ 842, training loss 0.562714159488678\n",
      "Epoch 3 -- Batch 654/ 842, training loss 0.5762321352958679\n",
      "Epoch 3 -- Batch 655/ 842, training loss 0.5624064207077026\n",
      "Epoch 3 -- Batch 656/ 842, training loss 0.5863211154937744\n",
      "Epoch 3 -- Batch 657/ 842, training loss 0.554308295249939\n",
      "Epoch 3 -- Batch 658/ 842, training loss 0.5625954866409302\n",
      "Epoch 3 -- Batch 659/ 842, training loss 0.5812810659408569\n",
      "Epoch 3 -- Batch 660/ 842, training loss 0.5593900680541992\n",
      "Epoch 3 -- Batch 661/ 842, training loss 0.572030782699585\n",
      "Epoch 3 -- Batch 662/ 842, training loss 0.5888779759407043\n",
      "Epoch 3 -- Batch 663/ 842, training loss 0.5824171900749207\n",
      "Epoch 3 -- Batch 664/ 842, training loss 0.5513167977333069\n",
      "Epoch 3 -- Batch 665/ 842, training loss 0.5674306154251099\n",
      "Epoch 3 -- Batch 666/ 842, training loss 0.5776366591453552\n",
      "Epoch 3 -- Batch 667/ 842, training loss 0.575036883354187\n",
      "Epoch 3 -- Batch 668/ 842, training loss 0.5553352236747742\n",
      "Epoch 3 -- Batch 669/ 842, training loss 0.5481546521186829\n",
      "Epoch 3 -- Batch 670/ 842, training loss 0.5661121010780334\n",
      "Epoch 3 -- Batch 671/ 842, training loss 0.5567655563354492\n",
      "Epoch 3 -- Batch 672/ 842, training loss 0.5722935795783997\n",
      "Epoch 3 -- Batch 673/ 842, training loss 0.5480881929397583\n",
      "Epoch 3 -- Batch 674/ 842, training loss 0.5637251734733582\n",
      "Epoch 3 -- Batch 675/ 842, training loss 0.5465734601020813\n",
      "Epoch 3 -- Batch 676/ 842, training loss 0.5547693967819214\n",
      "Epoch 3 -- Batch 677/ 842, training loss 0.5691576600074768\n",
      "Epoch 3 -- Batch 678/ 842, training loss 0.5753797888755798\n",
      "Epoch 3 -- Batch 679/ 842, training loss 0.5797946453094482\n",
      "Epoch 3 -- Batch 680/ 842, training loss 0.5508165955543518\n",
      "Epoch 3 -- Batch 681/ 842, training loss 0.5775365829467773\n",
      "Epoch 3 -- Batch 682/ 842, training loss 0.5685405135154724\n",
      "Epoch 3 -- Batch 683/ 842, training loss 0.5613624453544617\n",
      "Epoch 3 -- Batch 684/ 842, training loss 0.5477222204208374\n",
      "Epoch 3 -- Batch 685/ 842, training loss 0.5690088272094727\n",
      "Epoch 3 -- Batch 686/ 842, training loss 0.5706017017364502\n",
      "Epoch 3 -- Batch 687/ 842, training loss 0.5721848607063293\n",
      "Epoch 3 -- Batch 688/ 842, training loss 0.5611377358436584\n",
      "Epoch 3 -- Batch 689/ 842, training loss 0.572201132774353\n",
      "Epoch 3 -- Batch 690/ 842, training loss 0.5517022013664246\n",
      "Epoch 3 -- Batch 691/ 842, training loss 0.5596004128456116\n",
      "Epoch 3 -- Batch 692/ 842, training loss 0.5367820858955383\n",
      "Epoch 3 -- Batch 693/ 842, training loss 0.6204565167427063\n",
      "Epoch 3 -- Batch 694/ 842, training loss 0.5681484937667847\n",
      "Epoch 3 -- Batch 695/ 842, training loss 0.5570703148841858\n",
      "Epoch 3 -- Batch 696/ 842, training loss 0.5515791177749634\n",
      "Epoch 3 -- Batch 697/ 842, training loss 0.5659819841384888\n",
      "Epoch 3 -- Batch 698/ 842, training loss 0.5952747464179993\n",
      "Epoch 3 -- Batch 699/ 842, training loss 0.5665346384048462\n",
      "Epoch 3 -- Batch 700/ 842, training loss 0.5639854669570923\n",
      "Epoch 3 -- Batch 701/ 842, training loss 0.5447843670845032\n",
      "Epoch 3 -- Batch 702/ 842, training loss 0.5989769697189331\n",
      "Epoch 3 -- Batch 703/ 842, training loss 0.5482139587402344\n",
      "Epoch 3 -- Batch 704/ 842, training loss 0.5737419724464417\n",
      "Epoch 3 -- Batch 705/ 842, training loss 0.5791838765144348\n",
      "Epoch 3 -- Batch 706/ 842, training loss 0.5443596243858337\n",
      "Epoch 3 -- Batch 707/ 842, training loss 0.5706984996795654\n",
      "Epoch 3 -- Batch 708/ 842, training loss 0.5516327619552612\n",
      "Epoch 3 -- Batch 709/ 842, training loss 0.5557899475097656\n",
      "Epoch 3 -- Batch 710/ 842, training loss 0.5635961294174194\n",
      "Epoch 3 -- Batch 711/ 842, training loss 0.5473580956459045\n",
      "Epoch 3 -- Batch 712/ 842, training loss 0.578495979309082\n",
      "Epoch 3 -- Batch 713/ 842, training loss 0.5566563606262207\n",
      "Epoch 3 -- Batch 714/ 842, training loss 0.550094723701477\n",
      "Epoch 3 -- Batch 715/ 842, training loss 0.5470883846282959\n",
      "Epoch 3 -- Batch 716/ 842, training loss 0.5470118522644043\n",
      "Epoch 3 -- Batch 717/ 842, training loss 0.5662220120429993\n",
      "Epoch 3 -- Batch 718/ 842, training loss 0.5491799116134644\n",
      "Epoch 3 -- Batch 719/ 842, training loss 0.5470516085624695\n",
      "Epoch 3 -- Batch 720/ 842, training loss 0.5562716126441956\n",
      "Epoch 3 -- Batch 721/ 842, training loss 0.5408132672309875\n",
      "Epoch 3 -- Batch 722/ 842, training loss 0.5471371412277222\n",
      "Epoch 3 -- Batch 723/ 842, training loss 0.549075186252594\n",
      "Epoch 3 -- Batch 724/ 842, training loss 0.5673149228096008\n",
      "Epoch 3 -- Batch 725/ 842, training loss 0.5546718835830688\n",
      "Epoch 3 -- Batch 726/ 842, training loss 0.5566182136535645\n",
      "Epoch 3 -- Batch 727/ 842, training loss 0.557198166847229\n",
      "Epoch 3 -- Batch 728/ 842, training loss 0.5648438334465027\n",
      "Epoch 3 -- Batch 729/ 842, training loss 0.5692241191864014\n",
      "Epoch 3 -- Batch 730/ 842, training loss 0.5765656232833862\n",
      "Epoch 3 -- Batch 731/ 842, training loss 0.5680164098739624\n",
      "Epoch 3 -- Batch 732/ 842, training loss 0.5572795867919922\n",
      "Epoch 3 -- Batch 733/ 842, training loss 0.5643916726112366\n",
      "Epoch 3 -- Batch 734/ 842, training loss 0.5342472195625305\n",
      "Epoch 3 -- Batch 735/ 842, training loss 0.5496760010719299\n",
      "Epoch 3 -- Batch 736/ 842, training loss 0.5500696897506714\n",
      "Epoch 3 -- Batch 737/ 842, training loss 0.5481781363487244\n",
      "Epoch 3 -- Batch 738/ 842, training loss 0.5455876588821411\n",
      "Epoch 3 -- Batch 739/ 842, training loss 0.5769870281219482\n",
      "Epoch 3 -- Batch 740/ 842, training loss 0.5679666996002197\n",
      "Epoch 3 -- Batch 741/ 842, training loss 0.5517146587371826\n",
      "Epoch 3 -- Batch 742/ 842, training loss 0.567190945148468\n",
      "Epoch 3 -- Batch 743/ 842, training loss 0.5566220879554749\n",
      "Epoch 3 -- Batch 744/ 842, training loss 0.5616950988769531\n",
      "Epoch 3 -- Batch 745/ 842, training loss 0.5535566806793213\n",
      "Epoch 3 -- Batch 746/ 842, training loss 0.5465850234031677\n",
      "Epoch 3 -- Batch 747/ 842, training loss 0.5513273477554321\n",
      "Epoch 3 -- Batch 748/ 842, training loss 0.5699881315231323\n",
      "Epoch 3 -- Batch 749/ 842, training loss 0.5440020561218262\n",
      "Epoch 3 -- Batch 750/ 842, training loss 0.5470848679542542\n",
      "Epoch 3 -- Batch 751/ 842, training loss 0.5579106211662292\n",
      "Epoch 3 -- Batch 752/ 842, training loss 0.562248706817627\n",
      "Epoch 3 -- Batch 753/ 842, training loss 0.5533137321472168\n",
      "Epoch 3 -- Batch 754/ 842, training loss 0.5491242408752441\n",
      "Epoch 3 -- Batch 755/ 842, training loss 0.5803087949752808\n",
      "Epoch 3 -- Batch 756/ 842, training loss 0.5631043910980225\n",
      "Epoch 3 -- Batch 757/ 842, training loss 0.5662500858306885\n",
      "Epoch 3 -- Batch 758/ 842, training loss 0.5471482276916504\n",
      "Epoch 3 -- Batch 759/ 842, training loss 0.5569131374359131\n",
      "Epoch 3 -- Batch 760/ 842, training loss 0.5648165941238403\n",
      "Epoch 3 -- Batch 761/ 842, training loss 0.5712205171585083\n",
      "Epoch 3 -- Batch 762/ 842, training loss 0.5683386921882629\n",
      "Epoch 3 -- Batch 763/ 842, training loss 0.5590669512748718\n",
      "Epoch 3 -- Batch 764/ 842, training loss 0.5549011826515198\n",
      "Epoch 3 -- Batch 765/ 842, training loss 0.5795192718505859\n",
      "Epoch 3 -- Batch 766/ 842, training loss 0.5469500422477722\n",
      "Epoch 3 -- Batch 767/ 842, training loss 0.5711653828620911\n",
      "Epoch 3 -- Batch 768/ 842, training loss 0.5625338554382324\n",
      "Epoch 3 -- Batch 769/ 842, training loss 0.540233850479126\n",
      "Epoch 3 -- Batch 770/ 842, training loss 0.5738931894302368\n",
      "Epoch 3 -- Batch 771/ 842, training loss 0.5510963201522827\n",
      "Epoch 3 -- Batch 772/ 842, training loss 0.5864357352256775\n",
      "Epoch 3 -- Batch 773/ 842, training loss 0.5600375533103943\n",
      "Epoch 3 -- Batch 774/ 842, training loss 0.5639235973358154\n",
      "Epoch 3 -- Batch 775/ 842, training loss 0.5434682369232178\n",
      "Epoch 3 -- Batch 776/ 842, training loss 0.567966878414154\n",
      "Epoch 3 -- Batch 777/ 842, training loss 0.5664145946502686\n",
      "Epoch 3 -- Batch 778/ 842, training loss 0.5731440782546997\n",
      "Epoch 3 -- Batch 779/ 842, training loss 0.5659197568893433\n",
      "Epoch 3 -- Batch 780/ 842, training loss 0.5526912808418274\n",
      "Epoch 3 -- Batch 781/ 842, training loss 0.5534186363220215\n",
      "Epoch 3 -- Batch 782/ 842, training loss 0.5656722784042358\n",
      "Epoch 3 -- Batch 783/ 842, training loss 0.5817390084266663\n",
      "Epoch 3 -- Batch 784/ 842, training loss 0.5809713006019592\n",
      "Epoch 3 -- Batch 785/ 842, training loss 0.5671297311782837\n",
      "Epoch 3 -- Batch 786/ 842, training loss 0.5633803009986877\n",
      "Epoch 3 -- Batch 787/ 842, training loss 0.559686005115509\n",
      "Epoch 3 -- Batch 788/ 842, training loss 0.5580693483352661\n",
      "Epoch 3 -- Batch 789/ 842, training loss 0.5574687719345093\n",
      "Epoch 3 -- Batch 790/ 842, training loss 0.5675914883613586\n",
      "Epoch 3 -- Batch 791/ 842, training loss 0.5760727524757385\n",
      "Epoch 3 -- Batch 792/ 842, training loss 0.5532964468002319\n",
      "Epoch 3 -- Batch 793/ 842, training loss 0.5518506169319153\n",
      "Epoch 3 -- Batch 794/ 842, training loss 0.5552448630332947\n",
      "Epoch 3 -- Batch 795/ 842, training loss 0.5629057288169861\n",
      "Epoch 3 -- Batch 796/ 842, training loss 0.5400738716125488\n",
      "Epoch 3 -- Batch 797/ 842, training loss 0.5949608683586121\n",
      "Epoch 3 -- Batch 798/ 842, training loss 0.5536762475967407\n",
      "Epoch 3 -- Batch 799/ 842, training loss 0.591362714767456\n",
      "Epoch 3 -- Batch 800/ 842, training loss 0.5678973197937012\n",
      "Epoch 3 -- Batch 801/ 842, training loss 0.5629596710205078\n",
      "Epoch 3 -- Batch 802/ 842, training loss 0.5863861441612244\n",
      "Epoch 3 -- Batch 803/ 842, training loss 0.5244126319885254\n",
      "Epoch 3 -- Batch 804/ 842, training loss 0.5613850355148315\n",
      "Epoch 3 -- Batch 805/ 842, training loss 0.5682716369628906\n",
      "Epoch 3 -- Batch 806/ 842, training loss 0.5523199439048767\n",
      "Epoch 3 -- Batch 807/ 842, training loss 0.5590310096740723\n",
      "Epoch 3 -- Batch 808/ 842, training loss 0.5265851616859436\n",
      "Epoch 3 -- Batch 809/ 842, training loss 0.5858058929443359\n",
      "Epoch 3 -- Batch 810/ 842, training loss 0.5643345713615417\n",
      "Epoch 3 -- Batch 811/ 842, training loss 0.5561408996582031\n",
      "Epoch 3 -- Batch 812/ 842, training loss 0.5657267570495605\n",
      "Epoch 3 -- Batch 813/ 842, training loss 0.5597476363182068\n",
      "Epoch 3 -- Batch 814/ 842, training loss 0.565353512763977\n",
      "Epoch 3 -- Batch 815/ 842, training loss 0.5390480756759644\n",
      "Epoch 3 -- Batch 816/ 842, training loss 0.5550876259803772\n",
      "Epoch 3 -- Batch 817/ 842, training loss 0.5648475885391235\n",
      "Epoch 3 -- Batch 818/ 842, training loss 0.5645758509635925\n",
      "Epoch 3 -- Batch 819/ 842, training loss 0.5599930882453918\n",
      "Epoch 3 -- Batch 820/ 842, training loss 0.5577090978622437\n",
      "Epoch 3 -- Batch 821/ 842, training loss 0.5635900497436523\n",
      "Epoch 3 -- Batch 822/ 842, training loss 0.5554894208908081\n",
      "Epoch 3 -- Batch 823/ 842, training loss 0.5659781694412231\n",
      "Epoch 3 -- Batch 824/ 842, training loss 0.5602169632911682\n",
      "Epoch 3 -- Batch 825/ 842, training loss 0.5719560980796814\n",
      "Epoch 3 -- Batch 826/ 842, training loss 0.5561910271644592\n",
      "Epoch 3 -- Batch 827/ 842, training loss 0.5390247106552124\n",
      "Epoch 3 -- Batch 828/ 842, training loss 0.5599239468574524\n",
      "Epoch 3 -- Batch 829/ 842, training loss 0.558398425579071\n",
      "Epoch 3 -- Batch 830/ 842, training loss 0.5417433381080627\n",
      "Epoch 3 -- Batch 831/ 842, training loss 0.5378053188323975\n",
      "Epoch 3 -- Batch 832/ 842, training loss 0.5389996767044067\n",
      "Epoch 3 -- Batch 833/ 842, training loss 0.5459179282188416\n",
      "Epoch 3 -- Batch 834/ 842, training loss 0.5478916168212891\n",
      "Epoch 3 -- Batch 835/ 842, training loss 0.5412699580192566\n",
      "Epoch 3 -- Batch 836/ 842, training loss 0.5610718727111816\n",
      "Epoch 3 -- Batch 837/ 842, training loss 0.5568670630455017\n",
      "Epoch 3 -- Batch 838/ 842, training loss 0.5656955242156982\n",
      "Epoch 3 -- Batch 839/ 842, training loss 0.5422411561012268\n",
      "Epoch 3 -- Batch 840/ 842, training loss 0.556153416633606\n",
      "Epoch 3 -- Batch 841/ 842, training loss 0.5499168634414673\n",
      "Epoch 3 -- Batch 842/ 842, training loss 0.5414184331893921\n",
      "----------------------------------------------------------------------\n",
      "Epoch 3 -- Batch 1/ 94, validation loss 0.5731027722358704\n",
      "Epoch 3 -- Batch 2/ 94, validation loss 0.5499871969223022\n",
      "Epoch 3 -- Batch 3/ 94, validation loss 0.5527788996696472\n",
      "Epoch 3 -- Batch 4/ 94, validation loss 0.5454826354980469\n",
      "Epoch 3 -- Batch 5/ 94, validation loss 0.5369776487350464\n",
      "Epoch 3 -- Batch 6/ 94, validation loss 0.5446280837059021\n",
      "Epoch 3 -- Batch 7/ 94, validation loss 0.5571051836013794\n",
      "Epoch 3 -- Batch 8/ 94, validation loss 0.5202264189720154\n",
      "Epoch 3 -- Batch 9/ 94, validation loss 0.5540608763694763\n",
      "Epoch 3 -- Batch 10/ 94, validation loss 0.562236487865448\n",
      "Epoch 3 -- Batch 11/ 94, validation loss 0.5424458384513855\n",
      "Epoch 3 -- Batch 12/ 94, validation loss 0.5629559755325317\n",
      "Epoch 3 -- Batch 13/ 94, validation loss 0.5689508318901062\n",
      "Epoch 3 -- Batch 14/ 94, validation loss 0.5441397428512573\n",
      "Epoch 3 -- Batch 15/ 94, validation loss 0.5421162843704224\n",
      "Epoch 3 -- Batch 16/ 94, validation loss 0.5615459680557251\n",
      "Epoch 3 -- Batch 17/ 94, validation loss 0.5468575954437256\n",
      "Epoch 3 -- Batch 18/ 94, validation loss 0.5231634974479675\n",
      "Epoch 3 -- Batch 19/ 94, validation loss 0.5210357308387756\n",
      "Epoch 3 -- Batch 20/ 94, validation loss 0.5541329383850098\n",
      "Epoch 3 -- Batch 21/ 94, validation loss 0.5568338632583618\n",
      "Epoch 3 -- Batch 22/ 94, validation loss 0.5257618427276611\n",
      "Epoch 3 -- Batch 23/ 94, validation loss 0.5543453097343445\n",
      "Epoch 3 -- Batch 24/ 94, validation loss 0.5411702990531921\n",
      "Epoch 3 -- Batch 25/ 94, validation loss 0.5261425971984863\n",
      "Epoch 3 -- Batch 26/ 94, validation loss 0.5302901864051819\n",
      "Epoch 3 -- Batch 27/ 94, validation loss 0.5469122529029846\n",
      "Epoch 3 -- Batch 28/ 94, validation loss 0.5400156378746033\n",
      "Epoch 3 -- Batch 29/ 94, validation loss 0.5556826591491699\n",
      "Epoch 3 -- Batch 30/ 94, validation loss 0.5601044297218323\n",
      "Epoch 3 -- Batch 31/ 94, validation loss 0.5360274314880371\n",
      "Epoch 3 -- Batch 32/ 94, validation loss 0.5614395141601562\n",
      "Epoch 3 -- Batch 33/ 94, validation loss 0.547664999961853\n",
      "Epoch 3 -- Batch 34/ 94, validation loss 0.5318776369094849\n",
      "Epoch 3 -- Batch 35/ 94, validation loss 0.5447344183921814\n",
      "Epoch 3 -- Batch 36/ 94, validation loss 0.5348252058029175\n",
      "Epoch 3 -- Batch 37/ 94, validation loss 0.5392506718635559\n",
      "Epoch 3 -- Batch 38/ 94, validation loss 0.5272611975669861\n",
      "Epoch 3 -- Batch 39/ 94, validation loss 0.5458425879478455\n",
      "Epoch 3 -- Batch 40/ 94, validation loss 0.5527819991111755\n",
      "Epoch 3 -- Batch 41/ 94, validation loss 0.5487473011016846\n",
      "Epoch 3 -- Batch 42/ 94, validation loss 0.5601784586906433\n",
      "Epoch 3 -- Batch 43/ 94, validation loss 0.5594419836997986\n",
      "Epoch 3 -- Batch 44/ 94, validation loss 0.5409281253814697\n",
      "Epoch 3 -- Batch 45/ 94, validation loss 0.53547203540802\n",
      "Epoch 3 -- Batch 46/ 94, validation loss 0.5597291588783264\n",
      "Epoch 3 -- Batch 47/ 94, validation loss 0.5479811429977417\n",
      "Epoch 3 -- Batch 48/ 94, validation loss 0.5416342616081238\n",
      "Epoch 3 -- Batch 49/ 94, validation loss 0.5512434244155884\n",
      "Epoch 3 -- Batch 50/ 94, validation loss 0.5472660660743713\n",
      "Epoch 3 -- Batch 51/ 94, validation loss 0.5345531702041626\n",
      "Epoch 3 -- Batch 52/ 94, validation loss 0.5046605467796326\n",
      "Epoch 3 -- Batch 53/ 94, validation loss 0.5500351786613464\n",
      "Epoch 3 -- Batch 54/ 94, validation loss 0.5296568870544434\n",
      "Epoch 3 -- Batch 55/ 94, validation loss 0.5533192157745361\n",
      "Epoch 3 -- Batch 56/ 94, validation loss 0.5568587183952332\n",
      "Epoch 3 -- Batch 57/ 94, validation loss 0.5443927645683289\n",
      "Epoch 3 -- Batch 58/ 94, validation loss 0.552936851978302\n",
      "Epoch 3 -- Batch 59/ 94, validation loss 0.537233293056488\n",
      "Epoch 3 -- Batch 60/ 94, validation loss 0.5446023344993591\n",
      "Epoch 3 -- Batch 61/ 94, validation loss 0.5598792433738708\n",
      "Epoch 3 -- Batch 62/ 94, validation loss 0.5122288465499878\n",
      "Epoch 3 -- Batch 63/ 94, validation loss 0.5434350371360779\n",
      "Epoch 3 -- Batch 64/ 94, validation loss 0.5542991161346436\n",
      "Epoch 3 -- Batch 65/ 94, validation loss 0.5320960283279419\n",
      "Epoch 3 -- Batch 66/ 94, validation loss 0.5441247820854187\n",
      "Epoch 3 -- Batch 67/ 94, validation loss 0.5663527250289917\n",
      "Epoch 3 -- Batch 68/ 94, validation loss 0.5407409071922302\n",
      "Epoch 3 -- Batch 69/ 94, validation loss 0.5521606206893921\n",
      "Epoch 3 -- Batch 70/ 94, validation loss 0.5285870432853699\n",
      "Epoch 3 -- Batch 71/ 94, validation loss 0.5311906337738037\n",
      "Epoch 3 -- Batch 72/ 94, validation loss 0.5562169551849365\n",
      "Epoch 3 -- Batch 73/ 94, validation loss 0.5372207164764404\n",
      "Epoch 3 -- Batch 74/ 94, validation loss 0.5418820977210999\n",
      "Epoch 3 -- Batch 75/ 94, validation loss 0.5294027924537659\n",
      "Epoch 3 -- Batch 76/ 94, validation loss 0.546323835849762\n",
      "Epoch 3 -- Batch 77/ 94, validation loss 0.5178342461585999\n",
      "Epoch 3 -- Batch 78/ 94, validation loss 0.5412821173667908\n",
      "Epoch 3 -- Batch 79/ 94, validation loss 0.5640143752098083\n",
      "Epoch 3 -- Batch 80/ 94, validation loss 0.5327467918395996\n",
      "Epoch 3 -- Batch 81/ 94, validation loss 0.5215770602226257\n",
      "Epoch 3 -- Batch 82/ 94, validation loss 0.5551438927650452\n",
      "Epoch 3 -- Batch 83/ 94, validation loss 0.5330305695533752\n",
      "Epoch 3 -- Batch 84/ 94, validation loss 0.5126996040344238\n",
      "Epoch 3 -- Batch 85/ 94, validation loss 0.5238397121429443\n",
      "Epoch 3 -- Batch 86/ 94, validation loss 0.5316441059112549\n",
      "Epoch 3 -- Batch 87/ 94, validation loss 0.52910315990448\n",
      "Epoch 3 -- Batch 88/ 94, validation loss 0.528692901134491\n",
      "Epoch 3 -- Batch 89/ 94, validation loss 0.5269345641136169\n",
      "Epoch 3 -- Batch 90/ 94, validation loss 0.5260629057884216\n",
      "Epoch 3 -- Batch 91/ 94, validation loss 0.5417726039886475\n",
      "Epoch 3 -- Batch 92/ 94, validation loss 0.5198838710784912\n",
      "Epoch 3 -- Batch 93/ 94, validation loss 0.537056565284729\n",
      "Epoch 3 -- Batch 94/ 94, validation loss 0.5835490226745605\n",
      "----------------------------------------------------------------------\n",
      "Epoch 3 loss: Training 0.5762114524841309, Validation 0.5835490226745605\n",
      "----------------------------------------------------------------------\n",
      "Epoch 4/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 16\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 10 11 13 14 15 18\n",
      "[19:04:15] Explicit valence for atom # 23 O, 3, is greater than permitted\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 17 19 20 21 22 23 24\n",
      "[19:04:15] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CN(C)c1cccc(CNC(=O)C[C@@H]2C[C@@H]3c4ccc(C5=CC5CC(=O)N4CC5)ccc3[C@H]23)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'C[C@]12N=C(C(=O)N(CC)C3c2ccccc2Cl)N1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 5 7 8 20 21 22\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 22 23 24 25 27\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cc(S(=O)(=O)N2CCC2c3ccccc3)ccc2N(C(=O)CSC)C(C)C'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 18 19\n",
      "[19:04:15] Explicit valence for atom # 13 O, 3, is greater than permitted\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCCCC1=C(C(=O)OCC)C(c2cccc(C)c2)[N+]2(C)CC2CC(C)(C)CC(=O)N21'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc2[nH]cc([C@@H]3[C@@H](CO)N(C(=O)NCCOC)C[C@H]3CO)cc2=O'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCCNC(=O)N1C[C@@H](O)COC[C@H]2O[C@H](CC(=O)NCCCN3CCOCC3)C(=O)N22'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC1CC(=O)CC12N=C1C=C(N)O2'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccc(-c2c(CO)nc3c2c(=O)n(CCc4ccccc4)cnn23)c(OC)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 12 13 14 15 16 17 18\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCC(=O)Nc1cc(C(=O)OCC(=O)NCc2ccccc2C)ccc1N1CCN(CC#CCC2)CC1'\n",
      "[19:04:15] SMILES Parse Error: ring closure 2 duplicates bond between atom 10 and atom 11 for input: 'N#CC1=C4N=CCC(C)(O)C2(c2cccc(Cl)c2)C1'\n",
      "[19:04:15] non-ring atom 8 marked aromatic\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 11 25 26 27 28\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cc(NC(=O)C2CC23CCC4(C)C(=O)N(C)C4(C)O2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(c1cnc2ccccc2c1)N(Cc1ccccc1)C[C@H]1[C@H]2Oc3ccccc3[C@@H]2N2C(C)=CC1=O'\n",
      "[19:04:15] SMILES Parse Error: extra close parentheses while parsing: CCC1=NN2C(=S)/c3c4c(cc2C2)OC(c2ccc(C)cc2)N3)=C(C(=O)OC2CCS(=O)(=O)C1)C2\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'CCC1=NN2C(=S)/c3c4c(cc2C2)OC(c2ccc(C)cc2)N3)=C(C(=O)OC2CCS(=O)(=O)C1)C2' for input: 'CCC1=NN2C(=S)/c3c4c(cc2C2)OC(c2ccc(C)cc2)N3)=C(C(=O)OC2CCS(=O)(=O)C1)C2'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(NC(=O)c2cc3c(=O)n4cnc(C5CC6)cc4c3c3ccccc32)s1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 23 24 25\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(N2C(=O)C3C4c5ccccc5C(c5ccccc5O)C3O2)c(O)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cccc(N2CCN(C(=O)C3CCCN3Cc3cc(=O)n5ccc(C)cc4n3)CC2)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 21\n",
      "[19:04:15] Explicit valence for atom # 11 O, 4, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'c1ccc(-c2ccc(/C=C3\\N=C(c3ccccc3)N3CCOCC3)cc2)cc1'\n",
      "[19:04:15] SMILES Parse Error: syntax error while parsing: COc1ccc(NC(=O)c2cccc(NS==O)/c2ccccc2)cc1\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(NC(=O)c2cccc(NS==O)/c2ccccc2)cc1' for input: 'COc1ccc(NC(=O)c2cccc(NS==O)/c2ccccc2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCCC1=NN2c3ccccc3NC1(C)c1cccc(NS(=O)(=O)c2ccc(Cl)cc2)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(Cc1ccccc1)N1C[C@@H]2[C@H](c3ccc(-c4ccccc4)cc3)[C@@H]2CN1C2CC1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-n2ncc3c(SCC(=O)NC4CC5)nnc4c3ccccc32)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'N#Nc1cc(-c2cccc3c2OC(=O)c2ccccc21)c1cccnc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCC(=O)O[C@H]1C[C@@H]2CC[C@H]1[N+](=O)[O-]'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 28 29\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 23\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 7 17\n",
      "[19:04:15] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(-c2nc(=S)s(/C=C3/N=C(c3ccc(F)cc3)NC(=S)N2CCCCC2)cc1'\n",
      "[19:04:15] Explicit valence for atom # 7 C, 5, is greater than permitted\n",
      "[19:04:15] Explicit valence for atom # 8 Br, 2, is greater than permitted\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 21\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCn1c(=O)c2c(nc3n(-c3ccc(Cl)cc3)c(C)cn2[N+](=O)[O-])nn1C'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 25 26 27\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 11 21 22 23 24\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCCCCNC(=O)c1cc2c(n(-c3ccc(Br)cc3)c1-c1ccccc1)CCCC3'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)N1C(=O)C3(C(=O)OCc4ccccc42)C1CC2'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 25 27 28 29\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 21 22 23 24 25 26 27\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 13 17 18 19 20 21 22\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14\n",
      "[19:04:15] SMILES Parse Error: duplicated ring closure 3 bonds atom 17 to itself for input: 'COc1ccc(C2C3=C(CC(C)(C)CC3=O)OC33C(=O)CC(C)(C)C4(O)C2O3)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1nc2c3c(s1)CCCC3)C1(Cc2cc(Br)ccc2Cl)CC1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 12 13 23\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCc1oc2cc(OC)c(NC(=O)CC3CCN(C(=O)[C@@H]4CCO)C3)cc2c1=O'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'c1cc(CN2C[C@@H]3C2CCC[C@H]3C3)c(Br)n1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2c(C(=O)Nc3ccc(C(=O)O)c(Cl)c3)c(C(N)=O)oc2ccccc2F)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'C#CCSc1oc2c(c(Cl)c(C(=O)NCCC)c2C)C(c1ccccc1F)=C(N3)c2ccccc21'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2=NOC(CN3C(=O)C4C5C=C(C5)CC5c3ccccc3)C2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(CN2[C@@H]3CN[C@H](C2)c3ccc(-c4ccc(F)cc4)cn3[C@@H]2C(=O)N(C)C)cc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 16 17 20 22 23 24 25\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 18 19 20\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[19:04:15] Explicit valence for atom # 3 C, 5, is greater than permitted\n",
      "[19:04:15] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC1CCC(C)(CNC(=O)CN2CCCC3)(c2ccccc2)C1'\n",
      "[19:04:15] Explicit valence for atom # 8 F, 2, is greater than permitted\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24\n",
      "[19:04:15] SMILES Parse Error: syntax error while parsing: O=C1CC2(CCNC1)NCC1CCC(=O)N1/\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'O=C1CC2(CCNC1)NCC1CCC(=O)N1/' for input: 'O=C1CC2(CCNC1)NCC1CCC(=O)N1/'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 7 8 10 19 21\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 18 19 21 25 26 27 28 29 30\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1CCN(c2ccc(C(=O)N3CCC(n4nc(CO)c4ccc(F)cc4)cc3)CC2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCN1CCN(Cc2c(-c3ccccc3)n[nH]c3c2COc2ccccc2C)CC1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'NC(=O)c1cccc(-c2ccc([C@@H]3[C@H](CO)N4C(=O)CNC4=O)cc2)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 16\n",
      "[19:04:15] Explicit valence for atom # 8 N, 4, is greater than permitted\n",
      "[19:04:15] Explicit valence for atom # 21 C, 5, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C(=O)N2CC3(C2)CCCC3C2)c1'\n",
      "[19:04:15] SMILES Parse Error: syntax error while parsing: C#CCn1c(=O)c2c(-c3ccccc3Cl)nn(CCc3ccccc3)c2n(C)c1=\\\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'C#CCn1c(=O)c2c(-c3ccccc3Cl)nn(CCc3ccccc3)c2n(C)c1=\\' for input: 'C#CCn1c(=O)c2c(-c3ccccc3Cl)nn(CCc3ccccc3)c2n(C)c1=\\'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 7 8 17\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 11\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 16 17 24 25 26\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)N(Cc2ccc(-c2ccc(F)cc2)c2cccs2)C(C(C)C)C(=O)O)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(NCCS(=O)(=O)NCc1ccccc1)c1c(F)c2cc(Cl)c(F)c3ccccn2c12'\n",
      "[19:04:15] SMILES Parse Error: ring closure 4 duplicates bond between atom 12 and atom 13 for input: 'COc1ccc2ccc(-c3nc(C4C4c5ccccc5C4)cs3)oc2c1'\n",
      "[19:04:15] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 25 26 27\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1nn2c(/C=C/c3ccccc3F)nc3n(n2c2c1)-c1cccnc1-3'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C1c2cc([N+](=O)[O-])cc(Cl)c2N[C@@H]1[C@H](c1ccccc1)[C@@H]2CCCN12'\n",
      "[19:04:15] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(C(=O)NCCN2'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 18 19 22 23 24\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 19 20\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 17 18 19\n",
      "[19:04:15] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2cc(CN(C(=O)CC)s3)C(c3cccs3)C2)c(OC)c1\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2cc(CN(C(=O)CC)s3)C(c3cccs3)C2)c(OC)c1' for input: 'COc1ccc2cc(CN(C(=O)CC)s3)C(c3cccs3)C2)c(OC)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCCNC(=O)N1[C@H](CO)[C@@H](c2ccccc2)[C@@H]2C1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 7 8 9 13 14\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cccc2c1OCC(=O)c1c2c(c2c1CCCC3)C(=O)N2'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC(=O)C2=C(C1)N(c1ccccc1Cl)C(c2ccc(=O)ccc2)N1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nn(-c3ccccc3)c3c2OC(c2cc4c(cc2Br)OCO4)N3CCCCC2)cc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 20 21\n",
      "[19:04:15] Explicit valence for atom # 17 O, 3, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(NCCc1ccccc1)C1CCCN(S(=O)(=O)c2ccc(F)cc2)c1C1'\n",
      "[19:04:15] SMILES Parse Error: extra open parentheses for input: 'CCOc1ccc(CN2CCC(N3CCC(c4nc(-c5ccnc5ccc5cc[nH]c55)n3)CC2)CCC1C'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 3 15 17\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'NC(=O)C12CC3CC(CC(O)(CN1)C2)C2'\n",
      "[19:04:15] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(CN'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(N1CCN(C(=O)c2c(F)cccc2F)S(=O)(=O)C2)cc1'\n",
      "[19:04:15] SMILES Parse Error: extra close parentheses while parsing: CCN(CC)CC)c1cc2c(=O)n(-c3ccc(F)cc3)c(SCC(C)C)nc2n1\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'CCN(CC)CC)c1cc2c(=O)n(-c3ccc(F)cc3)c(SCC(C)C)nc2n1' for input: 'CCN(CC)CC)c1cc2c(=O)n(-c3ccc(F)cc3)c(SCC(C)C)nc2n1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CN([C@@H](C)CO)S(=O)(=O)c2ccc(-c3ccc4c(c3)OCC(=O)N3)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Nn2nccc2-c2ccc(/C=C3/C(=N)N4C(C)=C(c5cccnc4)CC3=O)o2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cc(C#Cc2cn[nH]c3-c2ccccc2)c(OC)cc1Cl'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 10 11\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 10 11\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)C(C)N2CCN3[C@H](CCN(C)C[C@@H]3O)[C@H]3Cc2ccc(Cl)cc2)o1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 3\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C#O)c1OC(=O)c1c(O)c2cccc3[nH]nc12'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1ccc(CN2C[C@@H]3C[C@@H](c4ccccc4)N4CCCC[C@@]422)cc1'\n",
      "[19:04:15] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1cc2c3c(c1o)OCCO3'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c(S(=O)(=O)N2CCSC23)cc1N1C(=O)C(O)=C(C(=O)c2cccnc2)C1c1cccc(N(C)C)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 8 9 10 19 21\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 22 23\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 19\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(N2CCN(c3c(O)nc4cc(C(=O)NC5CC5)n5ccccc44)CC2)cc1'\n",
      "[19:04:15] SMILES Parse Error: extra open parentheses for input: 'COCC1(C2OCC3CC(CO)OCC3=C'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 18 19 20 23 29\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 7 8 25 26\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2ccc([C@@H]3[C@@H](CO)N(C(=O)Nc4ccccc4)C4)cn2)cc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 8 9 11 12 14\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 7 8 20 21 22 23 24 25 26\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCC(=O)NN1C(=O)C2C3c4ccccc4S(=O)(c4ccccc44)C2C1=O'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 8 9 10 27 28\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCc1ccc2c(c1)N(C(=O)Cn1ccccn1)C(C)c1ccccc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 8 22\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CN1CCN(Cc2ccc3c(c2)OCCN2S(=O)(=O)c2ccccc2)CC1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2=CN(C3CC4)C(CCO)C3(C)Oc3ccccc32)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Oc1ccc(/C=C/CN2C[C@H]3C=C[C@H](CC4=O)[C@@H]3CN2)cc1'\n",
      "[19:04:15] Explicit valence for atom # 5 Br, 2, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cc2nn(CC(=O)N(C)C)c(=O)c3ccccc2c1O'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 11 12 20 21 22 23 24 25 26\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1cnnc(-n2nc(C(=O)NCc3ccco3)c3cc4ccccc4n3c23)n1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCCn1c2nc(=O)n(CCN3CCOCC3)c(=O)c2ccccc21'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)c(=O)n1CCC(C)CN1C(=O)CC1C(=O)Nc2ccc(C(C)(C)C)cc21'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 16 17 18\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 22\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 26 27 28\n",
      "[19:04:15] Explicit valence for atom # 6 C, 5, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: ring closure 1 duplicates bond between atom 8 and atom 9 for input: 'CC(=O)N1[C@H]2CC[C@H]1C1c1ccc(-c2ccc(F)cc2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCCn1c(C2CC(C)(C)NC(=O)N2C2Cc2ccccc2OC(C)O1)cnc1ccccc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)Nc2ccc3c(c2)C(=O)N(C)[C@@H]2C[C@H](C)[C@H](C(=O)Nc3ccc(F)cc3)[C@H](O)[C@H]2O)cc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 9 10 11\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC12CC(CC(=O)Nc3ccc4noc(-c4cccc(Cl)c4)c3C3)C1C2'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7\n",
      "[19:04:15] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1CCc2c1sc(Nc2ccccc2C(=O)NCc2ccc(C(C)=O)cc2)C(C)(C)NC12'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCCC(=O)N1CC2(CN(Cc3ccc4c(c3)OCO4)[C@H](CO)N2S3(C)C)C1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[19:04:15] SMILES Parse Error: extra open parentheses for input: 'CCOC(=O)C[C@@H]1C[C@@]23c4c(c(-c4ccccc5)c3ccccc3O[C@H]2C[C@@H](CO)[C@H]2CC1C[C@@H]3[C@@H]1CO2'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 29 30 37\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'N#Cc1c2cc3c(cc2n1CCCNC3)CCCCC2'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 15 16 27\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)C2C4CC=C(C4)C3C1=O'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2(CNC(=O)C(=O)N2CC4COCCO3)CCCC2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cn1nc(C(=O)N2CCc3c(cccn3)Cc2ccccc2)c(C)c1[N+](=O)[O-]'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 15 16 23\n",
      "[19:04:15] SMILES Parse Error: ring closure 3 duplicates bond between atom 9 and atom 10 for input: 'COC(=O)c1c(N2C(=O)C3C3CC(CC(C4)C2)C3)cc([N+](=O)[O-])c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 7 8 19 20 21 22 23 24\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 8 9 11\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(N/N=C/c1c2c3ccccc4cccc4n2n1)c1ccccc1F'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1=C(C)C(c2cc(Cl)ccc2Cl)C2=C(C(c3ccc(C)c(C)c3)NC2CCCC2)N2'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCc1c(C(=O)CC2(C)C)sc2c(C)n(-c3ccccc3)c(=S)n12'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)C(=O)NCC2COc3c(cnc3OC)CC3)cc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 21 22\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 6 8 20 22\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COC(=O)C(=O)N[C@H]1C[C@@H]2C(=O)N3Cc4ccccc4[C@H]3[C@H]2N(C(=O)NC(C)C)[C@H]2C1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cccc(NC(=O)CN[C@@]2[C@@H]3c4ccc(Oc5ccc(C)cc5)c(=O)n4C[C@H]35)cc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 10 21 22 27 28\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(-c2cnc3c(c2)C(=O)NC2)c1)c1ccccc1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 15 16 17 18 19\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 24\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COCCN1CN(Cc2ccccn2)C(=O)c2ccccc2C(=O)N(C)C[C@@H]1O2'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2nc3c(c(=O)n(-c4ccc5c(c4)OCCO5)c3c(CNc3ccc(F)cc3)n(C)c(=O)n3C)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 7 9 12 25 26 27 28 29 30\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cc(/C=C2C/N=C3/C=C(c3ccccc3)OC2=O)c(OC)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN([C@H](C)CO)C(=O)c2cn(nc3ccccc3)c2-[C@H](C)c1c3cc(OC)ccc31'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(Cn2c(=S)sc3c(=O)n(-c4ccccc4Cl)c(SCc4ccco4)nc42)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCC(=O)Nc1cccc(-c2cnc3c2n(c2-c2cc(C)c(O)c1)CSC44)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 21 22 23 24 34\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 11 12 20 21 22\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1cc(=O)n2nc(CCC(=O)N3CCC(C4)OCO3)c(C)n2c1=O'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1cccn2c(=O)c3c(n(C)c13)C(c1ccc(Cl)cc1)C2=C(CCCC1=O)N3'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 13 14 15 18 20 21 22\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc2cc(CN3CCN(CC4CC3)CC3)cc(=O)n12'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 9 18 26 27 28\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 15 16 17 18 32 33 34 35\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 11 12 17\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 10 11 13\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 17\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=c1c2c(ccc1-c1ccccc1)N1CCCCC1'\n",
      "[19:04:15] Explicit valence for atom # 28 O, 4, is greater than permitted\n",
      "[19:04:15] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'N#C[C@@H]1c2ccccc2C(=O)N2CCN(c3cnc(NCC4CC4)n4CCCCC4)C[C@H]12'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 22\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CN1CCN(c2cccc(-c3nc(C3CCCCC4)nn3)C2)cc1'\n",
      "[19:04:15] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C1CC(C(=O)c2cnncn2)\\c2cc(Br)cc31'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 11 13 15\n",
      "[19:04:15] SMILES Parse Error: extra close parentheses while parsing: O=C(COc1ccc2c(=O)n(Cc3ccco3)c(=O)c3ccccc3c21)NCC1CC2)S1Cc1ccccn1\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'O=C(COc1ccc2c(=O)n(Cc3ccco3)c(=O)c3ccccc3c21)NCC1CC2)S1Cc1ccccn1' for input: 'O=C(COc1ccc2c(=O)n(Cc3ccco3)c(=O)c3ccccc3c21)NCC1CC2)S1Cc1ccccn1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(COc2cccc3ccc(C(=O)Nc4ccc5c(c3)OCO5)c3c2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C1c2ccccc3c(S(=O)(=O)N4CCCc4ccccc43)CN1C1CCCCC1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(C)c1nc(CN2CCOC2(CNC(=O)Nc2ccccc2Cl)O3)cn1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 31 34\n",
      "[19:04:15] SMILES Parse Error: extra close parentheses while parsing: Cc1c([N+](=O)[O-])c(Cl)nn1CC(=O)N1CC(=O)N2CCc3ccccc3C2)1\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES 'Cc1c([N+](=O)[O-])c(Cl)nn1CC(=O)N1CC(=O)N2CCc3ccccc3C2)1' for input: 'Cc1c([N+](=O)[O-])c(Cl)nn1CC(=O)N1CC(=O)N2CCc3ccccc3C2)1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1cccc2cc(CN(CCN(C)C)C2=O)c1nnnn1'\n",
      "[19:04:15] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1=C(C)N=c2s/c(=C\\c2ccccc2F)n(CO)c1=O'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCOC1CCCN(C(=O)CN(Cc2csc(COc2ccc(Cl)cc2)C(C)=O)C(C)C)C1'\n",
      "[19:04:15] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C2=NOC(CN(Cc3nnnn3-c3ccccc3)C2)C2)c1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 5 6 7 30 32\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'c1ccc(C2C=C(c3ccoc3)C(c3cccs3)NC3=N)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCCCCCN1C(C(=O)O)SC(c2cc(Br)ccc1Cl)=C(C#N)CC(c2ccc(C)cc2)N1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 29 30\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(C#NCC(c3ccc(Br)cc3)N4CCC(C)(C)CC3)ccc2n1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 27 28\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 21 22 30\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 18\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCCCCc1nnc(CN(C)C(=O)c2c(O)n(C(C)c3ccccc3)c(C)n3C(C)(C)C)c(C(=O)O)n12'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 9 10 12 13 15 16 17\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'C=CCn1c(C2Cc3ccccc3C3CCCN(Cc3nc(=O)ccc3Cl)O2)c2ccccc21'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 2 3 7 8 10 11 12 13 14 15 16\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCc1ccc(OCCC2c3c(oc4ccccc4c3Cl)CC3c2ccccc2)cc1'\n",
      "[19:04:15] Explicit valence for atom # 6 O, 3, is greater than permitted\n",
      "[19:04:15] SMILES Parse Error: syntax error while parsing: =OCC1=C(COc2ccc(C)cc2)OC(=O)NC1c1ccc(Oc2ccccc2)cc1\n",
      "[19:04:15] SMILES Parse Error: Failed parsing SMILES '=OCC1=C(COc2ccc(C)cc2)OC(=O)NC1c1ccc(Oc2ccccc2)cc1' for input: '=OCC1=C(COc2ccc(C)cc2)OC(=O)NC1c1ccc(Oc2ccccc2)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCC(=O)N1CCC(N2CC3CC(CC(C3)C2)C3)CC1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 12 13 23\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc2oc(/C=C3\\OC(=O)OC33)c(C)nc21'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1c(NC(=O)Cc2ccccc2)sc2nc3c(c(=O)[nH]1)Cc1ccccc1-3'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CSc1ccccc1-n1c(=S)[nH]c2c(c(=O)oc2nc2c1CCCC3)C1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1CC[C@@H](CC(=O)NC2CCCC3)CC1'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 24\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)c(/C=N/N2CCC(N3CCN(C(=O)CNC(=O)c4c(F)cccc3Cl)Cl)CC2)c1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1nc2n1CCCNCc1ccccc1)C(O)c1ccc(F)cc1'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1cc2cc3c(cc2n2OCCCCC3)C(C#N)(C#N)C=N'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 10 11 13 24\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'Cc1c(N2CCC3)nc3c(NC(=O)C4CC5)noc3c3c2cc(-c2ccccc2)nn12'\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'CC(C)CCC#Cc1cnc2c(c1)C(=O)N([C@@H]([C@H](C)CO)C[C@H](C)[C@@H](C)CN(CC1CC1)[C@@H](C)CO2)N2'\n",
      "[19:04:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 25 26\n",
      "[19:04:15] SMILES Parse Error: unclosed ring for input: 'COc1ccc(N=C2S/N=C/c2ccc(C(=O)O)s2)cc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 -- Batch 1/ 842, training loss 0.5487864017486572\n",
      "Epoch 4 -- Batch 2/ 842, training loss 0.5496375560760498\n",
      "Epoch 4 -- Batch 3/ 842, training loss 0.5429785251617432\n",
      "Epoch 4 -- Batch 4/ 842, training loss 0.5710179805755615\n",
      "Epoch 4 -- Batch 5/ 842, training loss 0.5716375112533569\n",
      "Epoch 4 -- Batch 6/ 842, training loss 0.5560423135757446\n",
      "Epoch 4 -- Batch 7/ 842, training loss 0.5667555928230286\n",
      "Epoch 4 -- Batch 8/ 842, training loss 0.5533558130264282\n",
      "Epoch 4 -- Batch 9/ 842, training loss 0.5472891330718994\n",
      "Epoch 4 -- Batch 10/ 842, training loss 0.5442296266555786\n",
      "Epoch 4 -- Batch 11/ 842, training loss 0.5551198124885559\n",
      "Epoch 4 -- Batch 12/ 842, training loss 0.5374045372009277\n",
      "Epoch 4 -- Batch 13/ 842, training loss 0.5521165132522583\n",
      "Epoch 4 -- Batch 14/ 842, training loss 0.547013521194458\n",
      "Epoch 4 -- Batch 15/ 842, training loss 0.5495086908340454\n",
      "Epoch 4 -- Batch 16/ 842, training loss 0.5729098320007324\n",
      "Epoch 4 -- Batch 17/ 842, training loss 0.5376161932945251\n",
      "Epoch 4 -- Batch 18/ 842, training loss 0.5464005470275879\n",
      "Epoch 4 -- Batch 19/ 842, training loss 0.5522814989089966\n",
      "Epoch 4 -- Batch 20/ 842, training loss 0.5433257818222046\n",
      "Epoch 4 -- Batch 21/ 842, training loss 0.5506817698478699\n",
      "Epoch 4 -- Batch 22/ 842, training loss 0.5531574487686157\n",
      "Epoch 4 -- Batch 23/ 842, training loss 0.5584297776222229\n",
      "Epoch 4 -- Batch 24/ 842, training loss 0.5359240174293518\n",
      "Epoch 4 -- Batch 25/ 842, training loss 0.56011563539505\n",
      "Epoch 4 -- Batch 26/ 842, training loss 0.5205631852149963\n",
      "Epoch 4 -- Batch 27/ 842, training loss 0.542553186416626\n",
      "Epoch 4 -- Batch 28/ 842, training loss 0.5509573817253113\n",
      "Epoch 4 -- Batch 29/ 842, training loss 0.5342113375663757\n",
      "Epoch 4 -- Batch 30/ 842, training loss 0.5608385801315308\n",
      "Epoch 4 -- Batch 31/ 842, training loss 0.5676257014274597\n",
      "Epoch 4 -- Batch 32/ 842, training loss 0.5582698583602905\n",
      "Epoch 4 -- Batch 33/ 842, training loss 0.5335511565208435\n",
      "Epoch 4 -- Batch 34/ 842, training loss 0.5341755151748657\n",
      "Epoch 4 -- Batch 35/ 842, training loss 0.5472007989883423\n",
      "Epoch 4 -- Batch 36/ 842, training loss 0.535815954208374\n",
      "Epoch 4 -- Batch 37/ 842, training loss 0.5249819159507751\n",
      "Epoch 4 -- Batch 38/ 842, training loss 0.5333563685417175\n",
      "Epoch 4 -- Batch 39/ 842, training loss 0.5441909432411194\n",
      "Epoch 4 -- Batch 40/ 842, training loss 0.5170852541923523\n",
      "Epoch 4 -- Batch 41/ 842, training loss 0.5481129288673401\n",
      "Epoch 4 -- Batch 42/ 842, training loss 0.548431932926178\n",
      "Epoch 4 -- Batch 43/ 842, training loss 0.5334029197692871\n",
      "Epoch 4 -- Batch 44/ 842, training loss 0.5524458885192871\n",
      "Epoch 4 -- Batch 45/ 842, training loss 0.5299466252326965\n",
      "Epoch 4 -- Batch 46/ 842, training loss 0.5353419184684753\n",
      "Epoch 4 -- Batch 47/ 842, training loss 0.5643067955970764\n",
      "Epoch 4 -- Batch 48/ 842, training loss 0.5382874011993408\n",
      "Epoch 4 -- Batch 49/ 842, training loss 0.5550331473350525\n",
      "Epoch 4 -- Batch 50/ 842, training loss 0.5507757067680359\n",
      "Epoch 4 -- Batch 51/ 842, training loss 0.553759753704071\n",
      "Epoch 4 -- Batch 52/ 842, training loss 0.5476327538490295\n",
      "Epoch 4 -- Batch 53/ 842, training loss 0.5425347089767456\n",
      "Epoch 4 -- Batch 54/ 842, training loss 0.5292478799819946\n",
      "Epoch 4 -- Batch 55/ 842, training loss 0.5341485142707825\n",
      "Epoch 4 -- Batch 56/ 842, training loss 0.5369723439216614\n",
      "Epoch 4 -- Batch 57/ 842, training loss 0.546079695224762\n",
      "Epoch 4 -- Batch 58/ 842, training loss 0.5675130486488342\n",
      "Epoch 4 -- Batch 59/ 842, training loss 0.5339704751968384\n",
      "Epoch 4 -- Batch 60/ 842, training loss 0.5330880880355835\n",
      "Epoch 4 -- Batch 61/ 842, training loss 0.5599657297134399\n",
      "Epoch 4 -- Batch 62/ 842, training loss 0.5365796089172363\n",
      "Epoch 4 -- Batch 63/ 842, training loss 0.5216451287269592\n",
      "Epoch 4 -- Batch 64/ 842, training loss 0.5230957865715027\n",
      "Epoch 4 -- Batch 65/ 842, training loss 0.5719907879829407\n",
      "Epoch 4 -- Batch 66/ 842, training loss 0.5382696390151978\n",
      "Epoch 4 -- Batch 67/ 842, training loss 0.5464821457862854\n",
      "Epoch 4 -- Batch 68/ 842, training loss 0.5484630465507507\n",
      "Epoch 4 -- Batch 69/ 842, training loss 0.5311845541000366\n",
      "Epoch 4 -- Batch 70/ 842, training loss 0.5564638376235962\n",
      "Epoch 4 -- Batch 71/ 842, training loss 0.5309698581695557\n",
      "Epoch 4 -- Batch 72/ 842, training loss 0.5387457609176636\n",
      "Epoch 4 -- Batch 73/ 842, training loss 0.5441084504127502\n",
      "Epoch 4 -- Batch 74/ 842, training loss 0.5331958532333374\n",
      "Epoch 4 -- Batch 75/ 842, training loss 0.5377762913703918\n",
      "Epoch 4 -- Batch 76/ 842, training loss 0.5417986512184143\n",
      "Epoch 4 -- Batch 77/ 842, training loss 0.5572671294212341\n",
      "Epoch 4 -- Batch 78/ 842, training loss 0.5575280785560608\n",
      "Epoch 4 -- Batch 79/ 842, training loss 0.5613350868225098\n",
      "Epoch 4 -- Batch 80/ 842, training loss 0.5361738204956055\n",
      "Epoch 4 -- Batch 81/ 842, training loss 0.5521102547645569\n",
      "Epoch 4 -- Batch 82/ 842, training loss 0.523526132106781\n",
      "Epoch 4 -- Batch 83/ 842, training loss 0.5367693901062012\n",
      "Epoch 4 -- Batch 84/ 842, training loss 0.5318145751953125\n",
      "Epoch 4 -- Batch 85/ 842, training loss 0.527981698513031\n",
      "Epoch 4 -- Batch 86/ 842, training loss 0.5287805795669556\n",
      "Epoch 4 -- Batch 87/ 842, training loss 0.5638418197631836\n",
      "Epoch 4 -- Batch 88/ 842, training loss 0.5624289512634277\n",
      "Epoch 4 -- Batch 89/ 842, training loss 0.5414913296699524\n",
      "Epoch 4 -- Batch 90/ 842, training loss 0.5368724465370178\n",
      "Epoch 4 -- Batch 91/ 842, training loss 0.5130694508552551\n",
      "Epoch 4 -- Batch 92/ 842, training loss 0.5435531139373779\n",
      "Epoch 4 -- Batch 93/ 842, training loss 0.5168032050132751\n",
      "Epoch 4 -- Batch 94/ 842, training loss 0.5582016110420227\n",
      "Epoch 4 -- Batch 95/ 842, training loss 0.5399790406227112\n",
      "Epoch 4 -- Batch 96/ 842, training loss 0.5372262597084045\n",
      "Epoch 4 -- Batch 97/ 842, training loss 0.5172786712646484\n",
      "Epoch 4 -- Batch 98/ 842, training loss 0.5503389239311218\n",
      "Epoch 4 -- Batch 99/ 842, training loss 0.5359387397766113\n",
      "Epoch 4 -- Batch 100/ 842, training loss 0.5372797250747681\n",
      "Epoch 4 -- Batch 101/ 842, training loss 0.5495222210884094\n",
      "Epoch 4 -- Batch 102/ 842, training loss 0.5528401136398315\n",
      "Epoch 4 -- Batch 103/ 842, training loss 0.5509738922119141\n",
      "Epoch 4 -- Batch 104/ 842, training loss 0.5379292964935303\n",
      "Epoch 4 -- Batch 105/ 842, training loss 0.5492056012153625\n",
      "Epoch 4 -- Batch 106/ 842, training loss 0.5531414151191711\n",
      "Epoch 4 -- Batch 107/ 842, training loss 0.5341339111328125\n",
      "Epoch 4 -- Batch 108/ 842, training loss 0.5420588850975037\n",
      "Epoch 4 -- Batch 109/ 842, training loss 0.5443454384803772\n",
      "Epoch 4 -- Batch 110/ 842, training loss 0.5476672053337097\n",
      "Epoch 4 -- Batch 111/ 842, training loss 0.5703094005584717\n",
      "Epoch 4 -- Batch 112/ 842, training loss 0.5477921366691589\n",
      "Epoch 4 -- Batch 113/ 842, training loss 0.5350127220153809\n",
      "Epoch 4 -- Batch 114/ 842, training loss 0.5236796140670776\n",
      "Epoch 4 -- Batch 115/ 842, training loss 0.5478408336639404\n",
      "Epoch 4 -- Batch 116/ 842, training loss 0.552211344242096\n",
      "Epoch 4 -- Batch 117/ 842, training loss 0.5565336346626282\n",
      "Epoch 4 -- Batch 118/ 842, training loss 0.5565191507339478\n",
      "Epoch 4 -- Batch 119/ 842, training loss 0.5536094307899475\n",
      "Epoch 4 -- Batch 120/ 842, training loss 0.5416581630706787\n",
      "Epoch 4 -- Batch 121/ 842, training loss 0.528015673160553\n",
      "Epoch 4 -- Batch 122/ 842, training loss 0.5413049459457397\n",
      "Epoch 4 -- Batch 123/ 842, training loss 0.530275821685791\n",
      "Epoch 4 -- Batch 124/ 842, training loss 0.5694559812545776\n",
      "Epoch 4 -- Batch 125/ 842, training loss 0.5434045791625977\n",
      "Epoch 4 -- Batch 126/ 842, training loss 0.5254482626914978\n",
      "Epoch 4 -- Batch 127/ 842, training loss 0.5189768671989441\n",
      "Epoch 4 -- Batch 128/ 842, training loss 0.5324693918228149\n",
      "Epoch 4 -- Batch 129/ 842, training loss 0.5344531536102295\n",
      "Epoch 4 -- Batch 130/ 842, training loss 0.5344473123550415\n",
      "Epoch 4 -- Batch 131/ 842, training loss 0.5123671889305115\n",
      "Epoch 4 -- Batch 132/ 842, training loss 0.5402559041976929\n",
      "Epoch 4 -- Batch 133/ 842, training loss 0.5391589999198914\n",
      "Epoch 4 -- Batch 134/ 842, training loss 0.5406508445739746\n",
      "Epoch 4 -- Batch 135/ 842, training loss 0.5578839182853699\n",
      "Epoch 4 -- Batch 136/ 842, training loss 0.537346601486206\n",
      "Epoch 4 -- Batch 137/ 842, training loss 0.537247896194458\n",
      "Epoch 4 -- Batch 138/ 842, training loss 0.5323528051376343\n",
      "Epoch 4 -- Batch 139/ 842, training loss 0.5305770635604858\n",
      "Epoch 4 -- Batch 140/ 842, training loss 0.5088473558425903\n",
      "Epoch 4 -- Batch 141/ 842, training loss 0.5436269640922546\n",
      "Epoch 4 -- Batch 142/ 842, training loss 0.533631443977356\n",
      "Epoch 4 -- Batch 143/ 842, training loss 0.5392226576805115\n",
      "Epoch 4 -- Batch 144/ 842, training loss 0.5298944115638733\n",
      "Epoch 4 -- Batch 145/ 842, training loss 0.5402796268463135\n",
      "Epoch 4 -- Batch 146/ 842, training loss 0.5339885950088501\n",
      "Epoch 4 -- Batch 147/ 842, training loss 0.5405809879302979\n",
      "Epoch 4 -- Batch 148/ 842, training loss 0.5595453977584839\n",
      "Epoch 4 -- Batch 149/ 842, training loss 0.537613034248352\n",
      "Epoch 4 -- Batch 150/ 842, training loss 0.5574427247047424\n",
      "Epoch 4 -- Batch 151/ 842, training loss 0.5606262683868408\n",
      "Epoch 4 -- Batch 152/ 842, training loss 0.52700275182724\n",
      "Epoch 4 -- Batch 153/ 842, training loss 0.5454622507095337\n",
      "Epoch 4 -- Batch 154/ 842, training loss 0.5460634827613831\n",
      "Epoch 4 -- Batch 155/ 842, training loss 0.5307934880256653\n",
      "Epoch 4 -- Batch 156/ 842, training loss 0.5534919500350952\n",
      "Epoch 4 -- Batch 157/ 842, training loss 0.526695191860199\n",
      "Epoch 4 -- Batch 158/ 842, training loss 0.5424497127532959\n",
      "Epoch 4 -- Batch 159/ 842, training loss 0.529630184173584\n",
      "Epoch 4 -- Batch 160/ 842, training loss 0.5420237183570862\n",
      "Epoch 4 -- Batch 161/ 842, training loss 0.5242245197296143\n",
      "Epoch 4 -- Batch 162/ 842, training loss 0.5497117042541504\n",
      "Epoch 4 -- Batch 163/ 842, training loss 0.5473250150680542\n",
      "Epoch 4 -- Batch 164/ 842, training loss 0.5388442277908325\n",
      "Epoch 4 -- Batch 165/ 842, training loss 0.5454556941986084\n",
      "Epoch 4 -- Batch 166/ 842, training loss 0.550164520740509\n",
      "Epoch 4 -- Batch 167/ 842, training loss 0.5272433161735535\n",
      "Epoch 4 -- Batch 168/ 842, training loss 0.530041515827179\n",
      "Epoch 4 -- Batch 169/ 842, training loss 0.5426053404808044\n",
      "Epoch 4 -- Batch 170/ 842, training loss 0.532874345779419\n",
      "Epoch 4 -- Batch 171/ 842, training loss 0.5558276176452637\n",
      "Epoch 4 -- Batch 172/ 842, training loss 0.5439035296440125\n",
      "Epoch 4 -- Batch 173/ 842, training loss 0.5315414667129517\n",
      "Epoch 4 -- Batch 174/ 842, training loss 0.562938928604126\n",
      "Epoch 4 -- Batch 175/ 842, training loss 0.497472882270813\n",
      "Epoch 4 -- Batch 176/ 842, training loss 0.5331117510795593\n",
      "Epoch 4 -- Batch 177/ 842, training loss 0.5212407112121582\n",
      "Epoch 4 -- Batch 178/ 842, training loss 0.540695309638977\n",
      "Epoch 4 -- Batch 179/ 842, training loss 0.5257148742675781\n",
      "Epoch 4 -- Batch 180/ 842, training loss 0.5406968593597412\n",
      "Epoch 4 -- Batch 181/ 842, training loss 0.5494771599769592\n",
      "Epoch 4 -- Batch 182/ 842, training loss 0.5353144407272339\n",
      "Epoch 4 -- Batch 183/ 842, training loss 0.5469298958778381\n",
      "Epoch 4 -- Batch 184/ 842, training loss 0.5180234909057617\n",
      "Epoch 4 -- Batch 185/ 842, training loss 0.5524284839630127\n",
      "Epoch 4 -- Batch 186/ 842, training loss 0.5348821878433228\n",
      "Epoch 4 -- Batch 187/ 842, training loss 0.5488516092300415\n",
      "Epoch 4 -- Batch 188/ 842, training loss 0.5347371697425842\n",
      "Epoch 4 -- Batch 189/ 842, training loss 0.5333597660064697\n",
      "Epoch 4 -- Batch 190/ 842, training loss 0.5590815544128418\n",
      "Epoch 4 -- Batch 191/ 842, training loss 0.5224677920341492\n",
      "Epoch 4 -- Batch 192/ 842, training loss 0.5316246151924133\n",
      "Epoch 4 -- Batch 193/ 842, training loss 0.5450659990310669\n",
      "Epoch 4 -- Batch 194/ 842, training loss 0.5463371872901917\n",
      "Epoch 4 -- Batch 195/ 842, training loss 0.5518295168876648\n",
      "Epoch 4 -- Batch 196/ 842, training loss 0.5539924502372742\n",
      "Epoch 4 -- Batch 197/ 842, training loss 0.5497875213623047\n",
      "Epoch 4 -- Batch 198/ 842, training loss 0.5268006324768066\n",
      "Epoch 4 -- Batch 199/ 842, training loss 0.5350161194801331\n",
      "Epoch 4 -- Batch 200/ 842, training loss 0.5408560037612915\n",
      "Epoch 4 -- Batch 201/ 842, training loss 0.5167649388313293\n",
      "Epoch 4 -- Batch 202/ 842, training loss 0.5582922697067261\n",
      "Epoch 4 -- Batch 203/ 842, training loss 0.5630481839179993\n",
      "Epoch 4 -- Batch 204/ 842, training loss 0.5548526048660278\n",
      "Epoch 4 -- Batch 205/ 842, training loss 0.5249468088150024\n",
      "Epoch 4 -- Batch 206/ 842, training loss 0.5530781745910645\n",
      "Epoch 4 -- Batch 207/ 842, training loss 0.5575672388076782\n",
      "Epoch 4 -- Batch 208/ 842, training loss 0.5406123399734497\n",
      "Epoch 4 -- Batch 209/ 842, training loss 0.5324186682701111\n",
      "Epoch 4 -- Batch 210/ 842, training loss 0.5399449467658997\n",
      "Epoch 4 -- Batch 211/ 842, training loss 0.5483298301696777\n",
      "Epoch 4 -- Batch 212/ 842, training loss 0.5063908100128174\n",
      "Epoch 4 -- Batch 213/ 842, training loss 0.540764331817627\n",
      "Epoch 4 -- Batch 214/ 842, training loss 0.5465303063392639\n",
      "Epoch 4 -- Batch 215/ 842, training loss 0.5373796224594116\n",
      "Epoch 4 -- Batch 216/ 842, training loss 0.5479276180267334\n",
      "Epoch 4 -- Batch 217/ 842, training loss 0.5665055513381958\n",
      "Epoch 4 -- Batch 218/ 842, training loss 0.5463452339172363\n",
      "Epoch 4 -- Batch 219/ 842, training loss 0.5444298386573792\n",
      "Epoch 4 -- Batch 220/ 842, training loss 0.5203037261962891\n",
      "Epoch 4 -- Batch 221/ 842, training loss 0.5531780123710632\n",
      "Epoch 4 -- Batch 222/ 842, training loss 0.5398871302604675\n",
      "Epoch 4 -- Batch 223/ 842, training loss 0.5349681377410889\n",
      "Epoch 4 -- Batch 224/ 842, training loss 0.5317919254302979\n",
      "Epoch 4 -- Batch 225/ 842, training loss 0.5354937314987183\n",
      "Epoch 4 -- Batch 226/ 842, training loss 0.5478659272193909\n",
      "Epoch 4 -- Batch 227/ 842, training loss 0.543798565864563\n",
      "Epoch 4 -- Batch 228/ 842, training loss 0.530564546585083\n",
      "Epoch 4 -- Batch 229/ 842, training loss 0.5630859732627869\n",
      "Epoch 4 -- Batch 230/ 842, training loss 0.5369240045547485\n",
      "Epoch 4 -- Batch 231/ 842, training loss 0.5621868968009949\n",
      "Epoch 4 -- Batch 232/ 842, training loss 0.5471372008323669\n",
      "Epoch 4 -- Batch 233/ 842, training loss 0.5510376691818237\n",
      "Epoch 4 -- Batch 234/ 842, training loss 0.5460535287857056\n",
      "Epoch 4 -- Batch 235/ 842, training loss 0.5367856621742249\n",
      "Epoch 4 -- Batch 236/ 842, training loss 0.5524445176124573\n",
      "Epoch 4 -- Batch 237/ 842, training loss 0.5322170853614807\n",
      "Epoch 4 -- Batch 238/ 842, training loss 0.5347617864608765\n",
      "Epoch 4 -- Batch 239/ 842, training loss 0.5134553909301758\n",
      "Epoch 4 -- Batch 240/ 842, training loss 0.527331531047821\n",
      "Epoch 4 -- Batch 241/ 842, training loss 0.551992654800415\n",
      "Epoch 4 -- Batch 242/ 842, training loss 0.5354312062263489\n",
      "Epoch 4 -- Batch 243/ 842, training loss 0.5437408685684204\n",
      "Epoch 4 -- Batch 244/ 842, training loss 0.5177977085113525\n",
      "Epoch 4 -- Batch 245/ 842, training loss 0.5375441312789917\n",
      "Epoch 4 -- Batch 246/ 842, training loss 0.5399277806282043\n",
      "Epoch 4 -- Batch 247/ 842, training loss 0.5516526699066162\n",
      "Epoch 4 -- Batch 248/ 842, training loss 0.546354353427887\n",
      "Epoch 4 -- Batch 249/ 842, training loss 0.5265711545944214\n",
      "Epoch 4 -- Batch 250/ 842, training loss 0.5463632345199585\n",
      "Epoch 4 -- Batch 251/ 842, training loss 0.5332370400428772\n",
      "Epoch 4 -- Batch 252/ 842, training loss 0.5201104879379272\n",
      "Epoch 4 -- Batch 253/ 842, training loss 0.534030556678772\n",
      "Epoch 4 -- Batch 254/ 842, training loss 0.5400500297546387\n",
      "Epoch 4 -- Batch 255/ 842, training loss 0.5540280938148499\n",
      "Epoch 4 -- Batch 256/ 842, training loss 0.5484480857849121\n",
      "Epoch 4 -- Batch 257/ 842, training loss 0.51901775598526\n",
      "Epoch 4 -- Batch 258/ 842, training loss 0.5524517893791199\n",
      "Epoch 4 -- Batch 259/ 842, training loss 0.5395535826683044\n",
      "Epoch 4 -- Batch 260/ 842, training loss 0.546172559261322\n",
      "Epoch 4 -- Batch 261/ 842, training loss 0.5214336514472961\n",
      "Epoch 4 -- Batch 262/ 842, training loss 0.5486591458320618\n",
      "Epoch 4 -- Batch 263/ 842, training loss 0.5274229049682617\n",
      "Epoch 4 -- Batch 264/ 842, training loss 0.542097806930542\n",
      "Epoch 4 -- Batch 265/ 842, training loss 0.5365087985992432\n",
      "Epoch 4 -- Batch 266/ 842, training loss 0.5190708041191101\n",
      "Epoch 4 -- Batch 267/ 842, training loss 0.5229954123497009\n",
      "Epoch 4 -- Batch 268/ 842, training loss 0.5428801774978638\n",
      "Epoch 4 -- Batch 269/ 842, training loss 0.5155432820320129\n",
      "Epoch 4 -- Batch 270/ 842, training loss 0.5395146608352661\n",
      "Epoch 4 -- Batch 271/ 842, training loss 0.5341410040855408\n",
      "Epoch 4 -- Batch 272/ 842, training loss 0.5170297026634216\n",
      "Epoch 4 -- Batch 273/ 842, training loss 0.5247397422790527\n",
      "Epoch 4 -- Batch 274/ 842, training loss 0.5198796987533569\n",
      "Epoch 4 -- Batch 275/ 842, training loss 0.5425004363059998\n",
      "Epoch 4 -- Batch 276/ 842, training loss 0.522429883480072\n",
      "Epoch 4 -- Batch 277/ 842, training loss 0.5436949729919434\n",
      "Epoch 4 -- Batch 278/ 842, training loss 0.5330170392990112\n",
      "Epoch 4 -- Batch 279/ 842, training loss 0.5319051146507263\n",
      "Epoch 4 -- Batch 280/ 842, training loss 0.5353369116783142\n",
      "Epoch 4 -- Batch 281/ 842, training loss 0.564424991607666\n",
      "Epoch 4 -- Batch 282/ 842, training loss 0.5434243679046631\n",
      "Epoch 4 -- Batch 283/ 842, training loss 0.5383277535438538\n",
      "Epoch 4 -- Batch 284/ 842, training loss 0.556489884853363\n",
      "Epoch 4 -- Batch 285/ 842, training loss 0.5240452289581299\n",
      "Epoch 4 -- Batch 286/ 842, training loss 0.5379824042320251\n",
      "Epoch 4 -- Batch 287/ 842, training loss 0.5391920208930969\n",
      "Epoch 4 -- Batch 288/ 842, training loss 0.5268259048461914\n",
      "Epoch 4 -- Batch 289/ 842, training loss 0.5432371497154236\n",
      "Epoch 4 -- Batch 290/ 842, training loss 0.5196483731269836\n",
      "Epoch 4 -- Batch 291/ 842, training loss 0.5449590086936951\n",
      "Epoch 4 -- Batch 292/ 842, training loss 0.5544847846031189\n",
      "Epoch 4 -- Batch 293/ 842, training loss 0.521632730960846\n",
      "Epoch 4 -- Batch 294/ 842, training loss 0.5223532319068909\n",
      "Epoch 4 -- Batch 295/ 842, training loss 0.5549747347831726\n",
      "Epoch 4 -- Batch 296/ 842, training loss 0.5319778323173523\n",
      "Epoch 4 -- Batch 297/ 842, training loss 0.562229573726654\n",
      "Epoch 4 -- Batch 298/ 842, training loss 0.540020227432251\n",
      "Epoch 4 -- Batch 299/ 842, training loss 0.530123770236969\n",
      "Epoch 4 -- Batch 300/ 842, training loss 0.5370752811431885\n",
      "Epoch 4 -- Batch 301/ 842, training loss 0.5286878943443298\n",
      "Epoch 4 -- Batch 302/ 842, training loss 0.5220820903778076\n",
      "Epoch 4 -- Batch 303/ 842, training loss 0.5502293109893799\n",
      "Epoch 4 -- Batch 304/ 842, training loss 0.5451221466064453\n",
      "Epoch 4 -- Batch 305/ 842, training loss 0.5333055257797241\n",
      "Epoch 4 -- Batch 306/ 842, training loss 0.5498864650726318\n",
      "Epoch 4 -- Batch 307/ 842, training loss 0.5111235976219177\n",
      "Epoch 4 -- Batch 308/ 842, training loss 0.555535614490509\n",
      "Epoch 4 -- Batch 309/ 842, training loss 0.5236800312995911\n",
      "Epoch 4 -- Batch 310/ 842, training loss 0.5518914461135864\n",
      "Epoch 4 -- Batch 311/ 842, training loss 0.5414925217628479\n",
      "Epoch 4 -- Batch 312/ 842, training loss 0.5301598310470581\n",
      "Epoch 4 -- Batch 313/ 842, training loss 0.5368867516517639\n",
      "Epoch 4 -- Batch 314/ 842, training loss 0.5289832949638367\n",
      "Epoch 4 -- Batch 315/ 842, training loss 0.5691217184066772\n",
      "Epoch 4 -- Batch 316/ 842, training loss 0.5349304676055908\n",
      "Epoch 4 -- Batch 317/ 842, training loss 0.5333250761032104\n",
      "Epoch 4 -- Batch 318/ 842, training loss 0.5495729446411133\n",
      "Epoch 4 -- Batch 319/ 842, training loss 0.5385535359382629\n",
      "Epoch 4 -- Batch 320/ 842, training loss 0.5368750691413879\n",
      "Epoch 4 -- Batch 321/ 842, training loss 0.5106041431427002\n",
      "Epoch 4 -- Batch 322/ 842, training loss 0.5261702537536621\n",
      "Epoch 4 -- Batch 323/ 842, training loss 0.5067122578620911\n",
      "Epoch 4 -- Batch 324/ 842, training loss 0.5587722063064575\n",
      "Epoch 4 -- Batch 325/ 842, training loss 0.5471048951148987\n",
      "Epoch 4 -- Batch 326/ 842, training loss 0.5365373492240906\n",
      "Epoch 4 -- Batch 327/ 842, training loss 0.5449793934822083\n",
      "Epoch 4 -- Batch 328/ 842, training loss 0.5205704569816589\n",
      "Epoch 4 -- Batch 329/ 842, training loss 0.5391786694526672\n",
      "Epoch 4 -- Batch 330/ 842, training loss 0.529651403427124\n",
      "Epoch 4 -- Batch 331/ 842, training loss 0.5433691740036011\n",
      "Epoch 4 -- Batch 332/ 842, training loss 0.5408852100372314\n",
      "Epoch 4 -- Batch 333/ 842, training loss 0.5271445512771606\n",
      "Epoch 4 -- Batch 334/ 842, training loss 0.5546610951423645\n",
      "Epoch 4 -- Batch 335/ 842, training loss 0.5185677409172058\n",
      "Epoch 4 -- Batch 336/ 842, training loss 0.5573521256446838\n",
      "Epoch 4 -- Batch 337/ 842, training loss 0.5407816171646118\n",
      "Epoch 4 -- Batch 338/ 842, training loss 0.5511393547058105\n",
      "Epoch 4 -- Batch 339/ 842, training loss 0.5450001955032349\n",
      "Epoch 4 -- Batch 340/ 842, training loss 0.5307289958000183\n",
      "Epoch 4 -- Batch 341/ 842, training loss 0.5346001386642456\n",
      "Epoch 4 -- Batch 342/ 842, training loss 0.5456829071044922\n",
      "Epoch 4 -- Batch 343/ 842, training loss 0.5401731133460999\n",
      "Epoch 4 -- Batch 344/ 842, training loss 0.5188348293304443\n",
      "Epoch 4 -- Batch 345/ 842, training loss 0.5304173827171326\n",
      "Epoch 4 -- Batch 346/ 842, training loss 0.5536214113235474\n",
      "Epoch 4 -- Batch 347/ 842, training loss 0.5468875765800476\n",
      "Epoch 4 -- Batch 348/ 842, training loss 0.5347036719322205\n",
      "Epoch 4 -- Batch 349/ 842, training loss 0.531607985496521\n",
      "Epoch 4 -- Batch 350/ 842, training loss 0.5029942989349365\n",
      "Epoch 4 -- Batch 351/ 842, training loss 0.5239476561546326\n",
      "Epoch 4 -- Batch 352/ 842, training loss 0.5591708421707153\n",
      "Epoch 4 -- Batch 353/ 842, training loss 0.5404396057128906\n",
      "Epoch 4 -- Batch 354/ 842, training loss 0.5357428789138794\n",
      "Epoch 4 -- Batch 355/ 842, training loss 0.529712438583374\n",
      "Epoch 4 -- Batch 356/ 842, training loss 0.5212500095367432\n",
      "Epoch 4 -- Batch 357/ 842, training loss 0.5541850328445435\n",
      "Epoch 4 -- Batch 358/ 842, training loss 0.5513656139373779\n",
      "Epoch 4 -- Batch 359/ 842, training loss 0.5342716574668884\n",
      "Epoch 4 -- Batch 360/ 842, training loss 0.5336979627609253\n",
      "Epoch 4 -- Batch 361/ 842, training loss 0.5155205726623535\n",
      "Epoch 4 -- Batch 362/ 842, training loss 0.5225355625152588\n",
      "Epoch 4 -- Batch 363/ 842, training loss 0.5364136099815369\n",
      "Epoch 4 -- Batch 364/ 842, training loss 0.5499730706214905\n",
      "Epoch 4 -- Batch 365/ 842, training loss 0.5434591174125671\n",
      "Epoch 4 -- Batch 366/ 842, training loss 0.5504834651947021\n",
      "Epoch 4 -- Batch 367/ 842, training loss 0.5456542372703552\n",
      "Epoch 4 -- Batch 368/ 842, training loss 0.5317813754081726\n",
      "Epoch 4 -- Batch 369/ 842, training loss 0.5402326583862305\n",
      "Epoch 4 -- Batch 370/ 842, training loss 0.5375166535377502\n",
      "Epoch 4 -- Batch 371/ 842, training loss 0.5183521509170532\n",
      "Epoch 4 -- Batch 372/ 842, training loss 0.5172469019889832\n",
      "Epoch 4 -- Batch 373/ 842, training loss 0.5397644639015198\n",
      "Epoch 4 -- Batch 374/ 842, training loss 0.5457172989845276\n",
      "Epoch 4 -- Batch 375/ 842, training loss 0.5424777865409851\n",
      "Epoch 4 -- Batch 376/ 842, training loss 0.5367748737335205\n",
      "Epoch 4 -- Batch 377/ 842, training loss 0.5241237282752991\n",
      "Epoch 4 -- Batch 378/ 842, training loss 0.5144459009170532\n",
      "Epoch 4 -- Batch 379/ 842, training loss 0.5468367338180542\n",
      "Epoch 4 -- Batch 380/ 842, training loss 0.532599151134491\n",
      "Epoch 4 -- Batch 381/ 842, training loss 0.5177894234657288\n",
      "Epoch 4 -- Batch 382/ 842, training loss 0.5320932269096375\n",
      "Epoch 4 -- Batch 383/ 842, training loss 0.5226835608482361\n",
      "Epoch 4 -- Batch 384/ 842, training loss 0.5597786903381348\n",
      "Epoch 4 -- Batch 385/ 842, training loss 0.5162980556488037\n",
      "Epoch 4 -- Batch 386/ 842, training loss 0.5373381972312927\n",
      "Epoch 4 -- Batch 387/ 842, training loss 0.5676054358482361\n",
      "Epoch 4 -- Batch 388/ 842, training loss 0.5403003096580505\n",
      "Epoch 4 -- Batch 389/ 842, training loss 0.5422793030738831\n",
      "Epoch 4 -- Batch 390/ 842, training loss 0.5065470933914185\n",
      "Epoch 4 -- Batch 391/ 842, training loss 0.5391345024108887\n",
      "Epoch 4 -- Batch 392/ 842, training loss 0.5419144630432129\n",
      "Epoch 4 -- Batch 393/ 842, training loss 0.5179576873779297\n",
      "Epoch 4 -- Batch 394/ 842, training loss 0.5128606557846069\n",
      "Epoch 4 -- Batch 395/ 842, training loss 0.5285592675209045\n",
      "Epoch 4 -- Batch 396/ 842, training loss 0.5356134176254272\n",
      "Epoch 4 -- Batch 397/ 842, training loss 0.5398000478744507\n",
      "Epoch 4 -- Batch 398/ 842, training loss 0.5280213356018066\n",
      "Epoch 4 -- Batch 399/ 842, training loss 0.5234494805335999\n",
      "Epoch 4 -- Batch 400/ 842, training loss 0.5453637838363647\n",
      "Epoch 4 -- Batch 401/ 842, training loss 0.5491076707839966\n",
      "Epoch 4 -- Batch 402/ 842, training loss 0.5170590281486511\n",
      "Epoch 4 -- Batch 403/ 842, training loss 0.5333297252655029\n",
      "Epoch 4 -- Batch 404/ 842, training loss 0.5286605358123779\n",
      "Epoch 4 -- Batch 405/ 842, training loss 0.5561359524726868\n",
      "Epoch 4 -- Batch 406/ 842, training loss 0.5440285205841064\n",
      "Epoch 4 -- Batch 407/ 842, training loss 0.536821186542511\n",
      "Epoch 4 -- Batch 408/ 842, training loss 0.5406602025032043\n",
      "Epoch 4 -- Batch 409/ 842, training loss 0.5237594842910767\n",
      "Epoch 4 -- Batch 410/ 842, training loss 0.5465126037597656\n",
      "Epoch 4 -- Batch 411/ 842, training loss 0.5191821455955505\n",
      "Epoch 4 -- Batch 412/ 842, training loss 0.5237964987754822\n",
      "Epoch 4 -- Batch 413/ 842, training loss 0.5582302212715149\n",
      "Epoch 4 -- Batch 414/ 842, training loss 0.530110776424408\n",
      "Epoch 4 -- Batch 415/ 842, training loss 0.5329222083091736\n",
      "Epoch 4 -- Batch 416/ 842, training loss 0.5395601391792297\n",
      "Epoch 4 -- Batch 417/ 842, training loss 0.5414635539054871\n",
      "Epoch 4 -- Batch 418/ 842, training loss 0.5163602828979492\n",
      "Epoch 4 -- Batch 419/ 842, training loss 0.5253881812095642\n",
      "Epoch 4 -- Batch 420/ 842, training loss 0.5466280579566956\n",
      "Epoch 4 -- Batch 421/ 842, training loss 0.5385194420814514\n",
      "Epoch 4 -- Batch 422/ 842, training loss 0.5251612663269043\n",
      "Epoch 4 -- Batch 423/ 842, training loss 0.553801953792572\n",
      "Epoch 4 -- Batch 424/ 842, training loss 0.5239078402519226\n",
      "Epoch 4 -- Batch 425/ 842, training loss 0.5214093327522278\n",
      "Epoch 4 -- Batch 426/ 842, training loss 0.562324583530426\n",
      "Epoch 4 -- Batch 427/ 842, training loss 0.5437552332878113\n",
      "Epoch 4 -- Batch 428/ 842, training loss 0.5446845293045044\n",
      "Epoch 4 -- Batch 429/ 842, training loss 0.5563395023345947\n",
      "Epoch 4 -- Batch 430/ 842, training loss 0.5123288631439209\n",
      "Epoch 4 -- Batch 431/ 842, training loss 0.5373334884643555\n",
      "Epoch 4 -- Batch 432/ 842, training loss 0.5468845367431641\n",
      "Epoch 4 -- Batch 433/ 842, training loss 0.5375096201896667\n",
      "Epoch 4 -- Batch 434/ 842, training loss 0.5467578172683716\n",
      "Epoch 4 -- Batch 435/ 842, training loss 0.5110416412353516\n",
      "Epoch 4 -- Batch 436/ 842, training loss 0.5396881699562073\n",
      "Epoch 4 -- Batch 437/ 842, training loss 0.513365626335144\n",
      "Epoch 4 -- Batch 438/ 842, training loss 0.5385584831237793\n",
      "Epoch 4 -- Batch 439/ 842, training loss 0.5480331182479858\n",
      "Epoch 4 -- Batch 440/ 842, training loss 0.5508783459663391\n",
      "Epoch 4 -- Batch 441/ 842, training loss 0.5246284604072571\n",
      "Epoch 4 -- Batch 442/ 842, training loss 0.5691183805465698\n",
      "Epoch 4 -- Batch 443/ 842, training loss 0.5322796702384949\n",
      "Epoch 4 -- Batch 444/ 842, training loss 0.5251612067222595\n",
      "Epoch 4 -- Batch 445/ 842, training loss 0.5381590723991394\n",
      "Epoch 4 -- Batch 446/ 842, training loss 0.5344011187553406\n",
      "Epoch 4 -- Batch 447/ 842, training loss 0.5598956346511841\n",
      "Epoch 4 -- Batch 448/ 842, training loss 0.5505353212356567\n",
      "Epoch 4 -- Batch 449/ 842, training loss 0.5387325286865234\n",
      "Epoch 4 -- Batch 450/ 842, training loss 0.5283295512199402\n",
      "Epoch 4 -- Batch 451/ 842, training loss 0.5591698884963989\n",
      "Epoch 4 -- Batch 452/ 842, training loss 0.5326084494590759\n",
      "Epoch 4 -- Batch 453/ 842, training loss 0.5372732877731323\n",
      "Epoch 4 -- Batch 454/ 842, training loss 0.5158199667930603\n",
      "Epoch 4 -- Batch 455/ 842, training loss 0.5302824974060059\n",
      "Epoch 4 -- Batch 456/ 842, training loss 0.550483226776123\n",
      "Epoch 4 -- Batch 457/ 842, training loss 0.532848060131073\n",
      "Epoch 4 -- Batch 458/ 842, training loss 0.5284092426300049\n",
      "Epoch 4 -- Batch 459/ 842, training loss 0.5306149125099182\n",
      "Epoch 4 -- Batch 460/ 842, training loss 0.49774760007858276\n",
      "Epoch 4 -- Batch 461/ 842, training loss 0.5426039695739746\n",
      "Epoch 4 -- Batch 462/ 842, training loss 0.5485615134239197\n",
      "Epoch 4 -- Batch 463/ 842, training loss 0.5603224635124207\n",
      "Epoch 4 -- Batch 464/ 842, training loss 0.5353507995605469\n",
      "Epoch 4 -- Batch 465/ 842, training loss 0.5245795249938965\n",
      "Epoch 4 -- Batch 466/ 842, training loss 0.556489884853363\n",
      "Epoch 4 -- Batch 467/ 842, training loss 0.5594298243522644\n",
      "Epoch 4 -- Batch 468/ 842, training loss 0.5213446021080017\n",
      "Epoch 4 -- Batch 469/ 842, training loss 0.5261081457138062\n",
      "Epoch 4 -- Batch 470/ 842, training loss 0.5413402915000916\n",
      "Epoch 4 -- Batch 471/ 842, training loss 0.5467110276222229\n",
      "Epoch 4 -- Batch 472/ 842, training loss 0.5532200932502747\n",
      "Epoch 4 -- Batch 473/ 842, training loss 0.5336102247238159\n",
      "Epoch 4 -- Batch 474/ 842, training loss 0.5369591116905212\n",
      "Epoch 4 -- Batch 475/ 842, training loss 0.5183228254318237\n",
      "Epoch 4 -- Batch 476/ 842, training loss 0.5334618091583252\n",
      "Epoch 4 -- Batch 477/ 842, training loss 0.5139279365539551\n",
      "Epoch 4 -- Batch 478/ 842, training loss 0.5135414600372314\n",
      "Epoch 4 -- Batch 479/ 842, training loss 0.5290430188179016\n",
      "Epoch 4 -- Batch 480/ 842, training loss 0.5244055986404419\n",
      "Epoch 4 -- Batch 481/ 842, training loss 0.5356900095939636\n",
      "Epoch 4 -- Batch 482/ 842, training loss 0.5268247723579407\n",
      "Epoch 4 -- Batch 483/ 842, training loss 0.5291255116462708\n",
      "Epoch 4 -- Batch 484/ 842, training loss 0.5463613867759705\n",
      "Epoch 4 -- Batch 485/ 842, training loss 0.5133901834487915\n",
      "Epoch 4 -- Batch 486/ 842, training loss 0.5737035274505615\n",
      "Epoch 4 -- Batch 487/ 842, training loss 0.5244573950767517\n",
      "Epoch 4 -- Batch 488/ 842, training loss 0.5336019992828369\n",
      "Epoch 4 -- Batch 489/ 842, training loss 0.5200071334838867\n",
      "Epoch 4 -- Batch 490/ 842, training loss 0.533318817615509\n",
      "Epoch 4 -- Batch 491/ 842, training loss 0.5385794043540955\n",
      "Epoch 4 -- Batch 492/ 842, training loss 0.5316524505615234\n",
      "Epoch 4 -- Batch 493/ 842, training loss 0.5378472805023193\n",
      "Epoch 4 -- Batch 494/ 842, training loss 0.5499110817909241\n",
      "Epoch 4 -- Batch 495/ 842, training loss 0.5484551787376404\n",
      "Epoch 4 -- Batch 496/ 842, training loss 0.54757159948349\n",
      "Epoch 4 -- Batch 497/ 842, training loss 0.5457910299301147\n",
      "Epoch 4 -- Batch 498/ 842, training loss 0.5057839155197144\n",
      "Epoch 4 -- Batch 499/ 842, training loss 0.5235811471939087\n",
      "Epoch 4 -- Batch 500/ 842, training loss 0.5332277417182922\n",
      "Epoch 4 -- Batch 501/ 842, training loss 0.5253312587738037\n",
      "Epoch 4 -- Batch 502/ 842, training loss 0.5284455418586731\n",
      "Epoch 4 -- Batch 503/ 842, training loss 0.5416593551635742\n",
      "Epoch 4 -- Batch 504/ 842, training loss 0.53593510389328\n",
      "Epoch 4 -- Batch 505/ 842, training loss 0.544954776763916\n",
      "Epoch 4 -- Batch 506/ 842, training loss 0.52653568983078\n",
      "Epoch 4 -- Batch 507/ 842, training loss 0.5512306094169617\n",
      "Epoch 4 -- Batch 508/ 842, training loss 0.524497926235199\n",
      "Epoch 4 -- Batch 509/ 842, training loss 0.5239234566688538\n",
      "Epoch 4 -- Batch 510/ 842, training loss 0.5097880363464355\n",
      "Epoch 4 -- Batch 511/ 842, training loss 0.5341716408729553\n",
      "Epoch 4 -- Batch 512/ 842, training loss 0.5216853618621826\n",
      "Epoch 4 -- Batch 513/ 842, training loss 0.5318398475646973\n",
      "Epoch 4 -- Batch 514/ 842, training loss 0.5270412564277649\n",
      "Epoch 4 -- Batch 515/ 842, training loss 0.5342832803726196\n",
      "Epoch 4 -- Batch 516/ 842, training loss 0.5345432162284851\n",
      "Epoch 4 -- Batch 517/ 842, training loss 0.536340057849884\n",
      "Epoch 4 -- Batch 518/ 842, training loss 0.5368379354476929\n",
      "Epoch 4 -- Batch 519/ 842, training loss 0.536027193069458\n",
      "Epoch 4 -- Batch 520/ 842, training loss 0.5401167869567871\n",
      "Epoch 4 -- Batch 521/ 842, training loss 0.5480334758758545\n",
      "Epoch 4 -- Batch 522/ 842, training loss 0.5404837131500244\n",
      "Epoch 4 -- Batch 523/ 842, training loss 0.5296133756637573\n",
      "Epoch 4 -- Batch 524/ 842, training loss 0.5271852612495422\n",
      "Epoch 4 -- Batch 525/ 842, training loss 0.5554662942886353\n",
      "Epoch 4 -- Batch 526/ 842, training loss 0.546150803565979\n",
      "Epoch 4 -- Batch 527/ 842, training loss 0.5657811164855957\n",
      "Epoch 4 -- Batch 528/ 842, training loss 0.5259203910827637\n",
      "Epoch 4 -- Batch 529/ 842, training loss 0.5290507078170776\n",
      "Epoch 4 -- Batch 530/ 842, training loss 0.5479767918586731\n",
      "Epoch 4 -- Batch 531/ 842, training loss 0.5367326736450195\n",
      "Epoch 4 -- Batch 532/ 842, training loss 0.5170412659645081\n",
      "Epoch 4 -- Batch 533/ 842, training loss 0.5135815739631653\n",
      "Epoch 4 -- Batch 534/ 842, training loss 0.5267020463943481\n",
      "Epoch 4 -- Batch 535/ 842, training loss 0.5252298712730408\n",
      "Epoch 4 -- Batch 536/ 842, training loss 0.5225344300270081\n",
      "Epoch 4 -- Batch 537/ 842, training loss 0.49730291962623596\n",
      "Epoch 4 -- Batch 538/ 842, training loss 0.5557858347892761\n",
      "Epoch 4 -- Batch 539/ 842, training loss 0.5355688333511353\n",
      "Epoch 4 -- Batch 540/ 842, training loss 0.519507110118866\n",
      "Epoch 4 -- Batch 541/ 842, training loss 0.5312519073486328\n",
      "Epoch 4 -- Batch 542/ 842, training loss 0.5072863101959229\n",
      "Epoch 4 -- Batch 543/ 842, training loss 0.5369550585746765\n",
      "Epoch 4 -- Batch 544/ 842, training loss 0.5573114156723022\n",
      "Epoch 4 -- Batch 545/ 842, training loss 0.5282266736030579\n",
      "Epoch 4 -- Batch 546/ 842, training loss 0.5142320394515991\n",
      "Epoch 4 -- Batch 547/ 842, training loss 0.5164070725440979\n",
      "Epoch 4 -- Batch 548/ 842, training loss 0.5082400441169739\n",
      "Epoch 4 -- Batch 549/ 842, training loss 0.5150194764137268\n",
      "Epoch 4 -- Batch 550/ 842, training loss 0.5273065567016602\n",
      "Epoch 4 -- Batch 551/ 842, training loss 0.5137579441070557\n",
      "Epoch 4 -- Batch 552/ 842, training loss 0.5330409407615662\n",
      "Epoch 4 -- Batch 553/ 842, training loss 0.5292991995811462\n",
      "Epoch 4 -- Batch 554/ 842, training loss 0.5412519574165344\n",
      "Epoch 4 -- Batch 555/ 842, training loss 0.5160703659057617\n",
      "Epoch 4 -- Batch 556/ 842, training loss 0.5288068056106567\n",
      "Epoch 4 -- Batch 557/ 842, training loss 0.5355164408683777\n",
      "Epoch 4 -- Batch 558/ 842, training loss 0.5108648538589478\n",
      "Epoch 4 -- Batch 559/ 842, training loss 0.5317522883415222\n",
      "Epoch 4 -- Batch 560/ 842, training loss 0.5298341512680054\n",
      "Epoch 4 -- Batch 561/ 842, training loss 0.5334253311157227\n",
      "Epoch 4 -- Batch 562/ 842, training loss 0.5099397897720337\n",
      "Epoch 4 -- Batch 563/ 842, training loss 0.5420474410057068\n",
      "Epoch 4 -- Batch 564/ 842, training loss 0.5364550352096558\n",
      "Epoch 4 -- Batch 565/ 842, training loss 0.5496475100517273\n",
      "Epoch 4 -- Batch 566/ 842, training loss 0.5326221585273743\n",
      "Epoch 4 -- Batch 567/ 842, training loss 0.5374590754508972\n",
      "Epoch 4 -- Batch 568/ 842, training loss 0.5301899909973145\n",
      "Epoch 4 -- Batch 569/ 842, training loss 0.5440338253974915\n",
      "Epoch 4 -- Batch 570/ 842, training loss 0.543380856513977\n",
      "Epoch 4 -- Batch 571/ 842, training loss 0.5406430959701538\n",
      "Epoch 4 -- Batch 572/ 842, training loss 0.5436785817146301\n",
      "Epoch 4 -- Batch 573/ 842, training loss 0.5196518898010254\n",
      "Epoch 4 -- Batch 574/ 842, training loss 0.5434904098510742\n",
      "Epoch 4 -- Batch 575/ 842, training loss 0.5517591834068298\n",
      "Epoch 4 -- Batch 576/ 842, training loss 0.5426682233810425\n",
      "Epoch 4 -- Batch 577/ 842, training loss 0.5248993039131165\n",
      "Epoch 4 -- Batch 578/ 842, training loss 0.5163203477859497\n",
      "Epoch 4 -- Batch 579/ 842, training loss 0.5238254070281982\n",
      "Epoch 4 -- Batch 580/ 842, training loss 0.5305116772651672\n",
      "Epoch 4 -- Batch 581/ 842, training loss 0.5193804502487183\n",
      "Epoch 4 -- Batch 582/ 842, training loss 0.5178225040435791\n",
      "Epoch 4 -- Batch 583/ 842, training loss 0.5446780323982239\n",
      "Epoch 4 -- Batch 584/ 842, training loss 0.5122292041778564\n",
      "Epoch 4 -- Batch 585/ 842, training loss 0.5571300387382507\n",
      "Epoch 4 -- Batch 586/ 842, training loss 0.5463256239891052\n",
      "Epoch 4 -- Batch 587/ 842, training loss 0.5418229699134827\n",
      "Epoch 4 -- Batch 588/ 842, training loss 0.5349541306495667\n",
      "Epoch 4 -- Batch 589/ 842, training loss 0.547024130821228\n",
      "Epoch 4 -- Batch 590/ 842, training loss 0.5294491052627563\n",
      "Epoch 4 -- Batch 591/ 842, training loss 0.5474305748939514\n",
      "Epoch 4 -- Batch 592/ 842, training loss 0.5378963947296143\n",
      "Epoch 4 -- Batch 593/ 842, training loss 0.5225294828414917\n",
      "Epoch 4 -- Batch 594/ 842, training loss 0.5136707425117493\n",
      "Epoch 4 -- Batch 595/ 842, training loss 0.522253155708313\n",
      "Epoch 4 -- Batch 596/ 842, training loss 0.5628749132156372\n",
      "Epoch 4 -- Batch 597/ 842, training loss 0.5182865858078003\n",
      "Epoch 4 -- Batch 598/ 842, training loss 0.5337738394737244\n",
      "Epoch 4 -- Batch 599/ 842, training loss 0.5453906655311584\n",
      "Epoch 4 -- Batch 600/ 842, training loss 0.5226461887359619\n",
      "Epoch 4 -- Batch 601/ 842, training loss 0.5377176403999329\n",
      "Epoch 4 -- Batch 602/ 842, training loss 0.5195155739784241\n",
      "Epoch 4 -- Batch 603/ 842, training loss 0.5463177561759949\n",
      "Epoch 4 -- Batch 604/ 842, training loss 0.5356407761573792\n",
      "Epoch 4 -- Batch 605/ 842, training loss 0.52289217710495\n",
      "Epoch 4 -- Batch 606/ 842, training loss 0.5567139983177185\n",
      "Epoch 4 -- Batch 607/ 842, training loss 0.5186284780502319\n",
      "Epoch 4 -- Batch 608/ 842, training loss 0.518602728843689\n",
      "Epoch 4 -- Batch 609/ 842, training loss 0.5221793055534363\n",
      "Epoch 4 -- Batch 610/ 842, training loss 0.547882616519928\n",
      "Epoch 4 -- Batch 611/ 842, training loss 0.5242955684661865\n",
      "Epoch 4 -- Batch 612/ 842, training loss 0.5428273677825928\n",
      "Epoch 4 -- Batch 613/ 842, training loss 0.5571341514587402\n",
      "Epoch 4 -- Batch 614/ 842, training loss 0.5304853320121765\n",
      "Epoch 4 -- Batch 615/ 842, training loss 0.5605949759483337\n",
      "Epoch 4 -- Batch 616/ 842, training loss 0.5322455763816833\n",
      "Epoch 4 -- Batch 617/ 842, training loss 0.5468170046806335\n",
      "Epoch 4 -- Batch 618/ 842, training loss 0.5426548719406128\n",
      "Epoch 4 -- Batch 619/ 842, training loss 0.5163338780403137\n",
      "Epoch 4 -- Batch 620/ 842, training loss 0.5235217213630676\n",
      "Epoch 4 -- Batch 621/ 842, training loss 0.5375577211380005\n",
      "Epoch 4 -- Batch 622/ 842, training loss 0.5145567655563354\n",
      "Epoch 4 -- Batch 623/ 842, training loss 0.5281960368156433\n",
      "Epoch 4 -- Batch 624/ 842, training loss 0.5168402194976807\n",
      "Epoch 4 -- Batch 625/ 842, training loss 0.497445672750473\n",
      "Epoch 4 -- Batch 626/ 842, training loss 0.5469447374343872\n",
      "Epoch 4 -- Batch 627/ 842, training loss 0.5589219331741333\n",
      "Epoch 4 -- Batch 628/ 842, training loss 0.5286754965782166\n",
      "Epoch 4 -- Batch 629/ 842, training loss 0.5100598931312561\n",
      "Epoch 4 -- Batch 630/ 842, training loss 0.5408279299736023\n",
      "Epoch 4 -- Batch 631/ 842, training loss 0.5165805220603943\n",
      "Epoch 4 -- Batch 632/ 842, training loss 0.5223888158798218\n",
      "Epoch 4 -- Batch 633/ 842, training loss 0.522882342338562\n",
      "Epoch 4 -- Batch 634/ 842, training loss 0.5353848934173584\n",
      "Epoch 4 -- Batch 635/ 842, training loss 0.5230656862258911\n",
      "Epoch 4 -- Batch 636/ 842, training loss 0.544075608253479\n",
      "Epoch 4 -- Batch 637/ 842, training loss 0.5429731011390686\n",
      "Epoch 4 -- Batch 638/ 842, training loss 0.5357221961021423\n",
      "Epoch 4 -- Batch 639/ 842, training loss 0.5365065336227417\n",
      "Epoch 4 -- Batch 640/ 842, training loss 0.5162121057510376\n",
      "Epoch 4 -- Batch 641/ 842, training loss 0.5275105237960815\n",
      "Epoch 4 -- Batch 642/ 842, training loss 0.5186108350753784\n",
      "Epoch 4 -- Batch 643/ 842, training loss 0.5346634387969971\n",
      "Epoch 4 -- Batch 644/ 842, training loss 0.5258711576461792\n",
      "Epoch 4 -- Batch 645/ 842, training loss 0.5221222043037415\n",
      "Epoch 4 -- Batch 646/ 842, training loss 0.5212843418121338\n",
      "Epoch 4 -- Batch 647/ 842, training loss 0.5312581658363342\n",
      "Epoch 4 -- Batch 648/ 842, training loss 0.5517414808273315\n",
      "Epoch 4 -- Batch 649/ 842, training loss 0.5388218760490417\n",
      "Epoch 4 -- Batch 650/ 842, training loss 0.534723699092865\n",
      "Epoch 4 -- Batch 651/ 842, training loss 0.5205889940261841\n",
      "Epoch 4 -- Batch 652/ 842, training loss 0.5209495425224304\n",
      "Epoch 4 -- Batch 653/ 842, training loss 0.5396052002906799\n",
      "Epoch 4 -- Batch 654/ 842, training loss 0.5327463746070862\n",
      "Epoch 4 -- Batch 655/ 842, training loss 0.520606517791748\n",
      "Epoch 4 -- Batch 656/ 842, training loss 0.5180955529212952\n",
      "Epoch 4 -- Batch 657/ 842, training loss 0.5202106833457947\n",
      "Epoch 4 -- Batch 658/ 842, training loss 0.5169284343719482\n",
      "Epoch 4 -- Batch 659/ 842, training loss 0.5448182821273804\n",
      "Epoch 4 -- Batch 660/ 842, training loss 0.5333144664764404\n",
      "Epoch 4 -- Batch 661/ 842, training loss 0.5186567306518555\n",
      "Epoch 4 -- Batch 662/ 842, training loss 0.5317482948303223\n",
      "Epoch 4 -- Batch 663/ 842, training loss 0.5191786289215088\n",
      "Epoch 4 -- Batch 664/ 842, training loss 0.512261152267456\n",
      "Epoch 4 -- Batch 665/ 842, training loss 0.5018854737281799\n",
      "Epoch 4 -- Batch 666/ 842, training loss 0.5146268606185913\n",
      "Epoch 4 -- Batch 667/ 842, training loss 0.513124406337738\n",
      "Epoch 4 -- Batch 668/ 842, training loss 0.5312502384185791\n",
      "Epoch 4 -- Batch 669/ 842, training loss 0.529904842376709\n",
      "Epoch 4 -- Batch 670/ 842, training loss 0.5247480869293213\n",
      "Epoch 4 -- Batch 671/ 842, training loss 0.5492644906044006\n",
      "Epoch 4 -- Batch 672/ 842, training loss 0.5107252597808838\n",
      "Epoch 4 -- Batch 673/ 842, training loss 0.5246493816375732\n",
      "Epoch 4 -- Batch 674/ 842, training loss 0.5562548637390137\n",
      "Epoch 4 -- Batch 675/ 842, training loss 0.5311975479125977\n",
      "Epoch 4 -- Batch 676/ 842, training loss 0.5105372071266174\n",
      "Epoch 4 -- Batch 677/ 842, training loss 0.4998219609260559\n",
      "Epoch 4 -- Batch 678/ 842, training loss 0.5406864881515503\n",
      "Epoch 4 -- Batch 679/ 842, training loss 0.5183178782463074\n",
      "Epoch 4 -- Batch 680/ 842, training loss 0.5209442377090454\n",
      "Epoch 4 -- Batch 681/ 842, training loss 0.5317847728729248\n",
      "Epoch 4 -- Batch 682/ 842, training loss 0.5451925992965698\n",
      "Epoch 4 -- Batch 683/ 842, training loss 0.5498545169830322\n",
      "Epoch 4 -- Batch 684/ 842, training loss 0.5145204663276672\n",
      "Epoch 4 -- Batch 685/ 842, training loss 0.5050628185272217\n",
      "Epoch 4 -- Batch 686/ 842, training loss 0.5250899195671082\n",
      "Epoch 4 -- Batch 687/ 842, training loss 0.5368148684501648\n",
      "Epoch 4 -- Batch 688/ 842, training loss 0.5306506156921387\n",
      "Epoch 4 -- Batch 689/ 842, training loss 0.5430117249488831\n",
      "Epoch 4 -- Batch 690/ 842, training loss 0.5009489059448242\n",
      "Epoch 4 -- Batch 691/ 842, training loss 0.5194970369338989\n",
      "Epoch 4 -- Batch 692/ 842, training loss 0.5294822454452515\n",
      "Epoch 4 -- Batch 693/ 842, training loss 0.5348252058029175\n",
      "Epoch 4 -- Batch 694/ 842, training loss 0.5140925049781799\n",
      "Epoch 4 -- Batch 695/ 842, training loss 0.5161287188529968\n",
      "Epoch 4 -- Batch 696/ 842, training loss 0.5134955644607544\n",
      "Epoch 4 -- Batch 697/ 842, training loss 0.5550084710121155\n",
      "Epoch 4 -- Batch 698/ 842, training loss 0.5257934927940369\n",
      "Epoch 4 -- Batch 699/ 842, training loss 0.5268871784210205\n",
      "Epoch 4 -- Batch 700/ 842, training loss 0.5347188115119934\n",
      "Epoch 4 -- Batch 701/ 842, training loss 0.5122847557067871\n",
      "Epoch 4 -- Batch 702/ 842, training loss 0.5247547030448914\n",
      "Epoch 4 -- Batch 703/ 842, training loss 0.5289084315299988\n",
      "Epoch 4 -- Batch 704/ 842, training loss 0.5295416116714478\n",
      "Epoch 4 -- Batch 705/ 842, training loss 0.5346689820289612\n",
      "Epoch 4 -- Batch 706/ 842, training loss 0.5134496092796326\n",
      "Epoch 4 -- Batch 707/ 842, training loss 0.5272912979125977\n",
      "Epoch 4 -- Batch 708/ 842, training loss 0.5206928849220276\n",
      "Epoch 4 -- Batch 709/ 842, training loss 0.5440484285354614\n",
      "Epoch 4 -- Batch 710/ 842, training loss 0.5581157803535461\n",
      "Epoch 4 -- Batch 711/ 842, training loss 0.5245655179023743\n",
      "Epoch 4 -- Batch 712/ 842, training loss 0.5270573496818542\n",
      "Epoch 4 -- Batch 713/ 842, training loss 0.5277954339981079\n",
      "Epoch 4 -- Batch 714/ 842, training loss 0.5539740324020386\n",
      "Epoch 4 -- Batch 715/ 842, training loss 0.5472790598869324\n",
      "Epoch 4 -- Batch 716/ 842, training loss 0.5336487293243408\n",
      "Epoch 4 -- Batch 717/ 842, training loss 0.5502272248268127\n",
      "Epoch 4 -- Batch 718/ 842, training loss 0.5348082780838013\n",
      "Epoch 4 -- Batch 719/ 842, training loss 0.5109013915061951\n",
      "Epoch 4 -- Batch 720/ 842, training loss 0.5408943891525269\n",
      "Epoch 4 -- Batch 721/ 842, training loss 0.523002028465271\n",
      "Epoch 4 -- Batch 722/ 842, training loss 0.5253885984420776\n",
      "Epoch 4 -- Batch 723/ 842, training loss 0.5262425541877747\n",
      "Epoch 4 -- Batch 724/ 842, training loss 0.5138567686080933\n",
      "Epoch 4 -- Batch 725/ 842, training loss 0.527920663356781\n",
      "Epoch 4 -- Batch 726/ 842, training loss 0.5407344698905945\n",
      "Epoch 4 -- Batch 727/ 842, training loss 0.5069168210029602\n",
      "Epoch 4 -- Batch 728/ 842, training loss 0.5446974039077759\n",
      "Epoch 4 -- Batch 729/ 842, training loss 0.5484496355056763\n",
      "Epoch 4 -- Batch 730/ 842, training loss 0.5125744938850403\n",
      "Epoch 4 -- Batch 731/ 842, training loss 0.537717342376709\n",
      "Epoch 4 -- Batch 732/ 842, training loss 0.5052533149719238\n",
      "Epoch 4 -- Batch 733/ 842, training loss 0.5440506935119629\n",
      "Epoch 4 -- Batch 734/ 842, training loss 0.5083528757095337\n",
      "Epoch 4 -- Batch 735/ 842, training loss 0.5465535521507263\n",
      "Epoch 4 -- Batch 736/ 842, training loss 0.547417163848877\n",
      "Epoch 4 -- Batch 737/ 842, training loss 0.5273544788360596\n",
      "Epoch 4 -- Batch 738/ 842, training loss 0.521388590335846\n",
      "Epoch 4 -- Batch 739/ 842, training loss 0.5574472546577454\n",
      "Epoch 4 -- Batch 740/ 842, training loss 0.505710780620575\n",
      "Epoch 4 -- Batch 741/ 842, training loss 0.5485106110572815\n",
      "Epoch 4 -- Batch 742/ 842, training loss 0.5362772941589355\n",
      "Epoch 4 -- Batch 743/ 842, training loss 0.5294540524482727\n",
      "Epoch 4 -- Batch 744/ 842, training loss 0.5060021281242371\n",
      "Epoch 4 -- Batch 745/ 842, training loss 0.5232248902320862\n",
      "Epoch 4 -- Batch 746/ 842, training loss 0.5102198123931885\n",
      "Epoch 4 -- Batch 747/ 842, training loss 0.519250750541687\n",
      "Epoch 4 -- Batch 748/ 842, training loss 0.5098622441291809\n",
      "Epoch 4 -- Batch 749/ 842, training loss 0.5256580710411072\n",
      "Epoch 4 -- Batch 750/ 842, training loss 0.5223535895347595\n",
      "Epoch 4 -- Batch 751/ 842, training loss 0.5123878717422485\n",
      "Epoch 4 -- Batch 752/ 842, training loss 0.5127185583114624\n",
      "Epoch 4 -- Batch 753/ 842, training loss 0.5058133006095886\n",
      "Epoch 4 -- Batch 754/ 842, training loss 0.5391634106636047\n",
      "Epoch 4 -- Batch 755/ 842, training loss 0.510863184928894\n",
      "Epoch 4 -- Batch 756/ 842, training loss 0.5102587938308716\n",
      "Epoch 4 -- Batch 757/ 842, training loss 0.5131528377532959\n",
      "Epoch 4 -- Batch 758/ 842, training loss 0.5241404175758362\n",
      "Epoch 4 -- Batch 759/ 842, training loss 0.5277993083000183\n",
      "Epoch 4 -- Batch 760/ 842, training loss 0.5015629529953003\n",
      "Epoch 4 -- Batch 761/ 842, training loss 0.5197266936302185\n",
      "Epoch 4 -- Batch 762/ 842, training loss 0.5138639211654663\n",
      "Epoch 4 -- Batch 763/ 842, training loss 0.5362476110458374\n",
      "Epoch 4 -- Batch 764/ 842, training loss 0.5173166990280151\n",
      "Epoch 4 -- Batch 765/ 842, training loss 0.5294175744056702\n",
      "Epoch 4 -- Batch 766/ 842, training loss 0.5323420763015747\n",
      "Epoch 4 -- Batch 767/ 842, training loss 0.5310037136077881\n",
      "Epoch 4 -- Batch 768/ 842, training loss 0.5087431073188782\n",
      "Epoch 4 -- Batch 769/ 842, training loss 0.5352514982223511\n",
      "Epoch 4 -- Batch 770/ 842, training loss 0.5073389410972595\n",
      "Epoch 4 -- Batch 771/ 842, training loss 0.5127888917922974\n",
      "Epoch 4 -- Batch 772/ 842, training loss 0.5335180759429932\n",
      "Epoch 4 -- Batch 773/ 842, training loss 0.5445664525032043\n",
      "Epoch 4 -- Batch 774/ 842, training loss 0.5245853066444397\n",
      "Epoch 4 -- Batch 775/ 842, training loss 0.5321169495582581\n",
      "Epoch 4 -- Batch 776/ 842, training loss 0.5376588702201843\n",
      "Epoch 4 -- Batch 777/ 842, training loss 0.5244600176811218\n",
      "Epoch 4 -- Batch 778/ 842, training loss 0.5160227417945862\n",
      "Epoch 4 -- Batch 779/ 842, training loss 0.5034866333007812\n",
      "Epoch 4 -- Batch 780/ 842, training loss 0.5380761027336121\n",
      "Epoch 4 -- Batch 781/ 842, training loss 0.5409398674964905\n",
      "Epoch 4 -- Batch 782/ 842, training loss 0.5265249013900757\n",
      "Epoch 4 -- Batch 783/ 842, training loss 0.5162373185157776\n",
      "Epoch 4 -- Batch 784/ 842, training loss 0.5540462136268616\n",
      "Epoch 4 -- Batch 785/ 842, training loss 0.5235518217086792\n",
      "Epoch 4 -- Batch 786/ 842, training loss 0.5233678817749023\n",
      "Epoch 4 -- Batch 787/ 842, training loss 0.5281657576560974\n",
      "Epoch 4 -- Batch 788/ 842, training loss 0.5297775268554688\n",
      "Epoch 4 -- Batch 789/ 842, training loss 0.5244687795639038\n",
      "Epoch 4 -- Batch 790/ 842, training loss 0.5220105648040771\n",
      "Epoch 4 -- Batch 791/ 842, training loss 0.48899370431900024\n",
      "Epoch 4 -- Batch 792/ 842, training loss 0.5000723600387573\n",
      "Epoch 4 -- Batch 793/ 842, training loss 0.5379487872123718\n",
      "Epoch 4 -- Batch 794/ 842, training loss 0.5356796979904175\n",
      "Epoch 4 -- Batch 795/ 842, training loss 0.543437123298645\n",
      "Epoch 4 -- Batch 796/ 842, training loss 0.5269513130187988\n",
      "Epoch 4 -- Batch 797/ 842, training loss 0.5411882996559143\n",
      "Epoch 4 -- Batch 798/ 842, training loss 0.5207688808441162\n",
      "Epoch 4 -- Batch 799/ 842, training loss 0.5215640068054199\n",
      "Epoch 4 -- Batch 800/ 842, training loss 0.5434231758117676\n",
      "Epoch 4 -- Batch 801/ 842, training loss 0.5191521048545837\n",
      "Epoch 4 -- Batch 802/ 842, training loss 0.5475668907165527\n",
      "Epoch 4 -- Batch 803/ 842, training loss 0.5199388265609741\n",
      "Epoch 4 -- Batch 804/ 842, training loss 0.5237779021263123\n",
      "Epoch 4 -- Batch 805/ 842, training loss 0.5573828816413879\n",
      "Epoch 4 -- Batch 806/ 842, training loss 0.5248267650604248\n",
      "Epoch 4 -- Batch 807/ 842, training loss 0.5252054929733276\n",
      "Epoch 4 -- Batch 808/ 842, training loss 0.5418885946273804\n",
      "Epoch 4 -- Batch 809/ 842, training loss 0.550753653049469\n",
      "Epoch 4 -- Batch 810/ 842, training loss 0.5176823735237122\n",
      "Epoch 4 -- Batch 811/ 842, training loss 0.5460449457168579\n",
      "Epoch 4 -- Batch 812/ 842, training loss 0.5138911008834839\n",
      "Epoch 4 -- Batch 813/ 842, training loss 0.5441640615463257\n",
      "Epoch 4 -- Batch 814/ 842, training loss 0.5495776534080505\n",
      "Epoch 4 -- Batch 815/ 842, training loss 0.5094459056854248\n",
      "Epoch 4 -- Batch 816/ 842, training loss 0.5219928026199341\n",
      "Epoch 4 -- Batch 817/ 842, training loss 0.5298272371292114\n",
      "Epoch 4 -- Batch 818/ 842, training loss 0.517882764339447\n",
      "Epoch 4 -- Batch 819/ 842, training loss 0.5393500924110413\n",
      "Epoch 4 -- Batch 820/ 842, training loss 0.5097265243530273\n",
      "Epoch 4 -- Batch 821/ 842, training loss 0.5284385085105896\n",
      "Epoch 4 -- Batch 822/ 842, training loss 0.5161905884742737\n",
      "Epoch 4 -- Batch 823/ 842, training loss 0.5071671009063721\n",
      "Epoch 4 -- Batch 824/ 842, training loss 0.522792637348175\n",
      "Epoch 4 -- Batch 825/ 842, training loss 0.5188080668449402\n",
      "Epoch 4 -- Batch 826/ 842, training loss 0.5190333724021912\n",
      "Epoch 4 -- Batch 827/ 842, training loss 0.5268693566322327\n",
      "Epoch 4 -- Batch 828/ 842, training loss 0.5201296210289001\n",
      "Epoch 4 -- Batch 829/ 842, training loss 0.5136138200759888\n",
      "Epoch 4 -- Batch 830/ 842, training loss 0.5196179151535034\n",
      "Epoch 4 -- Batch 831/ 842, training loss 0.5250285267829895\n",
      "Epoch 4 -- Batch 832/ 842, training loss 0.5326105952262878\n",
      "Epoch 4 -- Batch 833/ 842, training loss 0.5372364521026611\n",
      "Epoch 4 -- Batch 834/ 842, training loss 0.5314469337463379\n",
      "Epoch 4 -- Batch 835/ 842, training loss 0.5422398447990417\n",
      "Epoch 4 -- Batch 836/ 842, training loss 0.5216130018234253\n",
      "Epoch 4 -- Batch 837/ 842, training loss 0.5249519348144531\n",
      "Epoch 4 -- Batch 838/ 842, training loss 0.5163049101829529\n",
      "Epoch 4 -- Batch 839/ 842, training loss 0.5067089200019836\n",
      "Epoch 4 -- Batch 840/ 842, training loss 0.5270386338233948\n",
      "Epoch 4 -- Batch 841/ 842, training loss 0.5248125791549683\n",
      "Epoch 4 -- Batch 842/ 842, training loss 0.54546719789505\n",
      "----------------------------------------------------------------------\n",
      "Epoch 4 -- Batch 1/ 94, validation loss 0.5080906748771667\n",
      "Epoch 4 -- Batch 2/ 94, validation loss 0.5175797939300537\n",
      "Epoch 4 -- Batch 3/ 94, validation loss 0.5066466927528381\n",
      "Epoch 4 -- Batch 4/ 94, validation loss 0.541480541229248\n",
      "Epoch 4 -- Batch 5/ 94, validation loss 0.5163341760635376\n",
      "Epoch 4 -- Batch 6/ 94, validation loss 0.5050439238548279\n",
      "Epoch 4 -- Batch 7/ 94, validation loss 0.5324020385742188\n",
      "Epoch 4 -- Batch 8/ 94, validation loss 0.5057486295700073\n",
      "Epoch 4 -- Batch 9/ 94, validation loss 0.5129451751708984\n",
      "Epoch 4 -- Batch 10/ 94, validation loss 0.5134050846099854\n",
      "Epoch 4 -- Batch 11/ 94, validation loss 0.49076351523399353\n",
      "Epoch 4 -- Batch 12/ 94, validation loss 0.5012330412864685\n",
      "Epoch 4 -- Batch 13/ 94, validation loss 0.5079314112663269\n",
      "Epoch 4 -- Batch 14/ 94, validation loss 0.522379457950592\n",
      "Epoch 4 -- Batch 15/ 94, validation loss 0.5058050751686096\n",
      "Epoch 4 -- Batch 16/ 94, validation loss 0.5080878138542175\n",
      "Epoch 4 -- Batch 17/ 94, validation loss 0.5009922981262207\n",
      "Epoch 4 -- Batch 18/ 94, validation loss 0.5185317397117615\n",
      "Epoch 4 -- Batch 19/ 94, validation loss 0.5138713121414185\n",
      "Epoch 4 -- Batch 20/ 94, validation loss 0.5076850056648254\n",
      "Epoch 4 -- Batch 21/ 94, validation loss 0.5108701586723328\n",
      "Epoch 4 -- Batch 22/ 94, validation loss 0.5196356177330017\n",
      "Epoch 4 -- Batch 23/ 94, validation loss 0.5233827233314514\n",
      "Epoch 4 -- Batch 24/ 94, validation loss 0.5221174359321594\n",
      "Epoch 4 -- Batch 25/ 94, validation loss 0.5190256237983704\n",
      "Epoch 4 -- Batch 26/ 94, validation loss 0.5206690430641174\n",
      "Epoch 4 -- Batch 27/ 94, validation loss 0.4999352693557739\n",
      "Epoch 4 -- Batch 28/ 94, validation loss 0.5340889096260071\n",
      "Epoch 4 -- Batch 29/ 94, validation loss 0.5115969777107239\n",
      "Epoch 4 -- Batch 30/ 94, validation loss 0.5151489973068237\n",
      "Epoch 4 -- Batch 31/ 94, validation loss 0.49200838804244995\n",
      "Epoch 4 -- Batch 32/ 94, validation loss 0.5093300342559814\n",
      "Epoch 4 -- Batch 33/ 94, validation loss 0.48889416456222534\n",
      "Epoch 4 -- Batch 34/ 94, validation loss 0.49439147114753723\n",
      "Epoch 4 -- Batch 35/ 94, validation loss 0.5097277164459229\n",
      "Epoch 4 -- Batch 36/ 94, validation loss 0.5089183449745178\n",
      "Epoch 4 -- Batch 37/ 94, validation loss 0.53528892993927\n",
      "Epoch 4 -- Batch 38/ 94, validation loss 0.5011867880821228\n",
      "Epoch 4 -- Batch 39/ 94, validation loss 0.5353670716285706\n",
      "Epoch 4 -- Batch 40/ 94, validation loss 0.5026895999908447\n",
      "Epoch 4 -- Batch 41/ 94, validation loss 0.500881552696228\n",
      "Epoch 4 -- Batch 42/ 94, validation loss 0.5360390543937683\n",
      "Epoch 4 -- Batch 43/ 94, validation loss 0.5156176686286926\n",
      "Epoch 4 -- Batch 44/ 94, validation loss 0.535957932472229\n",
      "Epoch 4 -- Batch 45/ 94, validation loss 0.5101642608642578\n",
      "Epoch 4 -- Batch 46/ 94, validation loss 0.513522207736969\n",
      "Epoch 4 -- Batch 47/ 94, validation loss 0.5139991044998169\n",
      "Epoch 4 -- Batch 48/ 94, validation loss 0.5121647119522095\n",
      "Epoch 4 -- Batch 49/ 94, validation loss 0.49951258301734924\n",
      "Epoch 4 -- Batch 50/ 94, validation loss 0.4997299909591675\n",
      "Epoch 4 -- Batch 51/ 94, validation loss 0.5136248469352722\n",
      "Epoch 4 -- Batch 52/ 94, validation loss 0.5171246528625488\n",
      "Epoch 4 -- Batch 53/ 94, validation loss 0.5105159878730774\n",
      "Epoch 4 -- Batch 54/ 94, validation loss 0.5207809805870056\n",
      "Epoch 4 -- Batch 55/ 94, validation loss 0.5083351135253906\n",
      "Epoch 4 -- Batch 56/ 94, validation loss 0.5080136656761169\n",
      "Epoch 4 -- Batch 57/ 94, validation loss 0.5094443559646606\n",
      "Epoch 4 -- Batch 58/ 94, validation loss 0.5218163132667542\n",
      "Epoch 4 -- Batch 59/ 94, validation loss 0.5068337321281433\n",
      "Epoch 4 -- Batch 60/ 94, validation loss 0.4849754571914673\n",
      "Epoch 4 -- Batch 61/ 94, validation loss 0.5095749497413635\n",
      "Epoch 4 -- Batch 62/ 94, validation loss 0.5118618011474609\n",
      "Epoch 4 -- Batch 63/ 94, validation loss 0.49077218770980835\n",
      "Epoch 4 -- Batch 64/ 94, validation loss 0.5490228533744812\n",
      "Epoch 4 -- Batch 65/ 94, validation loss 0.5009874105453491\n",
      "Epoch 4 -- Batch 66/ 94, validation loss 0.507175624370575\n",
      "Epoch 4 -- Batch 67/ 94, validation loss 0.5052978992462158\n",
      "Epoch 4 -- Batch 68/ 94, validation loss 0.524555504322052\n",
      "Epoch 4 -- Batch 69/ 94, validation loss 0.5177938342094421\n",
      "Epoch 4 -- Batch 70/ 94, validation loss 0.49174031615257263\n",
      "Epoch 4 -- Batch 71/ 94, validation loss 0.5098579525947571\n",
      "Epoch 4 -- Batch 72/ 94, validation loss 0.5104260444641113\n",
      "Epoch 4 -- Batch 73/ 94, validation loss 0.49719932675361633\n",
      "Epoch 4 -- Batch 74/ 94, validation loss 0.49463769793510437\n",
      "Epoch 4 -- Batch 75/ 94, validation loss 0.5216975212097168\n",
      "Epoch 4 -- Batch 76/ 94, validation loss 0.5117772817611694\n",
      "Epoch 4 -- Batch 77/ 94, validation loss 0.5167779922485352\n",
      "Epoch 4 -- Batch 78/ 94, validation loss 0.5109518766403198\n",
      "Epoch 4 -- Batch 79/ 94, validation loss 0.5077465176582336\n",
      "Epoch 4 -- Batch 80/ 94, validation loss 0.5228374600410461\n",
      "Epoch 4 -- Batch 81/ 94, validation loss 0.49996134638786316\n",
      "Epoch 4 -- Batch 82/ 94, validation loss 0.5318438410758972\n",
      "Epoch 4 -- Batch 83/ 94, validation loss 0.5154065489768982\n",
      "Epoch 4 -- Batch 84/ 94, validation loss 0.5217459797859192\n",
      "Epoch 4 -- Batch 85/ 94, validation loss 0.5370708703994751\n",
      "Epoch 4 -- Batch 86/ 94, validation loss 0.5098928213119507\n",
      "Epoch 4 -- Batch 87/ 94, validation loss 0.5105182528495789\n",
      "Epoch 4 -- Batch 88/ 94, validation loss 0.5172210931777954\n",
      "Epoch 4 -- Batch 89/ 94, validation loss 0.5189046263694763\n",
      "Epoch 4 -- Batch 90/ 94, validation loss 0.5003786087036133\n",
      "Epoch 4 -- Batch 91/ 94, validation loss 0.519895613193512\n",
      "Epoch 4 -- Batch 92/ 94, validation loss 0.49715620279312134\n",
      "Epoch 4 -- Batch 93/ 94, validation loss 0.5130400061607361\n",
      "Epoch 4 -- Batch 94/ 94, validation loss 0.5140824913978577\n",
      "----------------------------------------------------------------------\n",
      "Epoch 4 loss: Training 0.5348194241523743, Validation 0.5140824913978577\n",
      "----------------------------------------------------------------------\n",
      "Epoch 5/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 3 4 5 13 14\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CSc1ncc(CNc2cccn(CC)c2C)n2c1ccnc12'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 11 12 20\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCS(=O)(=O)N1CCCC1C(=O)NC1CCC2(C)CCN(C(=O)/C=C/c2cccs2)C1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CC[C@@H]1CN([C@@H](C)CO)S(=O)(=O)c2ccc(C#Cc3ccc(F)cc3)cc2O[C@@H]1CN(C)CC1C'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 6 7 8 14 15\n",
      "[19:04:26] Explicit valence for atom # 15 Cl, 2, is greater than permitted\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c3c(n(C2O)[C@@H](CO)c2c1c(=O)c1ccccc1n4C)OCO3'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc([N+](=O)[O-])c1)N[C@H]1O[C@@H]2c3ccc(c4ccccc5O)[C@@H](C)[C@H]1N(C(=O)Cc1cccnc1)C[C@@H](C)[C@H](OC)CN(C)C2=O'\n",
      "[19:04:26] SMILES Parse Error: extra close parentheses while parsing: CN(C)CCCN(C(=O)c1ccccc1)C1CC1)c1cccc(C)c1\n",
      "[19:04:26] SMILES Parse Error: Failed parsing SMILES 'CN(C)CCCN(C(=O)c1ccccc1)C1CC1)c1cccc(C)c1' for input: 'CN(C)CCCN(C(=O)c1ccccc1)C1CC1)c1cccc(C)c1'\n",
      "[19:04:26] SMILES Parse Error: extra close parentheses while parsing: O=C(Nc1ccc2nc3n(c(=O)c2c1)CCCCC3)C1c2cc[N+]cc2)c2ccccc21\n",
      "[19:04:26] SMILES Parse Error: Failed parsing SMILES 'O=C(Nc1ccc2nc3n(c(=O)c2c1)CCCCC3)C1c2cc[N+]cc2)c2ccccc21' for input: 'O=C(Nc1ccc2nc3n(c(=O)c2c1)CCCCC3)C1c2cc[N+]cc2)c2ccccc21'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 10 11 19 20 21\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CN1CCN(C(=O)c2sc2nc(CNC3CCCCCC3)c3ccc(Cl)cc3n2)CC1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CC(C)N(C(=O)CN1C(=O)C2C3C=CC(C4)C2C1=O)c1ccc(Cl)cc1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1cc2c(s1)-n1c(=I)c3ccccc3c(=O)n1CCc1ccccc1'\n",
      "[19:04:26] SMILES Parse Error: syntax error while parsing: COc1ccc(CCNc2c(C(-)Clcnc3ccncc23)OCC(C)C)on1\n",
      "[19:04:26] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(CCNc2c(C(-)Clcnc3ccncc23)OCC(C)C)on1' for input: 'COc1ccc(CCNc2c(C(-)Clcnc3ccncc23)OCC(C)C)on1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 7 15 16\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)Oc1ccc(-n2c(=O)n([C@@H](C)CN2C(=O)c3ccccc3C2=O)n2nc(-c3ccc(F)cc3)cc2C(F)(F)F)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 10 11 13\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'C/C(=C\\c1c(O)nc(SCC(=O)Nc2ccc(C(N)=O)cc2)(C#N)C(=O)C2)c1ccccc1'\n",
      "[19:04:26] Explicit valence for atom # 14 C, 5, is greater than permitted\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)N(Cc2ccccc2)C(=O)C13CCN(Cc1cccc3nonc13)C2'\n",
      "[19:04:26] Explicit valence for atom # 22 O, 3, is greater than permitted\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCCCn1c(=O)c2c(nc3n2CC(C)CC3C)c(=O)n1CCN(CC)CC2'\n",
      "[19:04:26] SMILES Parse Error: ring closure 1 duplicates bond between atom 23 and atom 24 for input: 'COc1ccc(S(=O)(=O)N(C)C)cc1NC(=O)Cn1nnnc1C1c1cc([N+](=O)[O-])cs1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 12 13 22\n",
      "[19:04:26] Explicit valence for atom # 18 Cl, 2, is greater than permitted\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 24 25 26 27\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(C2Nc3ccccc3Sc3nnc(SC)n(-c4ccsc4)c3=O)cc2OC'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 6 7 18\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 22 23 24 25 26 27\n",
      "[19:04:26] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(C#Cc2csc([C@H]3[C@@H](CO)N(C(=O)c4ccccn4)C[C@@H]32)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 8 9 11\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'c1ccc(Nc2nnc([C@@H]3C[C@H]4CC[C@H]3C3)s2)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 17 19 20 25 26\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 9 10 12 13 14 15 16 17 18\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 19 21 22 23\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C)cc1OCc1cc(C(=O)NCC2Cc3c([nH]c4ccc(Cl)cc44)CCO2)co1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1cccc2c1OC(C)C2=CCCC2'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1ccc(CN2CCN(c3nc(C4CC4)nc4ccnn44)CC2)nn1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10 11 17 18 19 20 21 22 23 24 25\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 11 27 28 29\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(OCC(=O)Nc2ccc3oc4c(c2c2)CCCC4)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 13 14 15 20 21 22 23 24 26\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1OC[C@@H]1O[C@H](CO)[C@H](O)[C@]16'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CO[C@@H]1CN(C)C(=O)c2ccc(NC(=O)C3CC3)cc2OC[C@H](C)N(CCC2CC=CCC24CC(F)(F)F)C[C@@H]21'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1cc(/C=C2/C(=N)N3N=C(C(=O)OC(C)C)C3=C2C(=O)CC(C)(C)C3)ccc1OC(C)C'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2cc(C(F)(F)F)n3nc(C(=O)NC4CC5)cc4n(C)c23)cc1'\n",
      "[19:04:26] SMILES Parse Error: extra open parentheses for input: 'CC1CCN(C(=O)COC(=O)c2cc3ccc(-c4cccc(N5C(=O)c5ccc(F)cc5)cc4C(=O)N3C)cc2C(=O)N(C)C1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 16 17 20 21 22\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 16 17 20\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 17 18 30\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 10 11 13\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 10 11 18 19 20 22 23 25 26\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)N(CN2CCOCC2)c2ccccc2c2S1(=O)=O'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 9 10 16 17 19 20 21\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 10 11 12 14 17 18 19 20\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19 20 21 28 29\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 31 32\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'C=CCn1cc(C(C)C)N[C@@H]1C[C@H](CC(=O)NCc2ccccc2OC)O[C@H](OCc2ccccc2)C1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1nn(C)cc1C(=O)Nc1ccc2oc(-c3ccc(Cl)cc3)cc(c2c2)c1C'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(Cl)c1)C1C2C=CC1C3C12CC3CC(CC(C3)C1)C2'\n",
      "[19:04:26] Explicit valence for atom # 16 N, 6, is greater than permitted\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Nc1nc2ccsc3c(=O)n2c1CCN1C(=O)c2ccccc2C1=O'\n",
      "[19:04:26] non-ring atom 0 marked aromatic\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 12\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 14 21 22 23 24\n",
      "[19:04:26] SMILES Parse Error: ring closure 1 duplicates bond between atom 17 and atom 18 for input: 'CCC(C)NC(=O)C12CN3CC(CC)(CN(C1)C1c1cccc(-c2ccc(Cl)cc2)o1)C2'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 25\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=c1[nH]c2sc3c(cc2c2c(=O)n1-c1ccccc1)CCCC3'\n",
      "[19:04:26] SMILES Parse Error: extra close parentheses while parsing: Cc1ccccc1N1C(=O)CN(C(=O)C2CCC2)S(=O)(=O)c2ccc([N+](=O)[O-])cc2)C1=O\n",
      "[19:04:26] SMILES Parse Error: Failed parsing SMILES 'Cc1ccccc1N1C(=O)CN(C(=O)C2CCC2)S(=O)(=O)c2ccc([N+](=O)[O-])cc2)C1=O' for input: 'Cc1ccccc1N1C(=O)CN(C(=O)C2CCC2)S(=O)(=O)c2ccc([N+](=O)[O-])cc2)C1=O'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cn1c(Sc2cnc3ccccc3c2=O)nc2sc3c4c(ccc3c13)CCC4'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(CNC(=O)C(C)(C)C)co2'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 7 8 9 21 22 23 24 31 32\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1[nH]c3ccccc3c2c1C(=O)C(=O)NCC(=O)O'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 21\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 10 11 13 14 16\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 20 22 23\n",
      "[19:04:26] SMILES Parse Error: extra open parentheses for input: 'N#Cc1ccc(Nc2ccccc2'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 14 15 16 31 34\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)c(C2C(C(=O)Nc3ccc(Br)cc3)C3C=CC2=O)c1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 11 12 13 16 17 21 22 23 24\n",
      "[19:04:26] SMILES Parse Error: ring closure 1 duplicates bond between atom 12 and atom 13 for input: 'CC1(C)CC(=O)C2=C1C2(C)CC=C1C1C2CC(C)=C(N)C1(C#N)=C(O)C2(C)C'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(-n2c(SCC(=O)Nc3ccc(N4CCCC4)cc4)n[nH]c2=S)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 12 13 14 15 16\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'C[C@]12C=CC3(=O)C(=O)[C@@H](c4ccc(Br)cc4)C(=O)C=C(N3CCC1)(C3)C2=O'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 22\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 18 19 20 21\n",
      "[19:04:26] SMILES Parse Error: ring closure 1 duplicates bond between atom 27 and atom 28 for input: 'COc1ccc(OC)c2c1Occ1cc(C)nc1c2oc3ccc(O)cc3c1c1-c1ccccc1'\n",
      "[19:04:26] SMILES Parse Error: extra open parentheses for input: 'O=C(NCc1nnc(SC/C(N)=N/O2CCCCCC2)s1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1cccc(NC(=O)C(C)n2c(-c3csc4ccccc33)csc2=O)c1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1c(CNC(=O)c2cc2c(s2)CCCCS2)CCCC2'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CC1=C(C)C(=O)c2c(nc(N)n3C(C)(C)C)C(=O)N1c1ccc(C2CCCCC2)cc1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'C=CCN(C(=O)c1c(O)c2cccc3cc(C)oc13)C1CC[C@@]2(OC)C(=O)N(C)C(=O)[C@@H]12'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 9 10 15 16 18\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCCN1C2c3ccc(OC)cc3CCC13CCNC2CCCO'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 7 22 23\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 8 10 14 15 16\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 4 23 24 25 26 27 28\n",
      "[19:04:26] SMILES Parse Error: ring closure 4 duplicates bond between atom 12 and atom 13 for input: 'Cc1ccc2nc(CN3CCCC4c4ccccc4O3)c(C#N)c(C)n2n1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Nc1nc2c4c(c(=O)oc2c1)CCCCC3'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 10 11 13 14 15\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1oc2c([N+](=O)[O-])c(C(F)(F)F)nn1CCN1CCOCC1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cn1c2c(c(=O)[nH]c1=O)C(=O)N(c1ccccc1)C(=O)c1ccccc1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CSc1cc(C(=O)N2c3ccccc3C2CCN(C(C)(C)C)CC2)ncn1C'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 18 19\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 8 12 13 14\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc2cc(C(=O)N3CCC[C@H]4c3ccsc43)cc(C)c2c1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 16 17 18 26 28\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 9 10 11 22 24\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=c1cc(-c2ccc(N3C(=O)CC4C4CCC(C5)C4)cc3)oc2ccccc12'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1c(CN2CCCC2)CN(C)C(=O)Cn1c(Cc2ccccc2)nc2ccccc21'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 10 11 12 14 15\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 7 8 9 12 13\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 22\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1cccc(NC(=O)N2CC(=O)N(CCc3c[nH]c4ccccc34)C(=O)[C@@H](C)C(=O)N(C)c2ccccc2)c1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'OCCn1c(N2CC(C(=O)N3CCOCC3)C=CC2c3ccccc3C2(F)F)nc2ccccc2c1=O'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 13 14 15\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1c(C)[nH]c(C(=O)COC(=O)c2ccc(N3C(=O)C4C5C=C(CC55)C4C3=O)cc2Cl)c1C'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@]12CN(C(=O)c2ccc(F)c(F)c2)[C@@H](CC(=O)NCCc1ccccn1)C2'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)C2CCN(Cc3cccc3ccccc34)CC2)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 6 7 8 14 15 16 19 20 21\n",
      "[19:04:26] Explicit valence for atom # 11 C, 5, is greater than permitted\n",
      "[19:04:26] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2CC(=O)N3C(=C4CCCCC4)NC3=C2C(=O)CCC3)cc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 15 16 26 27 28\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 6 7 8 16 24\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 3 15 16 17 18 19 20\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=C(CCN1C(=O)C2C3C=CC(C4)C2C1=O)=COc1ccccc1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 4 12 13 14 20 21 22\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 7 8\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CN(C)/C=C/c1ccc([C@@H]2[C@@H](CO)N(C(=O)C3CC2)[C@H](CO)S2)cc1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@]12CCCCC2=C1N(Cc1cc(Cl)ccc1Cl)C(=O)C2(C)C'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 25\n",
      "[19:04:26] SMILES Parse Error: extra close parentheses while parsing: Cc1ccccc1OCC(=O)O/N=C(\\C#N)c1nc2cc3c(cc2Br)OCO3)cc1C\n",
      "[19:04:26] SMILES Parse Error: Failed parsing SMILES 'Cc1ccccc1OCC(=O)O/N=C(\\C#N)c1nc2cc3c(cc2Br)OCO3)cc1C' for input: 'Cc1ccccc1OCC(=O)O/N=C(\\C#N)c1nc2cc3c(cc2Br)OCO3)cc1C'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)N=C2OC3C(CC2)OC)c(=O)[nH]c1=O'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=C(CN1C(=O)[C@H]2CCCN2C(=O)c2ccc3ccccc4c2c1=O)NCC1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2nc(C(=O)n3cccc4ccccc44)c(NCCN(C)C)c(=O)[nH]c2c1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=C(CN1C[C@@H]2CC[C@H]1C3)NS(=O)(=O)c1ccccc1Cl'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CN(Cc1ccco1)C(=O)CC1Nc2nc3ccccc4nc(C(c5ccc(F)cc5)c4=O)O[C@H]23'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCOc1ccccc1N2CC(NC(=O)NCCc3nncn3C2CCCCC2)C1=O'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 8 9 10 19 20 25\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 10\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 17 19 20\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-c1cnc2c(c1)C(=O)N([C@H](C)CO)C[C@@H](C)[C@H](CN(C)C(=O)C1CC2)O2'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2ccccc2c1C1C=C(/C(N)=N/N2CCN(C)CC2)C(=O)N2'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 7 8 10\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 3\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 22\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 23\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 24 25 28\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1OCCn1c(SCc2ccc([N+](=O)[O-])cc2)nc2c(s1c(=O)[nH]c1=O)CS2'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1cccc2cc(C(=O)Nc3cccc(C(F)(F)F)c3)c(=Nc3ccc(-c4cc[nH]c5)cc3)ccc12'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'O=C1CCC(CC(=O)N2CCN(c3ccccn3)CC2)c2cc1c(=O)o1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 3 4 5 16 17\n",
      "[19:04:26] SMILES Parse Error: extra open parentheses for input: 'O=C(CN1C(=O)NC2(CCS(=O)(=O)C2)CN1'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 15 16 17 27 28 29 30 31 33\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 17 25 27\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCCc1nnc(SCC(=O)Nc2sc3c(c2C(=O)N2CCCCCC2)CCCC3)n2C'\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 12 19\n",
      "[19:04:26] Can't kekulize mol.  Unkekulized atoms: 12 13 22 23 24\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COCC(=O)N1CCN(C(=O)[C@H]2CCCN2C(=O)c2cc(OC)c(OC)c3OC)ccc2C1=O'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCNC(=O)C1(C)CCCN(C(=O)c1cnn(C(C)(C)C)c1=O)CC1CCCC1'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)[C@H](CO)[C@@H]2[C@]3c4ccc(O)cc4CCC[C@]3CC[C@H]2[C@H]12'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1NC(=O)c1cc(-c2cnn3c([C@@H]2CO)csC1)n1cccc1-c1cc(C)ccc1N'\n",
      "[19:04:26] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1NC(=O)CSc1nc2c(c(=O)n1C1CCc2ccccc31)CCC2'\n",
      "[19:04:26] Explicit valence for atom # 1 N, 4, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 -- Batch 1/ 842, training loss 0.530566394329071\n",
      "Epoch 5 -- Batch 2/ 842, training loss 0.5108798146247864\n",
      "Epoch 5 -- Batch 3/ 842, training loss 0.5117610692977905\n",
      "Epoch 5 -- Batch 4/ 842, training loss 0.50933438539505\n",
      "Epoch 5 -- Batch 5/ 842, training loss 0.5013951063156128\n",
      "Epoch 5 -- Batch 6/ 842, training loss 0.5407463312149048\n",
      "Epoch 5 -- Batch 7/ 842, training loss 0.5334701538085938\n",
      "Epoch 5 -- Batch 8/ 842, training loss 0.5047807693481445\n",
      "Epoch 5 -- Batch 9/ 842, training loss 0.5363180041313171\n",
      "Epoch 5 -- Batch 10/ 842, training loss 0.5098373889923096\n",
      "Epoch 5 -- Batch 11/ 842, training loss 0.5127512812614441\n",
      "Epoch 5 -- Batch 12/ 842, training loss 0.5087909698486328\n",
      "Epoch 5 -- Batch 13/ 842, training loss 0.49858328700065613\n",
      "Epoch 5 -- Batch 14/ 842, training loss 0.5260335803031921\n",
      "Epoch 5 -- Batch 15/ 842, training loss 0.5238381028175354\n",
      "Epoch 5 -- Batch 16/ 842, training loss 0.5559899210929871\n",
      "Epoch 5 -- Batch 17/ 842, training loss 0.5156364440917969\n",
      "Epoch 5 -- Batch 18/ 842, training loss 0.5139384269714355\n",
      "Epoch 5 -- Batch 19/ 842, training loss 0.5042450428009033\n",
      "Epoch 5 -- Batch 20/ 842, training loss 0.5264173746109009\n",
      "Epoch 5 -- Batch 21/ 842, training loss 0.51502525806427\n",
      "Epoch 5 -- Batch 22/ 842, training loss 0.5013870596885681\n",
      "Epoch 5 -- Batch 23/ 842, training loss 0.509588897228241\n",
      "Epoch 5 -- Batch 24/ 842, training loss 0.5165822505950928\n",
      "Epoch 5 -- Batch 25/ 842, training loss 0.5293667316436768\n",
      "Epoch 5 -- Batch 26/ 842, training loss 0.5191003084182739\n",
      "Epoch 5 -- Batch 27/ 842, training loss 0.5222285985946655\n",
      "Epoch 5 -- Batch 28/ 842, training loss 0.49727779626846313\n",
      "Epoch 5 -- Batch 29/ 842, training loss 0.5191940069198608\n",
      "Epoch 5 -- Batch 30/ 842, training loss 0.5092570185661316\n",
      "Epoch 5 -- Batch 31/ 842, training loss 0.5225862860679626\n",
      "Epoch 5 -- Batch 32/ 842, training loss 0.5139997601509094\n",
      "Epoch 5 -- Batch 33/ 842, training loss 0.5228168964385986\n",
      "Epoch 5 -- Batch 34/ 842, training loss 0.5240601301193237\n",
      "Epoch 5 -- Batch 35/ 842, training loss 0.5038301348686218\n",
      "Epoch 5 -- Batch 36/ 842, training loss 0.5086308717727661\n",
      "Epoch 5 -- Batch 37/ 842, training loss 0.50004643201828\n",
      "Epoch 5 -- Batch 38/ 842, training loss 0.5080327391624451\n",
      "Epoch 5 -- Batch 39/ 842, training loss 0.5007021427154541\n",
      "Epoch 5 -- Batch 40/ 842, training loss 0.5193855166435242\n",
      "Epoch 5 -- Batch 41/ 842, training loss 0.5052826404571533\n",
      "Epoch 5 -- Batch 42/ 842, training loss 0.5192978382110596\n",
      "Epoch 5 -- Batch 43/ 842, training loss 0.5063155293464661\n",
      "Epoch 5 -- Batch 44/ 842, training loss 0.4890746772289276\n",
      "Epoch 5 -- Batch 45/ 842, training loss 0.5085114240646362\n",
      "Epoch 5 -- Batch 46/ 842, training loss 0.5195384621620178\n",
      "Epoch 5 -- Batch 47/ 842, training loss 0.5094347596168518\n",
      "Epoch 5 -- Batch 48/ 842, training loss 0.5142143964767456\n",
      "Epoch 5 -- Batch 49/ 842, training loss 0.4952284097671509\n",
      "Epoch 5 -- Batch 50/ 842, training loss 0.5210172533988953\n",
      "Epoch 5 -- Batch 51/ 842, training loss 0.5110835433006287\n",
      "Epoch 5 -- Batch 52/ 842, training loss 0.49498823285102844\n",
      "Epoch 5 -- Batch 53/ 842, training loss 0.519433856010437\n",
      "Epoch 5 -- Batch 54/ 842, training loss 0.49713289737701416\n",
      "Epoch 5 -- Batch 55/ 842, training loss 0.5004399418830872\n",
      "Epoch 5 -- Batch 56/ 842, training loss 0.512578010559082\n",
      "Epoch 5 -- Batch 57/ 842, training loss 0.5205804109573364\n",
      "Epoch 5 -- Batch 58/ 842, training loss 0.502947211265564\n",
      "Epoch 5 -- Batch 59/ 842, training loss 0.5071403980255127\n",
      "Epoch 5 -- Batch 60/ 842, training loss 0.5094774961471558\n",
      "Epoch 5 -- Batch 61/ 842, training loss 0.5204281210899353\n",
      "Epoch 5 -- Batch 62/ 842, training loss 0.49924397468566895\n",
      "Epoch 5 -- Batch 63/ 842, training loss 0.49054422974586487\n",
      "Epoch 5 -- Batch 64/ 842, training loss 0.505040168762207\n",
      "Epoch 5 -- Batch 65/ 842, training loss 0.5244296789169312\n",
      "Epoch 5 -- Batch 66/ 842, training loss 0.501257061958313\n",
      "Epoch 5 -- Batch 67/ 842, training loss 0.520730197429657\n",
      "Epoch 5 -- Batch 68/ 842, training loss 0.5289033055305481\n",
      "Epoch 5 -- Batch 69/ 842, training loss 0.512816309928894\n",
      "Epoch 5 -- Batch 70/ 842, training loss 0.5103128552436829\n",
      "Epoch 5 -- Batch 71/ 842, training loss 0.5271068811416626\n",
      "Epoch 5 -- Batch 72/ 842, training loss 0.5124855637550354\n",
      "Epoch 5 -- Batch 73/ 842, training loss 0.5078516006469727\n",
      "Epoch 5 -- Batch 74/ 842, training loss 0.5059333443641663\n",
      "Epoch 5 -- Batch 75/ 842, training loss 0.5411157608032227\n",
      "Epoch 5 -- Batch 76/ 842, training loss 0.5119144916534424\n",
      "Epoch 5 -- Batch 77/ 842, training loss 0.526301920413971\n",
      "Epoch 5 -- Batch 78/ 842, training loss 0.5046201348304749\n",
      "Epoch 5 -- Batch 79/ 842, training loss 0.541287362575531\n",
      "Epoch 5 -- Batch 80/ 842, training loss 0.5216372013092041\n",
      "Epoch 5 -- Batch 81/ 842, training loss 0.5316044092178345\n",
      "Epoch 5 -- Batch 82/ 842, training loss 0.5072358250617981\n",
      "Epoch 5 -- Batch 83/ 842, training loss 0.5423487424850464\n",
      "Epoch 5 -- Batch 84/ 842, training loss 0.5240653157234192\n",
      "Epoch 5 -- Batch 85/ 842, training loss 0.5201091170310974\n",
      "Epoch 5 -- Batch 86/ 842, training loss 0.5393877625465393\n",
      "Epoch 5 -- Batch 87/ 842, training loss 0.5299569368362427\n",
      "Epoch 5 -- Batch 88/ 842, training loss 0.505373477935791\n",
      "Epoch 5 -- Batch 89/ 842, training loss 0.5157068967819214\n",
      "Epoch 5 -- Batch 90/ 842, training loss 0.5006835460662842\n",
      "Epoch 5 -- Batch 91/ 842, training loss 0.4861815571784973\n",
      "Epoch 5 -- Batch 92/ 842, training loss 0.50319504737854\n",
      "Epoch 5 -- Batch 93/ 842, training loss 0.5026655197143555\n",
      "Epoch 5 -- Batch 94/ 842, training loss 0.5152090191841125\n",
      "Epoch 5 -- Batch 95/ 842, training loss 0.5249567031860352\n",
      "Epoch 5 -- Batch 96/ 842, training loss 0.5305591225624084\n",
      "Epoch 5 -- Batch 97/ 842, training loss 0.5021970868110657\n",
      "Epoch 5 -- Batch 98/ 842, training loss 0.5037993788719177\n",
      "Epoch 5 -- Batch 99/ 842, training loss 0.5148002505302429\n",
      "Epoch 5 -- Batch 100/ 842, training loss 0.5078158974647522\n",
      "Epoch 5 -- Batch 101/ 842, training loss 0.5093081593513489\n",
      "Epoch 5 -- Batch 102/ 842, training loss 0.4952256679534912\n",
      "Epoch 5 -- Batch 103/ 842, training loss 0.4826890528202057\n",
      "Epoch 5 -- Batch 104/ 842, training loss 0.5139642357826233\n",
      "Epoch 5 -- Batch 105/ 842, training loss 0.5048763751983643\n",
      "Epoch 5 -- Batch 106/ 842, training loss 0.5126968622207642\n",
      "Epoch 5 -- Batch 107/ 842, training loss 0.4982326328754425\n",
      "Epoch 5 -- Batch 108/ 842, training loss 0.5029147267341614\n",
      "Epoch 5 -- Batch 109/ 842, training loss 0.5027342438697815\n",
      "Epoch 5 -- Batch 110/ 842, training loss 0.5265019536018372\n",
      "Epoch 5 -- Batch 111/ 842, training loss 0.5019650459289551\n",
      "Epoch 5 -- Batch 112/ 842, training loss 0.49336934089660645\n",
      "Epoch 5 -- Batch 113/ 842, training loss 0.5117325186729431\n",
      "Epoch 5 -- Batch 114/ 842, training loss 0.5007238388061523\n",
      "Epoch 5 -- Batch 115/ 842, training loss 0.4958672523498535\n",
      "Epoch 5 -- Batch 116/ 842, training loss 0.5118986368179321\n",
      "Epoch 5 -- Batch 117/ 842, training loss 0.5098247528076172\n",
      "Epoch 5 -- Batch 118/ 842, training loss 0.5131681561470032\n",
      "Epoch 5 -- Batch 119/ 842, training loss 0.5011566877365112\n",
      "Epoch 5 -- Batch 120/ 842, training loss 0.5238175988197327\n",
      "Epoch 5 -- Batch 121/ 842, training loss 0.4893653094768524\n",
      "Epoch 5 -- Batch 122/ 842, training loss 0.5187573432922363\n",
      "Epoch 5 -- Batch 123/ 842, training loss 0.5040512084960938\n",
      "Epoch 5 -- Batch 124/ 842, training loss 0.5136831998825073\n",
      "Epoch 5 -- Batch 125/ 842, training loss 0.49512067437171936\n",
      "Epoch 5 -- Batch 126/ 842, training loss 0.5050977468490601\n",
      "Epoch 5 -- Batch 127/ 842, training loss 0.5226749777793884\n",
      "Epoch 5 -- Batch 128/ 842, training loss 0.4976384937763214\n",
      "Epoch 5 -- Batch 129/ 842, training loss 0.4910634756088257\n",
      "Epoch 5 -- Batch 130/ 842, training loss 0.49016425013542175\n",
      "Epoch 5 -- Batch 131/ 842, training loss 0.5269652009010315\n",
      "Epoch 5 -- Batch 132/ 842, training loss 0.5006822943687439\n",
      "Epoch 5 -- Batch 133/ 842, training loss 0.5109375715255737\n",
      "Epoch 5 -- Batch 134/ 842, training loss 0.5118485689163208\n",
      "Epoch 5 -- Batch 135/ 842, training loss 0.5247541069984436\n",
      "Epoch 5 -- Batch 136/ 842, training loss 0.49928921461105347\n",
      "Epoch 5 -- Batch 137/ 842, training loss 0.4944709539413452\n",
      "Epoch 5 -- Batch 138/ 842, training loss 0.49447715282440186\n",
      "Epoch 5 -- Batch 139/ 842, training loss 0.5123763084411621\n",
      "Epoch 5 -- Batch 140/ 842, training loss 0.487953782081604\n",
      "Epoch 5 -- Batch 141/ 842, training loss 0.5061724781990051\n",
      "Epoch 5 -- Batch 142/ 842, training loss 0.5291342735290527\n",
      "Epoch 5 -- Batch 143/ 842, training loss 0.534386932849884\n",
      "Epoch 5 -- Batch 144/ 842, training loss 0.48242881894111633\n",
      "Epoch 5 -- Batch 145/ 842, training loss 0.5011107921600342\n",
      "Epoch 5 -- Batch 146/ 842, training loss 0.5045412182807922\n",
      "Epoch 5 -- Batch 147/ 842, training loss 0.5132951736450195\n",
      "Epoch 5 -- Batch 148/ 842, training loss 0.5012407302856445\n",
      "Epoch 5 -- Batch 149/ 842, training loss 0.4986397325992584\n",
      "Epoch 5 -- Batch 150/ 842, training loss 0.5388094782829285\n",
      "Epoch 5 -- Batch 151/ 842, training loss 0.5106558799743652\n",
      "Epoch 5 -- Batch 152/ 842, training loss 0.5168850421905518\n",
      "Epoch 5 -- Batch 153/ 842, training loss 0.5202296376228333\n",
      "Epoch 5 -- Batch 154/ 842, training loss 0.5071356296539307\n",
      "Epoch 5 -- Batch 155/ 842, training loss 0.5092343688011169\n",
      "Epoch 5 -- Batch 156/ 842, training loss 0.49515894055366516\n",
      "Epoch 5 -- Batch 157/ 842, training loss 0.49408990144729614\n",
      "Epoch 5 -- Batch 158/ 842, training loss 0.5075127482414246\n",
      "Epoch 5 -- Batch 159/ 842, training loss 0.518700897693634\n",
      "Epoch 5 -- Batch 160/ 842, training loss 0.4959225058555603\n",
      "Epoch 5 -- Batch 161/ 842, training loss 0.5085651874542236\n",
      "Epoch 5 -- Batch 162/ 842, training loss 0.5111603736877441\n",
      "Epoch 5 -- Batch 163/ 842, training loss 0.5114964842796326\n",
      "Epoch 5 -- Batch 164/ 842, training loss 0.5047761797904968\n",
      "Epoch 5 -- Batch 165/ 842, training loss 0.49141356348991394\n",
      "Epoch 5 -- Batch 166/ 842, training loss 0.522734522819519\n",
      "Epoch 5 -- Batch 167/ 842, training loss 0.4960666596889496\n",
      "Epoch 5 -- Batch 168/ 842, training loss 0.48629721999168396\n",
      "Epoch 5 -- Batch 169/ 842, training loss 0.49037283658981323\n",
      "Epoch 5 -- Batch 170/ 842, training loss 0.5038779377937317\n",
      "Epoch 5 -- Batch 171/ 842, training loss 0.520171582698822\n",
      "Epoch 5 -- Batch 172/ 842, training loss 0.5491573810577393\n",
      "Epoch 5 -- Batch 173/ 842, training loss 0.482892245054245\n",
      "Epoch 5 -- Batch 174/ 842, training loss 0.5104783773422241\n",
      "Epoch 5 -- Batch 175/ 842, training loss 0.5003725290298462\n",
      "Epoch 5 -- Batch 176/ 842, training loss 0.4971686601638794\n",
      "Epoch 5 -- Batch 177/ 842, training loss 0.4997198283672333\n",
      "Epoch 5 -- Batch 178/ 842, training loss 0.4954589307308197\n",
      "Epoch 5 -- Batch 179/ 842, training loss 0.5352675318717957\n",
      "Epoch 5 -- Batch 180/ 842, training loss 0.5221880674362183\n",
      "Epoch 5 -- Batch 181/ 842, training loss 0.511107325553894\n",
      "Epoch 5 -- Batch 182/ 842, training loss 0.49735021591186523\n",
      "Epoch 5 -- Batch 183/ 842, training loss 0.5046951174736023\n",
      "Epoch 5 -- Batch 184/ 842, training loss 0.5222501754760742\n",
      "Epoch 5 -- Batch 185/ 842, training loss 0.5004037618637085\n",
      "Epoch 5 -- Batch 186/ 842, training loss 0.4901611804962158\n",
      "Epoch 5 -- Batch 187/ 842, training loss 0.49205097556114197\n",
      "Epoch 5 -- Batch 188/ 842, training loss 0.5054883360862732\n",
      "Epoch 5 -- Batch 189/ 842, training loss 0.5107959508895874\n",
      "Epoch 5 -- Batch 190/ 842, training loss 0.4902172386646271\n",
      "Epoch 5 -- Batch 191/ 842, training loss 0.5003410577774048\n",
      "Epoch 5 -- Batch 192/ 842, training loss 0.5047571063041687\n",
      "Epoch 5 -- Batch 193/ 842, training loss 0.5056374073028564\n",
      "Epoch 5 -- Batch 194/ 842, training loss 0.5050519704818726\n",
      "Epoch 5 -- Batch 195/ 842, training loss 0.505830705165863\n",
      "Epoch 5 -- Batch 196/ 842, training loss 0.4924948811531067\n",
      "Epoch 5 -- Batch 197/ 842, training loss 0.5088997483253479\n",
      "Epoch 5 -- Batch 198/ 842, training loss 0.522308349609375\n",
      "Epoch 5 -- Batch 199/ 842, training loss 0.504369854927063\n",
      "Epoch 5 -- Batch 200/ 842, training loss 0.5008062720298767\n",
      "Epoch 5 -- Batch 201/ 842, training loss 0.4986122250556946\n",
      "Epoch 5 -- Batch 202/ 842, training loss 0.5312337279319763\n",
      "Epoch 5 -- Batch 203/ 842, training loss 0.5075602531433105\n",
      "Epoch 5 -- Batch 204/ 842, training loss 0.5237297415733337\n",
      "Epoch 5 -- Batch 205/ 842, training loss 0.5155743360519409\n",
      "Epoch 5 -- Batch 206/ 842, training loss 0.4945306181907654\n",
      "Epoch 5 -- Batch 207/ 842, training loss 0.4856877028942108\n",
      "Epoch 5 -- Batch 208/ 842, training loss 0.5021501183509827\n",
      "Epoch 5 -- Batch 209/ 842, training loss 0.5066995620727539\n",
      "Epoch 5 -- Batch 210/ 842, training loss 0.5149155259132385\n",
      "Epoch 5 -- Batch 211/ 842, training loss 0.5014727115631104\n",
      "Epoch 5 -- Batch 212/ 842, training loss 0.5263748168945312\n",
      "Epoch 5 -- Batch 213/ 842, training loss 0.5045861005783081\n",
      "Epoch 5 -- Batch 214/ 842, training loss 0.5032827854156494\n",
      "Epoch 5 -- Batch 215/ 842, training loss 0.4980432987213135\n",
      "Epoch 5 -- Batch 216/ 842, training loss 0.5152772665023804\n",
      "Epoch 5 -- Batch 217/ 842, training loss 0.5108837485313416\n",
      "Epoch 5 -- Batch 218/ 842, training loss 0.4988933503627777\n",
      "Epoch 5 -- Batch 219/ 842, training loss 0.4907030463218689\n",
      "Epoch 5 -- Batch 220/ 842, training loss 0.5184445381164551\n",
      "Epoch 5 -- Batch 221/ 842, training loss 0.5288459062576294\n",
      "Epoch 5 -- Batch 222/ 842, training loss 0.4815604090690613\n",
      "Epoch 5 -- Batch 223/ 842, training loss 0.496550977230072\n",
      "Epoch 5 -- Batch 224/ 842, training loss 0.5267218351364136\n",
      "Epoch 5 -- Batch 225/ 842, training loss 0.5079073905944824\n",
      "Epoch 5 -- Batch 226/ 842, training loss 0.5437137484550476\n",
      "Epoch 5 -- Batch 227/ 842, training loss 0.512256920337677\n",
      "Epoch 5 -- Batch 228/ 842, training loss 0.5079118013381958\n",
      "Epoch 5 -- Batch 229/ 842, training loss 0.5043438673019409\n",
      "Epoch 5 -- Batch 230/ 842, training loss 0.48147910833358765\n",
      "Epoch 5 -- Batch 231/ 842, training loss 0.5065328478813171\n",
      "Epoch 5 -- Batch 232/ 842, training loss 0.4838515818119049\n",
      "Epoch 5 -- Batch 233/ 842, training loss 0.5027523636817932\n",
      "Epoch 5 -- Batch 234/ 842, training loss 0.504393458366394\n",
      "Epoch 5 -- Batch 235/ 842, training loss 0.5109302401542664\n",
      "Epoch 5 -- Batch 236/ 842, training loss 0.5142983794212341\n",
      "Epoch 5 -- Batch 237/ 842, training loss 0.5123399496078491\n",
      "Epoch 5 -- Batch 238/ 842, training loss 0.5132004022598267\n",
      "Epoch 5 -- Batch 239/ 842, training loss 0.506037712097168\n",
      "Epoch 5 -- Batch 240/ 842, training loss 0.5041294097900391\n",
      "Epoch 5 -- Batch 241/ 842, training loss 0.512689471244812\n",
      "Epoch 5 -- Batch 242/ 842, training loss 0.5162066221237183\n",
      "Epoch 5 -- Batch 243/ 842, training loss 0.5090635418891907\n",
      "Epoch 5 -- Batch 244/ 842, training loss 0.49236825108528137\n",
      "Epoch 5 -- Batch 245/ 842, training loss 0.4960576593875885\n",
      "Epoch 5 -- Batch 246/ 842, training loss 0.5035277605056763\n",
      "Epoch 5 -- Batch 247/ 842, training loss 0.5353223085403442\n",
      "Epoch 5 -- Batch 248/ 842, training loss 0.48974916338920593\n",
      "Epoch 5 -- Batch 249/ 842, training loss 0.5159906148910522\n",
      "Epoch 5 -- Batch 250/ 842, training loss 0.5084865689277649\n",
      "Epoch 5 -- Batch 251/ 842, training loss 0.5085709691047668\n",
      "Epoch 5 -- Batch 252/ 842, training loss 0.5076037049293518\n",
      "Epoch 5 -- Batch 253/ 842, training loss 0.4874666929244995\n",
      "Epoch 5 -- Batch 254/ 842, training loss 0.4833335876464844\n",
      "Epoch 5 -- Batch 255/ 842, training loss 0.49028533697128296\n",
      "Epoch 5 -- Batch 256/ 842, training loss 0.5014691352844238\n",
      "Epoch 5 -- Batch 257/ 842, training loss 0.47048747539520264\n",
      "Epoch 5 -- Batch 258/ 842, training loss 0.4987993836402893\n",
      "Epoch 5 -- Batch 259/ 842, training loss 0.5119748711585999\n",
      "Epoch 5 -- Batch 260/ 842, training loss 0.522162914276123\n",
      "Epoch 5 -- Batch 261/ 842, training loss 0.5040380954742432\n",
      "Epoch 5 -- Batch 262/ 842, training loss 0.4994814991950989\n",
      "Epoch 5 -- Batch 263/ 842, training loss 0.5205115079879761\n",
      "Epoch 5 -- Batch 264/ 842, training loss 0.5077066421508789\n",
      "Epoch 5 -- Batch 265/ 842, training loss 0.5193874835968018\n",
      "Epoch 5 -- Batch 266/ 842, training loss 0.5079635977745056\n",
      "Epoch 5 -- Batch 267/ 842, training loss 0.4982292652130127\n",
      "Epoch 5 -- Batch 268/ 842, training loss 0.5034003257751465\n",
      "Epoch 5 -- Batch 269/ 842, training loss 0.4936331808567047\n",
      "Epoch 5 -- Batch 270/ 842, training loss 0.5108568072319031\n",
      "Epoch 5 -- Batch 271/ 842, training loss 0.5081793665885925\n",
      "Epoch 5 -- Batch 272/ 842, training loss 0.5157951712608337\n",
      "Epoch 5 -- Batch 273/ 842, training loss 0.498933881521225\n",
      "Epoch 5 -- Batch 274/ 842, training loss 0.525175154209137\n",
      "Epoch 5 -- Batch 275/ 842, training loss 0.5095666646957397\n",
      "Epoch 5 -- Batch 276/ 842, training loss 0.49221253395080566\n",
      "Epoch 5 -- Batch 277/ 842, training loss 0.5180295705795288\n",
      "Epoch 5 -- Batch 278/ 842, training loss 0.5248650312423706\n",
      "Epoch 5 -- Batch 279/ 842, training loss 0.5157232880592346\n",
      "Epoch 5 -- Batch 280/ 842, training loss 0.5206284523010254\n",
      "Epoch 5 -- Batch 281/ 842, training loss 0.50356525182724\n",
      "Epoch 5 -- Batch 282/ 842, training loss 0.4999537765979767\n",
      "Epoch 5 -- Batch 283/ 842, training loss 0.49889421463012695\n",
      "Epoch 5 -- Batch 284/ 842, training loss 0.5021641850471497\n",
      "Epoch 5 -- Batch 285/ 842, training loss 0.4794345796108246\n",
      "Epoch 5 -- Batch 286/ 842, training loss 0.515587568283081\n",
      "Epoch 5 -- Batch 287/ 842, training loss 0.505206823348999\n",
      "Epoch 5 -- Batch 288/ 842, training loss 0.5049642324447632\n",
      "Epoch 5 -- Batch 289/ 842, training loss 0.5125563144683838\n",
      "Epoch 5 -- Batch 290/ 842, training loss 0.49518075585365295\n",
      "Epoch 5 -- Batch 291/ 842, training loss 0.5163754820823669\n",
      "Epoch 5 -- Batch 292/ 842, training loss 0.5021081566810608\n",
      "Epoch 5 -- Batch 293/ 842, training loss 0.508342444896698\n",
      "Epoch 5 -- Batch 294/ 842, training loss 0.5217525362968445\n",
      "Epoch 5 -- Batch 295/ 842, training loss 0.5033546090126038\n",
      "Epoch 5 -- Batch 296/ 842, training loss 0.5119447708129883\n",
      "Epoch 5 -- Batch 297/ 842, training loss 0.5083545446395874\n",
      "Epoch 5 -- Batch 298/ 842, training loss 0.524388313293457\n",
      "Epoch 5 -- Batch 299/ 842, training loss 0.4914805293083191\n",
      "Epoch 5 -- Batch 300/ 842, training loss 0.5087625980377197\n",
      "Epoch 5 -- Batch 301/ 842, training loss 0.4889545142650604\n",
      "Epoch 5 -- Batch 302/ 842, training loss 0.5106596946716309\n",
      "Epoch 5 -- Batch 303/ 842, training loss 0.4904485046863556\n",
      "Epoch 5 -- Batch 304/ 842, training loss 0.5153574347496033\n",
      "Epoch 5 -- Batch 305/ 842, training loss 0.5278275609016418\n",
      "Epoch 5 -- Batch 306/ 842, training loss 0.5081782937049866\n",
      "Epoch 5 -- Batch 307/ 842, training loss 0.49593812227249146\n",
      "Epoch 5 -- Batch 308/ 842, training loss 0.5125048756599426\n",
      "Epoch 5 -- Batch 309/ 842, training loss 0.5218692421913147\n",
      "Epoch 5 -- Batch 310/ 842, training loss 0.5041138529777527\n",
      "Epoch 5 -- Batch 311/ 842, training loss 0.5102946162223816\n",
      "Epoch 5 -- Batch 312/ 842, training loss 0.5002133250236511\n",
      "Epoch 5 -- Batch 313/ 842, training loss 0.5010890960693359\n",
      "Epoch 5 -- Batch 314/ 842, training loss 0.5123559236526489\n",
      "Epoch 5 -- Batch 315/ 842, training loss 0.5213966965675354\n",
      "Epoch 5 -- Batch 316/ 842, training loss 0.4960397183895111\n",
      "Epoch 5 -- Batch 317/ 842, training loss 0.48637324571609497\n",
      "Epoch 5 -- Batch 318/ 842, training loss 0.4865487217903137\n",
      "Epoch 5 -- Batch 319/ 842, training loss 0.5125173330307007\n",
      "Epoch 5 -- Batch 320/ 842, training loss 0.4928678274154663\n",
      "Epoch 5 -- Batch 321/ 842, training loss 0.4938127100467682\n",
      "Epoch 5 -- Batch 322/ 842, training loss 0.494489848613739\n",
      "Epoch 5 -- Batch 323/ 842, training loss 0.4928082525730133\n",
      "Epoch 5 -- Batch 324/ 842, training loss 0.4825163185596466\n",
      "Epoch 5 -- Batch 325/ 842, training loss 0.5166113376617432\n",
      "Epoch 5 -- Batch 326/ 842, training loss 0.504228949546814\n",
      "Epoch 5 -- Batch 327/ 842, training loss 0.5041587948799133\n",
      "Epoch 5 -- Batch 328/ 842, training loss 0.4993143379688263\n",
      "Epoch 5 -- Batch 329/ 842, training loss 0.5226500034332275\n",
      "Epoch 5 -- Batch 330/ 842, training loss 0.4930265545845032\n",
      "Epoch 5 -- Batch 331/ 842, training loss 0.5056685209274292\n",
      "Epoch 5 -- Batch 332/ 842, training loss 0.4902626872062683\n",
      "Epoch 5 -- Batch 333/ 842, training loss 0.4989793300628662\n",
      "Epoch 5 -- Batch 334/ 842, training loss 0.47899892926216125\n",
      "Epoch 5 -- Batch 335/ 842, training loss 0.5189507603645325\n",
      "Epoch 5 -- Batch 336/ 842, training loss 0.5105957388877869\n",
      "Epoch 5 -- Batch 337/ 842, training loss 0.5146886110305786\n",
      "Epoch 5 -- Batch 338/ 842, training loss 0.5119292140007019\n",
      "Epoch 5 -- Batch 339/ 842, training loss 0.5241925120353699\n",
      "Epoch 5 -- Batch 340/ 842, training loss 0.5114923119544983\n",
      "Epoch 5 -- Batch 341/ 842, training loss 0.5191583633422852\n",
      "Epoch 5 -- Batch 342/ 842, training loss 0.4983924925327301\n",
      "Epoch 5 -- Batch 343/ 842, training loss 0.4950506091117859\n",
      "Epoch 5 -- Batch 344/ 842, training loss 0.5178015232086182\n",
      "Epoch 5 -- Batch 345/ 842, training loss 0.4846706986427307\n",
      "Epoch 5 -- Batch 346/ 842, training loss 0.5084925889968872\n",
      "Epoch 5 -- Batch 347/ 842, training loss 0.5131074786186218\n",
      "Epoch 5 -- Batch 348/ 842, training loss 0.49699166417121887\n",
      "Epoch 5 -- Batch 349/ 842, training loss 0.5236691236495972\n",
      "Epoch 5 -- Batch 350/ 842, training loss 0.5011064410209656\n",
      "Epoch 5 -- Batch 351/ 842, training loss 0.5087592601776123\n",
      "Epoch 5 -- Batch 352/ 842, training loss 0.5058859586715698\n",
      "Epoch 5 -- Batch 353/ 842, training loss 0.49364474415779114\n",
      "Epoch 5 -- Batch 354/ 842, training loss 0.4994209408760071\n",
      "Epoch 5 -- Batch 355/ 842, training loss 0.4951789081096649\n",
      "Epoch 5 -- Batch 356/ 842, training loss 0.514361560344696\n",
      "Epoch 5 -- Batch 357/ 842, training loss 0.5083926320075989\n",
      "Epoch 5 -- Batch 358/ 842, training loss 0.49450036883354187\n",
      "Epoch 5 -- Batch 359/ 842, training loss 0.5118876099586487\n",
      "Epoch 5 -- Batch 360/ 842, training loss 0.5017448663711548\n",
      "Epoch 5 -- Batch 361/ 842, training loss 0.5075089335441589\n",
      "Epoch 5 -- Batch 362/ 842, training loss 0.5214899182319641\n",
      "Epoch 5 -- Batch 363/ 842, training loss 0.5221375823020935\n",
      "Epoch 5 -- Batch 364/ 842, training loss 0.4854225218296051\n",
      "Epoch 5 -- Batch 365/ 842, training loss 0.4958898425102234\n",
      "Epoch 5 -- Batch 366/ 842, training loss 0.5336245894432068\n",
      "Epoch 5 -- Batch 367/ 842, training loss 0.5059210658073425\n",
      "Epoch 5 -- Batch 368/ 842, training loss 0.5139814019203186\n",
      "Epoch 5 -- Batch 369/ 842, training loss 0.4924705922603607\n",
      "Epoch 5 -- Batch 370/ 842, training loss 0.49959883093833923\n",
      "Epoch 5 -- Batch 371/ 842, training loss 0.4923493564128876\n",
      "Epoch 5 -- Batch 372/ 842, training loss 0.5221612453460693\n",
      "Epoch 5 -- Batch 373/ 842, training loss 0.49657493829727173\n",
      "Epoch 5 -- Batch 374/ 842, training loss 0.5086669921875\n",
      "Epoch 5 -- Batch 375/ 842, training loss 0.5069753527641296\n",
      "Epoch 5 -- Batch 376/ 842, training loss 0.4975660741329193\n",
      "Epoch 5 -- Batch 377/ 842, training loss 0.5097639560699463\n",
      "Epoch 5 -- Batch 378/ 842, training loss 0.5131346583366394\n",
      "Epoch 5 -- Batch 379/ 842, training loss 0.4873942732810974\n",
      "Epoch 5 -- Batch 380/ 842, training loss 0.4992257356643677\n",
      "Epoch 5 -- Batch 381/ 842, training loss 0.49583497643470764\n",
      "Epoch 5 -- Batch 382/ 842, training loss 0.5030388832092285\n",
      "Epoch 5 -- Batch 383/ 842, training loss 0.4892844557762146\n",
      "Epoch 5 -- Batch 384/ 842, training loss 0.5151343941688538\n",
      "Epoch 5 -- Batch 385/ 842, training loss 0.5109213590621948\n",
      "Epoch 5 -- Batch 386/ 842, training loss 0.5177069902420044\n",
      "Epoch 5 -- Batch 387/ 842, training loss 0.478725403547287\n",
      "Epoch 5 -- Batch 388/ 842, training loss 0.5067705512046814\n",
      "Epoch 5 -- Batch 389/ 842, training loss 0.4955861270427704\n",
      "Epoch 5 -- Batch 390/ 842, training loss 0.5120664238929749\n",
      "Epoch 5 -- Batch 391/ 842, training loss 0.5184161067008972\n",
      "Epoch 5 -- Batch 392/ 842, training loss 0.5053507685661316\n",
      "Epoch 5 -- Batch 393/ 842, training loss 0.5277745127677917\n",
      "Epoch 5 -- Batch 394/ 842, training loss 0.5321550369262695\n",
      "Epoch 5 -- Batch 395/ 842, training loss 0.4958703815937042\n",
      "Epoch 5 -- Batch 396/ 842, training loss 0.49111810326576233\n",
      "Epoch 5 -- Batch 397/ 842, training loss 0.4932239353656769\n",
      "Epoch 5 -- Batch 398/ 842, training loss 0.49925798177719116\n",
      "Epoch 5 -- Batch 399/ 842, training loss 0.5180392265319824\n",
      "Epoch 5 -- Batch 400/ 842, training loss 0.5170354843139648\n",
      "Epoch 5 -- Batch 401/ 842, training loss 0.48866406083106995\n",
      "Epoch 5 -- Batch 402/ 842, training loss 0.49804696440696716\n",
      "Epoch 5 -- Batch 403/ 842, training loss 0.5165824890136719\n",
      "Epoch 5 -- Batch 404/ 842, training loss 0.5027904510498047\n",
      "Epoch 5 -- Batch 405/ 842, training loss 0.5124532580375671\n",
      "Epoch 5 -- Batch 406/ 842, training loss 0.51543790102005\n",
      "Epoch 5 -- Batch 407/ 842, training loss 0.4950447976589203\n",
      "Epoch 5 -- Batch 408/ 842, training loss 0.5047768354415894\n",
      "Epoch 5 -- Batch 409/ 842, training loss 0.5198773741722107\n",
      "Epoch 5 -- Batch 410/ 842, training loss 0.5122746825218201\n",
      "Epoch 5 -- Batch 411/ 842, training loss 0.49935418367385864\n",
      "Epoch 5 -- Batch 412/ 842, training loss 0.5013796091079712\n",
      "Epoch 5 -- Batch 413/ 842, training loss 0.47757893800735474\n",
      "Epoch 5 -- Batch 414/ 842, training loss 0.4626879096031189\n",
      "Epoch 5 -- Batch 415/ 842, training loss 0.5117861032485962\n",
      "Epoch 5 -- Batch 416/ 842, training loss 0.48615241050720215\n",
      "Epoch 5 -- Batch 417/ 842, training loss 0.5223214030265808\n",
      "Epoch 5 -- Batch 418/ 842, training loss 0.4893646240234375\n",
      "Epoch 5 -- Batch 419/ 842, training loss 0.5194942951202393\n",
      "Epoch 5 -- Batch 420/ 842, training loss 0.5164582133293152\n",
      "Epoch 5 -- Batch 421/ 842, training loss 0.5034529566764832\n",
      "Epoch 5 -- Batch 422/ 842, training loss 0.5317263603210449\n",
      "Epoch 5 -- Batch 423/ 842, training loss 0.4876919090747833\n",
      "Epoch 5 -- Batch 424/ 842, training loss 0.5117720365524292\n",
      "Epoch 5 -- Batch 425/ 842, training loss 0.5309416651725769\n",
      "Epoch 5 -- Batch 426/ 842, training loss 0.5091454386711121\n",
      "Epoch 5 -- Batch 427/ 842, training loss 0.5113142728805542\n",
      "Epoch 5 -- Batch 428/ 842, training loss 0.5014269351959229\n",
      "Epoch 5 -- Batch 429/ 842, training loss 0.5150588154792786\n",
      "Epoch 5 -- Batch 430/ 842, training loss 0.504961371421814\n",
      "Epoch 5 -- Batch 431/ 842, training loss 0.5036370754241943\n",
      "Epoch 5 -- Batch 432/ 842, training loss 0.48239588737487793\n",
      "Epoch 5 -- Batch 433/ 842, training loss 0.49968355894088745\n",
      "Epoch 5 -- Batch 434/ 842, training loss 0.5123623013496399\n",
      "Epoch 5 -- Batch 435/ 842, training loss 0.5296052098274231\n",
      "Epoch 5 -- Batch 436/ 842, training loss 0.5158216953277588\n",
      "Epoch 5 -- Batch 437/ 842, training loss 0.502522349357605\n",
      "Epoch 5 -- Batch 438/ 842, training loss 0.5030125379562378\n",
      "Epoch 5 -- Batch 439/ 842, training loss 0.5283203125\n",
      "Epoch 5 -- Batch 440/ 842, training loss 0.5086992383003235\n",
      "Epoch 5 -- Batch 441/ 842, training loss 0.5136380195617676\n",
      "Epoch 5 -- Batch 442/ 842, training loss 0.50752854347229\n",
      "Epoch 5 -- Batch 443/ 842, training loss 0.4908986985683441\n",
      "Epoch 5 -- Batch 444/ 842, training loss 0.5137407183647156\n",
      "Epoch 5 -- Batch 445/ 842, training loss 0.5036890506744385\n",
      "Epoch 5 -- Batch 446/ 842, training loss 0.5096172094345093\n",
      "Epoch 5 -- Batch 447/ 842, training loss 0.4979135990142822\n",
      "Epoch 5 -- Batch 448/ 842, training loss 0.4914468824863434\n",
      "Epoch 5 -- Batch 449/ 842, training loss 0.511601448059082\n",
      "Epoch 5 -- Batch 450/ 842, training loss 0.5188753008842468\n",
      "Epoch 5 -- Batch 451/ 842, training loss 0.5043678283691406\n",
      "Epoch 5 -- Batch 452/ 842, training loss 0.4861041307449341\n",
      "Epoch 5 -- Batch 453/ 842, training loss 0.5052458643913269\n",
      "Epoch 5 -- Batch 454/ 842, training loss 0.5030145049095154\n",
      "Epoch 5 -- Batch 455/ 842, training loss 0.4941999912261963\n",
      "Epoch 5 -- Batch 456/ 842, training loss 0.5027887225151062\n",
      "Epoch 5 -- Batch 457/ 842, training loss 0.5153599977493286\n",
      "Epoch 5 -- Batch 458/ 842, training loss 0.5050855875015259\n",
      "Epoch 5 -- Batch 459/ 842, training loss 0.48946258425712585\n",
      "Epoch 5 -- Batch 460/ 842, training loss 0.48801401257514954\n",
      "Epoch 5 -- Batch 461/ 842, training loss 0.49666857719421387\n",
      "Epoch 5 -- Batch 462/ 842, training loss 0.49731674790382385\n",
      "Epoch 5 -- Batch 463/ 842, training loss 0.4929455518722534\n",
      "Epoch 5 -- Batch 464/ 842, training loss 0.48198091983795166\n",
      "Epoch 5 -- Batch 465/ 842, training loss 0.5246850848197937\n",
      "Epoch 5 -- Batch 466/ 842, training loss 0.4783511459827423\n",
      "Epoch 5 -- Batch 467/ 842, training loss 0.4992634952068329\n",
      "Epoch 5 -- Batch 468/ 842, training loss 0.5040997266769409\n",
      "Epoch 5 -- Batch 469/ 842, training loss 0.48783668875694275\n",
      "Epoch 5 -- Batch 470/ 842, training loss 0.503671407699585\n",
      "Epoch 5 -- Batch 471/ 842, training loss 0.4929181933403015\n",
      "Epoch 5 -- Batch 472/ 842, training loss 0.5042610168457031\n",
      "Epoch 5 -- Batch 473/ 842, training loss 0.5063955187797546\n",
      "Epoch 5 -- Batch 474/ 842, training loss 0.523838222026825\n",
      "Epoch 5 -- Batch 475/ 842, training loss 0.5064742565155029\n",
      "Epoch 5 -- Batch 476/ 842, training loss 0.4996570348739624\n",
      "Epoch 5 -- Batch 477/ 842, training loss 0.49195316433906555\n",
      "Epoch 5 -- Batch 478/ 842, training loss 0.48610538244247437\n",
      "Epoch 5 -- Batch 479/ 842, training loss 0.5260058641433716\n",
      "Epoch 5 -- Batch 480/ 842, training loss 0.49655985832214355\n",
      "Epoch 5 -- Batch 481/ 842, training loss 0.4954785704612732\n",
      "Epoch 5 -- Batch 482/ 842, training loss 0.49982696771621704\n",
      "Epoch 5 -- Batch 483/ 842, training loss 0.49950912594795227\n",
      "Epoch 5 -- Batch 484/ 842, training loss 0.4983501136302948\n",
      "Epoch 5 -- Batch 485/ 842, training loss 0.5078796744346619\n",
      "Epoch 5 -- Batch 486/ 842, training loss 0.4858670234680176\n",
      "Epoch 5 -- Batch 487/ 842, training loss 0.47921526432037354\n",
      "Epoch 5 -- Batch 488/ 842, training loss 0.48984381556510925\n",
      "Epoch 5 -- Batch 489/ 842, training loss 0.4884464740753174\n",
      "Epoch 5 -- Batch 490/ 842, training loss 0.5103751420974731\n",
      "Epoch 5 -- Batch 491/ 842, training loss 0.495747834444046\n",
      "Epoch 5 -- Batch 492/ 842, training loss 0.5070825815200806\n",
      "Epoch 5 -- Batch 493/ 842, training loss 0.4950871467590332\n",
      "Epoch 5 -- Batch 494/ 842, training loss 0.5168328881263733\n",
      "Epoch 5 -- Batch 495/ 842, training loss 0.5141897797584534\n",
      "Epoch 5 -- Batch 496/ 842, training loss 0.5040826201438904\n",
      "Epoch 5 -- Batch 497/ 842, training loss 0.5098037123680115\n",
      "Epoch 5 -- Batch 498/ 842, training loss 0.5082103610038757\n",
      "Epoch 5 -- Batch 499/ 842, training loss 0.490867018699646\n",
      "Epoch 5 -- Batch 500/ 842, training loss 0.4882315993309021\n",
      "Epoch 5 -- Batch 501/ 842, training loss 0.5156162977218628\n",
      "Epoch 5 -- Batch 502/ 842, training loss 0.5094618201255798\n",
      "Epoch 5 -- Batch 503/ 842, training loss 0.4810328781604767\n",
      "Epoch 5 -- Batch 504/ 842, training loss 0.5108717679977417\n",
      "Epoch 5 -- Batch 505/ 842, training loss 0.49733155965805054\n",
      "Epoch 5 -- Batch 506/ 842, training loss 0.5164068937301636\n",
      "Epoch 5 -- Batch 507/ 842, training loss 0.5422027111053467\n",
      "Epoch 5 -- Batch 508/ 842, training loss 0.505890965461731\n",
      "Epoch 5 -- Batch 509/ 842, training loss 0.49032098054885864\n",
      "Epoch 5 -- Batch 510/ 842, training loss 0.5139070153236389\n",
      "Epoch 5 -- Batch 511/ 842, training loss 0.499005526304245\n",
      "Epoch 5 -- Batch 512/ 842, training loss 0.5084826946258545\n",
      "Epoch 5 -- Batch 513/ 842, training loss 0.4936205744743347\n",
      "Epoch 5 -- Batch 514/ 842, training loss 0.515139102935791\n",
      "Epoch 5 -- Batch 515/ 842, training loss 0.512928307056427\n",
      "Epoch 5 -- Batch 516/ 842, training loss 0.5067564845085144\n",
      "Epoch 5 -- Batch 517/ 842, training loss 0.48077622056007385\n",
      "Epoch 5 -- Batch 518/ 842, training loss 0.48929035663604736\n",
      "Epoch 5 -- Batch 519/ 842, training loss 0.5081741213798523\n",
      "Epoch 5 -- Batch 520/ 842, training loss 0.5092094540596008\n",
      "Epoch 5 -- Batch 521/ 842, training loss 0.5200566053390503\n",
      "Epoch 5 -- Batch 522/ 842, training loss 0.509730339050293\n",
      "Epoch 5 -- Batch 523/ 842, training loss 0.49313947558403015\n",
      "Epoch 5 -- Batch 524/ 842, training loss 0.5246612429618835\n",
      "Epoch 5 -- Batch 525/ 842, training loss 0.5354236364364624\n",
      "Epoch 5 -- Batch 526/ 842, training loss 0.5017721652984619\n",
      "Epoch 5 -- Batch 527/ 842, training loss 0.5034273266792297\n",
      "Epoch 5 -- Batch 528/ 842, training loss 0.5318094491958618\n",
      "Epoch 5 -- Batch 529/ 842, training loss 0.5094494819641113\n",
      "Epoch 5 -- Batch 530/ 842, training loss 0.49417486786842346\n",
      "Epoch 5 -- Batch 531/ 842, training loss 0.501863956451416\n",
      "Epoch 5 -- Batch 532/ 842, training loss 0.4950118958950043\n",
      "Epoch 5 -- Batch 533/ 842, training loss 0.5036802291870117\n",
      "Epoch 5 -- Batch 534/ 842, training loss 0.48992645740509033\n",
      "Epoch 5 -- Batch 535/ 842, training loss 0.504528820514679\n",
      "Epoch 5 -- Batch 536/ 842, training loss 0.5073045492172241\n",
      "Epoch 5 -- Batch 537/ 842, training loss 0.5157389044761658\n",
      "Epoch 5 -- Batch 538/ 842, training loss 0.49297934770584106\n",
      "Epoch 5 -- Batch 539/ 842, training loss 0.5008874535560608\n",
      "Epoch 5 -- Batch 540/ 842, training loss 0.49725228548049927\n",
      "Epoch 5 -- Batch 541/ 842, training loss 0.5145867466926575\n",
      "Epoch 5 -- Batch 542/ 842, training loss 0.5062321424484253\n",
      "Epoch 5 -- Batch 543/ 842, training loss 0.5035390853881836\n",
      "Epoch 5 -- Batch 544/ 842, training loss 0.49846792221069336\n",
      "Epoch 5 -- Batch 545/ 842, training loss 0.5036458969116211\n",
      "Epoch 5 -- Batch 546/ 842, training loss 0.5186821222305298\n",
      "Epoch 5 -- Batch 547/ 842, training loss 0.4902688264846802\n",
      "Epoch 5 -- Batch 548/ 842, training loss 0.518093466758728\n",
      "Epoch 5 -- Batch 549/ 842, training loss 0.5028489232063293\n",
      "Epoch 5 -- Batch 550/ 842, training loss 0.5007493495941162\n",
      "Epoch 5 -- Batch 551/ 842, training loss 0.5154185891151428\n",
      "Epoch 5 -- Batch 552/ 842, training loss 0.4802340865135193\n",
      "Epoch 5 -- Batch 553/ 842, training loss 0.5050969123840332\n",
      "Epoch 5 -- Batch 554/ 842, training loss 0.4930456280708313\n",
      "Epoch 5 -- Batch 555/ 842, training loss 0.4950868785381317\n",
      "Epoch 5 -- Batch 556/ 842, training loss 0.4913328289985657\n",
      "Epoch 5 -- Batch 557/ 842, training loss 0.5077767968177795\n",
      "Epoch 5 -- Batch 558/ 842, training loss 0.4849693477153778\n",
      "Epoch 5 -- Batch 559/ 842, training loss 0.5015066862106323\n",
      "Epoch 5 -- Batch 560/ 842, training loss 0.5180158019065857\n",
      "Epoch 5 -- Batch 561/ 842, training loss 0.5100408792495728\n",
      "Epoch 5 -- Batch 562/ 842, training loss 0.47747182846069336\n",
      "Epoch 5 -- Batch 563/ 842, training loss 0.5011156797409058\n",
      "Epoch 5 -- Batch 564/ 842, training loss 0.49546802043914795\n",
      "Epoch 5 -- Batch 565/ 842, training loss 0.5071608424186707\n",
      "Epoch 5 -- Batch 566/ 842, training loss 0.4977327883243561\n",
      "Epoch 5 -- Batch 567/ 842, training loss 0.49618831276893616\n",
      "Epoch 5 -- Batch 568/ 842, training loss 0.48648807406425476\n",
      "Epoch 5 -- Batch 569/ 842, training loss 0.49268704652786255\n",
      "Epoch 5 -- Batch 570/ 842, training loss 0.49748098850250244\n",
      "Epoch 5 -- Batch 571/ 842, training loss 0.49169921875\n",
      "Epoch 5 -- Batch 572/ 842, training loss 0.4939471483230591\n",
      "Epoch 5 -- Batch 573/ 842, training loss 0.4761244058609009\n",
      "Epoch 5 -- Batch 574/ 842, training loss 0.5080178380012512\n",
      "Epoch 5 -- Batch 575/ 842, training loss 0.4967667758464813\n",
      "Epoch 5 -- Batch 576/ 842, training loss 0.4918704628944397\n",
      "Epoch 5 -- Batch 577/ 842, training loss 0.5053404569625854\n",
      "Epoch 5 -- Batch 578/ 842, training loss 0.5076172351837158\n",
      "Epoch 5 -- Batch 579/ 842, training loss 0.5265130996704102\n",
      "Epoch 5 -- Batch 580/ 842, training loss 0.4849521219730377\n",
      "Epoch 5 -- Batch 581/ 842, training loss 0.5018367171287537\n",
      "Epoch 5 -- Batch 582/ 842, training loss 0.5236698389053345\n",
      "Epoch 5 -- Batch 583/ 842, training loss 0.4945373833179474\n",
      "Epoch 5 -- Batch 584/ 842, training loss 0.4849272668361664\n",
      "Epoch 5 -- Batch 585/ 842, training loss 0.4991037845611572\n",
      "Epoch 5 -- Batch 586/ 842, training loss 0.4861500859260559\n",
      "Epoch 5 -- Batch 587/ 842, training loss 0.5055883526802063\n",
      "Epoch 5 -- Batch 588/ 842, training loss 0.5054725408554077\n",
      "Epoch 5 -- Batch 589/ 842, training loss 0.4955214560031891\n",
      "Epoch 5 -- Batch 590/ 842, training loss 0.4997154474258423\n",
      "Epoch 5 -- Batch 591/ 842, training loss 0.48799505829811096\n",
      "Epoch 5 -- Batch 592/ 842, training loss 0.49566158652305603\n",
      "Epoch 5 -- Batch 593/ 842, training loss 0.48694249987602234\n",
      "Epoch 5 -- Batch 594/ 842, training loss 0.4978425204753876\n",
      "Epoch 5 -- Batch 595/ 842, training loss 0.47365206480026245\n",
      "Epoch 5 -- Batch 596/ 842, training loss 0.5068265199661255\n",
      "Epoch 5 -- Batch 597/ 842, training loss 0.4824340343475342\n",
      "Epoch 5 -- Batch 598/ 842, training loss 0.529225766658783\n",
      "Epoch 5 -- Batch 599/ 842, training loss 0.5096092224121094\n",
      "Epoch 5 -- Batch 600/ 842, training loss 0.4943537414073944\n",
      "Epoch 5 -- Batch 601/ 842, training loss 0.49672356247901917\n",
      "Epoch 5 -- Batch 602/ 842, training loss 0.4854719340801239\n",
      "Epoch 5 -- Batch 603/ 842, training loss 0.5127730369567871\n",
      "Epoch 5 -- Batch 604/ 842, training loss 0.4971461296081543\n",
      "Epoch 5 -- Batch 605/ 842, training loss 0.5179041624069214\n",
      "Epoch 5 -- Batch 606/ 842, training loss 0.5106773376464844\n",
      "Epoch 5 -- Batch 607/ 842, training loss 0.49874335527420044\n",
      "Epoch 5 -- Batch 608/ 842, training loss 0.49301281571388245\n",
      "Epoch 5 -- Batch 609/ 842, training loss 0.5082803964614868\n",
      "Epoch 5 -- Batch 610/ 842, training loss 0.5315927863121033\n",
      "Epoch 5 -- Batch 611/ 842, training loss 0.48915284872055054\n",
      "Epoch 5 -- Batch 612/ 842, training loss 0.5007545351982117\n",
      "Epoch 5 -- Batch 613/ 842, training loss 0.504245400428772\n",
      "Epoch 5 -- Batch 614/ 842, training loss 0.477723091840744\n",
      "Epoch 5 -- Batch 615/ 842, training loss 0.48812174797058105\n",
      "Epoch 5 -- Batch 616/ 842, training loss 0.500311553478241\n",
      "Epoch 5 -- Batch 617/ 842, training loss 0.5241841673851013\n",
      "Epoch 5 -- Batch 618/ 842, training loss 0.4913325607776642\n",
      "Epoch 5 -- Batch 619/ 842, training loss 0.4814627766609192\n",
      "Epoch 5 -- Batch 620/ 842, training loss 0.4903218150138855\n",
      "Epoch 5 -- Batch 621/ 842, training loss 0.5276845693588257\n",
      "Epoch 5 -- Batch 622/ 842, training loss 0.5076435804367065\n",
      "Epoch 5 -- Batch 623/ 842, training loss 0.47804710268974304\n",
      "Epoch 5 -- Batch 624/ 842, training loss 0.5026611089706421\n",
      "Epoch 5 -- Batch 625/ 842, training loss 0.5049887895584106\n",
      "Epoch 5 -- Batch 626/ 842, training loss 0.5046470761299133\n",
      "Epoch 5 -- Batch 627/ 842, training loss 0.5137383937835693\n",
      "Epoch 5 -- Batch 628/ 842, training loss 0.49385178089141846\n",
      "Epoch 5 -- Batch 629/ 842, training loss 0.48829352855682373\n",
      "Epoch 5 -- Batch 630/ 842, training loss 0.481698215007782\n",
      "Epoch 5 -- Batch 631/ 842, training loss 0.4960394501686096\n",
      "Epoch 5 -- Batch 632/ 842, training loss 0.5119924545288086\n",
      "Epoch 5 -- Batch 633/ 842, training loss 0.484485387802124\n",
      "Epoch 5 -- Batch 634/ 842, training loss 0.5067559480667114\n",
      "Epoch 5 -- Batch 635/ 842, training loss 0.4919034242630005\n",
      "Epoch 5 -- Batch 636/ 842, training loss 0.5101422667503357\n",
      "Epoch 5 -- Batch 637/ 842, training loss 0.4846000373363495\n",
      "Epoch 5 -- Batch 638/ 842, training loss 0.49031898379325867\n",
      "Epoch 5 -- Batch 639/ 842, training loss 0.5023088455200195\n",
      "Epoch 5 -- Batch 640/ 842, training loss 0.4834328591823578\n",
      "Epoch 5 -- Batch 641/ 842, training loss 0.4890725314617157\n",
      "Epoch 5 -- Batch 642/ 842, training loss 0.5219756364822388\n",
      "Epoch 5 -- Batch 643/ 842, training loss 0.4842795729637146\n",
      "Epoch 5 -- Batch 644/ 842, training loss 0.5149968862533569\n",
      "Epoch 5 -- Batch 645/ 842, training loss 0.4843772351741791\n",
      "Epoch 5 -- Batch 646/ 842, training loss 0.5105701088905334\n",
      "Epoch 5 -- Batch 647/ 842, training loss 0.47866958379745483\n",
      "Epoch 5 -- Batch 648/ 842, training loss 0.49248436093330383\n",
      "Epoch 5 -- Batch 649/ 842, training loss 0.5006054639816284\n",
      "Epoch 5 -- Batch 650/ 842, training loss 0.49827897548675537\n",
      "Epoch 5 -- Batch 651/ 842, training loss 0.5161881446838379\n",
      "Epoch 5 -- Batch 652/ 842, training loss 0.509338915348053\n",
      "Epoch 5 -- Batch 653/ 842, training loss 0.5107037425041199\n",
      "Epoch 5 -- Batch 654/ 842, training loss 0.48616117238998413\n",
      "Epoch 5 -- Batch 655/ 842, training loss 0.4951603412628174\n",
      "Epoch 5 -- Batch 656/ 842, training loss 0.5036037564277649\n",
      "Epoch 5 -- Batch 657/ 842, training loss 0.5046392679214478\n",
      "Epoch 5 -- Batch 658/ 842, training loss 0.512380063533783\n",
      "Epoch 5 -- Batch 659/ 842, training loss 0.4870826303958893\n",
      "Epoch 5 -- Batch 660/ 842, training loss 0.5052086710929871\n",
      "Epoch 5 -- Batch 661/ 842, training loss 0.5048732757568359\n",
      "Epoch 5 -- Batch 662/ 842, training loss 0.49386224150657654\n",
      "Epoch 5 -- Batch 663/ 842, training loss 0.475300133228302\n",
      "Epoch 5 -- Batch 664/ 842, training loss 0.4800169765949249\n",
      "Epoch 5 -- Batch 665/ 842, training loss 0.5098873376846313\n",
      "Epoch 5 -- Batch 666/ 842, training loss 0.5189245939254761\n",
      "Epoch 5 -- Batch 667/ 842, training loss 0.49199235439300537\n",
      "Epoch 5 -- Batch 668/ 842, training loss 0.5010964870452881\n",
      "Epoch 5 -- Batch 669/ 842, training loss 0.514420211315155\n",
      "Epoch 5 -- Batch 670/ 842, training loss 0.4985410273075104\n",
      "Epoch 5 -- Batch 671/ 842, training loss 0.48487573862075806\n",
      "Epoch 5 -- Batch 672/ 842, training loss 0.49910902976989746\n",
      "Epoch 5 -- Batch 673/ 842, training loss 0.5042220950126648\n",
      "Epoch 5 -- Batch 674/ 842, training loss 0.5036457777023315\n",
      "Epoch 5 -- Batch 675/ 842, training loss 0.5001675486564636\n",
      "Epoch 5 -- Batch 676/ 842, training loss 0.4911668300628662\n",
      "Epoch 5 -- Batch 677/ 842, training loss 0.4953060448169708\n",
      "Epoch 5 -- Batch 678/ 842, training loss 0.5058410167694092\n",
      "Epoch 5 -- Batch 679/ 842, training loss 0.501500129699707\n",
      "Epoch 5 -- Batch 680/ 842, training loss 0.5171588063240051\n",
      "Epoch 5 -- Batch 681/ 842, training loss 0.49856168031692505\n",
      "Epoch 5 -- Batch 682/ 842, training loss 0.5065538287162781\n",
      "Epoch 5 -- Batch 683/ 842, training loss 0.4763970971107483\n",
      "Epoch 5 -- Batch 684/ 842, training loss 0.5052439570426941\n",
      "Epoch 5 -- Batch 685/ 842, training loss 0.4877632260322571\n",
      "Epoch 5 -- Batch 686/ 842, training loss 0.5178039073944092\n",
      "Epoch 5 -- Batch 687/ 842, training loss 0.5145363211631775\n",
      "Epoch 5 -- Batch 688/ 842, training loss 0.4943620264530182\n",
      "Epoch 5 -- Batch 689/ 842, training loss 0.48662427067756653\n",
      "Epoch 5 -- Batch 690/ 842, training loss 0.4871950149536133\n",
      "Epoch 5 -- Batch 691/ 842, training loss 0.5025298595428467\n",
      "Epoch 5 -- Batch 692/ 842, training loss 0.5030704736709595\n",
      "Epoch 5 -- Batch 693/ 842, training loss 0.5080506801605225\n",
      "Epoch 5 -- Batch 694/ 842, training loss 0.5086625814437866\n",
      "Epoch 5 -- Batch 695/ 842, training loss 0.48322224617004395\n",
      "Epoch 5 -- Batch 696/ 842, training loss 0.4854962229728699\n",
      "Epoch 5 -- Batch 697/ 842, training loss 0.50485759973526\n",
      "Epoch 5 -- Batch 698/ 842, training loss 0.4952060580253601\n",
      "Epoch 5 -- Batch 699/ 842, training loss 0.5049855709075928\n",
      "Epoch 5 -- Batch 700/ 842, training loss 0.49542978405952454\n",
      "Epoch 5 -- Batch 701/ 842, training loss 0.4979858100414276\n",
      "Epoch 5 -- Batch 702/ 842, training loss 0.4934244751930237\n",
      "Epoch 5 -- Batch 703/ 842, training loss 0.4967747628688812\n",
      "Epoch 5 -- Batch 704/ 842, training loss 0.4858739674091339\n",
      "Epoch 5 -- Batch 705/ 842, training loss 0.4909343421459198\n",
      "Epoch 5 -- Batch 706/ 842, training loss 0.4822302460670471\n",
      "Epoch 5 -- Batch 707/ 842, training loss 0.49916771054267883\n",
      "Epoch 5 -- Batch 708/ 842, training loss 0.5119192600250244\n",
      "Epoch 5 -- Batch 709/ 842, training loss 0.49193549156188965\n",
      "Epoch 5 -- Batch 710/ 842, training loss 0.47457215189933777\n",
      "Epoch 5 -- Batch 711/ 842, training loss 0.48533162474632263\n",
      "Epoch 5 -- Batch 712/ 842, training loss 0.4965592622756958\n",
      "Epoch 5 -- Batch 713/ 842, training loss 0.49823620915412903\n",
      "Epoch 5 -- Batch 714/ 842, training loss 0.47895628213882446\n",
      "Epoch 5 -- Batch 715/ 842, training loss 0.49241605401039124\n",
      "Epoch 5 -- Batch 716/ 842, training loss 0.512263298034668\n",
      "Epoch 5 -- Batch 717/ 842, training loss 0.5139002203941345\n",
      "Epoch 5 -- Batch 718/ 842, training loss 0.4964716136455536\n",
      "Epoch 5 -- Batch 719/ 842, training loss 0.5027446150779724\n",
      "Epoch 5 -- Batch 720/ 842, training loss 0.5063992738723755\n",
      "Epoch 5 -- Batch 721/ 842, training loss 0.5124368667602539\n",
      "Epoch 5 -- Batch 722/ 842, training loss 0.5064907670021057\n",
      "Epoch 5 -- Batch 723/ 842, training loss 0.47152936458587646\n",
      "Epoch 5 -- Batch 724/ 842, training loss 0.5123663544654846\n",
      "Epoch 5 -- Batch 725/ 842, training loss 0.4960634112358093\n",
      "Epoch 5 -- Batch 726/ 842, training loss 0.5002394914627075\n",
      "Epoch 5 -- Batch 727/ 842, training loss 0.507763683795929\n",
      "Epoch 5 -- Batch 728/ 842, training loss 0.4863787889480591\n",
      "Epoch 5 -- Batch 729/ 842, training loss 0.5136533975601196\n",
      "Epoch 5 -- Batch 730/ 842, training loss 0.49074360728263855\n",
      "Epoch 5 -- Batch 731/ 842, training loss 0.48264315724372864\n",
      "Epoch 5 -- Batch 732/ 842, training loss 0.49755433201789856\n",
      "Epoch 5 -- Batch 733/ 842, training loss 0.48935121297836304\n",
      "Epoch 5 -- Batch 734/ 842, training loss 0.5113527178764343\n",
      "Epoch 5 -- Batch 735/ 842, training loss 0.49373215436935425\n",
      "Epoch 5 -- Batch 736/ 842, training loss 0.5052735209465027\n",
      "Epoch 5 -- Batch 737/ 842, training loss 0.4971402585506439\n",
      "Epoch 5 -- Batch 738/ 842, training loss 0.4980354309082031\n",
      "Epoch 5 -- Batch 739/ 842, training loss 0.5133529305458069\n",
      "Epoch 5 -- Batch 740/ 842, training loss 0.5013255476951599\n",
      "Epoch 5 -- Batch 741/ 842, training loss 0.5065658092498779\n",
      "Epoch 5 -- Batch 742/ 842, training loss 0.49195602536201477\n",
      "Epoch 5 -- Batch 743/ 842, training loss 0.486942857503891\n",
      "Epoch 5 -- Batch 744/ 842, training loss 0.48282527923583984\n",
      "Epoch 5 -- Batch 745/ 842, training loss 0.504598081111908\n",
      "Epoch 5 -- Batch 746/ 842, training loss 0.5145636796951294\n",
      "Epoch 5 -- Batch 747/ 842, training loss 0.4873044788837433\n",
      "Epoch 5 -- Batch 748/ 842, training loss 0.5141580104827881\n",
      "Epoch 5 -- Batch 749/ 842, training loss 0.4833729565143585\n",
      "Epoch 5 -- Batch 750/ 842, training loss 0.5147755146026611\n",
      "Epoch 5 -- Batch 751/ 842, training loss 0.48674672842025757\n",
      "Epoch 5 -- Batch 752/ 842, training loss 0.4972228407859802\n",
      "Epoch 5 -- Batch 753/ 842, training loss 0.494944304227829\n",
      "Epoch 5 -- Batch 754/ 842, training loss 0.4867458641529083\n",
      "Epoch 5 -- Batch 755/ 842, training loss 0.5120083093643188\n",
      "Epoch 5 -- Batch 756/ 842, training loss 0.5133668780326843\n",
      "Epoch 5 -- Batch 757/ 842, training loss 0.512050986289978\n",
      "Epoch 5 -- Batch 758/ 842, training loss 0.5068569183349609\n",
      "Epoch 5 -- Batch 759/ 842, training loss 0.46882084012031555\n",
      "Epoch 5 -- Batch 760/ 842, training loss 0.5076690316200256\n",
      "Epoch 5 -- Batch 761/ 842, training loss 0.4906142055988312\n",
      "Epoch 5 -- Batch 762/ 842, training loss 0.49688830971717834\n",
      "Epoch 5 -- Batch 763/ 842, training loss 0.5037279725074768\n",
      "Epoch 5 -- Batch 764/ 842, training loss 0.4915608763694763\n",
      "Epoch 5 -- Batch 765/ 842, training loss 0.49799516797065735\n",
      "Epoch 5 -- Batch 766/ 842, training loss 0.4753964841365814\n",
      "Epoch 5 -- Batch 767/ 842, training loss 0.47193777561187744\n",
      "Epoch 5 -- Batch 768/ 842, training loss 0.507152795791626\n",
      "Epoch 5 -- Batch 769/ 842, training loss 0.5143218636512756\n",
      "Epoch 5 -- Batch 770/ 842, training loss 0.4908808767795563\n",
      "Epoch 5 -- Batch 771/ 842, training loss 0.4903765022754669\n",
      "Epoch 5 -- Batch 772/ 842, training loss 0.5070608258247375\n",
      "Epoch 5 -- Batch 773/ 842, training loss 0.5019433498382568\n",
      "Epoch 5 -- Batch 774/ 842, training loss 0.5279158353805542\n",
      "Epoch 5 -- Batch 775/ 842, training loss 0.5001307725906372\n",
      "Epoch 5 -- Batch 776/ 842, training loss 0.4865588843822479\n",
      "Epoch 5 -- Batch 777/ 842, training loss 0.5062392950057983\n",
      "Epoch 5 -- Batch 778/ 842, training loss 0.49500739574432373\n",
      "Epoch 5 -- Batch 779/ 842, training loss 0.48720237612724304\n",
      "Epoch 5 -- Batch 780/ 842, training loss 0.5007843375205994\n",
      "Epoch 5 -- Batch 781/ 842, training loss 0.4814870357513428\n",
      "Epoch 5 -- Batch 782/ 842, training loss 0.47672778367996216\n",
      "Epoch 5 -- Batch 783/ 842, training loss 0.4919937551021576\n",
      "Epoch 5 -- Batch 784/ 842, training loss 0.4828251600265503\n",
      "Epoch 5 -- Batch 785/ 842, training loss 0.476126492023468\n",
      "Epoch 5 -- Batch 786/ 842, training loss 0.49506545066833496\n",
      "Epoch 5 -- Batch 787/ 842, training loss 0.48287585377693176\n",
      "Epoch 5 -- Batch 788/ 842, training loss 0.4843350052833557\n",
      "Epoch 5 -- Batch 789/ 842, training loss 0.4968518018722534\n",
      "Epoch 5 -- Batch 790/ 842, training loss 0.48913729190826416\n",
      "Epoch 5 -- Batch 791/ 842, training loss 0.4810647964477539\n",
      "Epoch 5 -- Batch 792/ 842, training loss 0.4977455139160156\n",
      "Epoch 5 -- Batch 793/ 842, training loss 0.5401135087013245\n",
      "Epoch 5 -- Batch 794/ 842, training loss 0.5260842442512512\n",
      "Epoch 5 -- Batch 795/ 842, training loss 0.5062858462333679\n",
      "Epoch 5 -- Batch 796/ 842, training loss 0.502435028553009\n",
      "Epoch 5 -- Batch 797/ 842, training loss 0.5030702352523804\n",
      "Epoch 5 -- Batch 798/ 842, training loss 0.4780488610267639\n",
      "Epoch 5 -- Batch 799/ 842, training loss 0.4834699332714081\n",
      "Epoch 5 -- Batch 800/ 842, training loss 0.4950351417064667\n",
      "Epoch 5 -- Batch 801/ 842, training loss 0.5089508295059204\n",
      "Epoch 5 -- Batch 802/ 842, training loss 0.49480143189430237\n",
      "Epoch 5 -- Batch 803/ 842, training loss 0.48965153098106384\n",
      "Epoch 5 -- Batch 804/ 842, training loss 0.49979057908058167\n",
      "Epoch 5 -- Batch 805/ 842, training loss 0.5077106952667236\n",
      "Epoch 5 -- Batch 806/ 842, training loss 0.4797636866569519\n",
      "Epoch 5 -- Batch 807/ 842, training loss 0.48937126994132996\n",
      "Epoch 5 -- Batch 808/ 842, training loss 0.48655465245246887\n",
      "Epoch 5 -- Batch 809/ 842, training loss 0.4914495050907135\n",
      "Epoch 5 -- Batch 810/ 842, training loss 0.48131316900253296\n",
      "Epoch 5 -- Batch 811/ 842, training loss 0.495638906955719\n",
      "Epoch 5 -- Batch 812/ 842, training loss 0.5092438459396362\n",
      "Epoch 5 -- Batch 813/ 842, training loss 0.49526211619377136\n",
      "Epoch 5 -- Batch 814/ 842, training loss 0.49047034978866577\n",
      "Epoch 5 -- Batch 815/ 842, training loss 0.4925243556499481\n",
      "Epoch 5 -- Batch 816/ 842, training loss 0.481281578540802\n",
      "Epoch 5 -- Batch 817/ 842, training loss 0.5006633996963501\n",
      "Epoch 5 -- Batch 818/ 842, training loss 0.4846288859844208\n",
      "Epoch 5 -- Batch 819/ 842, training loss 0.5016903281211853\n",
      "Epoch 5 -- Batch 820/ 842, training loss 0.5127776265144348\n",
      "Epoch 5 -- Batch 821/ 842, training loss 0.480939656496048\n",
      "Epoch 5 -- Batch 822/ 842, training loss 0.4911150634288788\n",
      "Epoch 5 -- Batch 823/ 842, training loss 0.481013685464859\n",
      "Epoch 5 -- Batch 824/ 842, training loss 0.5018323063850403\n",
      "Epoch 5 -- Batch 825/ 842, training loss 0.5133465528488159\n",
      "Epoch 5 -- Batch 826/ 842, training loss 0.5090076923370361\n",
      "Epoch 5 -- Batch 827/ 842, training loss 0.5096851587295532\n",
      "Epoch 5 -- Batch 828/ 842, training loss 0.5046977996826172\n",
      "Epoch 5 -- Batch 829/ 842, training loss 0.500729501247406\n",
      "Epoch 5 -- Batch 830/ 842, training loss 0.48790842294692993\n",
      "Epoch 5 -- Batch 831/ 842, training loss 0.4976526200771332\n",
      "Epoch 5 -- Batch 832/ 842, training loss 0.49778372049331665\n",
      "Epoch 5 -- Batch 833/ 842, training loss 0.48450976610183716\n",
      "Epoch 5 -- Batch 834/ 842, training loss 0.4940614700317383\n",
      "Epoch 5 -- Batch 835/ 842, training loss 0.5054124593734741\n",
      "Epoch 5 -- Batch 836/ 842, training loss 0.48586660623550415\n",
      "Epoch 5 -- Batch 837/ 842, training loss 0.4873819351196289\n",
      "Epoch 5 -- Batch 838/ 842, training loss 0.46989062428474426\n",
      "Epoch 5 -- Batch 839/ 842, training loss 0.5102469325065613\n",
      "Epoch 5 -- Batch 840/ 842, training loss 0.49605420231819153\n",
      "Epoch 5 -- Batch 841/ 842, training loss 0.5053434371948242\n",
      "Epoch 5 -- Batch 842/ 842, training loss 0.5182144641876221\n",
      "----------------------------------------------------------------------\n",
      "Epoch 5 -- Batch 1/ 94, validation loss 0.5078213214874268\n",
      "Epoch 5 -- Batch 2/ 94, validation loss 0.49938952922821045\n",
      "Epoch 5 -- Batch 3/ 94, validation loss 0.49006351828575134\n",
      "Epoch 5 -- Batch 4/ 94, validation loss 0.49403151869773865\n",
      "Epoch 5 -- Batch 5/ 94, validation loss 0.48069849610328674\n",
      "Epoch 5 -- Batch 6/ 94, validation loss 0.4802933633327484\n",
      "Epoch 5 -- Batch 7/ 94, validation loss 0.5096365213394165\n",
      "Epoch 5 -- Batch 8/ 94, validation loss 0.5014401078224182\n",
      "Epoch 5 -- Batch 9/ 94, validation loss 0.4669025242328644\n",
      "Epoch 5 -- Batch 10/ 94, validation loss 0.48804324865341187\n",
      "Epoch 5 -- Batch 11/ 94, validation loss 0.4910826086997986\n",
      "Epoch 5 -- Batch 12/ 94, validation loss 0.5151686072349548\n",
      "Epoch 5 -- Batch 13/ 94, validation loss 0.5201452374458313\n",
      "Epoch 5 -- Batch 14/ 94, validation loss 0.47604310512542725\n",
      "Epoch 5 -- Batch 15/ 94, validation loss 0.49596843123435974\n",
      "Epoch 5 -- Batch 16/ 94, validation loss 0.4917912185192108\n",
      "Epoch 5 -- Batch 17/ 94, validation loss 0.48250311613082886\n",
      "Epoch 5 -- Batch 18/ 94, validation loss 0.48999640345573425\n",
      "Epoch 5 -- Batch 19/ 94, validation loss 0.493171364068985\n",
      "Epoch 5 -- Batch 20/ 94, validation loss 0.4712109863758087\n",
      "Epoch 5 -- Batch 21/ 94, validation loss 0.4650726616382599\n",
      "Epoch 5 -- Batch 22/ 94, validation loss 0.48695001006126404\n",
      "Epoch 5 -- Batch 23/ 94, validation loss 0.5123984217643738\n",
      "Epoch 5 -- Batch 24/ 94, validation loss 0.47661951184272766\n",
      "Epoch 5 -- Batch 25/ 94, validation loss 0.4836220443248749\n",
      "Epoch 5 -- Batch 26/ 94, validation loss 0.4956885278224945\n",
      "Epoch 5 -- Batch 27/ 94, validation loss 0.4683014452457428\n",
      "Epoch 5 -- Batch 28/ 94, validation loss 0.48388129472732544\n",
      "Epoch 5 -- Batch 29/ 94, validation loss 0.48061224818229675\n",
      "Epoch 5 -- Batch 30/ 94, validation loss 0.4675295948982239\n",
      "Epoch 5 -- Batch 31/ 94, validation loss 0.4628027081489563\n",
      "Epoch 5 -- Batch 32/ 94, validation loss 0.4910741150379181\n",
      "Epoch 5 -- Batch 33/ 94, validation loss 0.5098249912261963\n",
      "Epoch 5 -- Batch 34/ 94, validation loss 0.47466522455215454\n",
      "Epoch 5 -- Batch 35/ 94, validation loss 0.4937761425971985\n",
      "Epoch 5 -- Batch 36/ 94, validation loss 0.5000941157341003\n",
      "Epoch 5 -- Batch 37/ 94, validation loss 0.5157892107963562\n",
      "Epoch 5 -- Batch 38/ 94, validation loss 0.48040974140167236\n",
      "Epoch 5 -- Batch 39/ 94, validation loss 0.46403807401657104\n",
      "Epoch 5 -- Batch 40/ 94, validation loss 0.4907186031341553\n",
      "Epoch 5 -- Batch 41/ 94, validation loss 0.48799917101860046\n",
      "Epoch 5 -- Batch 42/ 94, validation loss 0.4864726662635803\n",
      "Epoch 5 -- Batch 43/ 94, validation loss 0.488674134016037\n",
      "Epoch 5 -- Batch 44/ 94, validation loss 0.5132279396057129\n",
      "Epoch 5 -- Batch 45/ 94, validation loss 0.4911380708217621\n",
      "Epoch 5 -- Batch 46/ 94, validation loss 0.5016206502914429\n",
      "Epoch 5 -- Batch 47/ 94, validation loss 0.5050378441810608\n",
      "Epoch 5 -- Batch 48/ 94, validation loss 0.48072895407676697\n",
      "Epoch 5 -- Batch 49/ 94, validation loss 0.4670479893684387\n",
      "Epoch 5 -- Batch 50/ 94, validation loss 0.4875236451625824\n",
      "Epoch 5 -- Batch 51/ 94, validation loss 0.48205554485321045\n",
      "Epoch 5 -- Batch 52/ 94, validation loss 0.45690667629241943\n",
      "Epoch 5 -- Batch 53/ 94, validation loss 0.5012132525444031\n",
      "Epoch 5 -- Batch 54/ 94, validation loss 0.474051296710968\n",
      "Epoch 5 -- Batch 55/ 94, validation loss 0.4867536723613739\n",
      "Epoch 5 -- Batch 56/ 94, validation loss 0.5019592642784119\n",
      "Epoch 5 -- Batch 57/ 94, validation loss 0.4745233654975891\n",
      "Epoch 5 -- Batch 58/ 94, validation loss 0.4836496114730835\n",
      "Epoch 5 -- Batch 59/ 94, validation loss 0.4997209310531616\n",
      "Epoch 5 -- Batch 60/ 94, validation loss 0.4773343801498413\n",
      "Epoch 5 -- Batch 61/ 94, validation loss 0.47623780369758606\n",
      "Epoch 5 -- Batch 62/ 94, validation loss 0.4767689108848572\n",
      "Epoch 5 -- Batch 63/ 94, validation loss 0.4995385408401489\n",
      "Epoch 5 -- Batch 64/ 94, validation loss 0.5073655247688293\n",
      "Epoch 5 -- Batch 65/ 94, validation loss 0.49115845561027527\n",
      "Epoch 5 -- Batch 66/ 94, validation loss 0.4721210300922394\n",
      "Epoch 5 -- Batch 67/ 94, validation loss 0.49571460485458374\n",
      "Epoch 5 -- Batch 68/ 94, validation loss 0.49824896454811096\n",
      "Epoch 5 -- Batch 69/ 94, validation loss 0.49116888642311096\n",
      "Epoch 5 -- Batch 70/ 94, validation loss 0.485150009393692\n",
      "Epoch 5 -- Batch 71/ 94, validation loss 0.4800448715686798\n",
      "Epoch 5 -- Batch 72/ 94, validation loss 0.4881553649902344\n",
      "Epoch 5 -- Batch 73/ 94, validation loss 0.48430946469306946\n",
      "Epoch 5 -- Batch 74/ 94, validation loss 0.49018874764442444\n",
      "Epoch 5 -- Batch 75/ 94, validation loss 0.48128068447113037\n",
      "Epoch 5 -- Batch 76/ 94, validation loss 0.5052194595336914\n",
      "Epoch 5 -- Batch 77/ 94, validation loss 0.48923465609550476\n",
      "Epoch 5 -- Batch 78/ 94, validation loss 0.48836272954940796\n",
      "Epoch 5 -- Batch 79/ 94, validation loss 0.4727766811847687\n",
      "Epoch 5 -- Batch 80/ 94, validation loss 0.48807492852211\n",
      "Epoch 5 -- Batch 81/ 94, validation loss 0.5163681507110596\n",
      "Epoch 5 -- Batch 82/ 94, validation loss 0.45208826661109924\n",
      "Epoch 5 -- Batch 83/ 94, validation loss 0.4842784106731415\n",
      "Epoch 5 -- Batch 84/ 94, validation loss 0.47366949915885925\n",
      "Epoch 5 -- Batch 85/ 94, validation loss 0.48106300830841064\n",
      "Epoch 5 -- Batch 86/ 94, validation loss 0.48948243260383606\n",
      "Epoch 5 -- Batch 87/ 94, validation loss 0.5090784430503845\n",
      "Epoch 5 -- Batch 88/ 94, validation loss 0.4985324442386627\n",
      "Epoch 5 -- Batch 89/ 94, validation loss 0.47308799624443054\n",
      "Epoch 5 -- Batch 90/ 94, validation loss 0.4821912348270416\n",
      "Epoch 5 -- Batch 91/ 94, validation loss 0.4755846858024597\n",
      "Epoch 5 -- Batch 92/ 94, validation loss 0.4734324514865875\n",
      "Epoch 5 -- Batch 93/ 94, validation loss 0.5070784091949463\n",
      "Epoch 5 -- Batch 94/ 94, validation loss 0.5096958875656128\n",
      "----------------------------------------------------------------------\n",
      "Epoch 5 loss: Training 0.5036746859550476, Validation 0.5096958875656128\n",
      "----------------------------------------------------------------------\n",
      "Epoch 6/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:04:36] SMILES Parse Error: syntax error while parsing: CC((O)(c1ccccc1)c1ccccc1)C(=O)C(F)(F)F\n",
      "[19:04:36] SMILES Parse Error: Failed parsing SMILES 'CC((O)(c1ccccc1)c1ccccc1)C(=O)C(F)(F)F' for input: 'CC((O)(c1ccccc1)c1ccccc1)C(=O)C(F)(F)F'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 17\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)c(/C=N/NC(=O)CNC(=O)C23CC4CC(CC(C4)C2)C4)c1'\n",
      "[19:04:36] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(/C=c2\\sc3n(c2=O)NC2c2ccccc2-c2ccccc21'\n",
      "[19:04:36] SMILES Parse Error: ring closure 2 duplicates bond between atom 20 and atom 21 for input: 'NC(=O)c1ccc(COc2ccc3c(C2CCO4)noc2c2)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 10 11 12 14 18\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-c1noc(C2CCC3)n1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 22 23\n",
      "[19:04:36] SMILES Parse Error: extra open parentheses for input: 'COc1ccccc1-n1nnnc1SCCN(C'\n",
      "[19:04:36] SMILES Parse Error: extra close parentheses while parsing: CCC1CC2CCCN2C(=O)CCN1C(=O)OCC)CCc1ccccc1\n",
      "[19:04:36] SMILES Parse Error: Failed parsing SMILES 'CCC1CC2CCCN2C(=O)CCN1C(=O)OCC)CCc1ccccc1' for input: 'CCC1CC2CCCN2C(=O)CCN1C(=O)OCC)CCc1ccccc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 1 2 3 23 24 25 26 27 28\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cn1cnnc1SCc1cccc(C(=O)N1CCOCC2)o1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COCC(=O)Nc1nn(-c2ccc(C)cc2)c2c1C1(OC(O))CCN(CC3CC1)C2'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Cc2cc(C(=O)N3C[C@@H]4[C@@H](c3ccccc34)N(C)C)c3ccccc32)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 25 26 27\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COC(=O)CNc1ccc([C@@H]2N[C@@](CC(=O)O)C(=O)C2CCCCO3)cc1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'O=[N+]1[O-]c(cc1sccc1OC(F)(F)F)N1CCN(c2nc3ccccc3s2)CC1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 19 20 21 35\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCCCc1nnc(NC(=O)C2CCC3)s1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1cn(Nc2ccccc2)nc1-c1cc2c(cc1Cl)OCO3'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)Cn1ncc2c3c(-cccc3nc(NCCOC)ncn33)C1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1CCCCN1S(=O)(=O)c1ccc2c(c1)C(=O)N(c1ccccc1)N=C3C'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1(C)O[C@@]2c3c(cc4c(=O)oc4nc3n2)CCCC4=O1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Clc1cccc2sc(CCN3CCCC3c3cccc4cccnc33)n1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 14 15 25 26 27 28 29 30 31\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 10 12\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COCC(=O)Nc1ccc2c(c1)OC[C@H](C)N(Cc1cccnc1)C[C@H](C)[C@H](OO)c(OF)cc15'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)N1CCN(Cc2cccc(Oc3cc(N)nc(NC5CC[C@H]4CC[C@@H]4C)n3)c2)CC1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(C(=O)c1ccccc1)n1nc2c(c1=O)C(c1cccnc1)=NN1CCC2'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 16\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1nc(/C=C/c2nnc3c4CCCCC4)sc2nc1-c1ccccc1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1noc2c1C(c1cccc(Br)c1)C1(C#N)CC(C)(C)CCC1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'O=c1nc(CSc2ccccc2)[nH]c2c1C1CCCC2'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 10 11 12 20 24\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 28\n",
      "[19:04:36] SMILES Parse Error: extra open parentheses for input: 'CCOc1ccc(-n2c(OC)nc(C(C)(C(=O)NCCCC3CCC(C)CC3)n3cc(Cl)ccc32)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7 8 10 11 12\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 4 5 9 10 11 12 13\n",
      "[19:04:36] SMILES Parse Error: ring closure 3 duplicates bond between atom 24 and atom 26 for input: 'CCC1CCC2C3CCC4=CC(=O)CC=C5CCC[C@]4(C)C3CC[C@]3(C)C3CCC1C2'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 9 10 12 13 20 21 22 23 24 25 26\n",
      "[19:04:36] Explicit valence for atom # 10 C, 5, is greater than permitted\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 8 9 13 14 18\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1CCN(C(=O)c2cnn(C3CC(=O)N(c4ccccc4)c4ccccc4)c2O)CC1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 13\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(CO)CNC(=O)[C@@]12CCCCN1C(=O)Nc1cccc(F)c1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 33\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccncc1)N1CCN(Cc2ccc3c(c2)CN(Cc2ccccc2Cl)C(=O)C3CCO2)CC1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)Nc1ccc2c(c1)CC(=O)NC(=O)[C@@H](C(C)C)N2CCCCC1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 1 2 10 13 14 15 16 17 18 30\n",
      "[19:04:36] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2cc(C3C=C(c4ccc(Br)cc4)N4N=C(CPC)o4)SC3[C@@H]3CCCN22)c1\n",
      "[19:04:36] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2cc(C3C=C(c4ccc(Br)cc4)N4N=C(CPC)o4)SC3[C@@H]3CCCN22)c1' for input: 'COc1ccc2cc(C3C=C(c4ccc(Br)cc4)N4N=C(CPC)o4)SC3[C@@H]3CCCN22)c1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)CSc2nc3c(c(=O)n2Cc2ccccc2)c(=O)n(CC)c2=O)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 6 8 9 10 17\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1CC(C)CN(C(=O)Cn2cc3c(cnn2-c3ccccc3)c2=O)C1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COCCOC(=O)N(CCCOc1ccc(Cl)cc1)CC(=O)N1CCn2c1cccc12'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 0 1 2 12 13\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1=C(C(=O)O)C2(O)CC(C)(C)CC2C1[N+]1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(N2C(=O)CC(N3CCC3(CCC(=O)O)C3)C2=O)cc1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1cccc(CC2(O)CCN(C(=O)Nc3cc3c(c3)NC(=O)C(C)C4)CC2)c1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'N#Cc1ccc(-c2ccc(/C=C3\\Sc4nnnn5C4=C3COCCO4)o2)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 16 17\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'O=C1[C@H]2CCN(Cc3ccc(F)cc3)C2(CNCCc22C(=O)O)COC3=C1CN(CC(=O)NCc2ccco2)C1'\n",
      "[19:04:36] SMILES Parse Error: extra open parentheses for input: 'CCCCOc1ccc(-c2nc3s/c(=C/c3ccccc3Br)c(C)n2n1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1cccn2c(=O)c3c[n+](C#N)c5ccccc5cc3c3c12'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1cn2c(n1)c(=O)n(CC(=O)Nc1ccccc1)n1C'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCC(=O)Nc1ccc2c(c1)[C@@]1ccnc2n(CCC)c12'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)CSc2nnc3s(C)c4c(c23)c2ccc4ccccc4n2n3C)cc1'\n",
      "[19:04:36] SMILES Parse Error: syntax error while parsing: Cc1ccc(N2N/C(=N\\S(C)==O)c3ccc(OC(C)=O)cc3c2=O)cc1\n",
      "[19:04:36] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(N2N/C(=N\\S(C)==O)c3ccc(OC(C)=O)cc3c2=O)cc1' for input: 'Cc1ccc(N2N/C(=N\\S(C)==O)c3ccc(OC(C)=O)cc3c2=O)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCOc1cc(NC2OC(=O)c3c2cnc3ccccc3c2O)cc(OC)c1OC'\n",
      "[19:04:36] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 24\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCc1cccc2cc(C(=O)NCC3N4CCN(Cc4ccco4)CC3)c(=O)nc2c1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10 11 12\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(C)CC#Cc1cc2c(c1)C(=O)N(c1cc(F)ccc1F)CC3'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1ccco1)c1c(O)c2cccc3c2oc1c1CCCCC3'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'NC(=O)CSc1nc2c(c(=O)n1CCc1ccccc1)C(F)(F)F'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1cccc(CNC(=O)c2ccc3c(c2)ncn2C)c1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'NN(C(=O)c1cccc(N3C(=O)C4CCC(C4)C3C2=O)c1)c1nc2ccccc2s1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2Nc3ccccc3C3=[N+])C2c2ccccc2F'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1ccc(OCC(O)Cn2c3ccccc3c4ccccc4c32)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 8 14 15\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1cc2c(c3ccccc3c1SCCN3C(=O)CCC(=O)NCCCO)O2'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1cc2cccc3cccc2c1Br'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)C2CCCN(c3nc4ccccc4nc3C2CC2)c2ccccc2)c1OC'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 9 14 15 16 17 18 19\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10\n",
      "[19:04:36] Explicit valence for atom # 10 O, 3, is greater than permitted\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 6 7 11 20\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)[C@@]12CCCC=OC1(C)C[C@@]3(C)[C@H](OC(C)=O)C3SC1O3'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCN(CC)S(=O)(=O)c1ccc(C(=O)NC(C)c2nn2nc(C)cc2C)cc1'\n",
      "[19:04:36] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(-n2nc3n(c2=O)CCCCC3'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 15\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 16\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 19\n",
      "[19:04:36] Explicit valence for atom # 2 Br, 3, is greater than permitted\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 1 2 3 11 12 14 16\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2ccc(C3c4ccccc4NC3CC(=O)N(c4ccc5c(c4)OCO5)CC3)nc2c1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 18 20 22\n",
      "[19:04:36] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1OCc1nnc(SCc2cc(=O)c(OC)c(OC)c3)o2)n1\n",
      "[19:04:36] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1OCc1nnc(SCc2cc(=O)c(OC)c(OC)c3)o2)n1' for input: 'COc1ccccc1OCc1nnc(SCc2cc(=O)c(OC)c(OC)c3)o2)n1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 8 9 10 12 20 21 22 23 26\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 7\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 4 6 10 17 18 19 20 21 22\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 5 6 21 22 24 25 26\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)[C@H]2[C@@H]3CNCCN3C[C@@H]2[C@@H](c3ccc(-c4ccccc4)cc3)[C@@H]2C1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 4 5 16\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1sc2ncnc(-n3c(SCC#N)nc4cccnc33)c2c1C'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 25 26 27\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 18\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 12 14 15\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 14 15 16 17 18 19\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 3 4 13 14 16 17 18\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 23\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 7 8 9 13 24\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])c1c(Sc2ccc(F)cc2)c2cc3c(cc2O)c1ccccn12'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1C2NC(=O)NC2=C1C(=O)CC(C)(C)C2'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNC(=O)Cc2c3c(nc4ccccc3n3Cl)CCCC3)cc1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 13 14 15 17 18 21\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 17 18 25\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1N(CC(=O)Nc1ccc([N+](=O)[O-])cc1)c1nc2nc3cccc(-c4ccsc4)c(C)n2n1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC1(C)C2C=NC3C(C#N)=C(N)C(C#N)(C#N)[C@@H](c4ccc6c(c4)OCO5)[C@H]3OC12'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1[C@H](CO)[C@@H]/c1ccc(C#C[C@@H](OC)O)cc1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(c1cccc2c1CCOCCOC(=O)N2CCCC(C(=O)O)C1)N1CCCC1'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)N2Cc3cc(F)ccc3C2C(c2cccnc2)N2)cc1OC'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 8 9 16 17 21\n",
      "[19:04:36] SMILES Parse Error: extra close parentheses while parsing: CC(C)C[C@H]1NC(=O)[C@H]2C[C@H](NC(=O)/C=C/c3ccccc3)NC2=O)C1CN(C)C\n",
      "[19:04:36] SMILES Parse Error: Failed parsing SMILES 'CC(C)C[C@H]1NC(=O)[C@H]2C[C@H](NC(=O)/C=C/c3ccccc3)NC2=O)C1CN(C)C' for input: 'CC(C)C[C@H]1NC(=O)[C@H]2C[C@H](NC(=O)/C=C/c3ccccc3)NC2=O)C1CN(C)C'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCNC(=O)N1CC(=O)NC(Cc2c[nH]c3cc4c(cc23)OCO3)C1=O'\n",
      "[19:04:36] SMILES Parse Error: duplicated ring closure 2 bonds atom 33 to itself for input: 'CC1CCc2c(sc(NC(=O)c3ccc4[nH]c(=O)oc4c3)cc2C(=O)N2CCc2ccccc22)C1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 6 7 9\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)N1CC2(CN(Cc3ccccc3)C2)c2c(c(N4C)[nH]c2=O)C1=O'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CCc1[nH]c2ccccc2c1N(C1CCCC1)S(=O)C2'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'FC(F)(F)c1cc2nn(Cc3ccccc3)c(=O)c3ccccc23'\n",
      "[19:04:36] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)NC(=O)c1cccc(N2C(=O)C3C4C=CC(C5)C4C2=O)c1'\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 1 3 4 5 6 7 9 10 11 12 13 14 15 17 19\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 12 13 14 17 18\n",
      "[19:04:36] Can't kekulize mol.  Unkekulized atoms: 17 19 22 23 24 25 26 27 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 -- Batch 1/ 842, training loss 0.492296427488327\n",
      "Epoch 6 -- Batch 2/ 842, training loss 0.47159305214881897\n",
      "Epoch 6 -- Batch 3/ 842, training loss 0.47161224484443665\n",
      "Epoch 6 -- Batch 4/ 842, training loss 0.5132691264152527\n",
      "Epoch 6 -- Batch 5/ 842, training loss 0.49560967087745667\n",
      "Epoch 6 -- Batch 6/ 842, training loss 0.4806448221206665\n",
      "Epoch 6 -- Batch 7/ 842, training loss 0.4796527028083801\n",
      "Epoch 6 -- Batch 8/ 842, training loss 0.4840667247772217\n",
      "Epoch 6 -- Batch 9/ 842, training loss 0.47847262024879456\n",
      "Epoch 6 -- Batch 10/ 842, training loss 0.48538416624069214\n",
      "Epoch 6 -- Batch 11/ 842, training loss 0.4878973066806793\n",
      "Epoch 6 -- Batch 12/ 842, training loss 0.49798741936683655\n",
      "Epoch 6 -- Batch 13/ 842, training loss 0.4823172986507416\n",
      "Epoch 6 -- Batch 14/ 842, training loss 0.49365442991256714\n",
      "Epoch 6 -- Batch 15/ 842, training loss 0.4934558868408203\n",
      "Epoch 6 -- Batch 16/ 842, training loss 0.4941138029098511\n",
      "Epoch 6 -- Batch 17/ 842, training loss 0.4852067828178406\n",
      "Epoch 6 -- Batch 18/ 842, training loss 0.5070772171020508\n",
      "Epoch 6 -- Batch 19/ 842, training loss 0.5085316300392151\n",
      "Epoch 6 -- Batch 20/ 842, training loss 0.49554136395454407\n",
      "Epoch 6 -- Batch 21/ 842, training loss 0.48934119939804077\n",
      "Epoch 6 -- Batch 22/ 842, training loss 0.4842003881931305\n",
      "Epoch 6 -- Batch 23/ 842, training loss 0.4813216030597687\n",
      "Epoch 6 -- Batch 24/ 842, training loss 0.4731026291847229\n",
      "Epoch 6 -- Batch 25/ 842, training loss 0.47791823744773865\n",
      "Epoch 6 -- Batch 26/ 842, training loss 0.4948549270629883\n",
      "Epoch 6 -- Batch 27/ 842, training loss 0.4851835370063782\n",
      "Epoch 6 -- Batch 28/ 842, training loss 0.4925348162651062\n",
      "Epoch 6 -- Batch 29/ 842, training loss 0.4695391058921814\n",
      "Epoch 6 -- Batch 30/ 842, training loss 0.4971120059490204\n",
      "Epoch 6 -- Batch 31/ 842, training loss 0.47294461727142334\n",
      "Epoch 6 -- Batch 32/ 842, training loss 0.4774894118309021\n",
      "Epoch 6 -- Batch 33/ 842, training loss 0.47875821590423584\n",
      "Epoch 6 -- Batch 34/ 842, training loss 0.4983448088169098\n",
      "Epoch 6 -- Batch 35/ 842, training loss 0.47420066595077515\n",
      "Epoch 6 -- Batch 36/ 842, training loss 0.47448357939720154\n",
      "Epoch 6 -- Batch 37/ 842, training loss 0.4925558865070343\n",
      "Epoch 6 -- Batch 38/ 842, training loss 0.49467357993125916\n",
      "Epoch 6 -- Batch 39/ 842, training loss 0.5068988800048828\n",
      "Epoch 6 -- Batch 40/ 842, training loss 0.4731324315071106\n",
      "Epoch 6 -- Batch 41/ 842, training loss 0.47912219166755676\n",
      "Epoch 6 -- Batch 42/ 842, training loss 0.4989016652107239\n",
      "Epoch 6 -- Batch 43/ 842, training loss 0.48298341035842896\n",
      "Epoch 6 -- Batch 44/ 842, training loss 0.4744478464126587\n",
      "Epoch 6 -- Batch 45/ 842, training loss 0.4797796905040741\n",
      "Epoch 6 -- Batch 46/ 842, training loss 0.4995726943016052\n",
      "Epoch 6 -- Batch 47/ 842, training loss 0.4740784466266632\n",
      "Epoch 6 -- Batch 48/ 842, training loss 0.5038062334060669\n",
      "Epoch 6 -- Batch 49/ 842, training loss 0.45346799492836\n",
      "Epoch 6 -- Batch 50/ 842, training loss 0.4767148792743683\n",
      "Epoch 6 -- Batch 51/ 842, training loss 0.4875560700893402\n",
      "Epoch 6 -- Batch 52/ 842, training loss 0.48728325963020325\n",
      "Epoch 6 -- Batch 53/ 842, training loss 0.46133366227149963\n",
      "Epoch 6 -- Batch 54/ 842, training loss 0.48345667123794556\n",
      "Epoch 6 -- Batch 55/ 842, training loss 0.4567352831363678\n",
      "Epoch 6 -- Batch 56/ 842, training loss 0.46101686358451843\n",
      "Epoch 6 -- Batch 57/ 842, training loss 0.46678435802459717\n",
      "Epoch 6 -- Batch 58/ 842, training loss 0.5055887699127197\n",
      "Epoch 6 -- Batch 59/ 842, training loss 0.48930224776268005\n",
      "Epoch 6 -- Batch 60/ 842, training loss 0.4886018931865692\n",
      "Epoch 6 -- Batch 61/ 842, training loss 0.4761735796928406\n",
      "Epoch 6 -- Batch 62/ 842, training loss 0.4826803505420685\n",
      "Epoch 6 -- Batch 63/ 842, training loss 0.4848693311214447\n",
      "Epoch 6 -- Batch 64/ 842, training loss 0.5070083737373352\n",
      "Epoch 6 -- Batch 65/ 842, training loss 0.502162754535675\n",
      "Epoch 6 -- Batch 66/ 842, training loss 0.4784325659275055\n",
      "Epoch 6 -- Batch 67/ 842, training loss 0.4719468653202057\n",
      "Epoch 6 -- Batch 68/ 842, training loss 0.46693915128707886\n",
      "Epoch 6 -- Batch 69/ 842, training loss 0.48526597023010254\n",
      "Epoch 6 -- Batch 70/ 842, training loss 0.4879591464996338\n",
      "Epoch 6 -- Batch 71/ 842, training loss 0.491568386554718\n",
      "Epoch 6 -- Batch 72/ 842, training loss 0.47800660133361816\n",
      "Epoch 6 -- Batch 73/ 842, training loss 0.4796309769153595\n",
      "Epoch 6 -- Batch 74/ 842, training loss 0.4871596693992615\n",
      "Epoch 6 -- Batch 75/ 842, training loss 0.48319166898727417\n",
      "Epoch 6 -- Batch 76/ 842, training loss 0.4975007474422455\n",
      "Epoch 6 -- Batch 77/ 842, training loss 0.47461479902267456\n",
      "Epoch 6 -- Batch 78/ 842, training loss 0.4699813723564148\n",
      "Epoch 6 -- Batch 79/ 842, training loss 0.47680431604385376\n",
      "Epoch 6 -- Batch 80/ 842, training loss 0.48873186111450195\n",
      "Epoch 6 -- Batch 81/ 842, training loss 0.4873521327972412\n",
      "Epoch 6 -- Batch 82/ 842, training loss 0.5027165412902832\n",
      "Epoch 6 -- Batch 83/ 842, training loss 0.47281280159950256\n",
      "Epoch 6 -- Batch 84/ 842, training loss 0.4905775487422943\n",
      "Epoch 6 -- Batch 85/ 842, training loss 0.4917801022529602\n",
      "Epoch 6 -- Batch 86/ 842, training loss 0.4857524335384369\n",
      "Epoch 6 -- Batch 87/ 842, training loss 0.4741937816143036\n",
      "Epoch 6 -- Batch 88/ 842, training loss 0.4737470746040344\n",
      "Epoch 6 -- Batch 89/ 842, training loss 0.4531300365924835\n",
      "Epoch 6 -- Batch 90/ 842, training loss 0.47825178503990173\n",
      "Epoch 6 -- Batch 91/ 842, training loss 0.4974202513694763\n",
      "Epoch 6 -- Batch 92/ 842, training loss 0.49366751313209534\n",
      "Epoch 6 -- Batch 93/ 842, training loss 0.4733114242553711\n",
      "Epoch 6 -- Batch 94/ 842, training loss 0.48141729831695557\n",
      "Epoch 6 -- Batch 95/ 842, training loss 0.48455697298049927\n",
      "Epoch 6 -- Batch 96/ 842, training loss 0.4702368676662445\n",
      "Epoch 6 -- Batch 97/ 842, training loss 0.4789516031742096\n",
      "Epoch 6 -- Batch 98/ 842, training loss 0.4712587594985962\n",
      "Epoch 6 -- Batch 99/ 842, training loss 0.4916568100452423\n",
      "Epoch 6 -- Batch 100/ 842, training loss 0.46323299407958984\n",
      "Epoch 6 -- Batch 101/ 842, training loss 0.47876080870628357\n",
      "Epoch 6 -- Batch 102/ 842, training loss 0.48208367824554443\n",
      "Epoch 6 -- Batch 103/ 842, training loss 0.48396408557891846\n",
      "Epoch 6 -- Batch 104/ 842, training loss 0.4902450442314148\n",
      "Epoch 6 -- Batch 105/ 842, training loss 0.47924456000328064\n",
      "Epoch 6 -- Batch 106/ 842, training loss 0.496837854385376\n",
      "Epoch 6 -- Batch 107/ 842, training loss 0.47667399048805237\n",
      "Epoch 6 -- Batch 108/ 842, training loss 0.48972439765930176\n",
      "Epoch 6 -- Batch 109/ 842, training loss 0.5037661194801331\n",
      "Epoch 6 -- Batch 110/ 842, training loss 0.4741707146167755\n",
      "Epoch 6 -- Batch 111/ 842, training loss 0.4754785895347595\n",
      "Epoch 6 -- Batch 112/ 842, training loss 0.48627859354019165\n",
      "Epoch 6 -- Batch 113/ 842, training loss 0.47178924083709717\n",
      "Epoch 6 -- Batch 114/ 842, training loss 0.4711942970752716\n",
      "Epoch 6 -- Batch 115/ 842, training loss 0.4864906668663025\n",
      "Epoch 6 -- Batch 116/ 842, training loss 0.47426217794418335\n",
      "Epoch 6 -- Batch 117/ 842, training loss 0.47802960872650146\n",
      "Epoch 6 -- Batch 118/ 842, training loss 0.519025981426239\n",
      "Epoch 6 -- Batch 119/ 842, training loss 0.4871455430984497\n",
      "Epoch 6 -- Batch 120/ 842, training loss 0.4530996084213257\n",
      "Epoch 6 -- Batch 121/ 842, training loss 0.489105761051178\n",
      "Epoch 6 -- Batch 122/ 842, training loss 0.48138532042503357\n",
      "Epoch 6 -- Batch 123/ 842, training loss 0.48254233598709106\n",
      "Epoch 6 -- Batch 124/ 842, training loss 0.4805349111557007\n",
      "Epoch 6 -- Batch 125/ 842, training loss 0.4780923128128052\n",
      "Epoch 6 -- Batch 126/ 842, training loss 0.4698145389556885\n",
      "Epoch 6 -- Batch 127/ 842, training loss 0.4820428192615509\n",
      "Epoch 6 -- Batch 128/ 842, training loss 0.4830629825592041\n",
      "Epoch 6 -- Batch 129/ 842, training loss 0.4871949553489685\n",
      "Epoch 6 -- Batch 130/ 842, training loss 0.49258551001548767\n",
      "Epoch 6 -- Batch 131/ 842, training loss 0.47425344586372375\n",
      "Epoch 6 -- Batch 132/ 842, training loss 0.4634571969509125\n",
      "Epoch 6 -- Batch 133/ 842, training loss 0.4729144275188446\n",
      "Epoch 6 -- Batch 134/ 842, training loss 0.47167831659317017\n",
      "Epoch 6 -- Batch 135/ 842, training loss 0.46139052510261536\n",
      "Epoch 6 -- Batch 136/ 842, training loss 0.49663758277893066\n",
      "Epoch 6 -- Batch 137/ 842, training loss 0.4898878037929535\n",
      "Epoch 6 -- Batch 138/ 842, training loss 0.49026843905448914\n",
      "Epoch 6 -- Batch 139/ 842, training loss 0.4679214656352997\n",
      "Epoch 6 -- Batch 140/ 842, training loss 0.48530399799346924\n",
      "Epoch 6 -- Batch 141/ 842, training loss 0.46830612421035767\n",
      "Epoch 6 -- Batch 142/ 842, training loss 0.4604949355125427\n",
      "Epoch 6 -- Batch 143/ 842, training loss 0.4871986508369446\n",
      "Epoch 6 -- Batch 144/ 842, training loss 0.4822885990142822\n",
      "Epoch 6 -- Batch 145/ 842, training loss 0.48057714104652405\n",
      "Epoch 6 -- Batch 146/ 842, training loss 0.4763326644897461\n",
      "Epoch 6 -- Batch 147/ 842, training loss 0.4735516309738159\n",
      "Epoch 6 -- Batch 148/ 842, training loss 0.4784873127937317\n",
      "Epoch 6 -- Batch 149/ 842, training loss 0.47844284772872925\n",
      "Epoch 6 -- Batch 150/ 842, training loss 0.4707978665828705\n",
      "Epoch 6 -- Batch 151/ 842, training loss 0.5053098797798157\n",
      "Epoch 6 -- Batch 152/ 842, training loss 0.4852326810359955\n",
      "Epoch 6 -- Batch 153/ 842, training loss 0.47124147415161133\n",
      "Epoch 6 -- Batch 154/ 842, training loss 0.4911099970340729\n",
      "Epoch 6 -- Batch 155/ 842, training loss 0.4712531864643097\n",
      "Epoch 6 -- Batch 156/ 842, training loss 0.4888986349105835\n",
      "Epoch 6 -- Batch 157/ 842, training loss 0.5017151832580566\n",
      "Epoch 6 -- Batch 158/ 842, training loss 0.46995243430137634\n",
      "Epoch 6 -- Batch 159/ 842, training loss 0.4881022274494171\n",
      "Epoch 6 -- Batch 160/ 842, training loss 0.5134396553039551\n",
      "Epoch 6 -- Batch 161/ 842, training loss 0.456729531288147\n",
      "Epoch 6 -- Batch 162/ 842, training loss 0.48247474431991577\n",
      "Epoch 6 -- Batch 163/ 842, training loss 0.48160088062286377\n",
      "Epoch 6 -- Batch 164/ 842, training loss 0.47548675537109375\n",
      "Epoch 6 -- Batch 165/ 842, training loss 0.48287999629974365\n",
      "Epoch 6 -- Batch 166/ 842, training loss 0.46647128462791443\n",
      "Epoch 6 -- Batch 167/ 842, training loss 0.4836289882659912\n",
      "Epoch 6 -- Batch 168/ 842, training loss 0.4818551540374756\n",
      "Epoch 6 -- Batch 169/ 842, training loss 0.4867500960826874\n",
      "Epoch 6 -- Batch 170/ 842, training loss 0.4775908589363098\n",
      "Epoch 6 -- Batch 171/ 842, training loss 0.49049171805381775\n",
      "Epoch 6 -- Batch 172/ 842, training loss 0.4789341688156128\n",
      "Epoch 6 -- Batch 173/ 842, training loss 0.4922616183757782\n",
      "Epoch 6 -- Batch 174/ 842, training loss 0.48352354764938354\n",
      "Epoch 6 -- Batch 175/ 842, training loss 0.4652937948703766\n",
      "Epoch 6 -- Batch 176/ 842, training loss 0.5048941373825073\n",
      "Epoch 6 -- Batch 177/ 842, training loss 0.46300721168518066\n",
      "Epoch 6 -- Batch 178/ 842, training loss 0.47829678654670715\n",
      "Epoch 6 -- Batch 179/ 842, training loss 0.4971376657485962\n",
      "Epoch 6 -- Batch 180/ 842, training loss 0.4654332995414734\n",
      "Epoch 6 -- Batch 181/ 842, training loss 0.481052428483963\n",
      "Epoch 6 -- Batch 182/ 842, training loss 0.4899842441082001\n",
      "Epoch 6 -- Batch 183/ 842, training loss 0.4754680395126343\n",
      "Epoch 6 -- Batch 184/ 842, training loss 0.47359204292297363\n",
      "Epoch 6 -- Batch 185/ 842, training loss 0.47584518790245056\n",
      "Epoch 6 -- Batch 186/ 842, training loss 0.48527607321739197\n",
      "Epoch 6 -- Batch 187/ 842, training loss 0.4911747872829437\n",
      "Epoch 6 -- Batch 188/ 842, training loss 0.4760713577270508\n",
      "Epoch 6 -- Batch 189/ 842, training loss 0.4754587411880493\n",
      "Epoch 6 -- Batch 190/ 842, training loss 0.4584486484527588\n",
      "Epoch 6 -- Batch 191/ 842, training loss 0.47934192419052124\n",
      "Epoch 6 -- Batch 192/ 842, training loss 0.5005039572715759\n",
      "Epoch 6 -- Batch 193/ 842, training loss 0.46642130613327026\n",
      "Epoch 6 -- Batch 194/ 842, training loss 0.4506417512893677\n",
      "Epoch 6 -- Batch 195/ 842, training loss 0.47411414980888367\n",
      "Epoch 6 -- Batch 196/ 842, training loss 0.5020899176597595\n",
      "Epoch 6 -- Batch 197/ 842, training loss 0.4624764919281006\n",
      "Epoch 6 -- Batch 198/ 842, training loss 0.4646836221218109\n",
      "Epoch 6 -- Batch 199/ 842, training loss 0.4931001365184784\n",
      "Epoch 6 -- Batch 200/ 842, training loss 0.46435853838920593\n",
      "Epoch 6 -- Batch 201/ 842, training loss 0.48627662658691406\n",
      "Epoch 6 -- Batch 202/ 842, training loss 0.48356756567955017\n",
      "Epoch 6 -- Batch 203/ 842, training loss 0.4652496874332428\n",
      "Epoch 6 -- Batch 204/ 842, training loss 0.48347461223602295\n",
      "Epoch 6 -- Batch 205/ 842, training loss 0.4696730673313141\n",
      "Epoch 6 -- Batch 206/ 842, training loss 0.4866653382778168\n",
      "Epoch 6 -- Batch 207/ 842, training loss 0.47252357006073\n",
      "Epoch 6 -- Batch 208/ 842, training loss 0.47683560848236084\n",
      "Epoch 6 -- Batch 209/ 842, training loss 0.49579837918281555\n",
      "Epoch 6 -- Batch 210/ 842, training loss 0.4901999831199646\n",
      "Epoch 6 -- Batch 211/ 842, training loss 0.4688817858695984\n",
      "Epoch 6 -- Batch 212/ 842, training loss 0.4586867094039917\n",
      "Epoch 6 -- Batch 213/ 842, training loss 0.509648323059082\n",
      "Epoch 6 -- Batch 214/ 842, training loss 0.4728737771511078\n",
      "Epoch 6 -- Batch 215/ 842, training loss 0.47732236981391907\n",
      "Epoch 6 -- Batch 216/ 842, training loss 0.4794327914714813\n",
      "Epoch 6 -- Batch 217/ 842, training loss 0.47774001955986023\n",
      "Epoch 6 -- Batch 218/ 842, training loss 0.468037486076355\n",
      "Epoch 6 -- Batch 219/ 842, training loss 0.4911508858203888\n",
      "Epoch 6 -- Batch 220/ 842, training loss 0.47112658619880676\n",
      "Epoch 6 -- Batch 221/ 842, training loss 0.4784264862537384\n",
      "Epoch 6 -- Batch 222/ 842, training loss 0.48642104864120483\n",
      "Epoch 6 -- Batch 223/ 842, training loss 0.4854156970977783\n",
      "Epoch 6 -- Batch 224/ 842, training loss 0.47422096133232117\n",
      "Epoch 6 -- Batch 225/ 842, training loss 0.48688775300979614\n",
      "Epoch 6 -- Batch 226/ 842, training loss 0.4632315933704376\n",
      "Epoch 6 -- Batch 227/ 842, training loss 0.49448156356811523\n",
      "Epoch 6 -- Batch 228/ 842, training loss 0.4854387640953064\n",
      "Epoch 6 -- Batch 229/ 842, training loss 0.4589221477508545\n",
      "Epoch 6 -- Batch 230/ 842, training loss 0.4913160502910614\n",
      "Epoch 6 -- Batch 231/ 842, training loss 0.4786807596683502\n",
      "Epoch 6 -- Batch 232/ 842, training loss 0.47853055596351624\n",
      "Epoch 6 -- Batch 233/ 842, training loss 0.48408177495002747\n",
      "Epoch 6 -- Batch 234/ 842, training loss 0.47588545083999634\n",
      "Epoch 6 -- Batch 235/ 842, training loss 0.4957510232925415\n",
      "Epoch 6 -- Batch 236/ 842, training loss 0.4762450158596039\n",
      "Epoch 6 -- Batch 237/ 842, training loss 0.48186567425727844\n",
      "Epoch 6 -- Batch 238/ 842, training loss 0.49334463477134705\n",
      "Epoch 6 -- Batch 239/ 842, training loss 0.49166521430015564\n",
      "Epoch 6 -- Batch 240/ 842, training loss 0.49862176179885864\n",
      "Epoch 6 -- Batch 241/ 842, training loss 0.4896910786628723\n",
      "Epoch 6 -- Batch 242/ 842, training loss 0.47911161184310913\n",
      "Epoch 6 -- Batch 243/ 842, training loss 0.475467324256897\n",
      "Epoch 6 -- Batch 244/ 842, training loss 0.4841156303882599\n",
      "Epoch 6 -- Batch 245/ 842, training loss 0.480207622051239\n",
      "Epoch 6 -- Batch 246/ 842, training loss 0.47534751892089844\n",
      "Epoch 6 -- Batch 247/ 842, training loss 0.4901062846183777\n",
      "Epoch 6 -- Batch 248/ 842, training loss 0.4726567268371582\n",
      "Epoch 6 -- Batch 249/ 842, training loss 0.4909800887107849\n",
      "Epoch 6 -- Batch 250/ 842, training loss 0.4785577058792114\n",
      "Epoch 6 -- Batch 251/ 842, training loss 0.4863823652267456\n",
      "Epoch 6 -- Batch 252/ 842, training loss 0.47389036417007446\n",
      "Epoch 6 -- Batch 253/ 842, training loss 0.4888817369937897\n",
      "Epoch 6 -- Batch 254/ 842, training loss 0.4901118874549866\n",
      "Epoch 6 -- Batch 255/ 842, training loss 0.4875776767730713\n",
      "Epoch 6 -- Batch 256/ 842, training loss 0.4607662856578827\n",
      "Epoch 6 -- Batch 257/ 842, training loss 0.46673014760017395\n",
      "Epoch 6 -- Batch 258/ 842, training loss 0.4810905456542969\n",
      "Epoch 6 -- Batch 259/ 842, training loss 0.4834628701210022\n",
      "Epoch 6 -- Batch 260/ 842, training loss 0.49126747250556946\n",
      "Epoch 6 -- Batch 261/ 842, training loss 0.4762474298477173\n",
      "Epoch 6 -- Batch 262/ 842, training loss 0.46403273940086365\n",
      "Epoch 6 -- Batch 263/ 842, training loss 0.4980335831642151\n",
      "Epoch 6 -- Batch 264/ 842, training loss 0.4859088063240051\n",
      "Epoch 6 -- Batch 265/ 842, training loss 0.4713502526283264\n",
      "Epoch 6 -- Batch 266/ 842, training loss 0.4860720932483673\n",
      "Epoch 6 -- Batch 267/ 842, training loss 0.4890897870063782\n",
      "Epoch 6 -- Batch 268/ 842, training loss 0.49162933230400085\n",
      "Epoch 6 -- Batch 269/ 842, training loss 0.497464120388031\n",
      "Epoch 6 -- Batch 270/ 842, training loss 0.49164053797721863\n",
      "Epoch 6 -- Batch 271/ 842, training loss 0.48325634002685547\n",
      "Epoch 6 -- Batch 272/ 842, training loss 0.47302937507629395\n",
      "Epoch 6 -- Batch 273/ 842, training loss 0.4648597538471222\n",
      "Epoch 6 -- Batch 274/ 842, training loss 0.46200552582740784\n",
      "Epoch 6 -- Batch 275/ 842, training loss 0.46934494376182556\n",
      "Epoch 6 -- Batch 276/ 842, training loss 0.4643246829509735\n",
      "Epoch 6 -- Batch 277/ 842, training loss 0.49307170510292053\n",
      "Epoch 6 -- Batch 278/ 842, training loss 0.4613207280635834\n",
      "Epoch 6 -- Batch 279/ 842, training loss 0.4722229242324829\n",
      "Epoch 6 -- Batch 280/ 842, training loss 0.4754895567893982\n",
      "Epoch 6 -- Batch 281/ 842, training loss 0.5091652274131775\n",
      "Epoch 6 -- Batch 282/ 842, training loss 0.4869864881038666\n",
      "Epoch 6 -- Batch 283/ 842, training loss 0.5036830306053162\n",
      "Epoch 6 -- Batch 284/ 842, training loss 0.47177404165267944\n",
      "Epoch 6 -- Batch 285/ 842, training loss 0.47162896394729614\n",
      "Epoch 6 -- Batch 286/ 842, training loss 0.48068079352378845\n",
      "Epoch 6 -- Batch 287/ 842, training loss 0.4727891683578491\n",
      "Epoch 6 -- Batch 288/ 842, training loss 0.4991818368434906\n",
      "Epoch 6 -- Batch 289/ 842, training loss 0.4956103563308716\n",
      "Epoch 6 -- Batch 290/ 842, training loss 0.48185452818870544\n",
      "Epoch 6 -- Batch 291/ 842, training loss 0.47448602318763733\n",
      "Epoch 6 -- Batch 292/ 842, training loss 0.47643232345581055\n",
      "Epoch 6 -- Batch 293/ 842, training loss 0.47149503231048584\n",
      "Epoch 6 -- Batch 294/ 842, training loss 0.493692547082901\n",
      "Epoch 6 -- Batch 295/ 842, training loss 0.4853312075138092\n",
      "Epoch 6 -- Batch 296/ 842, training loss 0.4659573435783386\n",
      "Epoch 6 -- Batch 297/ 842, training loss 0.4764315187931061\n",
      "Epoch 6 -- Batch 298/ 842, training loss 0.4723692834377289\n",
      "Epoch 6 -- Batch 299/ 842, training loss 0.4799771308898926\n",
      "Epoch 6 -- Batch 300/ 842, training loss 0.47842907905578613\n",
      "Epoch 6 -- Batch 301/ 842, training loss 0.4717477858066559\n",
      "Epoch 6 -- Batch 302/ 842, training loss 0.4752408266067505\n",
      "Epoch 6 -- Batch 303/ 842, training loss 0.47291457653045654\n",
      "Epoch 6 -- Batch 304/ 842, training loss 0.4721955955028534\n",
      "Epoch 6 -- Batch 305/ 842, training loss 0.4641696512699127\n",
      "Epoch 6 -- Batch 306/ 842, training loss 0.4906858801841736\n",
      "Epoch 6 -- Batch 307/ 842, training loss 0.5013614892959595\n",
      "Epoch 6 -- Batch 308/ 842, training loss 0.5099446773529053\n",
      "Epoch 6 -- Batch 309/ 842, training loss 0.47932112216949463\n",
      "Epoch 6 -- Batch 310/ 842, training loss 0.47217315435409546\n",
      "Epoch 6 -- Batch 311/ 842, training loss 0.4962889552116394\n",
      "Epoch 6 -- Batch 312/ 842, training loss 0.4702271521091461\n",
      "Epoch 6 -- Batch 313/ 842, training loss 0.47633489966392517\n",
      "Epoch 6 -- Batch 314/ 842, training loss 0.49716100096702576\n",
      "Epoch 6 -- Batch 315/ 842, training loss 0.4839164614677429\n",
      "Epoch 6 -- Batch 316/ 842, training loss 0.4947577118873596\n",
      "Epoch 6 -- Batch 317/ 842, training loss 0.4730205535888672\n",
      "Epoch 6 -- Batch 318/ 842, training loss 0.4901747405529022\n",
      "Epoch 6 -- Batch 319/ 842, training loss 0.47585317492485046\n",
      "Epoch 6 -- Batch 320/ 842, training loss 0.4716432988643646\n",
      "Epoch 6 -- Batch 321/ 842, training loss 0.4732929766178131\n",
      "Epoch 6 -- Batch 322/ 842, training loss 0.48101842403411865\n",
      "Epoch 6 -- Batch 323/ 842, training loss 0.4662518799304962\n",
      "Epoch 6 -- Batch 324/ 842, training loss 0.4919663667678833\n",
      "Epoch 6 -- Batch 325/ 842, training loss 0.4888570308685303\n",
      "Epoch 6 -- Batch 326/ 842, training loss 0.4709673523902893\n",
      "Epoch 6 -- Batch 327/ 842, training loss 0.5075979232788086\n",
      "Epoch 6 -- Batch 328/ 842, training loss 0.47658124566078186\n",
      "Epoch 6 -- Batch 329/ 842, training loss 0.4796399772167206\n",
      "Epoch 6 -- Batch 330/ 842, training loss 0.49070125818252563\n",
      "Epoch 6 -- Batch 331/ 842, training loss 0.4684508740901947\n",
      "Epoch 6 -- Batch 332/ 842, training loss 0.46775829792022705\n",
      "Epoch 6 -- Batch 333/ 842, training loss 0.4831779897212982\n",
      "Epoch 6 -- Batch 334/ 842, training loss 0.5006838440895081\n",
      "Epoch 6 -- Batch 335/ 842, training loss 0.47373339533805847\n",
      "Epoch 6 -- Batch 336/ 842, training loss 0.4872888922691345\n",
      "Epoch 6 -- Batch 337/ 842, training loss 0.47408053278923035\n",
      "Epoch 6 -- Batch 338/ 842, training loss 0.47319498658180237\n",
      "Epoch 6 -- Batch 339/ 842, training loss 0.4847654700279236\n",
      "Epoch 6 -- Batch 340/ 842, training loss 0.4896973967552185\n",
      "Epoch 6 -- Batch 341/ 842, training loss 0.49021705985069275\n",
      "Epoch 6 -- Batch 342/ 842, training loss 0.479594886302948\n",
      "Epoch 6 -- Batch 343/ 842, training loss 0.5167490839958191\n",
      "Epoch 6 -- Batch 344/ 842, training loss 0.49478378891944885\n",
      "Epoch 6 -- Batch 345/ 842, training loss 0.4772236943244934\n",
      "Epoch 6 -- Batch 346/ 842, training loss 0.47824349999427795\n",
      "Epoch 6 -- Batch 347/ 842, training loss 0.4741717278957367\n",
      "Epoch 6 -- Batch 348/ 842, training loss 0.4898781478404999\n",
      "Epoch 6 -- Batch 349/ 842, training loss 0.475846529006958\n",
      "Epoch 6 -- Batch 350/ 842, training loss 0.4581111967563629\n",
      "Epoch 6 -- Batch 351/ 842, training loss 0.4537947177886963\n",
      "Epoch 6 -- Batch 352/ 842, training loss 0.48778441548347473\n",
      "Epoch 6 -- Batch 353/ 842, training loss 0.47918596863746643\n",
      "Epoch 6 -- Batch 354/ 842, training loss 0.4784119129180908\n",
      "Epoch 6 -- Batch 355/ 842, training loss 0.4723294973373413\n",
      "Epoch 6 -- Batch 356/ 842, training loss 0.48709043860435486\n",
      "Epoch 6 -- Batch 357/ 842, training loss 0.4940604865550995\n",
      "Epoch 6 -- Batch 358/ 842, training loss 0.46515941619873047\n",
      "Epoch 6 -- Batch 359/ 842, training loss 0.4640127122402191\n",
      "Epoch 6 -- Batch 360/ 842, training loss 0.5031387209892273\n",
      "Epoch 6 -- Batch 361/ 842, training loss 0.49058860540390015\n",
      "Epoch 6 -- Batch 362/ 842, training loss 0.47909775376319885\n",
      "Epoch 6 -- Batch 363/ 842, training loss 0.4824536442756653\n",
      "Epoch 6 -- Batch 364/ 842, training loss 0.4697926640510559\n",
      "Epoch 6 -- Batch 365/ 842, training loss 0.4725576639175415\n",
      "Epoch 6 -- Batch 366/ 842, training loss 0.4908447563648224\n",
      "Epoch 6 -- Batch 367/ 842, training loss 0.485636830329895\n",
      "Epoch 6 -- Batch 368/ 842, training loss 0.4854069948196411\n",
      "Epoch 6 -- Batch 369/ 842, training loss 0.5060135722160339\n",
      "Epoch 6 -- Batch 370/ 842, training loss 0.49132776260375977\n",
      "Epoch 6 -- Batch 371/ 842, training loss 0.47655653953552246\n",
      "Epoch 6 -- Batch 372/ 842, training loss 0.48131778836250305\n",
      "Epoch 6 -- Batch 373/ 842, training loss 0.44493740797042847\n",
      "Epoch 6 -- Batch 374/ 842, training loss 0.46187451481819153\n",
      "Epoch 6 -- Batch 375/ 842, training loss 0.4941089451313019\n",
      "Epoch 6 -- Batch 376/ 842, training loss 0.4976896643638611\n",
      "Epoch 6 -- Batch 377/ 842, training loss 0.48684003949165344\n",
      "Epoch 6 -- Batch 378/ 842, training loss 0.4759516417980194\n",
      "Epoch 6 -- Batch 379/ 842, training loss 0.4906032979488373\n",
      "Epoch 6 -- Batch 380/ 842, training loss 0.45749396085739136\n",
      "Epoch 6 -- Batch 381/ 842, training loss 0.4836040735244751\n",
      "Epoch 6 -- Batch 382/ 842, training loss 0.47702041268348694\n",
      "Epoch 6 -- Batch 383/ 842, training loss 0.46645933389663696\n",
      "Epoch 6 -- Batch 384/ 842, training loss 0.4887678027153015\n",
      "Epoch 6 -- Batch 385/ 842, training loss 0.46089455485343933\n",
      "Epoch 6 -- Batch 386/ 842, training loss 0.4890412986278534\n",
      "Epoch 6 -- Batch 387/ 842, training loss 0.47772079706192017\n",
      "Epoch 6 -- Batch 388/ 842, training loss 0.46010276675224304\n",
      "Epoch 6 -- Batch 389/ 842, training loss 0.5073690414428711\n",
      "Epoch 6 -- Batch 390/ 842, training loss 0.49698182940483093\n",
      "Epoch 6 -- Batch 391/ 842, training loss 0.493249773979187\n",
      "Epoch 6 -- Batch 392/ 842, training loss 0.46400895714759827\n",
      "Epoch 6 -- Batch 393/ 842, training loss 0.4709784686565399\n",
      "Epoch 6 -- Batch 394/ 842, training loss 0.4739314317703247\n",
      "Epoch 6 -- Batch 395/ 842, training loss 0.4927785098552704\n",
      "Epoch 6 -- Batch 396/ 842, training loss 0.4875761568546295\n",
      "Epoch 6 -- Batch 397/ 842, training loss 0.4890787899494171\n",
      "Epoch 6 -- Batch 398/ 842, training loss 0.46880003809928894\n",
      "Epoch 6 -- Batch 399/ 842, training loss 0.4819933772087097\n",
      "Epoch 6 -- Batch 400/ 842, training loss 0.46077030897140503\n",
      "Epoch 6 -- Batch 401/ 842, training loss 0.47205600142478943\n",
      "Epoch 6 -- Batch 402/ 842, training loss 0.47689157724380493\n",
      "Epoch 6 -- Batch 403/ 842, training loss 0.4787980914115906\n",
      "Epoch 6 -- Batch 404/ 842, training loss 0.4776912331581116\n",
      "Epoch 6 -- Batch 405/ 842, training loss 0.485211580991745\n",
      "Epoch 6 -- Batch 406/ 842, training loss 0.4795907437801361\n",
      "Epoch 6 -- Batch 407/ 842, training loss 0.493423730134964\n",
      "Epoch 6 -- Batch 408/ 842, training loss 0.4800032675266266\n",
      "Epoch 6 -- Batch 409/ 842, training loss 0.4774218797683716\n",
      "Epoch 6 -- Batch 410/ 842, training loss 0.48118701577186584\n",
      "Epoch 6 -- Batch 411/ 842, training loss 0.47982317209243774\n",
      "Epoch 6 -- Batch 412/ 842, training loss 0.48390546441078186\n",
      "Epoch 6 -- Batch 413/ 842, training loss 0.46423056721687317\n",
      "Epoch 6 -- Batch 414/ 842, training loss 0.4644577205181122\n",
      "Epoch 6 -- Batch 415/ 842, training loss 0.47063884139060974\n",
      "Epoch 6 -- Batch 416/ 842, training loss 0.48357003927230835\n",
      "Epoch 6 -- Batch 417/ 842, training loss 0.4868103861808777\n",
      "Epoch 6 -- Batch 418/ 842, training loss 0.47782453894615173\n",
      "Epoch 6 -- Batch 419/ 842, training loss 0.4721742868423462\n",
      "Epoch 6 -- Batch 420/ 842, training loss 0.46462297439575195\n",
      "Epoch 6 -- Batch 421/ 842, training loss 0.48250529170036316\n",
      "Epoch 6 -- Batch 422/ 842, training loss 0.4704095125198364\n",
      "Epoch 6 -- Batch 423/ 842, training loss 0.5004104971885681\n",
      "Epoch 6 -- Batch 424/ 842, training loss 0.5005095601081848\n",
      "Epoch 6 -- Batch 425/ 842, training loss 0.4803442358970642\n",
      "Epoch 6 -- Batch 426/ 842, training loss 0.46713462471961975\n",
      "Epoch 6 -- Batch 427/ 842, training loss 0.4860647916793823\n",
      "Epoch 6 -- Batch 428/ 842, training loss 0.47629764676094055\n",
      "Epoch 6 -- Batch 429/ 842, training loss 0.46927210688591003\n",
      "Epoch 6 -- Batch 430/ 842, training loss 0.5135126113891602\n",
      "Epoch 6 -- Batch 431/ 842, training loss 0.46842220425605774\n",
      "Epoch 6 -- Batch 432/ 842, training loss 0.49468204379081726\n",
      "Epoch 6 -- Batch 433/ 842, training loss 0.47449710965156555\n",
      "Epoch 6 -- Batch 434/ 842, training loss 0.4813093841075897\n",
      "Epoch 6 -- Batch 435/ 842, training loss 0.4799652695655823\n",
      "Epoch 6 -- Batch 436/ 842, training loss 0.4773194491863251\n",
      "Epoch 6 -- Batch 437/ 842, training loss 0.4683440029621124\n",
      "Epoch 6 -- Batch 438/ 842, training loss 0.48841360211372375\n",
      "Epoch 6 -- Batch 439/ 842, training loss 0.47522634267807007\n",
      "Epoch 6 -- Batch 440/ 842, training loss 0.4987868368625641\n",
      "Epoch 6 -- Batch 441/ 842, training loss 0.47141870856285095\n",
      "Epoch 6 -- Batch 442/ 842, training loss 0.4791831970214844\n",
      "Epoch 6 -- Batch 443/ 842, training loss 0.4701228141784668\n",
      "Epoch 6 -- Batch 444/ 842, training loss 0.4750896096229553\n",
      "Epoch 6 -- Batch 445/ 842, training loss 0.48353973031044006\n",
      "Epoch 6 -- Batch 446/ 842, training loss 0.4707906246185303\n",
      "Epoch 6 -- Batch 447/ 842, training loss 0.46448633074760437\n",
      "Epoch 6 -- Batch 448/ 842, training loss 0.46549665927886963\n",
      "Epoch 6 -- Batch 449/ 842, training loss 0.48053768277168274\n",
      "Epoch 6 -- Batch 450/ 842, training loss 0.4748287498950958\n",
      "Epoch 6 -- Batch 451/ 842, training loss 0.4751691520214081\n",
      "Epoch 6 -- Batch 452/ 842, training loss 0.44056829810142517\n",
      "Epoch 6 -- Batch 453/ 842, training loss 0.4498196840286255\n",
      "Epoch 6 -- Batch 454/ 842, training loss 0.47633761167526245\n",
      "Epoch 6 -- Batch 455/ 842, training loss 0.4762810170650482\n",
      "Epoch 6 -- Batch 456/ 842, training loss 0.4801088869571686\n",
      "Epoch 6 -- Batch 457/ 842, training loss 0.48196253180503845\n",
      "Epoch 6 -- Batch 458/ 842, training loss 0.47945767641067505\n",
      "Epoch 6 -- Batch 459/ 842, training loss 0.4962180554866791\n",
      "Epoch 6 -- Batch 460/ 842, training loss 0.46807730197906494\n",
      "Epoch 6 -- Batch 461/ 842, training loss 0.47865524888038635\n",
      "Epoch 6 -- Batch 462/ 842, training loss 0.46645161509513855\n",
      "Epoch 6 -- Batch 463/ 842, training loss 0.48934048414230347\n",
      "Epoch 6 -- Batch 464/ 842, training loss 0.4638935923576355\n",
      "Epoch 6 -- Batch 465/ 842, training loss 0.46233734488487244\n",
      "Epoch 6 -- Batch 466/ 842, training loss 0.4722306430339813\n",
      "Epoch 6 -- Batch 467/ 842, training loss 0.4746456444263458\n",
      "Epoch 6 -- Batch 468/ 842, training loss 0.4605115056037903\n",
      "Epoch 6 -- Batch 469/ 842, training loss 0.4590541124343872\n",
      "Epoch 6 -- Batch 470/ 842, training loss 0.4767968952655792\n",
      "Epoch 6 -- Batch 471/ 842, training loss 0.47993752360343933\n",
      "Epoch 6 -- Batch 472/ 842, training loss 0.4844239354133606\n",
      "Epoch 6 -- Batch 473/ 842, training loss 0.49008408188819885\n",
      "Epoch 6 -- Batch 474/ 842, training loss 0.4898071587085724\n",
      "Epoch 6 -- Batch 475/ 842, training loss 0.47090059518814087\n",
      "Epoch 6 -- Batch 476/ 842, training loss 0.5029075741767883\n",
      "Epoch 6 -- Batch 477/ 842, training loss 0.48282259702682495\n",
      "Epoch 6 -- Batch 478/ 842, training loss 0.4985921382904053\n",
      "Epoch 6 -- Batch 479/ 842, training loss 0.4891462028026581\n",
      "Epoch 6 -- Batch 480/ 842, training loss 0.4934317171573639\n",
      "Epoch 6 -- Batch 481/ 842, training loss 0.476401686668396\n",
      "Epoch 6 -- Batch 482/ 842, training loss 0.49716779589653015\n",
      "Epoch 6 -- Batch 483/ 842, training loss 0.4841695725917816\n",
      "Epoch 6 -- Batch 484/ 842, training loss 0.474514901638031\n",
      "Epoch 6 -- Batch 485/ 842, training loss 0.47916266322135925\n",
      "Epoch 6 -- Batch 486/ 842, training loss 0.4652860462665558\n",
      "Epoch 6 -- Batch 487/ 842, training loss 0.47636109590530396\n",
      "Epoch 6 -- Batch 488/ 842, training loss 0.4785611629486084\n",
      "Epoch 6 -- Batch 489/ 842, training loss 0.46897321939468384\n",
      "Epoch 6 -- Batch 490/ 842, training loss 0.4916086494922638\n",
      "Epoch 6 -- Batch 491/ 842, training loss 0.47715261578559875\n",
      "Epoch 6 -- Batch 492/ 842, training loss 0.48223868012428284\n",
      "Epoch 6 -- Batch 493/ 842, training loss 0.4862186312675476\n",
      "Epoch 6 -- Batch 494/ 842, training loss 0.4785948693752289\n",
      "Epoch 6 -- Batch 495/ 842, training loss 0.47346556186676025\n",
      "Epoch 6 -- Batch 496/ 842, training loss 0.4777137041091919\n",
      "Epoch 6 -- Batch 497/ 842, training loss 0.4443970322608948\n",
      "Epoch 6 -- Batch 498/ 842, training loss 0.48496904969215393\n",
      "Epoch 6 -- Batch 499/ 842, training loss 0.47032591700553894\n",
      "Epoch 6 -- Batch 500/ 842, training loss 0.5010836720466614\n",
      "Epoch 6 -- Batch 501/ 842, training loss 0.4797120988368988\n",
      "Epoch 6 -- Batch 502/ 842, training loss 0.4728226363658905\n",
      "Epoch 6 -- Batch 503/ 842, training loss 0.47436246275901794\n",
      "Epoch 6 -- Batch 504/ 842, training loss 0.4778392016887665\n",
      "Epoch 6 -- Batch 505/ 842, training loss 0.4684304893016815\n",
      "Epoch 6 -- Batch 506/ 842, training loss 0.4589272737503052\n",
      "Epoch 6 -- Batch 507/ 842, training loss 0.4594027101993561\n",
      "Epoch 6 -- Batch 508/ 842, training loss 0.47682642936706543\n",
      "Epoch 6 -- Batch 509/ 842, training loss 0.4745173156261444\n",
      "Epoch 6 -- Batch 510/ 842, training loss 0.4757465124130249\n",
      "Epoch 6 -- Batch 511/ 842, training loss 0.4715732932090759\n",
      "Epoch 6 -- Batch 512/ 842, training loss 0.4802284836769104\n",
      "Epoch 6 -- Batch 513/ 842, training loss 0.48714926838874817\n",
      "Epoch 6 -- Batch 514/ 842, training loss 0.47419166564941406\n",
      "Epoch 6 -- Batch 515/ 842, training loss 0.46348509192466736\n",
      "Epoch 6 -- Batch 516/ 842, training loss 0.4882856011390686\n",
      "Epoch 6 -- Batch 517/ 842, training loss 0.48843950033187866\n",
      "Epoch 6 -- Batch 518/ 842, training loss 0.4554098844528198\n",
      "Epoch 6 -- Batch 519/ 842, training loss 0.48275914788246155\n",
      "Epoch 6 -- Batch 520/ 842, training loss 0.47594329714775085\n",
      "Epoch 6 -- Batch 521/ 842, training loss 0.4747300446033478\n",
      "Epoch 6 -- Batch 522/ 842, training loss 0.4806232452392578\n",
      "Epoch 6 -- Batch 523/ 842, training loss 0.49319860339164734\n",
      "Epoch 6 -- Batch 524/ 842, training loss 0.47393113374710083\n",
      "Epoch 6 -- Batch 525/ 842, training loss 0.4816870093345642\n",
      "Epoch 6 -- Batch 526/ 842, training loss 0.5131325721740723\n",
      "Epoch 6 -- Batch 527/ 842, training loss 0.476995050907135\n",
      "Epoch 6 -- Batch 528/ 842, training loss 0.4744815528392792\n",
      "Epoch 6 -- Batch 529/ 842, training loss 0.48270460963249207\n",
      "Epoch 6 -- Batch 530/ 842, training loss 0.47185036540031433\n",
      "Epoch 6 -- Batch 531/ 842, training loss 0.4629219174385071\n",
      "Epoch 6 -- Batch 532/ 842, training loss 0.47619757056236267\n",
      "Epoch 6 -- Batch 533/ 842, training loss 0.46971839666366577\n",
      "Epoch 6 -- Batch 534/ 842, training loss 0.4878653585910797\n",
      "Epoch 6 -- Batch 535/ 842, training loss 0.4690514802932739\n",
      "Epoch 6 -- Batch 536/ 842, training loss 0.4900068938732147\n",
      "Epoch 6 -- Batch 537/ 842, training loss 0.4828254282474518\n",
      "Epoch 6 -- Batch 538/ 842, training loss 0.49394601583480835\n",
      "Epoch 6 -- Batch 539/ 842, training loss 0.4647972583770752\n",
      "Epoch 6 -- Batch 540/ 842, training loss 0.4791637063026428\n",
      "Epoch 6 -- Batch 541/ 842, training loss 0.4707677960395813\n",
      "Epoch 6 -- Batch 542/ 842, training loss 0.48213836550712585\n",
      "Epoch 6 -- Batch 543/ 842, training loss 0.4653468728065491\n",
      "Epoch 6 -- Batch 544/ 842, training loss 0.47470149397850037\n",
      "Epoch 6 -- Batch 545/ 842, training loss 0.48139065504074097\n",
      "Epoch 6 -- Batch 546/ 842, training loss 0.4885905683040619\n",
      "Epoch 6 -- Batch 547/ 842, training loss 0.48245733976364136\n",
      "Epoch 6 -- Batch 548/ 842, training loss 0.4773795008659363\n",
      "Epoch 6 -- Batch 549/ 842, training loss 0.4692574143409729\n",
      "Epoch 6 -- Batch 550/ 842, training loss 0.44815945625305176\n",
      "Epoch 6 -- Batch 551/ 842, training loss 0.4697713851928711\n",
      "Epoch 6 -- Batch 552/ 842, training loss 0.48748883605003357\n",
      "Epoch 6 -- Batch 553/ 842, training loss 0.4773533046245575\n",
      "Epoch 6 -- Batch 554/ 842, training loss 0.4925107955932617\n",
      "Epoch 6 -- Batch 555/ 842, training loss 0.4719070494174957\n",
      "Epoch 6 -- Batch 556/ 842, training loss 0.47040849924087524\n",
      "Epoch 6 -- Batch 557/ 842, training loss 0.501294732093811\n",
      "Epoch 6 -- Batch 558/ 842, training loss 0.4727170467376709\n",
      "Epoch 6 -- Batch 559/ 842, training loss 0.4853559732437134\n",
      "Epoch 6 -- Batch 560/ 842, training loss 0.47314757108688354\n",
      "Epoch 6 -- Batch 561/ 842, training loss 0.4886345863342285\n",
      "Epoch 6 -- Batch 562/ 842, training loss 0.47321709990501404\n",
      "Epoch 6 -- Batch 563/ 842, training loss 0.48790329694747925\n",
      "Epoch 6 -- Batch 564/ 842, training loss 0.4935495853424072\n",
      "Epoch 6 -- Batch 565/ 842, training loss 0.48211437463760376\n",
      "Epoch 6 -- Batch 566/ 842, training loss 0.46818098425865173\n",
      "Epoch 6 -- Batch 567/ 842, training loss 0.48281756043434143\n",
      "Epoch 6 -- Batch 568/ 842, training loss 0.4986271262168884\n",
      "Epoch 6 -- Batch 569/ 842, training loss 0.4641271233558655\n",
      "Epoch 6 -- Batch 570/ 842, training loss 0.4704741835594177\n",
      "Epoch 6 -- Batch 571/ 842, training loss 0.48341113328933716\n",
      "Epoch 6 -- Batch 572/ 842, training loss 0.4700847864151001\n",
      "Epoch 6 -- Batch 573/ 842, training loss 0.4682607352733612\n",
      "Epoch 6 -- Batch 574/ 842, training loss 0.48886510729789734\n",
      "Epoch 6 -- Batch 575/ 842, training loss 0.4580368399620056\n",
      "Epoch 6 -- Batch 576/ 842, training loss 0.45545750856399536\n",
      "Epoch 6 -- Batch 577/ 842, training loss 0.46685388684272766\n",
      "Epoch 6 -- Batch 578/ 842, training loss 0.46774381399154663\n",
      "Epoch 6 -- Batch 579/ 842, training loss 0.45766985416412354\n",
      "Epoch 6 -- Batch 580/ 842, training loss 0.4684101641178131\n",
      "Epoch 6 -- Batch 581/ 842, training loss 0.4694404900074005\n",
      "Epoch 6 -- Batch 582/ 842, training loss 0.4652993083000183\n",
      "Epoch 6 -- Batch 583/ 842, training loss 0.484919935464859\n",
      "Epoch 6 -- Batch 584/ 842, training loss 0.46831071376800537\n",
      "Epoch 6 -- Batch 585/ 842, training loss 0.45737147331237793\n",
      "Epoch 6 -- Batch 586/ 842, training loss 0.4795714020729065\n",
      "Epoch 6 -- Batch 587/ 842, training loss 0.4642196595668793\n",
      "Epoch 6 -- Batch 588/ 842, training loss 0.4768514335155487\n",
      "Epoch 6 -- Batch 589/ 842, training loss 0.4732593595981598\n",
      "Epoch 6 -- Batch 590/ 842, training loss 0.4982678294181824\n",
      "Epoch 6 -- Batch 591/ 842, training loss 0.4666599631309509\n",
      "Epoch 6 -- Batch 592/ 842, training loss 0.4797491431236267\n",
      "Epoch 6 -- Batch 593/ 842, training loss 0.46648651361465454\n",
      "Epoch 6 -- Batch 594/ 842, training loss 0.47016412019729614\n",
      "Epoch 6 -- Batch 595/ 842, training loss 0.4871825873851776\n",
      "Epoch 6 -- Batch 596/ 842, training loss 0.4590964913368225\n",
      "Epoch 6 -- Batch 597/ 842, training loss 0.4755111634731293\n",
      "Epoch 6 -- Batch 598/ 842, training loss 0.47529783844947815\n",
      "Epoch 6 -- Batch 599/ 842, training loss 0.4693630337715149\n",
      "Epoch 6 -- Batch 600/ 842, training loss 0.47476884722709656\n",
      "Epoch 6 -- Batch 601/ 842, training loss 0.48718634247779846\n",
      "Epoch 6 -- Batch 602/ 842, training loss 0.47623828053474426\n",
      "Epoch 6 -- Batch 603/ 842, training loss 0.4660835564136505\n",
      "Epoch 6 -- Batch 604/ 842, training loss 0.4670035243034363\n",
      "Epoch 6 -- Batch 605/ 842, training loss 0.476532518863678\n",
      "Epoch 6 -- Batch 606/ 842, training loss 0.46730929613113403\n",
      "Epoch 6 -- Batch 607/ 842, training loss 0.4683220684528351\n",
      "Epoch 6 -- Batch 608/ 842, training loss 0.485901415348053\n",
      "Epoch 6 -- Batch 609/ 842, training loss 0.4670526683330536\n",
      "Epoch 6 -- Batch 610/ 842, training loss 0.4952774941921234\n",
      "Epoch 6 -- Batch 611/ 842, training loss 0.47727012634277344\n",
      "Epoch 6 -- Batch 612/ 842, training loss 0.4739476442337036\n",
      "Epoch 6 -- Batch 613/ 842, training loss 0.4729512631893158\n",
      "Epoch 6 -- Batch 614/ 842, training loss 0.4933573305606842\n",
      "Epoch 6 -- Batch 615/ 842, training loss 0.4895392060279846\n",
      "Epoch 6 -- Batch 616/ 842, training loss 0.48411279916763306\n",
      "Epoch 6 -- Batch 617/ 842, training loss 0.49477818608283997\n",
      "Epoch 6 -- Batch 618/ 842, training loss 0.46340256929397583\n",
      "Epoch 6 -- Batch 619/ 842, training loss 0.47452491521835327\n",
      "Epoch 6 -- Batch 620/ 842, training loss 0.4675176739692688\n",
      "Epoch 6 -- Batch 621/ 842, training loss 0.48890602588653564\n",
      "Epoch 6 -- Batch 622/ 842, training loss 0.45723286271095276\n",
      "Epoch 6 -- Batch 623/ 842, training loss 0.4641939699649811\n",
      "Epoch 6 -- Batch 624/ 842, training loss 0.4730875790119171\n",
      "Epoch 6 -- Batch 625/ 842, training loss 0.4950231611728668\n",
      "Epoch 6 -- Batch 626/ 842, training loss 0.47823986411094666\n",
      "Epoch 6 -- Batch 627/ 842, training loss 0.4828854501247406\n",
      "Epoch 6 -- Batch 628/ 842, training loss 0.4673239588737488\n",
      "Epoch 6 -- Batch 629/ 842, training loss 0.46769145131111145\n",
      "Epoch 6 -- Batch 630/ 842, training loss 0.4895972013473511\n",
      "Epoch 6 -- Batch 631/ 842, training loss 0.48867449164390564\n",
      "Epoch 6 -- Batch 632/ 842, training loss 0.4697311222553253\n",
      "Epoch 6 -- Batch 633/ 842, training loss 0.459134578704834\n",
      "Epoch 6 -- Batch 634/ 842, training loss 0.46889984607696533\n",
      "Epoch 6 -- Batch 635/ 842, training loss 0.48230504989624023\n",
      "Epoch 6 -- Batch 636/ 842, training loss 0.46856689453125\n",
      "Epoch 6 -- Batch 637/ 842, training loss 0.45601174235343933\n",
      "Epoch 6 -- Batch 638/ 842, training loss 0.46764883399009705\n",
      "Epoch 6 -- Batch 639/ 842, training loss 0.487711638212204\n",
      "Epoch 6 -- Batch 640/ 842, training loss 0.48777535557746887\n",
      "Epoch 6 -- Batch 641/ 842, training loss 0.4779236614704132\n",
      "Epoch 6 -- Batch 642/ 842, training loss 0.4674451947212219\n",
      "Epoch 6 -- Batch 643/ 842, training loss 0.47426992654800415\n",
      "Epoch 6 -- Batch 644/ 842, training loss 0.46304136514663696\n",
      "Epoch 6 -- Batch 645/ 842, training loss 0.48177674412727356\n",
      "Epoch 6 -- Batch 646/ 842, training loss 0.4724108576774597\n",
      "Epoch 6 -- Batch 647/ 842, training loss 0.45508670806884766\n",
      "Epoch 6 -- Batch 648/ 842, training loss 0.49461549520492554\n",
      "Epoch 6 -- Batch 649/ 842, training loss 0.4831114709377289\n",
      "Epoch 6 -- Batch 650/ 842, training loss 0.47010400891304016\n",
      "Epoch 6 -- Batch 651/ 842, training loss 0.47361645102500916\n",
      "Epoch 6 -- Batch 652/ 842, training loss 0.4670833647251129\n",
      "Epoch 6 -- Batch 653/ 842, training loss 0.502090334892273\n",
      "Epoch 6 -- Batch 654/ 842, training loss 0.4506232440471649\n",
      "Epoch 6 -- Batch 655/ 842, training loss 0.46597450971603394\n",
      "Epoch 6 -- Batch 656/ 842, training loss 0.49686169624328613\n",
      "Epoch 6 -- Batch 657/ 842, training loss 0.46819010376930237\n",
      "Epoch 6 -- Batch 658/ 842, training loss 0.459770143032074\n",
      "Epoch 6 -- Batch 659/ 842, training loss 0.46439364552497864\n",
      "Epoch 6 -- Batch 660/ 842, training loss 0.47574886679649353\n",
      "Epoch 6 -- Batch 661/ 842, training loss 0.48985329270362854\n",
      "Epoch 6 -- Batch 662/ 842, training loss 0.4685962200164795\n",
      "Epoch 6 -- Batch 663/ 842, training loss 0.4631388187408447\n",
      "Epoch 6 -- Batch 664/ 842, training loss 0.491609662771225\n",
      "Epoch 6 -- Batch 665/ 842, training loss 0.4677915871143341\n",
      "Epoch 6 -- Batch 666/ 842, training loss 0.4738771319389343\n",
      "Epoch 6 -- Batch 667/ 842, training loss 0.4907943904399872\n",
      "Epoch 6 -- Batch 668/ 842, training loss 0.4736320674419403\n",
      "Epoch 6 -- Batch 669/ 842, training loss 0.46573537588119507\n",
      "Epoch 6 -- Batch 670/ 842, training loss 0.4560326337814331\n",
      "Epoch 6 -- Batch 671/ 842, training loss 0.46744421124458313\n",
      "Epoch 6 -- Batch 672/ 842, training loss 0.4818858504295349\n",
      "Epoch 6 -- Batch 673/ 842, training loss 0.47612378001213074\n",
      "Epoch 6 -- Batch 674/ 842, training loss 0.4678886830806732\n",
      "Epoch 6 -- Batch 675/ 842, training loss 0.4812096059322357\n",
      "Epoch 6 -- Batch 676/ 842, training loss 0.47591105103492737\n",
      "Epoch 6 -- Batch 677/ 842, training loss 0.4636518061161041\n",
      "Epoch 6 -- Batch 678/ 842, training loss 0.4922267198562622\n",
      "Epoch 6 -- Batch 679/ 842, training loss 0.4773605465888977\n",
      "Epoch 6 -- Batch 680/ 842, training loss 0.48803484439849854\n",
      "Epoch 6 -- Batch 681/ 842, training loss 0.4855261743068695\n",
      "Epoch 6 -- Batch 682/ 842, training loss 0.47471243143081665\n",
      "Epoch 6 -- Batch 683/ 842, training loss 0.4595133364200592\n",
      "Epoch 6 -- Batch 684/ 842, training loss 0.4744976758956909\n",
      "Epoch 6 -- Batch 685/ 842, training loss 0.45819592475891113\n",
      "Epoch 6 -- Batch 686/ 842, training loss 0.4900660514831543\n",
      "Epoch 6 -- Batch 687/ 842, training loss 0.48855826258659363\n",
      "Epoch 6 -- Batch 688/ 842, training loss 0.46231964230537415\n",
      "Epoch 6 -- Batch 689/ 842, training loss 0.4806901514530182\n",
      "Epoch 6 -- Batch 690/ 842, training loss 0.4873600900173187\n",
      "Epoch 6 -- Batch 691/ 842, training loss 0.491079717874527\n",
      "Epoch 6 -- Batch 692/ 842, training loss 0.49690020084381104\n",
      "Epoch 6 -- Batch 693/ 842, training loss 0.4789750874042511\n",
      "Epoch 6 -- Batch 694/ 842, training loss 0.46596211194992065\n",
      "Epoch 6 -- Batch 695/ 842, training loss 0.46179041266441345\n",
      "Epoch 6 -- Batch 696/ 842, training loss 0.4623771607875824\n",
      "Epoch 6 -- Batch 697/ 842, training loss 0.4669042229652405\n",
      "Epoch 6 -- Batch 698/ 842, training loss 0.4821034371852875\n",
      "Epoch 6 -- Batch 699/ 842, training loss 0.46817851066589355\n",
      "Epoch 6 -- Batch 700/ 842, training loss 0.48364317417144775\n",
      "Epoch 6 -- Batch 701/ 842, training loss 0.4725109934806824\n",
      "Epoch 6 -- Batch 702/ 842, training loss 0.4633643329143524\n",
      "Epoch 6 -- Batch 703/ 842, training loss 0.49423685669898987\n",
      "Epoch 6 -- Batch 704/ 842, training loss 0.49274200201034546\n",
      "Epoch 6 -- Batch 705/ 842, training loss 0.4669536352157593\n",
      "Epoch 6 -- Batch 706/ 842, training loss 0.46943235397338867\n",
      "Epoch 6 -- Batch 707/ 842, training loss 0.45113298296928406\n",
      "Epoch 6 -- Batch 708/ 842, training loss 0.47438347339630127\n",
      "Epoch 6 -- Batch 709/ 842, training loss 0.47230812907218933\n",
      "Epoch 6 -- Batch 710/ 842, training loss 0.47589871287345886\n",
      "Epoch 6 -- Batch 711/ 842, training loss 0.4683251976966858\n",
      "Epoch 6 -- Batch 712/ 842, training loss 0.46271979808807373\n",
      "Epoch 6 -- Batch 713/ 842, training loss 0.48709434270858765\n",
      "Epoch 6 -- Batch 714/ 842, training loss 0.48123031854629517\n",
      "Epoch 6 -- Batch 715/ 842, training loss 0.47957104444503784\n",
      "Epoch 6 -- Batch 716/ 842, training loss 0.4729970097541809\n",
      "Epoch 6 -- Batch 717/ 842, training loss 0.473455011844635\n",
      "Epoch 6 -- Batch 718/ 842, training loss 0.4721994996070862\n",
      "Epoch 6 -- Batch 719/ 842, training loss 0.5005304217338562\n",
      "Epoch 6 -- Batch 720/ 842, training loss 0.4582104980945587\n",
      "Epoch 6 -- Batch 721/ 842, training loss 0.4663676619529724\n",
      "Epoch 6 -- Batch 722/ 842, training loss 0.4781288802623749\n",
      "Epoch 6 -- Batch 723/ 842, training loss 0.47621244192123413\n",
      "Epoch 6 -- Batch 724/ 842, training loss 0.49283862113952637\n",
      "Epoch 6 -- Batch 725/ 842, training loss 0.45945996046066284\n",
      "Epoch 6 -- Batch 726/ 842, training loss 0.4743497967720032\n",
      "Epoch 6 -- Batch 727/ 842, training loss 0.47801870107650757\n",
      "Epoch 6 -- Batch 728/ 842, training loss 0.47689393162727356\n",
      "Epoch 6 -- Batch 729/ 842, training loss 0.4491537809371948\n",
      "Epoch 6 -- Batch 730/ 842, training loss 0.4726305902004242\n",
      "Epoch 6 -- Batch 731/ 842, training loss 0.4574195444583893\n",
      "Epoch 6 -- Batch 732/ 842, training loss 0.4744759500026703\n",
      "Epoch 6 -- Batch 733/ 842, training loss 0.45974817872047424\n",
      "Epoch 6 -- Batch 734/ 842, training loss 0.4783889949321747\n",
      "Epoch 6 -- Batch 735/ 842, training loss 0.4686693549156189\n",
      "Epoch 6 -- Batch 736/ 842, training loss 0.445041298866272\n",
      "Epoch 6 -- Batch 737/ 842, training loss 0.49387627840042114\n",
      "Epoch 6 -- Batch 738/ 842, training loss 0.47868409752845764\n",
      "Epoch 6 -- Batch 739/ 842, training loss 0.4530266225337982\n",
      "Epoch 6 -- Batch 740/ 842, training loss 0.4554702937602997\n",
      "Epoch 6 -- Batch 741/ 842, training loss 0.47254955768585205\n",
      "Epoch 6 -- Batch 742/ 842, training loss 0.4725009500980377\n",
      "Epoch 6 -- Batch 743/ 842, training loss 0.47583499550819397\n",
      "Epoch 6 -- Batch 744/ 842, training loss 0.47091037034988403\n",
      "Epoch 6 -- Batch 745/ 842, training loss 0.4645870327949524\n",
      "Epoch 6 -- Batch 746/ 842, training loss 0.46158474683761597\n",
      "Epoch 6 -- Batch 747/ 842, training loss 0.48373663425445557\n",
      "Epoch 6 -- Batch 748/ 842, training loss 0.4714043438434601\n",
      "Epoch 6 -- Batch 749/ 842, training loss 0.4758535921573639\n",
      "Epoch 6 -- Batch 750/ 842, training loss 0.4592497944831848\n",
      "Epoch 6 -- Batch 751/ 842, training loss 0.4660891890525818\n",
      "Epoch 6 -- Batch 752/ 842, training loss 0.46816718578338623\n",
      "Epoch 6 -- Batch 753/ 842, training loss 0.4717625081539154\n",
      "Epoch 6 -- Batch 754/ 842, training loss 0.4744170904159546\n",
      "Epoch 6 -- Batch 755/ 842, training loss 0.47048380970954895\n",
      "Epoch 6 -- Batch 756/ 842, training loss 0.4762987196445465\n",
      "Epoch 6 -- Batch 757/ 842, training loss 0.4632735848426819\n",
      "Epoch 6 -- Batch 758/ 842, training loss 0.4895663261413574\n",
      "Epoch 6 -- Batch 759/ 842, training loss 0.47376081347465515\n",
      "Epoch 6 -- Batch 760/ 842, training loss 0.4692346751689911\n",
      "Epoch 6 -- Batch 761/ 842, training loss 0.46039146184921265\n",
      "Epoch 6 -- Batch 762/ 842, training loss 0.4914812445640564\n",
      "Epoch 6 -- Batch 763/ 842, training loss 0.47206011414527893\n",
      "Epoch 6 -- Batch 764/ 842, training loss 0.4702524244785309\n",
      "Epoch 6 -- Batch 765/ 842, training loss 0.4531289339065552\n",
      "Epoch 6 -- Batch 766/ 842, training loss 0.4583721160888672\n",
      "Epoch 6 -- Batch 767/ 842, training loss 0.48334741592407227\n",
      "Epoch 6 -- Batch 768/ 842, training loss 0.4563860595226288\n",
      "Epoch 6 -- Batch 769/ 842, training loss 0.4701536297798157\n",
      "Epoch 6 -- Batch 770/ 842, training loss 0.46635010838508606\n",
      "Epoch 6 -- Batch 771/ 842, training loss 0.49845924973487854\n",
      "Epoch 6 -- Batch 772/ 842, training loss 0.46383193135261536\n",
      "Epoch 6 -- Batch 773/ 842, training loss 0.47932198643684387\n",
      "Epoch 6 -- Batch 774/ 842, training loss 0.4933620095252991\n",
      "Epoch 6 -- Batch 775/ 842, training loss 0.47466447949409485\n",
      "Epoch 6 -- Batch 776/ 842, training loss 0.47877368330955505\n",
      "Epoch 6 -- Batch 777/ 842, training loss 0.49271872639656067\n",
      "Epoch 6 -- Batch 778/ 842, training loss 0.46290746331214905\n",
      "Epoch 6 -- Batch 779/ 842, training loss 0.47784993052482605\n",
      "Epoch 6 -- Batch 780/ 842, training loss 0.47019848227500916\n",
      "Epoch 6 -- Batch 781/ 842, training loss 0.4839272201061249\n",
      "Epoch 6 -- Batch 782/ 842, training loss 0.4768654704093933\n",
      "Epoch 6 -- Batch 783/ 842, training loss 0.4651329517364502\n",
      "Epoch 6 -- Batch 784/ 842, training loss 0.4841797351837158\n",
      "Epoch 6 -- Batch 785/ 842, training loss 0.49281203746795654\n",
      "Epoch 6 -- Batch 786/ 842, training loss 0.4850202202796936\n",
      "Epoch 6 -- Batch 787/ 842, training loss 0.47237589955329895\n",
      "Epoch 6 -- Batch 788/ 842, training loss 0.4562091827392578\n",
      "Epoch 6 -- Batch 789/ 842, training loss 0.4635053873062134\n",
      "Epoch 6 -- Batch 790/ 842, training loss 0.4688384532928467\n",
      "Epoch 6 -- Batch 791/ 842, training loss 0.4646521210670471\n",
      "Epoch 6 -- Batch 792/ 842, training loss 0.48276737332344055\n",
      "Epoch 6 -- Batch 793/ 842, training loss 0.454272985458374\n",
      "Epoch 6 -- Batch 794/ 842, training loss 0.4633062481880188\n",
      "Epoch 6 -- Batch 795/ 842, training loss 0.4901123344898224\n",
      "Epoch 6 -- Batch 796/ 842, training loss 0.46449577808380127\n",
      "Epoch 6 -- Batch 797/ 842, training loss 0.4546147882938385\n",
      "Epoch 6 -- Batch 798/ 842, training loss 0.4719882011413574\n",
      "Epoch 6 -- Batch 799/ 842, training loss 0.4537012279033661\n",
      "Epoch 6 -- Batch 800/ 842, training loss 0.48320239782333374\n",
      "Epoch 6 -- Batch 801/ 842, training loss 0.47906798124313354\n",
      "Epoch 6 -- Batch 802/ 842, training loss 0.4736473858356476\n",
      "Epoch 6 -- Batch 803/ 842, training loss 0.4578787386417389\n",
      "Epoch 6 -- Batch 804/ 842, training loss 0.45506608486175537\n",
      "Epoch 6 -- Batch 805/ 842, training loss 0.4814695119857788\n",
      "Epoch 6 -- Batch 806/ 842, training loss 0.4764000177383423\n",
      "Epoch 6 -- Batch 807/ 842, training loss 0.46667057275772095\n",
      "Epoch 6 -- Batch 808/ 842, training loss 0.48807913064956665\n",
      "Epoch 6 -- Batch 809/ 842, training loss 0.46745067834854126\n",
      "Epoch 6 -- Batch 810/ 842, training loss 0.4606097340583801\n",
      "Epoch 6 -- Batch 811/ 842, training loss 0.47526952624320984\n",
      "Epoch 6 -- Batch 812/ 842, training loss 0.48515835404396057\n",
      "Epoch 6 -- Batch 813/ 842, training loss 0.47940701246261597\n",
      "Epoch 6 -- Batch 814/ 842, training loss 0.4757712781429291\n",
      "Epoch 6 -- Batch 815/ 842, training loss 0.47966742515563965\n",
      "Epoch 6 -- Batch 816/ 842, training loss 0.466709703207016\n",
      "Epoch 6 -- Batch 817/ 842, training loss 0.4656490385532379\n",
      "Epoch 6 -- Batch 818/ 842, training loss 0.4502708315849304\n",
      "Epoch 6 -- Batch 819/ 842, training loss 0.47312721610069275\n",
      "Epoch 6 -- Batch 820/ 842, training loss 0.47443461418151855\n",
      "Epoch 6 -- Batch 821/ 842, training loss 0.46071746945381165\n",
      "Epoch 6 -- Batch 822/ 842, training loss 0.4869438111782074\n",
      "Epoch 6 -- Batch 823/ 842, training loss 0.4709475338459015\n",
      "Epoch 6 -- Batch 824/ 842, training loss 0.4783736765384674\n",
      "Epoch 6 -- Batch 825/ 842, training loss 0.476507306098938\n",
      "Epoch 6 -- Batch 826/ 842, training loss 0.4650227427482605\n",
      "Epoch 6 -- Batch 827/ 842, training loss 0.47038426995277405\n",
      "Epoch 6 -- Batch 828/ 842, training loss 0.46013393998146057\n",
      "Epoch 6 -- Batch 829/ 842, training loss 0.46570277214050293\n",
      "Epoch 6 -- Batch 830/ 842, training loss 0.47701767086982727\n",
      "Epoch 6 -- Batch 831/ 842, training loss 0.47323355078697205\n",
      "Epoch 6 -- Batch 832/ 842, training loss 0.4584946036338806\n",
      "Epoch 6 -- Batch 833/ 842, training loss 0.48034483194351196\n",
      "Epoch 6 -- Batch 834/ 842, training loss 0.4782300293445587\n",
      "Epoch 6 -- Batch 835/ 842, training loss 0.48756444454193115\n",
      "Epoch 6 -- Batch 836/ 842, training loss 0.48028403520584106\n",
      "Epoch 6 -- Batch 837/ 842, training loss 0.47771498560905457\n",
      "Epoch 6 -- Batch 838/ 842, training loss 0.4685315191745758\n",
      "Epoch 6 -- Batch 839/ 842, training loss 0.47975820302963257\n",
      "Epoch 6 -- Batch 840/ 842, training loss 0.47115641832351685\n",
      "Epoch 6 -- Batch 841/ 842, training loss 0.46822547912597656\n",
      "Epoch 6 -- Batch 842/ 842, training loss 0.4563587009906769\n",
      "----------------------------------------------------------------------\n",
      "Epoch 6 -- Batch 1/ 94, validation loss 0.4340073764324188\n",
      "Epoch 6 -- Batch 2/ 94, validation loss 0.48507919907569885\n",
      "Epoch 6 -- Batch 3/ 94, validation loss 0.494679719209671\n",
      "Epoch 6 -- Batch 4/ 94, validation loss 0.46598875522613525\n",
      "Epoch 6 -- Batch 5/ 94, validation loss 0.44439226388931274\n",
      "Epoch 6 -- Batch 6/ 94, validation loss 0.45259010791778564\n",
      "Epoch 6 -- Batch 7/ 94, validation loss 0.47071364521980286\n",
      "Epoch 6 -- Batch 8/ 94, validation loss 0.48691412806510925\n",
      "Epoch 6 -- Batch 9/ 94, validation loss 0.48037493228912354\n",
      "Epoch 6 -- Batch 10/ 94, validation loss 0.47992897033691406\n",
      "Epoch 6 -- Batch 11/ 94, validation loss 0.4802766442298889\n",
      "Epoch 6 -- Batch 12/ 94, validation loss 0.47727397084236145\n",
      "Epoch 6 -- Batch 13/ 94, validation loss 0.4557560682296753\n",
      "Epoch 6 -- Batch 14/ 94, validation loss 0.4596405625343323\n",
      "Epoch 6 -- Batch 15/ 94, validation loss 0.44838207960128784\n",
      "Epoch 6 -- Batch 16/ 94, validation loss 0.445589542388916\n",
      "Epoch 6 -- Batch 17/ 94, validation loss 0.475870817899704\n",
      "Epoch 6 -- Batch 18/ 94, validation loss 0.47154122591018677\n",
      "Epoch 6 -- Batch 19/ 94, validation loss 0.4717232584953308\n",
      "Epoch 6 -- Batch 20/ 94, validation loss 0.4759734570980072\n",
      "Epoch 6 -- Batch 21/ 94, validation loss 0.4500208795070648\n",
      "Epoch 6 -- Batch 22/ 94, validation loss 0.46414563059806824\n",
      "Epoch 6 -- Batch 23/ 94, validation loss 0.45058563351631165\n",
      "Epoch 6 -- Batch 24/ 94, validation loss 0.4499857425689697\n",
      "Epoch 6 -- Batch 25/ 94, validation loss 0.49380484223365784\n",
      "Epoch 6 -- Batch 26/ 94, validation loss 0.4571398198604584\n",
      "Epoch 6 -- Batch 27/ 94, validation loss 0.44341012835502625\n",
      "Epoch 6 -- Batch 28/ 94, validation loss 0.4561462700366974\n",
      "Epoch 6 -- Batch 29/ 94, validation loss 0.4796040654182434\n",
      "Epoch 6 -- Batch 30/ 94, validation loss 0.4555390477180481\n",
      "Epoch 6 -- Batch 31/ 94, validation loss 0.46776309609413147\n",
      "Epoch 6 -- Batch 32/ 94, validation loss 0.4462910592556\n",
      "Epoch 6 -- Batch 33/ 94, validation loss 0.43696147203445435\n",
      "Epoch 6 -- Batch 34/ 94, validation loss 0.48675429821014404\n",
      "Epoch 6 -- Batch 35/ 94, validation loss 0.4563390612602234\n",
      "Epoch 6 -- Batch 36/ 94, validation loss 0.4605902433395386\n",
      "Epoch 6 -- Batch 37/ 94, validation loss 0.45679107308387756\n",
      "Epoch 6 -- Batch 38/ 94, validation loss 0.4650236666202545\n",
      "Epoch 6 -- Batch 39/ 94, validation loss 0.4846210181713104\n",
      "Epoch 6 -- Batch 40/ 94, validation loss 0.47252872586250305\n",
      "Epoch 6 -- Batch 41/ 94, validation loss 0.4814213812351227\n",
      "Epoch 6 -- Batch 42/ 94, validation loss 0.4576720893383026\n",
      "Epoch 6 -- Batch 43/ 94, validation loss 0.46282002329826355\n",
      "Epoch 6 -- Batch 44/ 94, validation loss 0.4367883503437042\n",
      "Epoch 6 -- Batch 45/ 94, validation loss 0.4726485013961792\n",
      "Epoch 6 -- Batch 46/ 94, validation loss 0.4412061274051666\n",
      "Epoch 6 -- Batch 47/ 94, validation loss 0.4793570041656494\n",
      "Epoch 6 -- Batch 48/ 94, validation loss 0.4734781086444855\n",
      "Epoch 6 -- Batch 49/ 94, validation loss 0.4673425555229187\n",
      "Epoch 6 -- Batch 50/ 94, validation loss 0.49583229422569275\n",
      "Epoch 6 -- Batch 51/ 94, validation loss 0.4421815872192383\n",
      "Epoch 6 -- Batch 52/ 94, validation loss 0.4632772207260132\n",
      "Epoch 6 -- Batch 53/ 94, validation loss 0.444980651140213\n",
      "Epoch 6 -- Batch 54/ 94, validation loss 0.44214826822280884\n",
      "Epoch 6 -- Batch 55/ 94, validation loss 0.4528786242008209\n",
      "Epoch 6 -- Batch 56/ 94, validation loss 0.45891618728637695\n",
      "Epoch 6 -- Batch 57/ 94, validation loss 0.47242194414138794\n",
      "Epoch 6 -- Batch 58/ 94, validation loss 0.46028071641921997\n",
      "Epoch 6 -- Batch 59/ 94, validation loss 0.46579936146736145\n",
      "Epoch 6 -- Batch 60/ 94, validation loss 0.4737955629825592\n",
      "Epoch 6 -- Batch 61/ 94, validation loss 0.4674597680568695\n",
      "Epoch 6 -- Batch 62/ 94, validation loss 0.4612872302532196\n",
      "Epoch 6 -- Batch 63/ 94, validation loss 0.4555469751358032\n",
      "Epoch 6 -- Batch 64/ 94, validation loss 0.486795037984848\n",
      "Epoch 6 -- Batch 65/ 94, validation loss 0.4788229465484619\n",
      "Epoch 6 -- Batch 66/ 94, validation loss 0.4696612060070038\n",
      "Epoch 6 -- Batch 67/ 94, validation loss 0.45961737632751465\n",
      "Epoch 6 -- Batch 68/ 94, validation loss 0.480037122964859\n",
      "Epoch 6 -- Batch 69/ 94, validation loss 0.4628985822200775\n",
      "Epoch 6 -- Batch 70/ 94, validation loss 0.4662509560585022\n",
      "Epoch 6 -- Batch 71/ 94, validation loss 0.4679582715034485\n",
      "Epoch 6 -- Batch 72/ 94, validation loss 0.47880932688713074\n",
      "Epoch 6 -- Batch 73/ 94, validation loss 0.45068126916885376\n",
      "Epoch 6 -- Batch 74/ 94, validation loss 0.47824326157569885\n",
      "Epoch 6 -- Batch 75/ 94, validation loss 0.4738723933696747\n",
      "Epoch 6 -- Batch 76/ 94, validation loss 0.4783289134502411\n",
      "Epoch 6 -- Batch 77/ 94, validation loss 0.452424019575119\n",
      "Epoch 6 -- Batch 78/ 94, validation loss 0.46753162145614624\n",
      "Epoch 6 -- Batch 79/ 94, validation loss 0.47376930713653564\n",
      "Epoch 6 -- Batch 80/ 94, validation loss 0.48375603556632996\n",
      "Epoch 6 -- Batch 81/ 94, validation loss 0.4810314178466797\n",
      "Epoch 6 -- Batch 82/ 94, validation loss 0.49020591378211975\n",
      "Epoch 6 -- Batch 83/ 94, validation loss 0.4774031937122345\n",
      "Epoch 6 -- Batch 84/ 94, validation loss 0.45308759808540344\n",
      "Epoch 6 -- Batch 85/ 94, validation loss 0.4764675796031952\n",
      "Epoch 6 -- Batch 86/ 94, validation loss 0.4438677430152893\n",
      "Epoch 6 -- Batch 87/ 94, validation loss 0.5005060434341431\n",
      "Epoch 6 -- Batch 88/ 94, validation loss 0.4547440707683563\n",
      "Epoch 6 -- Batch 89/ 94, validation loss 0.45782607793807983\n",
      "Epoch 6 -- Batch 90/ 94, validation loss 0.45772936940193176\n",
      "Epoch 6 -- Batch 91/ 94, validation loss 0.46101510524749756\n",
      "Epoch 6 -- Batch 92/ 94, validation loss 0.47908705472946167\n",
      "Epoch 6 -- Batch 93/ 94, validation loss 0.456775963306427\n",
      "Epoch 6 -- Batch 94/ 94, validation loss 0.4461893141269684\n",
      "----------------------------------------------------------------------\n",
      "Epoch 6 loss: Training 0.47816625237464905, Validation 0.446189284324646\n",
      "----------------------------------------------------------------------\n",
      "Epoch 7/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 14 15 16 17 18 19\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 5 7 26 27 29 32\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)C2(c3ccccc3N2C(=O)Nc2ccccc21)C(C#N)=C(N)O2'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4 15 16 18 19\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)N1CCC2(CC1)CC2OC(=O)CNC2c1ccc(OCc2ccc(C(F)(F)F)cc2)cc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 9 10 17 18 19\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'N#Cc1c(NC(=O)CCCSc2ccc([N+](=O)[O-])cc2ss2)sc2c1CCCC2'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)C(C(C)C)N1CCN(c2nnc(Cc3ccccc3)n3CCCCC2)CC1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCOc1ccccc1N1CCn2c1nc1c2c(=O)n(CCc3ccccc3)c(=O)n2C'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Cn2ncc3c([nH]c4cccc(C(F)(F)F)c4)c(=O)c3c2c2=O)c(=O)[nH]1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CC(C)N(Cc1ccc(-n2cccn2)cc1)Cc1cccn2C'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C1c2ccc3ccc(c3c2C1)CC(O)COC31'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1=C(C(=O)OC)SC(=C2C(=S)C(C(=O)OCC)=C(OC(=O)c3ccccc3)C2C)c1ccccc12'\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'COc1cc(C=O)ccc1COc1ccc(C'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 4 13\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNc2nc(CNc3ccc4[nH]c(=O)n(CC5CC4)n3)cc(=O)n2C2CCC(C)CC2)cc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(Cc2nnc(SCC(=O)c3cc(-c4ccccc4)nc4ccccc4c34)o2)cc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1nc2oc(c(=O)c3ccccc33)n2c(C)c(Cc3ccccc3)c(=O)c12'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 22\n",
      "[19:04:46] Explicit valence for atom # 28 N, 6, is greater than permitted\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(CC(=O)N3CCSC4(C)CC3)c[nH]c2c1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CC12CC3CC(C1)CC(C(=O)Nc1ccc(Cl)c(C(F)(F)F)c1)N2'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C(CC12CC3CC(C1)CC(O)C2)(O)C(F)(F)F'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 21 22 23\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 7 13 14\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 9 22 23 24 27 28 29\n",
      "[19:04:46] SMILES Parse Error: syntax error while parsing: CCn1[nH]c(SC)c(/C=N/O)c1-\n",
      "[19:04:46] SMILES Parse Error: Failed parsing SMILES 'CCn1[nH]c(SC)c(/C=N/O)c1-' for input: 'CCn1[nH]c(SC)c(/C=N/O)c1-'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2c(c1)[C@H]1C[C@@H](CC(=O)N3CCc5ccccc4C3)O[C@H](CO)[C@@H]1O2)NCc1ccc2c(c1)OCO2'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1c([N+](=O)[O-])ccc2c1Br)c1ccccc1C2CC2'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Nc2nc3ncnn3c(NCc4cccc(C(F)(F)F)c4)cn23)cc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cn(cnc1C(F)(F)F)n1cccc1C(O)c1ccccc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C(CC(c1ccccc1Oc1ccccc1)C1)OCC1CCCCC1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])c1cc(Br)cc(N2C3CC4CC(C2)CC3C(=O)N2CCN(C2CCCCCC3)CC2)c1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'C/C=C/c1ccc(-c2ccc([C@@H]3[C@@H](CO)N4CCCCN(C(=O)Nc5cccc(F)c5)C[C@H]3C)nc2)cc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)NC2CC3CCCCC2C2)c(OC)c1'\n",
      "[19:04:46] SMILES Parse Error: duplicated ring closure 3 bonds atom 30 to itself for input: 'COc1cccnc1-n1nnc(C(=O)N2CCC(C(=O)NC3CCCc3ccccc33)CC2)n1'\n",
      "[19:04:46] SMILES Parse Error: ring closure 2 duplicates bond between atom 11 and atom 12 for input: 'Clc1ccc(-c2cnc3n2[nH]c2c2CCCCC3)cc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24 26 27\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 14 15 16\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=c1c(-c2ccccc2)c[nH]c2c1C(c1ccsc1)C1C=CC2C12'\n",
      "[19:04:46] SMILES Parse Error: extra close parentheses while parsing: CC1(C)CC(C(=O)N2CCC3(C(=O)Nc4cccc(-c5ccccc5)c4)C3)CCO2)cc1Cl\n",
      "[19:04:46] SMILES Parse Error: Failed parsing SMILES 'CC1(C)CC(C(=O)N2CCC3(C(=O)Nc4cccc(-c5ccccc5)c4)C3)CCO2)cc1Cl' for input: 'CC1(C)CC(C(=O)N2CCC3(C(=O)Nc4cccc(-c5ccccc5)c4)C3)CCO2)cc1Cl'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1cc(CN2CCOC3=O)c(=O)[nH]c2cc1C(=O)N(C)C/C=C/c1ccccc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 24 27 28\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 15\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'O=C(CC1(O)C(=O)N(Cc2ccccc2)C(=O)C12CCN(CC(=O)N1CCCCC1)CC2'\n",
      "[19:04:46] SMILES Parse Error: ring closure 2 duplicates bond between atom 5 and atom 6 for input: 'CN(Cc1cc2n2n(C)c(=O)c3ccccc3n2n1)c1cccc(Br)c1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 3 17 18 19 32 33 34\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 1 2 3 15 21 23 25\n",
      "[19:04:46] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2nc(-c3ccc(NC(=S)Nc4c(C)n(C)n(-c5ccccc5)c4=O)cn3)s2)c(C)c1\n",
      "[19:04:46] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2nc(-c3ccc(NC(=S)Nc4c(C)n(C)n(-c5ccccc5)c4=O)cn3)s2)c(C)c1' for input: 'Cc1ccc2nc(-c3ccc(NC(=S)Nc4c(C)n(C)n(-c5ccccc5)c4=O)cn3)s2)c(C)c1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCc1cc(C2[C@H]CCN2C(=O)C2=C[C@H](c2ccsc2)CC(=O)O3)ccc1C'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 3 11 15 16 17 18 19 20 21\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C=C2/Oc3cc(=O)P(=O)COc4ccccc42)cc2CO1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 12 13 14 23 25\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1CCC(c2ccc(-c3cc4[nH]c(C(F)(F)F)cc4n3-c3ccn(C)c4=O)cc2)CC1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 24\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C1C(C(=O)N2CCc3c(nc(-c4ccccn4)[nH]c3=O)C2C=CCC2)NC1=O'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 7\n",
      "[19:04:46] SMILES Parse Error: ring closure 1 duplicates bond between atom 25 and atom 26 for input: 'O=C(CC1C(=O)NCCN1Cc1ccc(Cl)cc1Cl)N1CCCCC1C1c1ccccc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)N1CCN(C2CC2c2ccc(-c3ccn[nH]3)cc2CC3)CC1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(NS(=O)(=O)c2cc3c4c(c2)CCN3C(=O)CC3)c1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 22\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(N3CCC(C(=O)NCc4cccc4c(c4)OCO5)CC3)ccc2c1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc3ccc4c(c3cc2NC(=O)c2ccccc2C)OCO3)cc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 11\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 19 20 21\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 3 4 24 25 27 28 29\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1cc(/C=N/NC(=O)CSc2nc3cc(Cl)c(OC(C)=O)c(C)c3cc2c2C)cc(OC)c1OC'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 5 6 8 10 17 28 29\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 8 10 11 17 18\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])c1ccc(CSc2n[nH]c(-c3ccc(-c4c(O)nc5cc(Br)ccc6c4)nn3)cc2)cc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 7 8 9 16 17 22 23 24 26\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2oc(=O)c3c(c2c1)C(c1ccccn1)N=C1CC(c2ccccc2)OC1=O'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCCc1cc2c(=O)n3c(nc2c1-c1ccc(OCC)cc1)CC(C(=O)N3CCC(C)CC1)S2'\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(NC(=O)c2c3c(nn4C(C)(C)c4ccccc43)c(=O)n(C)c2cc1C'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 22\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 13 14 19\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 9 18 19 20 21 22 23 24\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)N(CC(=O)Nc2ccc3c(c2)CC(=O)N3)CC2)c(C)c1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1/C=C/CN1CCc2nnc(C(NC(=O)C(C)C)n3CC2CC2)CC1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 17\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCc1ccc(C2(C(C)C(=O)NCCOC(c3ccco3)C3)CC(=O)N(C2CCCC2)C2=O)cc1'\n",
      "[19:04:46] Explicit valence for atom # 12 Cl, 2, is greater than permitted\n",
      "[19:04:46] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[19:04:46] SMILES Parse Error: ring closure 6 duplicates bond between atom 16 and atom 17 for input: 'CC1(C)Cc2c(sc3ncn4nc(NSc5n6n6CCCCC5)ccc4c23)C(=O)N1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1cccc(CNC(=O)[C@@H]2C[C@@H]3c4[nH]c5cccc(Cl)cc5c3C2N2C(=O)C2CC2)c1'\n",
      "[19:04:46] SMILES Parse Error: ring closure 1 duplicates bond between atom 10 and atom 11 for input: 'COc1ccc2c(c1)CNc1c1c(=O)n(C)c(=O)n2C'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 16 17 18 23 24\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 24 25\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 8 9 11 12 13\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 23 24 25\n",
      "[19:04:46] Explicit valence for atom # 27 N, 6, is greater than permitted\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CC(c1nc2ccccc2[nH]1)P(=O)(OCc1ccccc1)C(=O)N2'\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(OC)c(-n2c(SCC(=O)N2CCc3ccccc3C2)nnc1N'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 15 17 18 20 21\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'CN1c2ccc(-c3ccc(C(=O)NCC[C@@H]4CC[C@H](C)N(C(=O)C(C)(C)C)CC3)cc2OC[C@H](C)N(C(=O)Cc2ccccn2)C[C@H]1C'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 10 12\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1CN1CCn2c(C(=O)NCc3ccco3)cccc2c2c1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 5 6 17 18 27\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 8 21 23 24\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 19 20 21\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 20 21 32 33 34\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 18\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 7 8 10 11 12 24 25 26 27\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CCCCC(C)C(=O)N1CCc2cc(S(=O)(=O)N3CCN(c4ccc(F)cc4)CC3)ccc21'\n",
      "[19:04:46] SMILES Parse Error: ring closure 2 duplicates bond between atom 24 and atom 25 for input: 'CC(C)(C)c1ccc(C2CC(=O)C=C(c2ccco2)n2c(=O)[nH]nc2-c2ccccc2)cc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4 18 19 20 21 22 23\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C(C1CC2CCC1C2C1)N1CCN(c2ccccc2)CC1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'O=C1NC(=O)c2c1c1c3c3c([nH]c4c1Cc1ccccc13)OCCC3'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(CNC(=O)c2ccc3nc(-c4cccc(NC(=O)c5ccc(Oc5ccccc5)o4)s3)sc2c1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 14 15 20 21 22\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'C=C(C)COc1ccc(-n2c(=O)c(C(=O)NCCN3CCOCC3)cc3c(=O)n(Cc4ccco3)cnc32)cc1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C2Cc3ccccc3C(C(=O)N(CC(=O)c4ccc(OC)c(OC)c4)C3)=C2C)c1'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'COc1cc(OC)cc(C(=O)NNC(=O)C23CC4CC(C2)CC(C(=O)N3CCCCCC3)C3)c1'\n",
      "[19:04:46] Explicit valence for atom # 18 O, 3, is greater than permitted\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CCn1c(CC(C)n2cncnc2=O)cc2c(=O)n3c(nc3cc2C)CCCC4'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'CC12CN3CC(C1)CC(Cn1c3ccccc3n1)C(C#N)=C(N)O2'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 3 4 6 24 25 26 27 28 29\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'CC#Cc1ccc(c2c(O)c(C)[nH]c2c(=O)c2cc(OC)c(OC)cc2c1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1c(C(N)=O)nc2c3c4c(sc3nc1c1)CCCC4'\n",
      "[19:04:46] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Oc2cccc(N3C(=O)NC(=O)/C(=C\\c3ccccc3)C2=O)c2)cc1'\n",
      "[19:04:46] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 8\n",
      "[19:04:46] SMILES Parse Error: extra open parentheses for input: 'COc1cc(C2/C(=C(/C#N)=C(N)OC3=NC(C)(C)Cc3ccccc32)cccc1O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 -- Batch 1/ 842, training loss 0.45569559931755066\n",
      "Epoch 7 -- Batch 2/ 842, training loss 0.45760148763656616\n",
      "Epoch 7 -- Batch 3/ 842, training loss 0.4716483950614929\n",
      "Epoch 7 -- Batch 4/ 842, training loss 0.4496441185474396\n",
      "Epoch 7 -- Batch 5/ 842, training loss 0.4595576524734497\n",
      "Epoch 7 -- Batch 6/ 842, training loss 0.4729387164115906\n",
      "Epoch 7 -- Batch 7/ 842, training loss 0.4662638306617737\n",
      "Epoch 7 -- Batch 8/ 842, training loss 0.47524675726890564\n",
      "Epoch 7 -- Batch 9/ 842, training loss 0.46441495418548584\n",
      "Epoch 7 -- Batch 10/ 842, training loss 0.4423176348209381\n",
      "Epoch 7 -- Batch 11/ 842, training loss 0.4655456840991974\n",
      "Epoch 7 -- Batch 12/ 842, training loss 0.45928412675857544\n",
      "Epoch 7 -- Batch 13/ 842, training loss 0.46625304222106934\n",
      "Epoch 7 -- Batch 14/ 842, training loss 0.44134417176246643\n",
      "Epoch 7 -- Batch 15/ 842, training loss 0.4607570767402649\n",
      "Epoch 7 -- Batch 16/ 842, training loss 0.4700063169002533\n",
      "Epoch 7 -- Batch 17/ 842, training loss 0.4612807631492615\n",
      "Epoch 7 -- Batch 18/ 842, training loss 0.44541752338409424\n",
      "Epoch 7 -- Batch 19/ 842, training loss 0.46441519260406494\n",
      "Epoch 7 -- Batch 20/ 842, training loss 0.46019142866134644\n",
      "Epoch 7 -- Batch 21/ 842, training loss 0.47637155652046204\n",
      "Epoch 7 -- Batch 22/ 842, training loss 0.46419626474380493\n",
      "Epoch 7 -- Batch 23/ 842, training loss 0.4646490514278412\n",
      "Epoch 7 -- Batch 24/ 842, training loss 0.45463865995407104\n",
      "Epoch 7 -- Batch 25/ 842, training loss 0.45440658926963806\n",
      "Epoch 7 -- Batch 26/ 842, training loss 0.46107786893844604\n",
      "Epoch 7 -- Batch 27/ 842, training loss 0.4501015245914459\n",
      "Epoch 7 -- Batch 28/ 842, training loss 0.4535348117351532\n",
      "Epoch 7 -- Batch 29/ 842, training loss 0.4693562388420105\n",
      "Epoch 7 -- Batch 30/ 842, training loss 0.4323926866054535\n",
      "Epoch 7 -- Batch 31/ 842, training loss 0.4585421681404114\n",
      "Epoch 7 -- Batch 32/ 842, training loss 0.4650384485721588\n",
      "Epoch 7 -- Batch 33/ 842, training loss 0.4702686667442322\n",
      "Epoch 7 -- Batch 34/ 842, training loss 0.4677724242210388\n",
      "Epoch 7 -- Batch 35/ 842, training loss 0.46948158740997314\n",
      "Epoch 7 -- Batch 36/ 842, training loss 0.4537688195705414\n",
      "Epoch 7 -- Batch 37/ 842, training loss 0.45019981265068054\n",
      "Epoch 7 -- Batch 38/ 842, training loss 0.4449195861816406\n",
      "Epoch 7 -- Batch 39/ 842, training loss 0.4814140200614929\n",
      "Epoch 7 -- Batch 40/ 842, training loss 0.45713573694229126\n",
      "Epoch 7 -- Batch 41/ 842, training loss 0.4383012652397156\n",
      "Epoch 7 -- Batch 42/ 842, training loss 0.4894144535064697\n",
      "Epoch 7 -- Batch 43/ 842, training loss 0.4483630657196045\n",
      "Epoch 7 -- Batch 44/ 842, training loss 0.4633011519908905\n",
      "Epoch 7 -- Batch 45/ 842, training loss 0.4449966251850128\n",
      "Epoch 7 -- Batch 46/ 842, training loss 0.4561140537261963\n",
      "Epoch 7 -- Batch 47/ 842, training loss 0.4395672380924225\n",
      "Epoch 7 -- Batch 48/ 842, training loss 0.47074094414711\n",
      "Epoch 7 -- Batch 49/ 842, training loss 0.4596386253833771\n",
      "Epoch 7 -- Batch 50/ 842, training loss 0.4789143204689026\n",
      "Epoch 7 -- Batch 51/ 842, training loss 0.4549061059951782\n",
      "Epoch 7 -- Batch 52/ 842, training loss 0.465335488319397\n",
      "Epoch 7 -- Batch 53/ 842, training loss 0.45131322741508484\n",
      "Epoch 7 -- Batch 54/ 842, training loss 0.47348272800445557\n",
      "Epoch 7 -- Batch 55/ 842, training loss 0.44991615414619446\n",
      "Epoch 7 -- Batch 56/ 842, training loss 0.4429575800895691\n",
      "Epoch 7 -- Batch 57/ 842, training loss 0.4536932408809662\n",
      "Epoch 7 -- Batch 58/ 842, training loss 0.44857749342918396\n",
      "Epoch 7 -- Batch 59/ 842, training loss 0.46022340655326843\n",
      "Epoch 7 -- Batch 60/ 842, training loss 0.45961394906044006\n",
      "Epoch 7 -- Batch 61/ 842, training loss 0.4447089731693268\n",
      "Epoch 7 -- Batch 62/ 842, training loss 0.4388349950313568\n",
      "Epoch 7 -- Batch 63/ 842, training loss 0.48680999875068665\n",
      "Epoch 7 -- Batch 64/ 842, training loss 0.47704169154167175\n",
      "Epoch 7 -- Batch 65/ 842, training loss 0.44868192076683044\n",
      "Epoch 7 -- Batch 66/ 842, training loss 0.45265814661979675\n",
      "Epoch 7 -- Batch 67/ 842, training loss 0.4726043939590454\n",
      "Epoch 7 -- Batch 68/ 842, training loss 0.4654283821582794\n",
      "Epoch 7 -- Batch 69/ 842, training loss 0.46177953481674194\n",
      "Epoch 7 -- Batch 70/ 842, training loss 0.47505855560302734\n",
      "Epoch 7 -- Batch 71/ 842, training loss 0.4694776237010956\n",
      "Epoch 7 -- Batch 72/ 842, training loss 0.46416500210762024\n",
      "Epoch 7 -- Batch 73/ 842, training loss 0.47516995668411255\n",
      "Epoch 7 -- Batch 74/ 842, training loss 0.4544430375099182\n",
      "Epoch 7 -- Batch 75/ 842, training loss 0.46219193935394287\n",
      "Epoch 7 -- Batch 76/ 842, training loss 0.452160120010376\n",
      "Epoch 7 -- Batch 77/ 842, training loss 0.4457336366176605\n",
      "Epoch 7 -- Batch 78/ 842, training loss 0.4547911286354065\n",
      "Epoch 7 -- Batch 79/ 842, training loss 0.4624672532081604\n",
      "Epoch 7 -- Batch 80/ 842, training loss 0.4612523317337036\n",
      "Epoch 7 -- Batch 81/ 842, training loss 0.47349706292152405\n",
      "Epoch 7 -- Batch 82/ 842, training loss 0.4739426374435425\n",
      "Epoch 7 -- Batch 83/ 842, training loss 0.45062175393104553\n",
      "Epoch 7 -- Batch 84/ 842, training loss 0.4678884446620941\n",
      "Epoch 7 -- Batch 85/ 842, training loss 0.4605857729911804\n",
      "Epoch 7 -- Batch 86/ 842, training loss 0.46793341636657715\n",
      "Epoch 7 -- Batch 87/ 842, training loss 0.4508965313434601\n",
      "Epoch 7 -- Batch 88/ 842, training loss 0.4657762348651886\n",
      "Epoch 7 -- Batch 89/ 842, training loss 0.4586479067802429\n",
      "Epoch 7 -- Batch 90/ 842, training loss 0.4549451470375061\n",
      "Epoch 7 -- Batch 91/ 842, training loss 0.4456063210964203\n",
      "Epoch 7 -- Batch 92/ 842, training loss 0.46903231739997864\n",
      "Epoch 7 -- Batch 93/ 842, training loss 0.4351873993873596\n",
      "Epoch 7 -- Batch 94/ 842, training loss 0.45820680260658264\n",
      "Epoch 7 -- Batch 95/ 842, training loss 0.44516047835350037\n",
      "Epoch 7 -- Batch 96/ 842, training loss 0.47651517391204834\n",
      "Epoch 7 -- Batch 97/ 842, training loss 0.4478829801082611\n",
      "Epoch 7 -- Batch 98/ 842, training loss 0.4724167585372925\n",
      "Epoch 7 -- Batch 99/ 842, training loss 0.4613488018512726\n",
      "Epoch 7 -- Batch 100/ 842, training loss 0.46989908814430237\n",
      "Epoch 7 -- Batch 101/ 842, training loss 0.4654192626476288\n",
      "Epoch 7 -- Batch 102/ 842, training loss 0.48193010687828064\n",
      "Epoch 7 -- Batch 103/ 842, training loss 0.4375627636909485\n",
      "Epoch 7 -- Batch 104/ 842, training loss 0.4530333876609802\n",
      "Epoch 7 -- Batch 105/ 842, training loss 0.472419798374176\n",
      "Epoch 7 -- Batch 106/ 842, training loss 0.4725322723388672\n",
      "Epoch 7 -- Batch 107/ 842, training loss 0.456737220287323\n",
      "Epoch 7 -- Batch 108/ 842, training loss 0.46521633863449097\n",
      "Epoch 7 -- Batch 109/ 842, training loss 0.46691322326660156\n",
      "Epoch 7 -- Batch 110/ 842, training loss 0.4567728340625763\n",
      "Epoch 7 -- Batch 111/ 842, training loss 0.45488518476486206\n",
      "Epoch 7 -- Batch 112/ 842, training loss 0.4717739522457123\n",
      "Epoch 7 -- Batch 113/ 842, training loss 0.4604584276676178\n",
      "Epoch 7 -- Batch 114/ 842, training loss 0.46915167570114136\n",
      "Epoch 7 -- Batch 115/ 842, training loss 0.4658496081829071\n",
      "Epoch 7 -- Batch 116/ 842, training loss 0.4650298058986664\n",
      "Epoch 7 -- Batch 117/ 842, training loss 0.4458274245262146\n",
      "Epoch 7 -- Batch 118/ 842, training loss 0.4768829047679901\n",
      "Epoch 7 -- Batch 119/ 842, training loss 0.46451467275619507\n",
      "Epoch 7 -- Batch 120/ 842, training loss 0.4600106477737427\n",
      "Epoch 7 -- Batch 121/ 842, training loss 0.46530649065971375\n",
      "Epoch 7 -- Batch 122/ 842, training loss 0.4626635015010834\n",
      "Epoch 7 -- Batch 123/ 842, training loss 0.47262823581695557\n",
      "Epoch 7 -- Batch 124/ 842, training loss 0.4619626998901367\n",
      "Epoch 7 -- Batch 125/ 842, training loss 0.4777792990207672\n",
      "Epoch 7 -- Batch 126/ 842, training loss 0.4617927670478821\n",
      "Epoch 7 -- Batch 127/ 842, training loss 0.43758055567741394\n",
      "Epoch 7 -- Batch 128/ 842, training loss 0.4733639061450958\n",
      "Epoch 7 -- Batch 129/ 842, training loss 0.4534757137298584\n",
      "Epoch 7 -- Batch 130/ 842, training loss 0.4520063102245331\n",
      "Epoch 7 -- Batch 131/ 842, training loss 0.45824652910232544\n",
      "Epoch 7 -- Batch 132/ 842, training loss 0.4564134180545807\n",
      "Epoch 7 -- Batch 133/ 842, training loss 0.450946182012558\n",
      "Epoch 7 -- Batch 134/ 842, training loss 0.4537198543548584\n",
      "Epoch 7 -- Batch 135/ 842, training loss 0.4563550651073456\n",
      "Epoch 7 -- Batch 136/ 842, training loss 0.4414594769477844\n",
      "Epoch 7 -- Batch 137/ 842, training loss 0.4650171101093292\n",
      "Epoch 7 -- Batch 138/ 842, training loss 0.4488561451435089\n",
      "Epoch 7 -- Batch 139/ 842, training loss 0.46257972717285156\n",
      "Epoch 7 -- Batch 140/ 842, training loss 0.45708727836608887\n",
      "Epoch 7 -- Batch 141/ 842, training loss 0.46337783336639404\n",
      "Epoch 7 -- Batch 142/ 842, training loss 0.45385244488716125\n",
      "Epoch 7 -- Batch 143/ 842, training loss 0.4597310721874237\n",
      "Epoch 7 -- Batch 144/ 842, training loss 0.464821994304657\n",
      "Epoch 7 -- Batch 145/ 842, training loss 0.45420321822166443\n",
      "Epoch 7 -- Batch 146/ 842, training loss 0.46253153681755066\n",
      "Epoch 7 -- Batch 147/ 842, training loss 0.4620262682437897\n",
      "Epoch 7 -- Batch 148/ 842, training loss 0.45716989040374756\n",
      "Epoch 7 -- Batch 149/ 842, training loss 0.47275567054748535\n",
      "Epoch 7 -- Batch 150/ 842, training loss 0.4414832293987274\n",
      "Epoch 7 -- Batch 151/ 842, training loss 0.46894294023513794\n",
      "Epoch 7 -- Batch 152/ 842, training loss 0.46258407831192017\n",
      "Epoch 7 -- Batch 153/ 842, training loss 0.4738611578941345\n",
      "Epoch 7 -- Batch 154/ 842, training loss 0.444202721118927\n",
      "Epoch 7 -- Batch 155/ 842, training loss 0.4479461908340454\n",
      "Epoch 7 -- Batch 156/ 842, training loss 0.47245413064956665\n",
      "Epoch 7 -- Batch 157/ 842, training loss 0.45314425230026245\n",
      "Epoch 7 -- Batch 158/ 842, training loss 0.4609776735305786\n",
      "Epoch 7 -- Batch 159/ 842, training loss 0.44424498081207275\n",
      "Epoch 7 -- Batch 160/ 842, training loss 0.4619212746620178\n",
      "Epoch 7 -- Batch 161/ 842, training loss 0.4709983766078949\n",
      "Epoch 7 -- Batch 162/ 842, training loss 0.4606149196624756\n",
      "Epoch 7 -- Batch 163/ 842, training loss 0.4492134153842926\n",
      "Epoch 7 -- Batch 164/ 842, training loss 0.45993560552597046\n",
      "Epoch 7 -- Batch 165/ 842, training loss 0.4665587246417999\n",
      "Epoch 7 -- Batch 166/ 842, training loss 0.46215125918388367\n",
      "Epoch 7 -- Batch 167/ 842, training loss 0.46132853627204895\n",
      "Epoch 7 -- Batch 168/ 842, training loss 0.45127207040786743\n",
      "Epoch 7 -- Batch 169/ 842, training loss 0.44921237230300903\n",
      "Epoch 7 -- Batch 170/ 842, training loss 0.4602912366390228\n",
      "Epoch 7 -- Batch 171/ 842, training loss 0.4526805877685547\n",
      "Epoch 7 -- Batch 172/ 842, training loss 0.4699425995349884\n",
      "Epoch 7 -- Batch 173/ 842, training loss 0.44095122814178467\n",
      "Epoch 7 -- Batch 174/ 842, training loss 0.4549659788608551\n",
      "Epoch 7 -- Batch 175/ 842, training loss 0.48351067304611206\n",
      "Epoch 7 -- Batch 176/ 842, training loss 0.4660716950893402\n",
      "Epoch 7 -- Batch 177/ 842, training loss 0.478575736284256\n",
      "Epoch 7 -- Batch 178/ 842, training loss 0.4568609297275543\n",
      "Epoch 7 -- Batch 179/ 842, training loss 0.46567827463150024\n",
      "Epoch 7 -- Batch 180/ 842, training loss 0.4596725106239319\n",
      "Epoch 7 -- Batch 181/ 842, training loss 0.4540867805480957\n",
      "Epoch 7 -- Batch 182/ 842, training loss 0.43490275740623474\n",
      "Epoch 7 -- Batch 183/ 842, training loss 0.4846755564212799\n",
      "Epoch 7 -- Batch 184/ 842, training loss 0.46764418482780457\n",
      "Epoch 7 -- Batch 185/ 842, training loss 0.45148566365242004\n",
      "Epoch 7 -- Batch 186/ 842, training loss 0.45830675959587097\n",
      "Epoch 7 -- Batch 187/ 842, training loss 0.48151957988739014\n",
      "Epoch 7 -- Batch 188/ 842, training loss 0.4488338828086853\n",
      "Epoch 7 -- Batch 189/ 842, training loss 0.45280036330223083\n",
      "Epoch 7 -- Batch 190/ 842, training loss 0.4619062840938568\n",
      "Epoch 7 -- Batch 191/ 842, training loss 0.4616551995277405\n",
      "Epoch 7 -- Batch 192/ 842, training loss 0.45524320006370544\n",
      "Epoch 7 -- Batch 193/ 842, training loss 0.4664453864097595\n",
      "Epoch 7 -- Batch 194/ 842, training loss 0.46386799216270447\n",
      "Epoch 7 -- Batch 195/ 842, training loss 0.4535449147224426\n",
      "Epoch 7 -- Batch 196/ 842, training loss 0.46110621094703674\n",
      "Epoch 7 -- Batch 197/ 842, training loss 0.4560457468032837\n",
      "Epoch 7 -- Batch 198/ 842, training loss 0.45804962515830994\n",
      "Epoch 7 -- Batch 199/ 842, training loss 0.47240909934043884\n",
      "Epoch 7 -- Batch 200/ 842, training loss 0.45055580139160156\n",
      "Epoch 7 -- Batch 201/ 842, training loss 0.4705713093280792\n",
      "Epoch 7 -- Batch 202/ 842, training loss 0.4437667429447174\n",
      "Epoch 7 -- Batch 203/ 842, training loss 0.4543309807777405\n",
      "Epoch 7 -- Batch 204/ 842, training loss 0.4559333026409149\n",
      "Epoch 7 -- Batch 205/ 842, training loss 0.4664134383201599\n",
      "Epoch 7 -- Batch 206/ 842, training loss 0.4483548700809479\n",
      "Epoch 7 -- Batch 207/ 842, training loss 0.4471897780895233\n",
      "Epoch 7 -- Batch 208/ 842, training loss 0.46537837386131287\n",
      "Epoch 7 -- Batch 209/ 842, training loss 0.46692436933517456\n",
      "Epoch 7 -- Batch 210/ 842, training loss 0.45375099778175354\n",
      "Epoch 7 -- Batch 211/ 842, training loss 0.48382315039634705\n",
      "Epoch 7 -- Batch 212/ 842, training loss 0.44953539967536926\n",
      "Epoch 7 -- Batch 213/ 842, training loss 0.4510434567928314\n",
      "Epoch 7 -- Batch 214/ 842, training loss 0.4698542654514313\n",
      "Epoch 7 -- Batch 215/ 842, training loss 0.45498543977737427\n",
      "Epoch 7 -- Batch 216/ 842, training loss 0.4523294270038605\n",
      "Epoch 7 -- Batch 217/ 842, training loss 0.45065605640411377\n",
      "Epoch 7 -- Batch 218/ 842, training loss 0.4579808712005615\n",
      "Epoch 7 -- Batch 219/ 842, training loss 0.4465673863887787\n",
      "Epoch 7 -- Batch 220/ 842, training loss 0.4619898796081543\n",
      "Epoch 7 -- Batch 221/ 842, training loss 0.46520400047302246\n",
      "Epoch 7 -- Batch 222/ 842, training loss 0.4498806595802307\n",
      "Epoch 7 -- Batch 223/ 842, training loss 0.4589356780052185\n",
      "Epoch 7 -- Batch 224/ 842, training loss 0.46185413002967834\n",
      "Epoch 7 -- Batch 225/ 842, training loss 0.43727484345436096\n",
      "Epoch 7 -- Batch 226/ 842, training loss 0.4529927670955658\n",
      "Epoch 7 -- Batch 227/ 842, training loss 0.45312434434890747\n",
      "Epoch 7 -- Batch 228/ 842, training loss 0.45320457220077515\n",
      "Epoch 7 -- Batch 229/ 842, training loss 0.48537954688072205\n",
      "Epoch 7 -- Batch 230/ 842, training loss 0.4604889750480652\n",
      "Epoch 7 -- Batch 231/ 842, training loss 0.44902849197387695\n",
      "Epoch 7 -- Batch 232/ 842, training loss 0.47258538007736206\n",
      "Epoch 7 -- Batch 233/ 842, training loss 0.4466857612133026\n",
      "Epoch 7 -- Batch 234/ 842, training loss 0.4688884913921356\n",
      "Epoch 7 -- Batch 235/ 842, training loss 0.4712592363357544\n",
      "Epoch 7 -- Batch 236/ 842, training loss 0.44977688789367676\n",
      "Epoch 7 -- Batch 237/ 842, training loss 0.44823935627937317\n",
      "Epoch 7 -- Batch 238/ 842, training loss 0.4537643790245056\n",
      "Epoch 7 -- Batch 239/ 842, training loss 0.4749640226364136\n",
      "Epoch 7 -- Batch 240/ 842, training loss 0.444979190826416\n",
      "Epoch 7 -- Batch 241/ 842, training loss 0.44803348183631897\n",
      "Epoch 7 -- Batch 242/ 842, training loss 0.48108482360839844\n",
      "Epoch 7 -- Batch 243/ 842, training loss 0.45353516936302185\n",
      "Epoch 7 -- Batch 244/ 842, training loss 0.45711931586265564\n",
      "Epoch 7 -- Batch 245/ 842, training loss 0.47404924035072327\n",
      "Epoch 7 -- Batch 246/ 842, training loss 0.46426254510879517\n",
      "Epoch 7 -- Batch 247/ 842, training loss 0.47156909108161926\n",
      "Epoch 7 -- Batch 248/ 842, training loss 0.4740348756313324\n",
      "Epoch 7 -- Batch 249/ 842, training loss 0.45976904034614563\n",
      "Epoch 7 -- Batch 250/ 842, training loss 0.4623873829841614\n",
      "Epoch 7 -- Batch 251/ 842, training loss 0.4686562716960907\n",
      "Epoch 7 -- Batch 252/ 842, training loss 0.455983966588974\n",
      "Epoch 7 -- Batch 253/ 842, training loss 0.4597308337688446\n",
      "Epoch 7 -- Batch 254/ 842, training loss 0.46648773550987244\n",
      "Epoch 7 -- Batch 255/ 842, training loss 0.471746563911438\n",
      "Epoch 7 -- Batch 256/ 842, training loss 0.45978060364723206\n",
      "Epoch 7 -- Batch 257/ 842, training loss 0.4582769274711609\n",
      "Epoch 7 -- Batch 258/ 842, training loss 0.46619942784309387\n",
      "Epoch 7 -- Batch 259/ 842, training loss 0.4713890850543976\n",
      "Epoch 7 -- Batch 260/ 842, training loss 0.45149874687194824\n",
      "Epoch 7 -- Batch 261/ 842, training loss 0.43650415539741516\n",
      "Epoch 7 -- Batch 262/ 842, training loss 0.4568651020526886\n",
      "Epoch 7 -- Batch 263/ 842, training loss 0.4555616080760956\n",
      "Epoch 7 -- Batch 264/ 842, training loss 0.4670407176017761\n",
      "Epoch 7 -- Batch 265/ 842, training loss 0.4492545425891876\n",
      "Epoch 7 -- Batch 266/ 842, training loss 0.47434958815574646\n",
      "Epoch 7 -- Batch 267/ 842, training loss 0.4546869695186615\n",
      "Epoch 7 -- Batch 268/ 842, training loss 0.4419920742511749\n",
      "Epoch 7 -- Batch 269/ 842, training loss 0.446457177400589\n",
      "Epoch 7 -- Batch 270/ 842, training loss 0.4775530695915222\n",
      "Epoch 7 -- Batch 271/ 842, training loss 0.46415427327156067\n",
      "Epoch 7 -- Batch 272/ 842, training loss 0.45467570424079895\n",
      "Epoch 7 -- Batch 273/ 842, training loss 0.45586124062538147\n",
      "Epoch 7 -- Batch 274/ 842, training loss 0.46517154574394226\n",
      "Epoch 7 -- Batch 275/ 842, training loss 0.469750314950943\n",
      "Epoch 7 -- Batch 276/ 842, training loss 0.4434218406677246\n",
      "Epoch 7 -- Batch 277/ 842, training loss 0.4641145169734955\n",
      "Epoch 7 -- Batch 278/ 842, training loss 0.4444137215614319\n",
      "Epoch 7 -- Batch 279/ 842, training loss 0.46050867438316345\n",
      "Epoch 7 -- Batch 280/ 842, training loss 0.4334321916103363\n",
      "Epoch 7 -- Batch 281/ 842, training loss 0.4541797935962677\n",
      "Epoch 7 -- Batch 282/ 842, training loss 0.4603850841522217\n",
      "Epoch 7 -- Batch 283/ 842, training loss 0.4593546688556671\n",
      "Epoch 7 -- Batch 284/ 842, training loss 0.4632267653942108\n",
      "Epoch 7 -- Batch 285/ 842, training loss 0.44210192561149597\n",
      "Epoch 7 -- Batch 286/ 842, training loss 0.4602277874946594\n",
      "Epoch 7 -- Batch 287/ 842, training loss 0.4653160572052002\n",
      "Epoch 7 -- Batch 288/ 842, training loss 0.4556846022605896\n",
      "Epoch 7 -- Batch 289/ 842, training loss 0.45084360241889954\n",
      "Epoch 7 -- Batch 290/ 842, training loss 0.4434065520763397\n",
      "Epoch 7 -- Batch 291/ 842, training loss 0.4500426948070526\n",
      "Epoch 7 -- Batch 292/ 842, training loss 0.45104628801345825\n",
      "Epoch 7 -- Batch 293/ 842, training loss 0.45456579327583313\n",
      "Epoch 7 -- Batch 294/ 842, training loss 0.4654068648815155\n",
      "Epoch 7 -- Batch 295/ 842, training loss 0.46893906593322754\n",
      "Epoch 7 -- Batch 296/ 842, training loss 0.455230712890625\n",
      "Epoch 7 -- Batch 297/ 842, training loss 0.4451846182346344\n",
      "Epoch 7 -- Batch 298/ 842, training loss 0.45483213663101196\n",
      "Epoch 7 -- Batch 299/ 842, training loss 0.4552781283855438\n",
      "Epoch 7 -- Batch 300/ 842, training loss 0.4404277205467224\n",
      "Epoch 7 -- Batch 301/ 842, training loss 0.46111056208610535\n",
      "Epoch 7 -- Batch 302/ 842, training loss 0.4544598162174225\n",
      "Epoch 7 -- Batch 303/ 842, training loss 0.47182342410087585\n",
      "Epoch 7 -- Batch 304/ 842, training loss 0.4642588794231415\n",
      "Epoch 7 -- Batch 305/ 842, training loss 0.4629722237586975\n",
      "Epoch 7 -- Batch 306/ 842, training loss 0.46768075227737427\n",
      "Epoch 7 -- Batch 307/ 842, training loss 0.45479562878608704\n",
      "Epoch 7 -- Batch 308/ 842, training loss 0.4597448706626892\n",
      "Epoch 7 -- Batch 309/ 842, training loss 0.4722748398780823\n",
      "Epoch 7 -- Batch 310/ 842, training loss 0.469294011592865\n",
      "Epoch 7 -- Batch 311/ 842, training loss 0.46757593750953674\n",
      "Epoch 7 -- Batch 312/ 842, training loss 0.4721831977367401\n",
      "Epoch 7 -- Batch 313/ 842, training loss 0.45649346709251404\n",
      "Epoch 7 -- Batch 314/ 842, training loss 0.4633430540561676\n",
      "Epoch 7 -- Batch 315/ 842, training loss 0.4496642053127289\n",
      "Epoch 7 -- Batch 316/ 842, training loss 0.45473548769950867\n",
      "Epoch 7 -- Batch 317/ 842, training loss 0.4596012234687805\n",
      "Epoch 7 -- Batch 318/ 842, training loss 0.4433961808681488\n",
      "Epoch 7 -- Batch 319/ 842, training loss 0.47103142738342285\n",
      "Epoch 7 -- Batch 320/ 842, training loss 0.4546125829219818\n",
      "Epoch 7 -- Batch 321/ 842, training loss 0.45048490166664124\n",
      "Epoch 7 -- Batch 322/ 842, training loss 0.4446602463722229\n",
      "Epoch 7 -- Batch 323/ 842, training loss 0.4588705599308014\n",
      "Epoch 7 -- Batch 324/ 842, training loss 0.4562399983406067\n",
      "Epoch 7 -- Batch 325/ 842, training loss 0.4498811960220337\n",
      "Epoch 7 -- Batch 326/ 842, training loss 0.44756293296813965\n",
      "Epoch 7 -- Batch 327/ 842, training loss 0.45649829506874084\n",
      "Epoch 7 -- Batch 328/ 842, training loss 0.44854429364204407\n",
      "Epoch 7 -- Batch 329/ 842, training loss 0.4610365331172943\n",
      "Epoch 7 -- Batch 330/ 842, training loss 0.4417356848716736\n",
      "Epoch 7 -- Batch 331/ 842, training loss 0.45949918031692505\n",
      "Epoch 7 -- Batch 332/ 842, training loss 0.4634357988834381\n",
      "Epoch 7 -- Batch 333/ 842, training loss 0.4509357213973999\n",
      "Epoch 7 -- Batch 334/ 842, training loss 0.46113690733909607\n",
      "Epoch 7 -- Batch 335/ 842, training loss 0.4619365930557251\n",
      "Epoch 7 -- Batch 336/ 842, training loss 0.4573124051094055\n",
      "Epoch 7 -- Batch 337/ 842, training loss 0.4569784700870514\n",
      "Epoch 7 -- Batch 338/ 842, training loss 0.45521676540374756\n",
      "Epoch 7 -- Batch 339/ 842, training loss 0.45268043875694275\n",
      "Epoch 7 -- Batch 340/ 842, training loss 0.4662328362464905\n",
      "Epoch 7 -- Batch 341/ 842, training loss 0.4563634693622589\n",
      "Epoch 7 -- Batch 342/ 842, training loss 0.4434930980205536\n",
      "Epoch 7 -- Batch 343/ 842, training loss 0.4561048448085785\n",
      "Epoch 7 -- Batch 344/ 842, training loss 0.451872318983078\n",
      "Epoch 7 -- Batch 345/ 842, training loss 0.4706652760505676\n",
      "Epoch 7 -- Batch 346/ 842, training loss 0.441729873418808\n",
      "Epoch 7 -- Batch 347/ 842, training loss 0.4368870258331299\n",
      "Epoch 7 -- Batch 348/ 842, training loss 0.45645496249198914\n",
      "Epoch 7 -- Batch 349/ 842, training loss 0.4541816711425781\n",
      "Epoch 7 -- Batch 350/ 842, training loss 0.4537244439125061\n",
      "Epoch 7 -- Batch 351/ 842, training loss 0.46189263463020325\n",
      "Epoch 7 -- Batch 352/ 842, training loss 0.4294901192188263\n",
      "Epoch 7 -- Batch 353/ 842, training loss 0.47495946288108826\n",
      "Epoch 7 -- Batch 354/ 842, training loss 0.4649829566478729\n",
      "Epoch 7 -- Batch 355/ 842, training loss 0.44940292835235596\n",
      "Epoch 7 -- Batch 356/ 842, training loss 0.46022000908851624\n",
      "Epoch 7 -- Batch 357/ 842, training loss 0.4624713063240051\n",
      "Epoch 7 -- Batch 358/ 842, training loss 0.46858397126197815\n",
      "Epoch 7 -- Batch 359/ 842, training loss 0.45355507731437683\n",
      "Epoch 7 -- Batch 360/ 842, training loss 0.45221570134162903\n",
      "Epoch 7 -- Batch 361/ 842, training loss 0.4524734318256378\n",
      "Epoch 7 -- Batch 362/ 842, training loss 0.471797913312912\n",
      "Epoch 7 -- Batch 363/ 842, training loss 0.4614454209804535\n",
      "Epoch 7 -- Batch 364/ 842, training loss 0.4729931056499481\n",
      "Epoch 7 -- Batch 365/ 842, training loss 0.453607976436615\n",
      "Epoch 7 -- Batch 366/ 842, training loss 0.4391063451766968\n",
      "Epoch 7 -- Batch 367/ 842, training loss 0.4446880519390106\n",
      "Epoch 7 -- Batch 368/ 842, training loss 0.47569385170936584\n",
      "Epoch 7 -- Batch 369/ 842, training loss 0.4387974143028259\n",
      "Epoch 7 -- Batch 370/ 842, training loss 0.46624964475631714\n",
      "Epoch 7 -- Batch 371/ 842, training loss 0.45635005831718445\n",
      "Epoch 7 -- Batch 372/ 842, training loss 0.46443942189216614\n",
      "Epoch 7 -- Batch 373/ 842, training loss 0.44264280796051025\n",
      "Epoch 7 -- Batch 374/ 842, training loss 0.44137272238731384\n",
      "Epoch 7 -- Batch 375/ 842, training loss 0.4654514193534851\n",
      "Epoch 7 -- Batch 376/ 842, training loss 0.47856229543685913\n",
      "Epoch 7 -- Batch 377/ 842, training loss 0.4674670100212097\n",
      "Epoch 7 -- Batch 378/ 842, training loss 0.4639724791049957\n",
      "Epoch 7 -- Batch 379/ 842, training loss 0.4694839119911194\n",
      "Epoch 7 -- Batch 380/ 842, training loss 0.45074141025543213\n",
      "Epoch 7 -- Batch 381/ 842, training loss 0.4573981463909149\n",
      "Epoch 7 -- Batch 382/ 842, training loss 0.46010932326316833\n",
      "Epoch 7 -- Batch 383/ 842, training loss 0.4457893669605255\n",
      "Epoch 7 -- Batch 384/ 842, training loss 0.4585283398628235\n",
      "Epoch 7 -- Batch 385/ 842, training loss 0.4560905396938324\n",
      "Epoch 7 -- Batch 386/ 842, training loss 0.4560495615005493\n",
      "Epoch 7 -- Batch 387/ 842, training loss 0.45819446444511414\n",
      "Epoch 7 -- Batch 388/ 842, training loss 0.44899287819862366\n",
      "Epoch 7 -- Batch 389/ 842, training loss 0.4530934989452362\n",
      "Epoch 7 -- Batch 390/ 842, training loss 0.46330004930496216\n",
      "Epoch 7 -- Batch 391/ 842, training loss 0.46063730120658875\n",
      "Epoch 7 -- Batch 392/ 842, training loss 0.47655656933784485\n",
      "Epoch 7 -- Batch 393/ 842, training loss 0.4691784977912903\n",
      "Epoch 7 -- Batch 394/ 842, training loss 0.48026448488235474\n",
      "Epoch 7 -- Batch 395/ 842, training loss 0.4381343424320221\n",
      "Epoch 7 -- Batch 396/ 842, training loss 0.4425410032272339\n",
      "Epoch 7 -- Batch 397/ 842, training loss 0.4653034210205078\n",
      "Epoch 7 -- Batch 398/ 842, training loss 0.4436657130718231\n",
      "Epoch 7 -- Batch 399/ 842, training loss 0.4507104456424713\n",
      "Epoch 7 -- Batch 400/ 842, training loss 0.46640488505363464\n",
      "Epoch 7 -- Batch 401/ 842, training loss 0.4551050364971161\n",
      "Epoch 7 -- Batch 402/ 842, training loss 0.4811291992664337\n",
      "Epoch 7 -- Batch 403/ 842, training loss 0.4468821883201599\n",
      "Epoch 7 -- Batch 404/ 842, training loss 0.44599393010139465\n",
      "Epoch 7 -- Batch 405/ 842, training loss 0.47023898363113403\n",
      "Epoch 7 -- Batch 406/ 842, training loss 0.4609123170375824\n",
      "Epoch 7 -- Batch 407/ 842, training loss 0.45989537239074707\n",
      "Epoch 7 -- Batch 408/ 842, training loss 0.4583446681499481\n",
      "Epoch 7 -- Batch 409/ 842, training loss 0.4619271755218506\n",
      "Epoch 7 -- Batch 410/ 842, training loss 0.454059898853302\n",
      "Epoch 7 -- Batch 411/ 842, training loss 0.46943604946136475\n",
      "Epoch 7 -- Batch 412/ 842, training loss 0.45429733395576477\n",
      "Epoch 7 -- Batch 413/ 842, training loss 0.469021201133728\n",
      "Epoch 7 -- Batch 414/ 842, training loss 0.4629761278629303\n",
      "Epoch 7 -- Batch 415/ 842, training loss 0.45604610443115234\n",
      "Epoch 7 -- Batch 416/ 842, training loss 0.44092532992362976\n",
      "Epoch 7 -- Batch 417/ 842, training loss 0.4543037414550781\n",
      "Epoch 7 -- Batch 418/ 842, training loss 0.44812026619911194\n",
      "Epoch 7 -- Batch 419/ 842, training loss 0.45493611693382263\n",
      "Epoch 7 -- Batch 420/ 842, training loss 0.46536803245544434\n",
      "Epoch 7 -- Batch 421/ 842, training loss 0.4679799973964691\n",
      "Epoch 7 -- Batch 422/ 842, training loss 0.4584202766418457\n",
      "Epoch 7 -- Batch 423/ 842, training loss 0.45548808574676514\n",
      "Epoch 7 -- Batch 424/ 842, training loss 0.4717395007610321\n",
      "Epoch 7 -- Batch 425/ 842, training loss 0.4464377164840698\n",
      "Epoch 7 -- Batch 426/ 842, training loss 0.4392956793308258\n",
      "Epoch 7 -- Batch 427/ 842, training loss 0.4381854832172394\n",
      "Epoch 7 -- Batch 428/ 842, training loss 0.4480714797973633\n",
      "Epoch 7 -- Batch 429/ 842, training loss 0.4547448456287384\n",
      "Epoch 7 -- Batch 430/ 842, training loss 0.46505412459373474\n",
      "Epoch 7 -- Batch 431/ 842, training loss 0.45147082209587097\n",
      "Epoch 7 -- Batch 432/ 842, training loss 0.44075390696525574\n",
      "Epoch 7 -- Batch 433/ 842, training loss 0.48745959997177124\n",
      "Epoch 7 -- Batch 434/ 842, training loss 0.4645858108997345\n",
      "Epoch 7 -- Batch 435/ 842, training loss 0.44594356417655945\n",
      "Epoch 7 -- Batch 436/ 842, training loss 0.4622558355331421\n",
      "Epoch 7 -- Batch 437/ 842, training loss 0.4449513256549835\n",
      "Epoch 7 -- Batch 438/ 842, training loss 0.46254125237464905\n",
      "Epoch 7 -- Batch 439/ 842, training loss 0.45274588465690613\n",
      "Epoch 7 -- Batch 440/ 842, training loss 0.47338375449180603\n",
      "Epoch 7 -- Batch 441/ 842, training loss 0.4641474485397339\n",
      "Epoch 7 -- Batch 442/ 842, training loss 0.4466921091079712\n",
      "Epoch 7 -- Batch 443/ 842, training loss 0.46225082874298096\n",
      "Epoch 7 -- Batch 444/ 842, training loss 0.4453446865081787\n",
      "Epoch 7 -- Batch 445/ 842, training loss 0.4558374583721161\n",
      "Epoch 7 -- Batch 446/ 842, training loss 0.44284316897392273\n",
      "Epoch 7 -- Batch 447/ 842, training loss 0.45826777815818787\n",
      "Epoch 7 -- Batch 448/ 842, training loss 0.46436992287635803\n",
      "Epoch 7 -- Batch 449/ 842, training loss 0.4571513235569\n",
      "Epoch 7 -- Batch 450/ 842, training loss 0.4547295570373535\n",
      "Epoch 7 -- Batch 451/ 842, training loss 0.4526260793209076\n",
      "Epoch 7 -- Batch 452/ 842, training loss 0.4490335285663605\n",
      "Epoch 7 -- Batch 453/ 842, training loss 0.4456596374511719\n",
      "Epoch 7 -- Batch 454/ 842, training loss 0.46877625584602356\n",
      "Epoch 7 -- Batch 455/ 842, training loss 0.47057923674583435\n",
      "Epoch 7 -- Batch 456/ 842, training loss 0.4647637605667114\n",
      "Epoch 7 -- Batch 457/ 842, training loss 0.4441837668418884\n",
      "Epoch 7 -- Batch 458/ 842, training loss 0.4633204936981201\n",
      "Epoch 7 -- Batch 459/ 842, training loss 0.4489935636520386\n",
      "Epoch 7 -- Batch 460/ 842, training loss 0.4763919413089752\n",
      "Epoch 7 -- Batch 461/ 842, training loss 0.4512282609939575\n",
      "Epoch 7 -- Batch 462/ 842, training loss 0.4484454095363617\n",
      "Epoch 7 -- Batch 463/ 842, training loss 0.47552070021629333\n",
      "Epoch 7 -- Batch 464/ 842, training loss 0.4467352628707886\n",
      "Epoch 7 -- Batch 465/ 842, training loss 0.4486364722251892\n",
      "Epoch 7 -- Batch 466/ 842, training loss 0.4562729299068451\n",
      "Epoch 7 -- Batch 467/ 842, training loss 0.46152716875076294\n",
      "Epoch 7 -- Batch 468/ 842, training loss 0.4587397575378418\n",
      "Epoch 7 -- Batch 469/ 842, training loss 0.4639236629009247\n",
      "Epoch 7 -- Batch 470/ 842, training loss 0.4522053003311157\n",
      "Epoch 7 -- Batch 471/ 842, training loss 0.454641193151474\n",
      "Epoch 7 -- Batch 472/ 842, training loss 0.4474557042121887\n",
      "Epoch 7 -- Batch 473/ 842, training loss 0.45781582593917847\n",
      "Epoch 7 -- Batch 474/ 842, training loss 0.46704956889152527\n",
      "Epoch 7 -- Batch 475/ 842, training loss 0.4727937877178192\n",
      "Epoch 7 -- Batch 476/ 842, training loss 0.4580143392086029\n",
      "Epoch 7 -- Batch 477/ 842, training loss 0.46699032187461853\n",
      "Epoch 7 -- Batch 478/ 842, training loss 0.45709243416786194\n",
      "Epoch 7 -- Batch 479/ 842, training loss 0.4466504752635956\n",
      "Epoch 7 -- Batch 480/ 842, training loss 0.48578670620918274\n",
      "Epoch 7 -- Batch 481/ 842, training loss 0.4756143391132355\n",
      "Epoch 7 -- Batch 482/ 842, training loss 0.450945109128952\n",
      "Epoch 7 -- Batch 483/ 842, training loss 0.44697362184524536\n",
      "Epoch 7 -- Batch 484/ 842, training loss 0.4552972912788391\n",
      "Epoch 7 -- Batch 485/ 842, training loss 0.4309921860694885\n",
      "Epoch 7 -- Batch 486/ 842, training loss 0.4476446509361267\n",
      "Epoch 7 -- Batch 487/ 842, training loss 0.4461304545402527\n",
      "Epoch 7 -- Batch 488/ 842, training loss 0.4497250020503998\n",
      "Epoch 7 -- Batch 489/ 842, training loss 0.4480507969856262\n",
      "Epoch 7 -- Batch 490/ 842, training loss 0.44566166400909424\n",
      "Epoch 7 -- Batch 491/ 842, training loss 0.4660183787345886\n",
      "Epoch 7 -- Batch 492/ 842, training loss 0.4493680000305176\n",
      "Epoch 7 -- Batch 493/ 842, training loss 0.4607923626899719\n",
      "Epoch 7 -- Batch 494/ 842, training loss 0.43832775950431824\n",
      "Epoch 7 -- Batch 495/ 842, training loss 0.4472256302833557\n",
      "Epoch 7 -- Batch 496/ 842, training loss 0.43241599202156067\n",
      "Epoch 7 -- Batch 497/ 842, training loss 0.4406842291355133\n",
      "Epoch 7 -- Batch 498/ 842, training loss 0.4562320411205292\n",
      "Epoch 7 -- Batch 499/ 842, training loss 0.44182541966438293\n",
      "Epoch 7 -- Batch 500/ 842, training loss 0.4356439709663391\n",
      "Epoch 7 -- Batch 501/ 842, training loss 0.4579645097255707\n",
      "Epoch 7 -- Batch 502/ 842, training loss 0.4515428841114044\n",
      "Epoch 7 -- Batch 503/ 842, training loss 0.4298957288265228\n",
      "Epoch 7 -- Batch 504/ 842, training loss 0.4512874484062195\n",
      "Epoch 7 -- Batch 505/ 842, training loss 0.44134652614593506\n",
      "Epoch 7 -- Batch 506/ 842, training loss 0.4491409361362457\n",
      "Epoch 7 -- Batch 507/ 842, training loss 0.45997393131256104\n",
      "Epoch 7 -- Batch 508/ 842, training loss 0.44457629323005676\n",
      "Epoch 7 -- Batch 509/ 842, training loss 0.4573841094970703\n",
      "Epoch 7 -- Batch 510/ 842, training loss 0.4638129770755768\n",
      "Epoch 7 -- Batch 511/ 842, training loss 0.45373523235321045\n",
      "Epoch 7 -- Batch 512/ 842, training loss 0.46848952770233154\n",
      "Epoch 7 -- Batch 513/ 842, training loss 0.4621620178222656\n",
      "Epoch 7 -- Batch 514/ 842, training loss 0.4598327875137329\n",
      "Epoch 7 -- Batch 515/ 842, training loss 0.45092299580574036\n",
      "Epoch 7 -- Batch 516/ 842, training loss 0.46346133947372437\n",
      "Epoch 7 -- Batch 517/ 842, training loss 0.4360576868057251\n",
      "Epoch 7 -- Batch 518/ 842, training loss 0.4561042785644531\n",
      "Epoch 7 -- Batch 519/ 842, training loss 0.4683324694633484\n",
      "Epoch 7 -- Batch 520/ 842, training loss 0.43482521176338196\n",
      "Epoch 7 -- Batch 521/ 842, training loss 0.4522719383239746\n",
      "Epoch 7 -- Batch 522/ 842, training loss 0.45742008090019226\n",
      "Epoch 7 -- Batch 523/ 842, training loss 0.46814560890197754\n",
      "Epoch 7 -- Batch 524/ 842, training loss 0.45183253288269043\n",
      "Epoch 7 -- Batch 525/ 842, training loss 0.45351123809814453\n",
      "Epoch 7 -- Batch 526/ 842, training loss 0.45428842306137085\n",
      "Epoch 7 -- Batch 527/ 842, training loss 0.45567238330841064\n",
      "Epoch 7 -- Batch 528/ 842, training loss 0.4423421621322632\n",
      "Epoch 7 -- Batch 529/ 842, training loss 0.4486469030380249\n",
      "Epoch 7 -- Batch 530/ 842, training loss 0.4591955542564392\n",
      "Epoch 7 -- Batch 531/ 842, training loss 0.4564169645309448\n",
      "Epoch 7 -- Batch 532/ 842, training loss 0.4469657838344574\n",
      "Epoch 7 -- Batch 533/ 842, training loss 0.44701313972473145\n",
      "Epoch 7 -- Batch 534/ 842, training loss 0.4456872344017029\n",
      "Epoch 7 -- Batch 535/ 842, training loss 0.48366203904151917\n",
      "Epoch 7 -- Batch 536/ 842, training loss 0.46306684613227844\n",
      "Epoch 7 -- Batch 537/ 842, training loss 0.4412595331668854\n",
      "Epoch 7 -- Batch 538/ 842, training loss 0.47514328360557556\n",
      "Epoch 7 -- Batch 539/ 842, training loss 0.447853147983551\n",
      "Epoch 7 -- Batch 540/ 842, training loss 0.4386109411716461\n",
      "Epoch 7 -- Batch 541/ 842, training loss 0.4561655819416046\n",
      "Epoch 7 -- Batch 542/ 842, training loss 0.4515440762042999\n",
      "Epoch 7 -- Batch 543/ 842, training loss 0.4517872929573059\n",
      "Epoch 7 -- Batch 544/ 842, training loss 0.4574320912361145\n",
      "Epoch 7 -- Batch 545/ 842, training loss 0.48102453351020813\n",
      "Epoch 7 -- Batch 546/ 842, training loss 0.45675748586654663\n",
      "Epoch 7 -- Batch 547/ 842, training loss 0.4399872124195099\n",
      "Epoch 7 -- Batch 548/ 842, training loss 0.4543158710002899\n",
      "Epoch 7 -- Batch 549/ 842, training loss 0.45537683367729187\n",
      "Epoch 7 -- Batch 550/ 842, training loss 0.46715760231018066\n",
      "Epoch 7 -- Batch 551/ 842, training loss 0.4638895094394684\n",
      "Epoch 7 -- Batch 552/ 842, training loss 0.4420502185821533\n",
      "Epoch 7 -- Batch 553/ 842, training loss 0.46337640285491943\n",
      "Epoch 7 -- Batch 554/ 842, training loss 0.43895870447158813\n",
      "Epoch 7 -- Batch 555/ 842, training loss 0.4433774948120117\n",
      "Epoch 7 -- Batch 556/ 842, training loss 0.45458778738975525\n",
      "Epoch 7 -- Batch 557/ 842, training loss 0.4471627473831177\n",
      "Epoch 7 -- Batch 558/ 842, training loss 0.4562147557735443\n",
      "Epoch 7 -- Batch 559/ 842, training loss 0.4519873857498169\n",
      "Epoch 7 -- Batch 560/ 842, training loss 0.44762229919433594\n",
      "Epoch 7 -- Batch 561/ 842, training loss 0.44587811827659607\n",
      "Epoch 7 -- Batch 562/ 842, training loss 0.4413411021232605\n",
      "Epoch 7 -- Batch 563/ 842, training loss 0.46311700344085693\n",
      "Epoch 7 -- Batch 564/ 842, training loss 0.4598664343357086\n",
      "Epoch 7 -- Batch 565/ 842, training loss 0.4465160667896271\n",
      "Epoch 7 -- Batch 566/ 842, training loss 0.46566295623779297\n",
      "Epoch 7 -- Batch 567/ 842, training loss 0.460849791765213\n",
      "Epoch 7 -- Batch 568/ 842, training loss 0.4593481719493866\n",
      "Epoch 7 -- Batch 569/ 842, training loss 0.4516865015029907\n",
      "Epoch 7 -- Batch 570/ 842, training loss 0.4485209584236145\n",
      "Epoch 7 -- Batch 571/ 842, training loss 0.470767080783844\n",
      "Epoch 7 -- Batch 572/ 842, training loss 0.4546278715133667\n",
      "Epoch 7 -- Batch 573/ 842, training loss 0.47087720036506653\n",
      "Epoch 7 -- Batch 574/ 842, training loss 0.46149811148643494\n",
      "Epoch 7 -- Batch 575/ 842, training loss 0.43538540601730347\n",
      "Epoch 7 -- Batch 576/ 842, training loss 0.46798038482666016\n",
      "Epoch 7 -- Batch 577/ 842, training loss 0.4555616080760956\n",
      "Epoch 7 -- Batch 578/ 842, training loss 0.4566115438938141\n",
      "Epoch 7 -- Batch 579/ 842, training loss 0.45589661598205566\n",
      "Epoch 7 -- Batch 580/ 842, training loss 0.47257232666015625\n",
      "Epoch 7 -- Batch 581/ 842, training loss 0.45832979679107666\n",
      "Epoch 7 -- Batch 582/ 842, training loss 0.45927831530570984\n",
      "Epoch 7 -- Batch 583/ 842, training loss 0.4602817893028259\n",
      "Epoch 7 -- Batch 584/ 842, training loss 0.45989468693733215\n",
      "Epoch 7 -- Batch 585/ 842, training loss 0.43991369009017944\n",
      "Epoch 7 -- Batch 586/ 842, training loss 0.4432874917984009\n",
      "Epoch 7 -- Batch 587/ 842, training loss 0.45788484811782837\n",
      "Epoch 7 -- Batch 588/ 842, training loss 0.4482100307941437\n",
      "Epoch 7 -- Batch 589/ 842, training loss 0.44880083203315735\n",
      "Epoch 7 -- Batch 590/ 842, training loss 0.47560447454452515\n",
      "Epoch 7 -- Batch 591/ 842, training loss 0.44416216015815735\n",
      "Epoch 7 -- Batch 592/ 842, training loss 0.4392552077770233\n",
      "Epoch 7 -- Batch 593/ 842, training loss 0.434689462184906\n",
      "Epoch 7 -- Batch 594/ 842, training loss 0.4735249876976013\n",
      "Epoch 7 -- Batch 595/ 842, training loss 0.44951945543289185\n",
      "Epoch 7 -- Batch 596/ 842, training loss 0.469135582447052\n",
      "Epoch 7 -- Batch 597/ 842, training loss 0.44601932168006897\n",
      "Epoch 7 -- Batch 598/ 842, training loss 0.4485788345336914\n",
      "Epoch 7 -- Batch 599/ 842, training loss 0.460419625043869\n",
      "Epoch 7 -- Batch 600/ 842, training loss 0.44037124514579773\n",
      "Epoch 7 -- Batch 601/ 842, training loss 0.45618677139282227\n",
      "Epoch 7 -- Batch 602/ 842, training loss 0.4414370059967041\n",
      "Epoch 7 -- Batch 603/ 842, training loss 0.46035686135292053\n",
      "Epoch 7 -- Batch 604/ 842, training loss 0.46758490800857544\n",
      "Epoch 7 -- Batch 605/ 842, training loss 0.4558787941932678\n",
      "Epoch 7 -- Batch 606/ 842, training loss 0.44736212491989136\n",
      "Epoch 7 -- Batch 607/ 842, training loss 0.448299765586853\n",
      "Epoch 7 -- Batch 608/ 842, training loss 0.4637567400932312\n",
      "Epoch 7 -- Batch 609/ 842, training loss 0.4534274935722351\n",
      "Epoch 7 -- Batch 610/ 842, training loss 0.44202539324760437\n",
      "Epoch 7 -- Batch 611/ 842, training loss 0.45786213874816895\n",
      "Epoch 7 -- Batch 612/ 842, training loss 0.46716994047164917\n",
      "Epoch 7 -- Batch 613/ 842, training loss 0.4624025523662567\n",
      "Epoch 7 -- Batch 614/ 842, training loss 0.465782105922699\n",
      "Epoch 7 -- Batch 615/ 842, training loss 0.45984503626823425\n",
      "Epoch 7 -- Batch 616/ 842, training loss 0.44006481766700745\n",
      "Epoch 7 -- Batch 617/ 842, training loss 0.4635026156902313\n",
      "Epoch 7 -- Batch 618/ 842, training loss 0.45508936047554016\n",
      "Epoch 7 -- Batch 619/ 842, training loss 0.4703481197357178\n",
      "Epoch 7 -- Batch 620/ 842, training loss 0.4715321362018585\n",
      "Epoch 7 -- Batch 621/ 842, training loss 0.4384070932865143\n",
      "Epoch 7 -- Batch 622/ 842, training loss 0.43485885858535767\n",
      "Epoch 7 -- Batch 623/ 842, training loss 0.44791027903556824\n",
      "Epoch 7 -- Batch 624/ 842, training loss 0.44238466024398804\n",
      "Epoch 7 -- Batch 625/ 842, training loss 0.47015151381492615\n",
      "Epoch 7 -- Batch 626/ 842, training loss 0.45241451263427734\n",
      "Epoch 7 -- Batch 627/ 842, training loss 0.4553753137588501\n",
      "Epoch 7 -- Batch 628/ 842, training loss 0.46094655990600586\n",
      "Epoch 7 -- Batch 629/ 842, training loss 0.45542410016059875\n",
      "Epoch 7 -- Batch 630/ 842, training loss 0.447118878364563\n",
      "Epoch 7 -- Batch 631/ 842, training loss 0.46439674496650696\n",
      "Epoch 7 -- Batch 632/ 842, training loss 0.4507317841053009\n",
      "Epoch 7 -- Batch 633/ 842, training loss 0.4310823976993561\n",
      "Epoch 7 -- Batch 634/ 842, training loss 0.4555045962333679\n",
      "Epoch 7 -- Batch 635/ 842, training loss 0.45147186517715454\n",
      "Epoch 7 -- Batch 636/ 842, training loss 0.46106383204460144\n",
      "Epoch 7 -- Batch 637/ 842, training loss 0.4504557251930237\n",
      "Epoch 7 -- Batch 638/ 842, training loss 0.4661008417606354\n",
      "Epoch 7 -- Batch 639/ 842, training loss 0.45671582221984863\n",
      "Epoch 7 -- Batch 640/ 842, training loss 0.4569515585899353\n",
      "Epoch 7 -- Batch 641/ 842, training loss 0.44280776381492615\n",
      "Epoch 7 -- Batch 642/ 842, training loss 0.4527512788772583\n",
      "Epoch 7 -- Batch 643/ 842, training loss 0.4691115915775299\n",
      "Epoch 7 -- Batch 644/ 842, training loss 0.44013848900794983\n",
      "Epoch 7 -- Batch 645/ 842, training loss 0.46562501788139343\n",
      "Epoch 7 -- Batch 646/ 842, training loss 0.45697104930877686\n",
      "Epoch 7 -- Batch 647/ 842, training loss 0.4405343532562256\n",
      "Epoch 7 -- Batch 648/ 842, training loss 0.47026997804641724\n",
      "Epoch 7 -- Batch 649/ 842, training loss 0.439902663230896\n",
      "Epoch 7 -- Batch 650/ 842, training loss 0.44763392210006714\n",
      "Epoch 7 -- Batch 651/ 842, training loss 0.459556519985199\n",
      "Epoch 7 -- Batch 652/ 842, training loss 0.45234039425849915\n",
      "Epoch 7 -- Batch 653/ 842, training loss 0.43580037355422974\n",
      "Epoch 7 -- Batch 654/ 842, training loss 0.4575487971305847\n",
      "Epoch 7 -- Batch 655/ 842, training loss 0.46534624695777893\n",
      "Epoch 7 -- Batch 656/ 842, training loss 0.4396913945674896\n",
      "Epoch 7 -- Batch 657/ 842, training loss 0.46122896671295166\n",
      "Epoch 7 -- Batch 658/ 842, training loss 0.4699782133102417\n",
      "Epoch 7 -- Batch 659/ 842, training loss 0.4603325128555298\n",
      "Epoch 7 -- Batch 660/ 842, training loss 0.4491942226886749\n",
      "Epoch 7 -- Batch 661/ 842, training loss 0.44845861196517944\n",
      "Epoch 7 -- Batch 662/ 842, training loss 0.4575424790382385\n",
      "Epoch 7 -- Batch 663/ 842, training loss 0.4446565806865692\n",
      "Epoch 7 -- Batch 664/ 842, training loss 0.453299343585968\n",
      "Epoch 7 -- Batch 665/ 842, training loss 0.4514533281326294\n",
      "Epoch 7 -- Batch 666/ 842, training loss 0.4652915298938751\n",
      "Epoch 7 -- Batch 667/ 842, training loss 0.503451943397522\n",
      "Epoch 7 -- Batch 668/ 842, training loss 0.4577014744281769\n",
      "Epoch 7 -- Batch 669/ 842, training loss 0.4459468424320221\n",
      "Epoch 7 -- Batch 670/ 842, training loss 0.4556167423725128\n",
      "Epoch 7 -- Batch 671/ 842, training loss 0.4354233145713806\n",
      "Epoch 7 -- Batch 672/ 842, training loss 0.42916810512542725\n",
      "Epoch 7 -- Batch 673/ 842, training loss 0.445209801197052\n",
      "Epoch 7 -- Batch 674/ 842, training loss 0.4681070148944855\n",
      "Epoch 7 -- Batch 675/ 842, training loss 0.46769529581069946\n",
      "Epoch 7 -- Batch 676/ 842, training loss 0.4596845805644989\n",
      "Epoch 7 -- Batch 677/ 842, training loss 0.4769054353237152\n",
      "Epoch 7 -- Batch 678/ 842, training loss 0.45648419857025146\n",
      "Epoch 7 -- Batch 679/ 842, training loss 0.44400641322135925\n",
      "Epoch 7 -- Batch 680/ 842, training loss 0.45995962619781494\n",
      "Epoch 7 -- Batch 681/ 842, training loss 0.46898728609085083\n",
      "Epoch 7 -- Batch 682/ 842, training loss 0.4612022340297699\n",
      "Epoch 7 -- Batch 683/ 842, training loss 0.4324598014354706\n",
      "Epoch 7 -- Batch 684/ 842, training loss 0.44051432609558105\n",
      "Epoch 7 -- Batch 685/ 842, training loss 0.42964592576026917\n",
      "Epoch 7 -- Batch 686/ 842, training loss 0.44036152958869934\n",
      "Epoch 7 -- Batch 687/ 842, training loss 0.4512823522090912\n",
      "Epoch 7 -- Batch 688/ 842, training loss 0.4465124011039734\n",
      "Epoch 7 -- Batch 689/ 842, training loss 0.44439759850502014\n",
      "Epoch 7 -- Batch 690/ 842, training loss 0.45427635312080383\n",
      "Epoch 7 -- Batch 691/ 842, training loss 0.4675721526145935\n",
      "Epoch 7 -- Batch 692/ 842, training loss 0.4552888870239258\n",
      "Epoch 7 -- Batch 693/ 842, training loss 0.457853227853775\n",
      "Epoch 7 -- Batch 694/ 842, training loss 0.44316384196281433\n",
      "Epoch 7 -- Batch 695/ 842, training loss 0.46710848808288574\n",
      "Epoch 7 -- Batch 696/ 842, training loss 0.44555824995040894\n",
      "Epoch 7 -- Batch 697/ 842, training loss 0.43956154584884644\n",
      "Epoch 7 -- Batch 698/ 842, training loss 0.428434818983078\n",
      "Epoch 7 -- Batch 699/ 842, training loss 0.44218286871910095\n",
      "Epoch 7 -- Batch 700/ 842, training loss 0.45154619216918945\n",
      "Epoch 7 -- Batch 701/ 842, training loss 0.4508315920829773\n",
      "Epoch 7 -- Batch 702/ 842, training loss 0.4416327476501465\n",
      "Epoch 7 -- Batch 703/ 842, training loss 0.4461103677749634\n",
      "Epoch 7 -- Batch 704/ 842, training loss 0.45714786648750305\n",
      "Epoch 7 -- Batch 705/ 842, training loss 0.4586661756038666\n",
      "Epoch 7 -- Batch 706/ 842, training loss 0.4567233920097351\n",
      "Epoch 7 -- Batch 707/ 842, training loss 0.43721023201942444\n",
      "Epoch 7 -- Batch 708/ 842, training loss 0.4446079134941101\n",
      "Epoch 7 -- Batch 709/ 842, training loss 0.44767242670059204\n",
      "Epoch 7 -- Batch 710/ 842, training loss 0.4746566414833069\n",
      "Epoch 7 -- Batch 711/ 842, training loss 0.4454227685928345\n",
      "Epoch 7 -- Batch 712/ 842, training loss 0.4527408182621002\n",
      "Epoch 7 -- Batch 713/ 842, training loss 0.45279431343078613\n",
      "Epoch 7 -- Batch 714/ 842, training loss 0.44195806980133057\n",
      "Epoch 7 -- Batch 715/ 842, training loss 0.4556809961795807\n",
      "Epoch 7 -- Batch 716/ 842, training loss 0.44452419877052307\n",
      "Epoch 7 -- Batch 717/ 842, training loss 0.45083919167518616\n",
      "Epoch 7 -- Batch 718/ 842, training loss 0.43652698397636414\n",
      "Epoch 7 -- Batch 719/ 842, training loss 0.4439162015914917\n",
      "Epoch 7 -- Batch 720/ 842, training loss 0.45518970489501953\n",
      "Epoch 7 -- Batch 721/ 842, training loss 0.4366474449634552\n",
      "Epoch 7 -- Batch 722/ 842, training loss 0.4609760642051697\n",
      "Epoch 7 -- Batch 723/ 842, training loss 0.45539891719818115\n",
      "Epoch 7 -- Batch 724/ 842, training loss 0.4700844883918762\n",
      "Epoch 7 -- Batch 725/ 842, training loss 0.4463879466056824\n",
      "Epoch 7 -- Batch 726/ 842, training loss 0.46316853165626526\n",
      "Epoch 7 -- Batch 727/ 842, training loss 0.44467470049858093\n",
      "Epoch 7 -- Batch 728/ 842, training loss 0.4754120111465454\n",
      "Epoch 7 -- Batch 729/ 842, training loss 0.439196914434433\n",
      "Epoch 7 -- Batch 730/ 842, training loss 0.4463386833667755\n",
      "Epoch 7 -- Batch 731/ 842, training loss 0.4656495153903961\n",
      "Epoch 7 -- Batch 732/ 842, training loss 0.442779541015625\n",
      "Epoch 7 -- Batch 733/ 842, training loss 0.4487361013889313\n",
      "Epoch 7 -- Batch 734/ 842, training loss 0.4510612189769745\n",
      "Epoch 7 -- Batch 735/ 842, training loss 0.43500152230262756\n",
      "Epoch 7 -- Batch 736/ 842, training loss 0.45071348547935486\n",
      "Epoch 7 -- Batch 737/ 842, training loss 0.45110446214675903\n",
      "Epoch 7 -- Batch 738/ 842, training loss 0.4403681457042694\n",
      "Epoch 7 -- Batch 739/ 842, training loss 0.4508553743362427\n",
      "Epoch 7 -- Batch 740/ 842, training loss 0.4311445355415344\n",
      "Epoch 7 -- Batch 741/ 842, training loss 0.4324665665626526\n",
      "Epoch 7 -- Batch 742/ 842, training loss 0.4595586359500885\n",
      "Epoch 7 -- Batch 743/ 842, training loss 0.4523239731788635\n",
      "Epoch 7 -- Batch 744/ 842, training loss 0.44225698709487915\n",
      "Epoch 7 -- Batch 745/ 842, training loss 0.4428597688674927\n",
      "Epoch 7 -- Batch 746/ 842, training loss 0.4447155296802521\n",
      "Epoch 7 -- Batch 747/ 842, training loss 0.4621548056602478\n",
      "Epoch 7 -- Batch 748/ 842, training loss 0.44934001564979553\n",
      "Epoch 7 -- Batch 749/ 842, training loss 0.45888039469718933\n",
      "Epoch 7 -- Batch 750/ 842, training loss 0.45293962955474854\n",
      "Epoch 7 -- Batch 751/ 842, training loss 0.44705361127853394\n",
      "Epoch 7 -- Batch 752/ 842, training loss 0.4645161032676697\n",
      "Epoch 7 -- Batch 753/ 842, training loss 0.43711283802986145\n",
      "Epoch 7 -- Batch 754/ 842, training loss 0.47454485297203064\n",
      "Epoch 7 -- Batch 755/ 842, training loss 0.4633691608905792\n",
      "Epoch 7 -- Batch 756/ 842, training loss 0.4552972614765167\n",
      "Epoch 7 -- Batch 757/ 842, training loss 0.4439120590686798\n",
      "Epoch 7 -- Batch 758/ 842, training loss 0.44330981373786926\n",
      "Epoch 7 -- Batch 759/ 842, training loss 0.45396775007247925\n",
      "Epoch 7 -- Batch 760/ 842, training loss 0.43799301981925964\n",
      "Epoch 7 -- Batch 761/ 842, training loss 0.44194918870925903\n",
      "Epoch 7 -- Batch 762/ 842, training loss 0.4572903513908386\n",
      "Epoch 7 -- Batch 763/ 842, training loss 0.4501403570175171\n",
      "Epoch 7 -- Batch 764/ 842, training loss 0.4465022683143616\n",
      "Epoch 7 -- Batch 765/ 842, training loss 0.46765050292015076\n",
      "Epoch 7 -- Batch 766/ 842, training loss 0.449372798204422\n",
      "Epoch 7 -- Batch 767/ 842, training loss 0.4462328553199768\n",
      "Epoch 7 -- Batch 768/ 842, training loss 0.46123117208480835\n",
      "Epoch 7 -- Batch 769/ 842, training loss 0.43150612711906433\n",
      "Epoch 7 -- Batch 770/ 842, training loss 0.45342743396759033\n",
      "Epoch 7 -- Batch 771/ 842, training loss 0.44531941413879395\n",
      "Epoch 7 -- Batch 772/ 842, training loss 0.44690611958503723\n",
      "Epoch 7 -- Batch 773/ 842, training loss 0.43858540058135986\n",
      "Epoch 7 -- Batch 774/ 842, training loss 0.44429323077201843\n",
      "Epoch 7 -- Batch 775/ 842, training loss 0.43834438920021057\n",
      "Epoch 7 -- Batch 776/ 842, training loss 0.46300771832466125\n",
      "Epoch 7 -- Batch 777/ 842, training loss 0.44373735785484314\n",
      "Epoch 7 -- Batch 778/ 842, training loss 0.44102272391319275\n",
      "Epoch 7 -- Batch 779/ 842, training loss 0.456625759601593\n",
      "Epoch 7 -- Batch 780/ 842, training loss 0.4502682387828827\n",
      "Epoch 7 -- Batch 781/ 842, training loss 0.4498215615749359\n",
      "Epoch 7 -- Batch 782/ 842, training loss 0.44686028361320496\n",
      "Epoch 7 -- Batch 783/ 842, training loss 0.46218207478523254\n",
      "Epoch 7 -- Batch 784/ 842, training loss 0.4452233910560608\n",
      "Epoch 7 -- Batch 785/ 842, training loss 0.4474765956401825\n",
      "Epoch 7 -- Batch 786/ 842, training loss 0.44987624883651733\n",
      "Epoch 7 -- Batch 787/ 842, training loss 0.4583967626094818\n",
      "Epoch 7 -- Batch 788/ 842, training loss 0.4500969350337982\n",
      "Epoch 7 -- Batch 789/ 842, training loss 0.4540434777736664\n",
      "Epoch 7 -- Batch 790/ 842, training loss 0.461237370967865\n",
      "Epoch 7 -- Batch 791/ 842, training loss 0.45022934675216675\n",
      "Epoch 7 -- Batch 792/ 842, training loss 0.4564502537250519\n",
      "Epoch 7 -- Batch 793/ 842, training loss 0.4599282741546631\n",
      "Epoch 7 -- Batch 794/ 842, training loss 0.45891231298446655\n",
      "Epoch 7 -- Batch 795/ 842, training loss 0.44932955503463745\n",
      "Epoch 7 -- Batch 796/ 842, training loss 0.4571440815925598\n",
      "Epoch 7 -- Batch 797/ 842, training loss 0.46525654196739197\n",
      "Epoch 7 -- Batch 798/ 842, training loss 0.44992998242378235\n",
      "Epoch 7 -- Batch 799/ 842, training loss 0.45138511061668396\n",
      "Epoch 7 -- Batch 800/ 842, training loss 0.44823721051216125\n",
      "Epoch 7 -- Batch 801/ 842, training loss 0.45216676592826843\n",
      "Epoch 7 -- Batch 802/ 842, training loss 0.4498676657676697\n",
      "Epoch 7 -- Batch 803/ 842, training loss 0.4519522786140442\n",
      "Epoch 7 -- Batch 804/ 842, training loss 0.47299090027809143\n",
      "Epoch 7 -- Batch 805/ 842, training loss 0.4398539066314697\n",
      "Epoch 7 -- Batch 806/ 842, training loss 0.46058890223503113\n",
      "Epoch 7 -- Batch 807/ 842, training loss 0.45919373631477356\n",
      "Epoch 7 -- Batch 808/ 842, training loss 0.44102734327316284\n",
      "Epoch 7 -- Batch 809/ 842, training loss 0.44355979561805725\n",
      "Epoch 7 -- Batch 810/ 842, training loss 0.4487488269805908\n",
      "Epoch 7 -- Batch 811/ 842, training loss 0.4501762092113495\n",
      "Epoch 7 -- Batch 812/ 842, training loss 0.4668789207935333\n",
      "Epoch 7 -- Batch 813/ 842, training loss 0.45180970430374146\n",
      "Epoch 7 -- Batch 814/ 842, training loss 0.46154946088790894\n",
      "Epoch 7 -- Batch 815/ 842, training loss 0.4513114094734192\n",
      "Epoch 7 -- Batch 816/ 842, training loss 0.4522903263568878\n",
      "Epoch 7 -- Batch 817/ 842, training loss 0.4589485824108124\n",
      "Epoch 7 -- Batch 818/ 842, training loss 0.45878052711486816\n",
      "Epoch 7 -- Batch 819/ 842, training loss 0.44313693046569824\n",
      "Epoch 7 -- Batch 820/ 842, training loss 0.4467658996582031\n",
      "Epoch 7 -- Batch 821/ 842, training loss 0.43253374099731445\n",
      "Epoch 7 -- Batch 822/ 842, training loss 0.43487003445625305\n",
      "Epoch 7 -- Batch 823/ 842, training loss 0.45862171053886414\n",
      "Epoch 7 -- Batch 824/ 842, training loss 0.4602327346801758\n",
      "Epoch 7 -- Batch 825/ 842, training loss 0.4742495119571686\n",
      "Epoch 7 -- Batch 826/ 842, training loss 0.4440012276172638\n",
      "Epoch 7 -- Batch 827/ 842, training loss 0.4499487280845642\n",
      "Epoch 7 -- Batch 828/ 842, training loss 0.44066211581230164\n",
      "Epoch 7 -- Batch 829/ 842, training loss 0.43712472915649414\n",
      "Epoch 7 -- Batch 830/ 842, training loss 0.44202563166618347\n",
      "Epoch 7 -- Batch 831/ 842, training loss 0.45966193079948425\n",
      "Epoch 7 -- Batch 832/ 842, training loss 0.458894819021225\n",
      "Epoch 7 -- Batch 833/ 842, training loss 0.44672298431396484\n",
      "Epoch 7 -- Batch 834/ 842, training loss 0.449880987405777\n",
      "Epoch 7 -- Batch 835/ 842, training loss 0.4499019384384155\n",
      "Epoch 7 -- Batch 836/ 842, training loss 0.45421648025512695\n",
      "Epoch 7 -- Batch 837/ 842, training loss 0.45554324984550476\n",
      "Epoch 7 -- Batch 838/ 842, training loss 0.45797279477119446\n",
      "Epoch 7 -- Batch 839/ 842, training loss 0.44032400846481323\n",
      "Epoch 7 -- Batch 840/ 842, training loss 0.4717329144477844\n",
      "Epoch 7 -- Batch 841/ 842, training loss 0.434935063123703\n",
      "Epoch 7 -- Batch 842/ 842, training loss 0.493912935256958\n",
      "----------------------------------------------------------------------\n",
      "Epoch 7 -- Batch 1/ 94, validation loss 0.43537256121635437\n",
      "Epoch 7 -- Batch 2/ 94, validation loss 0.43221044540405273\n",
      "Epoch 7 -- Batch 3/ 94, validation loss 0.46894654631614685\n",
      "Epoch 7 -- Batch 4/ 94, validation loss 0.4453628361225128\n",
      "Epoch 7 -- Batch 5/ 94, validation loss 0.42895445227622986\n",
      "Epoch 7 -- Batch 6/ 94, validation loss 0.44449636340141296\n",
      "Epoch 7 -- Batch 7/ 94, validation loss 0.44655025005340576\n",
      "Epoch 7 -- Batch 8/ 94, validation loss 0.4309309720993042\n",
      "Epoch 7 -- Batch 9/ 94, validation loss 0.4442512094974518\n",
      "Epoch 7 -- Batch 10/ 94, validation loss 0.4515077769756317\n",
      "Epoch 7 -- Batch 11/ 94, validation loss 0.44055619835853577\n",
      "Epoch 7 -- Batch 12/ 94, validation loss 0.4422532916069031\n",
      "Epoch 7 -- Batch 13/ 94, validation loss 0.45298439264297485\n",
      "Epoch 7 -- Batch 14/ 94, validation loss 0.44986510276794434\n",
      "Epoch 7 -- Batch 15/ 94, validation loss 0.44899457693099976\n",
      "Epoch 7 -- Batch 16/ 94, validation loss 0.45832356810569763\n",
      "Epoch 7 -- Batch 17/ 94, validation loss 0.43952706456184387\n",
      "Epoch 7 -- Batch 18/ 94, validation loss 0.43033817410469055\n",
      "Epoch 7 -- Batch 19/ 94, validation loss 0.44539037346839905\n",
      "Epoch 7 -- Batch 20/ 94, validation loss 0.44614937901496887\n",
      "Epoch 7 -- Batch 21/ 94, validation loss 0.45682215690612793\n",
      "Epoch 7 -- Batch 22/ 94, validation loss 0.413198858499527\n",
      "Epoch 7 -- Batch 23/ 94, validation loss 0.4445021450519562\n",
      "Epoch 7 -- Batch 24/ 94, validation loss 0.44426050782203674\n",
      "Epoch 7 -- Batch 25/ 94, validation loss 0.45261913537979126\n",
      "Epoch 7 -- Batch 26/ 94, validation loss 0.4482302665710449\n",
      "Epoch 7 -- Batch 27/ 94, validation loss 0.44666004180908203\n",
      "Epoch 7 -- Batch 28/ 94, validation loss 0.44767749309539795\n",
      "Epoch 7 -- Batch 29/ 94, validation loss 0.4342052936553955\n",
      "Epoch 7 -- Batch 30/ 94, validation loss 0.4458809792995453\n",
      "Epoch 7 -- Batch 31/ 94, validation loss 0.42658185958862305\n",
      "Epoch 7 -- Batch 32/ 94, validation loss 0.4544384181499481\n",
      "Epoch 7 -- Batch 33/ 94, validation loss 0.43725666403770447\n",
      "Epoch 7 -- Batch 34/ 94, validation loss 0.4258630573749542\n",
      "Epoch 7 -- Batch 35/ 94, validation loss 0.45985156297683716\n",
      "Epoch 7 -- Batch 36/ 94, validation loss 0.4296132028102875\n",
      "Epoch 7 -- Batch 37/ 94, validation loss 0.4652189612388611\n",
      "Epoch 7 -- Batch 38/ 94, validation loss 0.4563090205192566\n",
      "Epoch 7 -- Batch 39/ 94, validation loss 0.44679248332977295\n",
      "Epoch 7 -- Batch 40/ 94, validation loss 0.4521467089653015\n",
      "Epoch 7 -- Batch 41/ 94, validation loss 0.4518307149410248\n",
      "Epoch 7 -- Batch 42/ 94, validation loss 0.45593371987342834\n",
      "Epoch 7 -- Batch 43/ 94, validation loss 0.43036356568336487\n",
      "Epoch 7 -- Batch 44/ 94, validation loss 0.44323796033859253\n",
      "Epoch 7 -- Batch 45/ 94, validation loss 0.4278409481048584\n",
      "Epoch 7 -- Batch 46/ 94, validation loss 0.42954298853874207\n",
      "Epoch 7 -- Batch 47/ 94, validation loss 0.4479236900806427\n",
      "Epoch 7 -- Batch 48/ 94, validation loss 0.44200295209884644\n",
      "Epoch 7 -- Batch 49/ 94, validation loss 0.43245068192481995\n",
      "Epoch 7 -- Batch 50/ 94, validation loss 0.4566778242588043\n",
      "Epoch 7 -- Batch 51/ 94, validation loss 0.453237920999527\n",
      "Epoch 7 -- Batch 52/ 94, validation loss 0.439683735370636\n",
      "Epoch 7 -- Batch 53/ 94, validation loss 0.46349114179611206\n",
      "Epoch 7 -- Batch 54/ 94, validation loss 0.4485737979412079\n",
      "Epoch 7 -- Batch 55/ 94, validation loss 0.4463789463043213\n",
      "Epoch 7 -- Batch 56/ 94, validation loss 0.45923861861228943\n",
      "Epoch 7 -- Batch 57/ 94, validation loss 0.44115689396858215\n",
      "Epoch 7 -- Batch 58/ 94, validation loss 0.44396358728408813\n",
      "Epoch 7 -- Batch 59/ 94, validation loss 0.45867887139320374\n",
      "Epoch 7 -- Batch 60/ 94, validation loss 0.44657641649246216\n",
      "Epoch 7 -- Batch 61/ 94, validation loss 0.4704975187778473\n",
      "Epoch 7 -- Batch 62/ 94, validation loss 0.46010661125183105\n",
      "Epoch 7 -- Batch 63/ 94, validation loss 0.4358440935611725\n",
      "Epoch 7 -- Batch 64/ 94, validation loss 0.44894322752952576\n",
      "Epoch 7 -- Batch 65/ 94, validation loss 0.4650341272354126\n",
      "Epoch 7 -- Batch 66/ 94, validation loss 0.44969406723976135\n",
      "Epoch 7 -- Batch 67/ 94, validation loss 0.4342583119869232\n",
      "Epoch 7 -- Batch 68/ 94, validation loss 0.4381208121776581\n",
      "Epoch 7 -- Batch 69/ 94, validation loss 0.44444411993026733\n",
      "Epoch 7 -- Batch 70/ 94, validation loss 0.435031533241272\n",
      "Epoch 7 -- Batch 71/ 94, validation loss 0.43125614523887634\n",
      "Epoch 7 -- Batch 72/ 94, validation loss 0.45975369215011597\n",
      "Epoch 7 -- Batch 73/ 94, validation loss 0.44853475689888\n",
      "Epoch 7 -- Batch 74/ 94, validation loss 0.45272964239120483\n",
      "Epoch 7 -- Batch 75/ 94, validation loss 0.4466201364994049\n",
      "Epoch 7 -- Batch 76/ 94, validation loss 0.4477880001068115\n",
      "Epoch 7 -- Batch 77/ 94, validation loss 0.4321456551551819\n",
      "Epoch 7 -- Batch 78/ 94, validation loss 0.436389297246933\n",
      "Epoch 7 -- Batch 79/ 94, validation loss 0.4326224625110626\n",
      "Epoch 7 -- Batch 80/ 94, validation loss 0.44548895955085754\n",
      "Epoch 7 -- Batch 81/ 94, validation loss 0.439992755651474\n",
      "Epoch 7 -- Batch 82/ 94, validation loss 0.4533412754535675\n",
      "Epoch 7 -- Batch 83/ 94, validation loss 0.4405391812324524\n",
      "Epoch 7 -- Batch 84/ 94, validation loss 0.46345508098602295\n",
      "Epoch 7 -- Batch 85/ 94, validation loss 0.45446014404296875\n",
      "Epoch 7 -- Batch 86/ 94, validation loss 0.4337177276611328\n",
      "Epoch 7 -- Batch 87/ 94, validation loss 0.43842577934265137\n",
      "Epoch 7 -- Batch 88/ 94, validation loss 0.4487937390804291\n",
      "Epoch 7 -- Batch 89/ 94, validation loss 0.44443273544311523\n",
      "Epoch 7 -- Batch 90/ 94, validation loss 0.4413839280605316\n",
      "Epoch 7 -- Batch 91/ 94, validation loss 0.4368560314178467\n",
      "Epoch 7 -- Batch 92/ 94, validation loss 0.44260165095329285\n",
      "Epoch 7 -- Batch 93/ 94, validation loss 0.4383038580417633\n",
      "Epoch 7 -- Batch 94/ 94, validation loss 0.44100266695022583\n",
      "----------------------------------------------------------------------\n",
      "Epoch 7 loss: Training 0.4560558795928955, Validation 0.44100266695022583\n",
      "----------------------------------------------------------------------\n",
      "Epoch 8/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:04:57] SMILES Parse Error: extra open parentheses for input: 'c1cc2c(c3c1CCN(Cc1ccco1)C(=O)CO2'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 13 14 15 17 18\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 16 18\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 5 6 7 16 17 18 19 20 21\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 16 17 19 20 21\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 21 23\n",
      "[19:04:57] Explicit valence for atom # 12 Cl, 2, is greater than permitted\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1cccc(OCCOc2ccc(/C=C3/C(=N)N4N=C5C=CSC4=NC3=O)cc2)c1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)n(CCc2nnc(CCC(=O)NCC3CC4)n(C)c2=O)n1'\n",
      "[19:04:57] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(-c2nc3n(n2)C(c2ccc(OC)cc2)C(C(=O)Nc2ccc(Cl)c(C(=O)O)c2)c2ccccc12'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2nc3c(c(=O)o1)CCCC3)C(F)(F)F'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c([nH]c1c1)[C@@H](CO)N(C(=O)CN(C)C)CC3'\n",
      "[19:04:57] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1cc(=O)[nH]c2nnc(SCC(=O)Nc3nccc3-c4ccccc4)nc3c12'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1[C@@H]1CC[C@@H]2Cc3c(cc(NC(=O)C4CCOCC4)c3C)N3C1=O'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1sc2c3c(cccc24)C(c2ccccc2C(F)(F)F)n1C'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 7 11\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CC(Sc1nc2sc3c(c2c(=O)[nH]1)CCCCC3)C(N)C(C)C2'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'N#Cc1ccc(C(=O)N2CCCC(c3nc(-c4cccs4)no3)CC2)c2c1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C2(C)n3c(CCC4=O)no3)n(C)c2cc1C#O'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2c(-c3ccccc3)c(C#N)c(SCC(=O)Nc3nccs3)nc2ccccc23)cc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'O=C(C[C@H]1C=C[C@@H](N(C)C)CC[C@@H]1C1)c1ccccc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1cccc(NC(=O)C2CC2CCC2C3c2ccccc2F)c1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 15 16 17 27 28\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 6 25\n",
      "[19:04:57] SMILES Parse Error: ring closure 1 duplicates bond between atom 29 and atom 30 for input: 'CCOC(=O)c1c(NC(=O)CSc2nc(C(C)C)cs1)c(-c2ccc(OC)cc2)n1-c1cccc(C)c1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CCC1CCCCN1C(=O)C1CC1(c1cccc(OC)c1)c1ccccc12'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 22 23 24\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 4 5 6 7 9 10 11 21\n",
      "[19:04:57] Explicit valence for atom # 12 N, 4, is greater than permitted\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(=O)NC2CC2CCC(CNC(=O)Nc2ccnn3)CC2)cc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Cn2ccc3c(SCC4CO5)nc(-c4ccccc4Cl)nc32)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 5 6 8\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 13 14 15 16 17 18\n",
      "[19:04:57] SMILES Parse Error: extra close parentheses while parsing: CC1CCN(c2ccc3nc(SCC(=O)NCc4ccccc4)C4)c(=O)[nH]c2=O)CC1\n",
      "[19:04:57] SMILES Parse Error: Failed parsing SMILES 'CC1CCN(c2ccc3nc(SCC(=O)NCc4ccccc4)C4)c(=O)[nH]c2=O)CC1' for input: 'CC1CCN(c2ccc3nc(SCC(=O)NCc4ccccc4)C4)c(=O)[nH]c2=O)CC1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1CCN(C1=NS(=O)(=O)c2ccc([N+](=O)[O-])cc2)CC1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccsc1CN1CCC2(CCC(C(=O)NCC3CCCO3)O2)CC2'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 3 10 11 18\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 29 30\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 17 18 20 21 22 23\n",
      "[19:04:57] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[19:04:57] Explicit valence for atom # 9 O, 4, is greater than permitted\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 18 19 20\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 12\n",
      "[19:04:57] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 6 7 17 18 19 20 30 31 32\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-n2c(C)cc(/C=C3\\C(=N)N4C(=S)NC(C)=C4C(=O)OC4=NC3=O)c2C)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 6 8\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 21 22\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 20 21\n",
      "[19:04:57] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1c2c(nn1-c1cccc(F)c1CCN2CCOCC2)c1ccccc1Oc1ccccc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 3 4 33 34\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 5 6 8 23 24 25 26 27 28\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2c(c1)c1ccccc1[N+](=O)[O-])c1ccccc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(C(COC(=O)NCC2FCCOC2)O2)c1ccccc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CC1(C)C2CC(C(=O)Nc3cccc(Br)c3)(C(=O)N2Cc3ccccc3C1=O)N2'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(cc1OC)CC(C(=O)Nc1cnc(C3CCCC3)cc1)(CO2)C2'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCN2C(=O)CCC2=C2C=C(C)SC3)c(OCCOC)c1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 25 27\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 7 8 9 26 27 28 29 30 39\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'C[C@H](CO)N1C[C@H](C)[C@@H](CN(C)C(=O)C2CC2)Oc2c(NC(=O)NCC3CC4)cccc2C1=O'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 19 20 22\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CC(C)c1ccc(NC(=O)COC(=O)c2ccc3c(c2)NC(=O)C(CO)=C3SC3)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[19:04:57] SMILES Parse Error: ring closure 5 duplicates bond between atom 16 and atom 17 for input: 'CCC(=O)c1ccc(N2CCc3nc4nc(C5c5c[nH]c(=O)[nH]c5c4)[nH]c(=O)n3C2)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 6 7 13 14 15\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 16 17 22 23 24\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(NC(=O)CSc3nnc(-c4c(C)oc(C)n5C4CC4)s3)sc2c1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COC1CCCc2c1cc(Br)c2c(=O)c(-c3ccc(C)cc3)c(C)oc2c1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)[C@H]2[C@@H](c3ccc(-c4ccc(F)cc4)cc3)N[C@@H]3C(=O)N(Cc3ccccc3)C(=O)[C@@H]21'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'OCc1ccc2c(c1)C1CC=CCC1C2(C(=O)Nc1cccc(C(F)(F)F)c1)C2=O'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 8 24\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2[nH]c4nc(SCC(=O)NCc5ccccc5)ncnc3c3c1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C2CC(=O)C3=C(C2)Nc2nc(C(F)(F)F)cc(Br)n2)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 4 5 7 8 13 20 21\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'OCC12CC3CC(C1)CC(n1nnc(-c3ccco3)c1-c1ccc(Cl)cc1)[C@@H]2O'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1cc2nc3c(cn2C)c(=O)n(-c2ccccc2)c1N'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 4 5 12 21 22\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 8 10 11 12 13 14 15\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 3 4 5 17 20\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COCC(=O)N[C@H]1CC[C@@]23CCCN2C(=O)c3cc(F)ccc3N2C(=O)[C@H]1O'\n",
      "[19:04:57] SMILES Parse Error: extra close parentheses while parsing: N/C(Nc1cccc([N+](=O)[O-])c1)C(=O)O)c1ccc(Br)cc1\n",
      "[19:04:57] SMILES Parse Error: Failed parsing SMILES 'N/C(Nc1cccc([N+](=O)[O-])c1)C(=O)O)c1ccc(Br)cc1' for input: 'N/C(Nc1cccc([N+](=O)[O-])c1)C(=O)O)c1ccc(Br)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 21 22\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Nn1c(=NC(=O)CCCl)cs2'\n",
      "[19:04:57] SMILES Parse Error: syntax error while parsing: C/C(C)=C\\/C=C\\C(=O)NCc1ccccc1OC\n",
      "[19:04:57] SMILES Parse Error: Failed parsing SMILES 'C/C(C)=C\\/C=C\\C(=O)NCc1ccccc1OC' for input: 'C/C(C)=C\\/C=C\\C(=O)NCc1ccccc1OC'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 13 14 16\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(N2CCn3c2cnc(C)c2CN(C(C)=O)C3)c1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 16 17 18 19 20 22 23 24 25\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(C)oc2ccc(OC(=O)c3cccc(OC)c3)c1N2CCN(C)CC1'\n",
      "[19:04:57] SMILES Parse Error: extra open parentheses for input: 'O=C1CC(c2ccccc2)CC2=C1C(c1ccc(Cl)cc1F)NC2=C(C(=O)C1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 18 20\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 6 7 23\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 14 15 20\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(CN2C(=S)NC(C)(C3CC2)C2=O)cc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 3 25\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 22 23 24\n",
      "[19:04:57] SMILES Parse Error: extra open parentheses for input: 'Cc1cccc(NC(=O)Nc2cccn3c(C2CCCO3)nc1-c1ccccc1'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 3 4 15 16 17 37 38\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'COC(=O)CCCn1c(=O)c(-c2ccccc2)c(O)n(N1CC(C)CC(C)C1)c1ccc(F)cc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CCOCCCNC(=O)CC1S/C(=N2N=C/c2ccccc2Cl)N=C(N(C)C)C1=O'\n",
      "[19:04:57] Can't kekulize mol.  Unkekulized atoms: 2 12 19 20 21 22 23 24 25\n",
      "[19:04:57] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[19:04:57] Explicit valence for atom # 7 Cl, 2, is greater than permitted\n",
      "[19:04:57] SMILES Parse Error: extra close parentheses while parsing: Cc1nc(-c2ccc(Cl)cc2)c(N2CCN(C(=O)c3c(F)c(F)ccc3F)CC2)n1)c1ccccc1\n",
      "[19:04:57] SMILES Parse Error: Failed parsing SMILES 'Cc1nc(-c2ccc(Cl)cc2)c(N2CCN(C(=O)c3c(F)c(F)ccc3F)CC2)n1)c1ccccc1' for input: 'Cc1nc(-c2ccc(Cl)cc2)c(N2CCN(C(=O)c3c(F)c(F)ccc3F)CC2)n1)c1ccccc1'\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CN(C)c1ccc(N/O)c(=O)c1ccccc1O'\n",
      "[19:04:57] non-ring atom 22 marked aromatic\n",
      "[19:04:57] SMILES Parse Error: unclosed ring for input: 'CC1(C)C(=O)N2C(=C(C3C#C)SCC(=O)N2Cc2cc(OC)ccc2OC)c2ccccc21'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 -- Batch 1/ 842, training loss 0.42428427934646606\n",
      "Epoch 8 -- Batch 2/ 842, training loss 0.45552051067352295\n",
      "Epoch 8 -- Batch 3/ 842, training loss 0.45405200123786926\n",
      "Epoch 8 -- Batch 4/ 842, training loss 0.44858989119529724\n",
      "Epoch 8 -- Batch 5/ 842, training loss 0.4605257213115692\n",
      "Epoch 8 -- Batch 6/ 842, training loss 0.44884026050567627\n",
      "Epoch 8 -- Batch 7/ 842, training loss 0.44002315402030945\n",
      "Epoch 8 -- Batch 8/ 842, training loss 0.43489113450050354\n",
      "Epoch 8 -- Batch 9/ 842, training loss 0.44847923517227173\n",
      "Epoch 8 -- Batch 10/ 842, training loss 0.4423667788505554\n",
      "Epoch 8 -- Batch 11/ 842, training loss 0.4344923794269562\n",
      "Epoch 8 -- Batch 12/ 842, training loss 0.4598734676837921\n",
      "Epoch 8 -- Batch 13/ 842, training loss 0.45212602615356445\n",
      "Epoch 8 -- Batch 14/ 842, training loss 0.44335272908210754\n",
      "Epoch 8 -- Batch 15/ 842, training loss 0.46586400270462036\n",
      "Epoch 8 -- Batch 16/ 842, training loss 0.4415931701660156\n",
      "Epoch 8 -- Batch 17/ 842, training loss 0.44421011209487915\n",
      "Epoch 8 -- Batch 18/ 842, training loss 0.43948277831077576\n",
      "Epoch 8 -- Batch 19/ 842, training loss 0.4331330358982086\n",
      "Epoch 8 -- Batch 20/ 842, training loss 0.4341965615749359\n",
      "Epoch 8 -- Batch 21/ 842, training loss 0.4354357421398163\n",
      "Epoch 8 -- Batch 22/ 842, training loss 0.4618462920188904\n",
      "Epoch 8 -- Batch 23/ 842, training loss 0.4300656318664551\n",
      "Epoch 8 -- Batch 24/ 842, training loss 0.43554964661598206\n",
      "Epoch 8 -- Batch 25/ 842, training loss 0.44370096921920776\n",
      "Epoch 8 -- Batch 26/ 842, training loss 0.4489547908306122\n",
      "Epoch 8 -- Batch 27/ 842, training loss 0.43498674035072327\n",
      "Epoch 8 -- Batch 28/ 842, training loss 0.4378150701522827\n",
      "Epoch 8 -- Batch 29/ 842, training loss 0.4333004057407379\n",
      "Epoch 8 -- Batch 30/ 842, training loss 0.41163673996925354\n",
      "Epoch 8 -- Batch 31/ 842, training loss 0.4280814230442047\n",
      "Epoch 8 -- Batch 32/ 842, training loss 0.43926867842674255\n",
      "Epoch 8 -- Batch 33/ 842, training loss 0.4419863820075989\n",
      "Epoch 8 -- Batch 34/ 842, training loss 0.42072418332099915\n",
      "Epoch 8 -- Batch 35/ 842, training loss 0.450279176235199\n",
      "Epoch 8 -- Batch 36/ 842, training loss 0.4379654824733734\n",
      "Epoch 8 -- Batch 37/ 842, training loss 0.4389097988605499\n",
      "Epoch 8 -- Batch 38/ 842, training loss 0.44366636872291565\n",
      "Epoch 8 -- Batch 39/ 842, training loss 0.4501148760318756\n",
      "Epoch 8 -- Batch 40/ 842, training loss 0.4373011291027069\n",
      "Epoch 8 -- Batch 41/ 842, training loss 0.426847368478775\n",
      "Epoch 8 -- Batch 42/ 842, training loss 0.46654292941093445\n",
      "Epoch 8 -- Batch 43/ 842, training loss 0.414029598236084\n",
      "Epoch 8 -- Batch 44/ 842, training loss 0.42730095982551575\n",
      "Epoch 8 -- Batch 45/ 842, training loss 0.43066778779029846\n",
      "Epoch 8 -- Batch 46/ 842, training loss 0.45419642329216003\n",
      "Epoch 8 -- Batch 47/ 842, training loss 0.4416652023792267\n",
      "Epoch 8 -- Batch 48/ 842, training loss 0.4503954350948334\n",
      "Epoch 8 -- Batch 49/ 842, training loss 0.4205455183982849\n",
      "Epoch 8 -- Batch 50/ 842, training loss 0.44733089208602905\n",
      "Epoch 8 -- Batch 51/ 842, training loss 0.42723068594932556\n",
      "Epoch 8 -- Batch 52/ 842, training loss 0.4334825873374939\n",
      "Epoch 8 -- Batch 53/ 842, training loss 0.44729554653167725\n",
      "Epoch 8 -- Batch 54/ 842, training loss 0.4229530692100525\n",
      "Epoch 8 -- Batch 55/ 842, training loss 0.42806440591812134\n",
      "Epoch 8 -- Batch 56/ 842, training loss 0.4395717680454254\n",
      "Epoch 8 -- Batch 57/ 842, training loss 0.4400479793548584\n",
      "Epoch 8 -- Batch 58/ 842, training loss 0.46102526783943176\n",
      "Epoch 8 -- Batch 59/ 842, training loss 0.46594202518463135\n",
      "Epoch 8 -- Batch 60/ 842, training loss 0.4319397509098053\n",
      "Epoch 8 -- Batch 61/ 842, training loss 0.4195542633533478\n",
      "Epoch 8 -- Batch 62/ 842, training loss 0.4514434039592743\n",
      "Epoch 8 -- Batch 63/ 842, training loss 0.4296422302722931\n",
      "Epoch 8 -- Batch 64/ 842, training loss 0.4239150583744049\n",
      "Epoch 8 -- Batch 65/ 842, training loss 0.4431507885456085\n",
      "Epoch 8 -- Batch 66/ 842, training loss 0.42872855067253113\n",
      "Epoch 8 -- Batch 67/ 842, training loss 0.4382654130458832\n",
      "Epoch 8 -- Batch 68/ 842, training loss 0.4476611614227295\n",
      "Epoch 8 -- Batch 69/ 842, training loss 0.451637864112854\n",
      "Epoch 8 -- Batch 70/ 842, training loss 0.434310644865036\n",
      "Epoch 8 -- Batch 71/ 842, training loss 0.4507935643196106\n",
      "Epoch 8 -- Batch 72/ 842, training loss 0.44779810309410095\n",
      "Epoch 8 -- Batch 73/ 842, training loss 0.4233938157558441\n",
      "Epoch 8 -- Batch 74/ 842, training loss 0.42846769094467163\n",
      "Epoch 8 -- Batch 75/ 842, training loss 0.43346893787384033\n",
      "Epoch 8 -- Batch 76/ 842, training loss 0.431873619556427\n",
      "Epoch 8 -- Batch 77/ 842, training loss 0.4100929796695709\n",
      "Epoch 8 -- Batch 78/ 842, training loss 0.4128926992416382\n",
      "Epoch 8 -- Batch 79/ 842, training loss 0.43659311532974243\n",
      "Epoch 8 -- Batch 80/ 842, training loss 0.4347341060638428\n",
      "Epoch 8 -- Batch 81/ 842, training loss 0.4403868615627289\n",
      "Epoch 8 -- Batch 82/ 842, training loss 0.459807425737381\n",
      "Epoch 8 -- Batch 83/ 842, training loss 0.4388186037540436\n",
      "Epoch 8 -- Batch 84/ 842, training loss 0.4253387451171875\n",
      "Epoch 8 -- Batch 85/ 842, training loss 0.44759997725486755\n",
      "Epoch 8 -- Batch 86/ 842, training loss 0.4194963276386261\n",
      "Epoch 8 -- Batch 87/ 842, training loss 0.4592157006263733\n",
      "Epoch 8 -- Batch 88/ 842, training loss 0.44364503026008606\n",
      "Epoch 8 -- Batch 89/ 842, training loss 0.42918628454208374\n",
      "Epoch 8 -- Batch 90/ 842, training loss 0.44854894280433655\n",
      "Epoch 8 -- Batch 91/ 842, training loss 0.4423274099826813\n",
      "Epoch 8 -- Batch 92/ 842, training loss 0.43750861287117004\n",
      "Epoch 8 -- Batch 93/ 842, training loss 0.4276319444179535\n",
      "Epoch 8 -- Batch 94/ 842, training loss 0.4447983205318451\n",
      "Epoch 8 -- Batch 95/ 842, training loss 0.4334963262081146\n",
      "Epoch 8 -- Batch 96/ 842, training loss 0.4335692226886749\n",
      "Epoch 8 -- Batch 97/ 842, training loss 0.4332798421382904\n",
      "Epoch 8 -- Batch 98/ 842, training loss 0.43389514088630676\n",
      "Epoch 8 -- Batch 99/ 842, training loss 0.4414597749710083\n",
      "Epoch 8 -- Batch 100/ 842, training loss 0.4530251622200012\n",
      "Epoch 8 -- Batch 101/ 842, training loss 0.45066243410110474\n",
      "Epoch 8 -- Batch 102/ 842, training loss 0.42173096537590027\n",
      "Epoch 8 -- Batch 103/ 842, training loss 0.44228121638298035\n",
      "Epoch 8 -- Batch 104/ 842, training loss 0.4674765467643738\n",
      "Epoch 8 -- Batch 105/ 842, training loss 0.4530870318412781\n",
      "Epoch 8 -- Batch 106/ 842, training loss 0.4403258264064789\n",
      "Epoch 8 -- Batch 107/ 842, training loss 0.43812212347984314\n",
      "Epoch 8 -- Batch 108/ 842, training loss 0.4232041537761688\n",
      "Epoch 8 -- Batch 109/ 842, training loss 0.4285447895526886\n",
      "Epoch 8 -- Batch 110/ 842, training loss 0.4360339939594269\n",
      "Epoch 8 -- Batch 111/ 842, training loss 0.44034814834594727\n",
      "Epoch 8 -- Batch 112/ 842, training loss 0.4247814416885376\n",
      "Epoch 8 -- Batch 113/ 842, training loss 0.44356203079223633\n",
      "Epoch 8 -- Batch 114/ 842, training loss 0.441043883562088\n",
      "Epoch 8 -- Batch 115/ 842, training loss 0.44703325629234314\n",
      "Epoch 8 -- Batch 116/ 842, training loss 0.44246143102645874\n",
      "Epoch 8 -- Batch 117/ 842, training loss 0.4444921910762787\n",
      "Epoch 8 -- Batch 118/ 842, training loss 0.44158774614334106\n",
      "Epoch 8 -- Batch 119/ 842, training loss 0.4248760938644409\n",
      "Epoch 8 -- Batch 120/ 842, training loss 0.42481857538223267\n",
      "Epoch 8 -- Batch 121/ 842, training loss 0.4527864158153534\n",
      "Epoch 8 -- Batch 122/ 842, training loss 0.43098214268684387\n",
      "Epoch 8 -- Batch 123/ 842, training loss 0.4200056195259094\n",
      "Epoch 8 -- Batch 124/ 842, training loss 0.422268807888031\n",
      "Epoch 8 -- Batch 125/ 842, training loss 0.44623857736587524\n",
      "Epoch 8 -- Batch 126/ 842, training loss 0.44360342621803284\n",
      "Epoch 8 -- Batch 127/ 842, training loss 0.4606741964817047\n",
      "Epoch 8 -- Batch 128/ 842, training loss 0.4381035566329956\n",
      "Epoch 8 -- Batch 129/ 842, training loss 0.4390450119972229\n",
      "Epoch 8 -- Batch 130/ 842, training loss 0.4526311457157135\n",
      "Epoch 8 -- Batch 131/ 842, training loss 0.418378621339798\n",
      "Epoch 8 -- Batch 132/ 842, training loss 0.44031718373298645\n",
      "Epoch 8 -- Batch 133/ 842, training loss 0.4292196035385132\n",
      "Epoch 8 -- Batch 134/ 842, training loss 0.4315791130065918\n",
      "Epoch 8 -- Batch 135/ 842, training loss 0.44602519273757935\n",
      "Epoch 8 -- Batch 136/ 842, training loss 0.4272348880767822\n",
      "Epoch 8 -- Batch 137/ 842, training loss 0.4455759823322296\n",
      "Epoch 8 -- Batch 138/ 842, training loss 0.43446359038352966\n",
      "Epoch 8 -- Batch 139/ 842, training loss 0.43242383003234863\n",
      "Epoch 8 -- Batch 140/ 842, training loss 0.4328904151916504\n",
      "Epoch 8 -- Batch 141/ 842, training loss 0.43415549397468567\n",
      "Epoch 8 -- Batch 142/ 842, training loss 0.43584126234054565\n",
      "Epoch 8 -- Batch 143/ 842, training loss 0.4395216703414917\n",
      "Epoch 8 -- Batch 144/ 842, training loss 0.46678078174591064\n",
      "Epoch 8 -- Batch 145/ 842, training loss 0.43824687600135803\n",
      "Epoch 8 -- Batch 146/ 842, training loss 0.453726589679718\n",
      "Epoch 8 -- Batch 147/ 842, training loss 0.44360819458961487\n",
      "Epoch 8 -- Batch 148/ 842, training loss 0.43976014852523804\n",
      "Epoch 8 -- Batch 149/ 842, training loss 0.4189804196357727\n",
      "Epoch 8 -- Batch 150/ 842, training loss 0.44929319620132446\n",
      "Epoch 8 -- Batch 151/ 842, training loss 0.44330036640167236\n",
      "Epoch 8 -- Batch 152/ 842, training loss 0.447734534740448\n",
      "Epoch 8 -- Batch 153/ 842, training loss 0.43181049823760986\n",
      "Epoch 8 -- Batch 154/ 842, training loss 0.4360816180706024\n",
      "Epoch 8 -- Batch 155/ 842, training loss 0.4368482530117035\n",
      "Epoch 8 -- Batch 156/ 842, training loss 0.43712911009788513\n",
      "Epoch 8 -- Batch 157/ 842, training loss 0.4300256669521332\n",
      "Epoch 8 -- Batch 158/ 842, training loss 0.41560056805610657\n",
      "Epoch 8 -- Batch 159/ 842, training loss 0.4358394145965576\n",
      "Epoch 8 -- Batch 160/ 842, training loss 0.4366154670715332\n",
      "Epoch 8 -- Batch 161/ 842, training loss 0.43078821897506714\n",
      "Epoch 8 -- Batch 162/ 842, training loss 0.43595513701438904\n",
      "Epoch 8 -- Batch 163/ 842, training loss 0.4454452395439148\n",
      "Epoch 8 -- Batch 164/ 842, training loss 0.44149070978164673\n",
      "Epoch 8 -- Batch 165/ 842, training loss 0.44872063398361206\n",
      "Epoch 8 -- Batch 166/ 842, training loss 0.4390943646430969\n",
      "Epoch 8 -- Batch 167/ 842, training loss 0.45705467462539673\n",
      "Epoch 8 -- Batch 168/ 842, training loss 0.43885189294815063\n",
      "Epoch 8 -- Batch 169/ 842, training loss 0.4421102702617645\n",
      "Epoch 8 -- Batch 170/ 842, training loss 0.46113264560699463\n",
      "Epoch 8 -- Batch 171/ 842, training loss 0.43547385931015015\n",
      "Epoch 8 -- Batch 172/ 842, training loss 0.44958963990211487\n",
      "Epoch 8 -- Batch 173/ 842, training loss 0.43504950404167175\n",
      "Epoch 8 -- Batch 174/ 842, training loss 0.4248521029949188\n",
      "Epoch 8 -- Batch 175/ 842, training loss 0.4535563886165619\n",
      "Epoch 8 -- Batch 176/ 842, training loss 0.4131506681442261\n",
      "Epoch 8 -- Batch 177/ 842, training loss 0.4340183734893799\n",
      "Epoch 8 -- Batch 178/ 842, training loss 0.4175224006175995\n",
      "Epoch 8 -- Batch 179/ 842, training loss 0.4534924030303955\n",
      "Epoch 8 -- Batch 180/ 842, training loss 0.44772467017173767\n",
      "Epoch 8 -- Batch 181/ 842, training loss 0.4492749273777008\n",
      "Epoch 8 -- Batch 182/ 842, training loss 0.4524318277835846\n",
      "Epoch 8 -- Batch 183/ 842, training loss 0.44360142946243286\n",
      "Epoch 8 -- Batch 184/ 842, training loss 0.4418555200099945\n",
      "Epoch 8 -- Batch 185/ 842, training loss 0.4420037269592285\n",
      "Epoch 8 -- Batch 186/ 842, training loss 0.4445362687110901\n",
      "Epoch 8 -- Batch 187/ 842, training loss 0.4398157596588135\n",
      "Epoch 8 -- Batch 188/ 842, training loss 0.4301462173461914\n",
      "Epoch 8 -- Batch 189/ 842, training loss 0.4384676516056061\n",
      "Epoch 8 -- Batch 190/ 842, training loss 0.4536047577857971\n",
      "Epoch 8 -- Batch 191/ 842, training loss 0.437776654958725\n",
      "Epoch 8 -- Batch 192/ 842, training loss 0.4263976812362671\n",
      "Epoch 8 -- Batch 193/ 842, training loss 0.44137462973594666\n",
      "Epoch 8 -- Batch 194/ 842, training loss 0.4359663724899292\n",
      "Epoch 8 -- Batch 195/ 842, training loss 0.4357130527496338\n",
      "Epoch 8 -- Batch 196/ 842, training loss 0.4367135167121887\n",
      "Epoch 8 -- Batch 197/ 842, training loss 0.42166897654533386\n",
      "Epoch 8 -- Batch 198/ 842, training loss 0.434664249420166\n",
      "Epoch 8 -- Batch 199/ 842, training loss 0.4545401632785797\n",
      "Epoch 8 -- Batch 200/ 842, training loss 0.41813406348228455\n",
      "Epoch 8 -- Batch 201/ 842, training loss 0.43109434843063354\n",
      "Epoch 8 -- Batch 202/ 842, training loss 0.45135146379470825\n",
      "Epoch 8 -- Batch 203/ 842, training loss 0.44006550312042236\n",
      "Epoch 8 -- Batch 204/ 842, training loss 0.4326896667480469\n",
      "Epoch 8 -- Batch 205/ 842, training loss 0.43161138892173767\n",
      "Epoch 8 -- Batch 206/ 842, training loss 0.43131396174430847\n",
      "Epoch 8 -- Batch 207/ 842, training loss 0.4204569458961487\n",
      "Epoch 8 -- Batch 208/ 842, training loss 0.43262779712677\n",
      "Epoch 8 -- Batch 209/ 842, training loss 0.45790448784828186\n",
      "Epoch 8 -- Batch 210/ 842, training loss 0.43741023540496826\n",
      "Epoch 8 -- Batch 211/ 842, training loss 0.43535104393959045\n",
      "Epoch 8 -- Batch 212/ 842, training loss 0.4365288317203522\n",
      "Epoch 8 -- Batch 213/ 842, training loss 0.4310723841190338\n",
      "Epoch 8 -- Batch 214/ 842, training loss 0.45040130615234375\n",
      "Epoch 8 -- Batch 215/ 842, training loss 0.42731645703315735\n",
      "Epoch 8 -- Batch 216/ 842, training loss 0.42962244153022766\n",
      "Epoch 8 -- Batch 217/ 842, training loss 0.4280426502227783\n",
      "Epoch 8 -- Batch 218/ 842, training loss 0.429582417011261\n",
      "Epoch 8 -- Batch 219/ 842, training loss 0.4470410645008087\n",
      "Epoch 8 -- Batch 220/ 842, training loss 0.4315711557865143\n",
      "Epoch 8 -- Batch 221/ 842, training loss 0.4358958303928375\n",
      "Epoch 8 -- Batch 222/ 842, training loss 0.4303332567214966\n",
      "Epoch 8 -- Batch 223/ 842, training loss 0.43447205424308777\n",
      "Epoch 8 -- Batch 224/ 842, training loss 0.43097275495529175\n",
      "Epoch 8 -- Batch 225/ 842, training loss 0.43035492300987244\n",
      "Epoch 8 -- Batch 226/ 842, training loss 0.4353956878185272\n",
      "Epoch 8 -- Batch 227/ 842, training loss 0.4090224802494049\n",
      "Epoch 8 -- Batch 228/ 842, training loss 0.43382060527801514\n",
      "Epoch 8 -- Batch 229/ 842, training loss 0.4452188313007355\n",
      "Epoch 8 -- Batch 230/ 842, training loss 0.4458598494529724\n",
      "Epoch 8 -- Batch 231/ 842, training loss 0.43596121668815613\n",
      "Epoch 8 -- Batch 232/ 842, training loss 0.4513961374759674\n",
      "Epoch 8 -- Batch 233/ 842, training loss 0.4514419138431549\n",
      "Epoch 8 -- Batch 234/ 842, training loss 0.4300926923751831\n",
      "Epoch 8 -- Batch 235/ 842, training loss 0.43466639518737793\n",
      "Epoch 8 -- Batch 236/ 842, training loss 0.4360957145690918\n",
      "Epoch 8 -- Batch 237/ 842, training loss 0.4449431300163269\n",
      "Epoch 8 -- Batch 238/ 842, training loss 0.4196510910987854\n",
      "Epoch 8 -- Batch 239/ 842, training loss 0.43032291531562805\n",
      "Epoch 8 -- Batch 240/ 842, training loss 0.43749934434890747\n",
      "Epoch 8 -- Batch 241/ 842, training loss 0.42567142844200134\n",
      "Epoch 8 -- Batch 242/ 842, training loss 0.45243039727211\n",
      "Epoch 8 -- Batch 243/ 842, training loss 0.4569515585899353\n",
      "Epoch 8 -- Batch 244/ 842, training loss 0.4372017979621887\n",
      "Epoch 8 -- Batch 245/ 842, training loss 0.43429818749427795\n",
      "Epoch 8 -- Batch 246/ 842, training loss 0.4320727586746216\n",
      "Epoch 8 -- Batch 247/ 842, training loss 0.42012637853622437\n",
      "Epoch 8 -- Batch 248/ 842, training loss 0.43534961342811584\n",
      "Epoch 8 -- Batch 249/ 842, training loss 0.4295530319213867\n",
      "Epoch 8 -- Batch 250/ 842, training loss 0.43006691336631775\n",
      "Epoch 8 -- Batch 251/ 842, training loss 0.4205613434314728\n",
      "Epoch 8 -- Batch 252/ 842, training loss 0.4273276627063751\n",
      "Epoch 8 -- Batch 253/ 842, training loss 0.444166898727417\n",
      "Epoch 8 -- Batch 254/ 842, training loss 0.4316929280757904\n",
      "Epoch 8 -- Batch 255/ 842, training loss 0.43164995312690735\n",
      "Epoch 8 -- Batch 256/ 842, training loss 0.43436452746391296\n",
      "Epoch 8 -- Batch 257/ 842, training loss 0.44164782762527466\n",
      "Epoch 8 -- Batch 258/ 842, training loss 0.43262383341789246\n",
      "Epoch 8 -- Batch 259/ 842, training loss 0.4588066041469574\n",
      "Epoch 8 -- Batch 260/ 842, training loss 0.42652273178100586\n",
      "Epoch 8 -- Batch 261/ 842, training loss 0.42452237010002136\n",
      "Epoch 8 -- Batch 262/ 842, training loss 0.42436549067497253\n",
      "Epoch 8 -- Batch 263/ 842, training loss 0.4387856125831604\n",
      "Epoch 8 -- Batch 264/ 842, training loss 0.4609735906124115\n",
      "Epoch 8 -- Batch 265/ 842, training loss 0.44302526116371155\n",
      "Epoch 8 -- Batch 266/ 842, training loss 0.4467780888080597\n",
      "Epoch 8 -- Batch 267/ 842, training loss 0.44721731543540955\n",
      "Epoch 8 -- Batch 268/ 842, training loss 0.4482765793800354\n",
      "Epoch 8 -- Batch 269/ 842, training loss 0.4420582056045532\n",
      "Epoch 8 -- Batch 270/ 842, training loss 0.43689078092575073\n",
      "Epoch 8 -- Batch 271/ 842, training loss 0.4202860891819\n",
      "Epoch 8 -- Batch 272/ 842, training loss 0.44058841466903687\n",
      "Epoch 8 -- Batch 273/ 842, training loss 0.4513636529445648\n",
      "Epoch 8 -- Batch 274/ 842, training loss 0.4257635772228241\n",
      "Epoch 8 -- Batch 275/ 842, training loss 0.42755675315856934\n",
      "Epoch 8 -- Batch 276/ 842, training loss 0.44338470697402954\n",
      "Epoch 8 -- Batch 277/ 842, training loss 0.44224274158477783\n",
      "Epoch 8 -- Batch 278/ 842, training loss 0.452448308467865\n",
      "Epoch 8 -- Batch 279/ 842, training loss 0.44066891074180603\n",
      "Epoch 8 -- Batch 280/ 842, training loss 0.4335445165634155\n",
      "Epoch 8 -- Batch 281/ 842, training loss 0.44508713483810425\n",
      "Epoch 8 -- Batch 282/ 842, training loss 0.4198955297470093\n",
      "Epoch 8 -- Batch 283/ 842, training loss 0.4336792826652527\n",
      "Epoch 8 -- Batch 284/ 842, training loss 0.43668732047080994\n",
      "Epoch 8 -- Batch 285/ 842, training loss 0.4579479396343231\n",
      "Epoch 8 -- Batch 286/ 842, training loss 0.4466407299041748\n",
      "Epoch 8 -- Batch 287/ 842, training loss 0.4316118657588959\n",
      "Epoch 8 -- Batch 288/ 842, training loss 0.44341567158699036\n",
      "Epoch 8 -- Batch 289/ 842, training loss 0.44113409519195557\n",
      "Epoch 8 -- Batch 290/ 842, training loss 0.44261103868484497\n",
      "Epoch 8 -- Batch 291/ 842, training loss 0.43518710136413574\n",
      "Epoch 8 -- Batch 292/ 842, training loss 0.44646745920181274\n",
      "Epoch 8 -- Batch 293/ 842, training loss 0.44076812267303467\n",
      "Epoch 8 -- Batch 294/ 842, training loss 0.42685726284980774\n",
      "Epoch 8 -- Batch 295/ 842, training loss 0.44380757212638855\n",
      "Epoch 8 -- Batch 296/ 842, training loss 0.447363942861557\n",
      "Epoch 8 -- Batch 297/ 842, training loss 0.42588454484939575\n",
      "Epoch 8 -- Batch 298/ 842, training loss 0.4171050786972046\n",
      "Epoch 8 -- Batch 299/ 842, training loss 0.452330619096756\n",
      "Epoch 8 -- Batch 300/ 842, training loss 0.41935640573501587\n",
      "Epoch 8 -- Batch 301/ 842, training loss 0.44974586367607117\n",
      "Epoch 8 -- Batch 302/ 842, training loss 0.43184253573417664\n",
      "Epoch 8 -- Batch 303/ 842, training loss 0.44137170910835266\n",
      "Epoch 8 -- Batch 304/ 842, training loss 0.4381878972053528\n",
      "Epoch 8 -- Batch 305/ 842, training loss 0.44101306796073914\n",
      "Epoch 8 -- Batch 306/ 842, training loss 0.4493604004383087\n",
      "Epoch 8 -- Batch 307/ 842, training loss 0.4269097149372101\n",
      "Epoch 8 -- Batch 308/ 842, training loss 0.4315222501754761\n",
      "Epoch 8 -- Batch 309/ 842, training loss 0.4378310441970825\n",
      "Epoch 8 -- Batch 310/ 842, training loss 0.4493911564350128\n",
      "Epoch 8 -- Batch 311/ 842, training loss 0.4492267668247223\n",
      "Epoch 8 -- Batch 312/ 842, training loss 0.44265830516815186\n",
      "Epoch 8 -- Batch 313/ 842, training loss 0.4234945476055145\n",
      "Epoch 8 -- Batch 314/ 842, training loss 0.44960296154022217\n",
      "Epoch 8 -- Batch 315/ 842, training loss 0.45534810423851013\n",
      "Epoch 8 -- Batch 316/ 842, training loss 0.44349390268325806\n",
      "Epoch 8 -- Batch 317/ 842, training loss 0.4454118609428406\n",
      "Epoch 8 -- Batch 318/ 842, training loss 0.44734758138656616\n",
      "Epoch 8 -- Batch 319/ 842, training loss 0.44141846895217896\n",
      "Epoch 8 -- Batch 320/ 842, training loss 0.4404074549674988\n",
      "Epoch 8 -- Batch 321/ 842, training loss 0.4569653570652008\n",
      "Epoch 8 -- Batch 322/ 842, training loss 0.4287945628166199\n",
      "Epoch 8 -- Batch 323/ 842, training loss 0.4390370547771454\n",
      "Epoch 8 -- Batch 324/ 842, training loss 0.44650742411613464\n",
      "Epoch 8 -- Batch 325/ 842, training loss 0.44072476029396057\n",
      "Epoch 8 -- Batch 326/ 842, training loss 0.45002296566963196\n",
      "Epoch 8 -- Batch 327/ 842, training loss 0.4456997215747833\n",
      "Epoch 8 -- Batch 328/ 842, training loss 0.4397345781326294\n",
      "Epoch 8 -- Batch 329/ 842, training loss 0.43416836857795715\n",
      "Epoch 8 -- Batch 330/ 842, training loss 0.4321412742137909\n",
      "Epoch 8 -- Batch 331/ 842, training loss 0.4376271069049835\n",
      "Epoch 8 -- Batch 332/ 842, training loss 0.427664577960968\n",
      "Epoch 8 -- Batch 333/ 842, training loss 0.4317282736301422\n",
      "Epoch 8 -- Batch 334/ 842, training loss 0.43648141622543335\n",
      "Epoch 8 -- Batch 335/ 842, training loss 0.4276202321052551\n",
      "Epoch 8 -- Batch 336/ 842, training loss 0.44264519214630127\n",
      "Epoch 8 -- Batch 337/ 842, training loss 0.44896963238716125\n",
      "Epoch 8 -- Batch 338/ 842, training loss 0.44525331258773804\n",
      "Epoch 8 -- Batch 339/ 842, training loss 0.4281301498413086\n",
      "Epoch 8 -- Batch 340/ 842, training loss 0.44663161039352417\n",
      "Epoch 8 -- Batch 341/ 842, training loss 0.43584203720092773\n",
      "Epoch 8 -- Batch 342/ 842, training loss 0.43898656964302063\n",
      "Epoch 8 -- Batch 343/ 842, training loss 0.4417422413825989\n",
      "Epoch 8 -- Batch 344/ 842, training loss 0.43991267681121826\n",
      "Epoch 8 -- Batch 345/ 842, training loss 0.43135544657707214\n",
      "Epoch 8 -- Batch 346/ 842, training loss 0.470710813999176\n",
      "Epoch 8 -- Batch 347/ 842, training loss 0.4385813772678375\n",
      "Epoch 8 -- Batch 348/ 842, training loss 0.4342659115791321\n",
      "Epoch 8 -- Batch 349/ 842, training loss 0.4336724579334259\n",
      "Epoch 8 -- Batch 350/ 842, training loss 0.4317215085029602\n",
      "Epoch 8 -- Batch 351/ 842, training loss 0.44549360871315\n",
      "Epoch 8 -- Batch 352/ 842, training loss 0.44609421491622925\n",
      "Epoch 8 -- Batch 353/ 842, training loss 0.42265230417251587\n",
      "Epoch 8 -- Batch 354/ 842, training loss 0.44257745146751404\n",
      "Epoch 8 -- Batch 355/ 842, training loss 0.44015976786613464\n",
      "Epoch 8 -- Batch 356/ 842, training loss 0.4583907127380371\n",
      "Epoch 8 -- Batch 357/ 842, training loss 0.4418051540851593\n",
      "Epoch 8 -- Batch 358/ 842, training loss 0.43736374378204346\n",
      "Epoch 8 -- Batch 359/ 842, training loss 0.4391211271286011\n",
      "Epoch 8 -- Batch 360/ 842, training loss 0.42912086844444275\n",
      "Epoch 8 -- Batch 361/ 842, training loss 0.46989062428474426\n",
      "Epoch 8 -- Batch 362/ 842, training loss 0.43325918912887573\n",
      "Epoch 8 -- Batch 363/ 842, training loss 0.4510233998298645\n",
      "Epoch 8 -- Batch 364/ 842, training loss 0.4463970959186554\n",
      "Epoch 8 -- Batch 365/ 842, training loss 0.436200886964798\n",
      "Epoch 8 -- Batch 366/ 842, training loss 0.432963103055954\n",
      "Epoch 8 -- Batch 367/ 842, training loss 0.42709723114967346\n",
      "Epoch 8 -- Batch 368/ 842, training loss 0.4608730971813202\n",
      "Epoch 8 -- Batch 369/ 842, training loss 0.446332186460495\n",
      "Epoch 8 -- Batch 370/ 842, training loss 0.44574660062789917\n",
      "Epoch 8 -- Batch 371/ 842, training loss 0.4355925917625427\n",
      "Epoch 8 -- Batch 372/ 842, training loss 0.4226705729961395\n",
      "Epoch 8 -- Batch 373/ 842, training loss 0.4251243770122528\n",
      "Epoch 8 -- Batch 374/ 842, training loss 0.4308519959449768\n",
      "Epoch 8 -- Batch 375/ 842, training loss 0.43301644921302795\n",
      "Epoch 8 -- Batch 376/ 842, training loss 0.4455776512622833\n",
      "Epoch 8 -- Batch 377/ 842, training loss 0.44344159960746765\n",
      "Epoch 8 -- Batch 378/ 842, training loss 0.443609356880188\n",
      "Epoch 8 -- Batch 379/ 842, training loss 0.43048378825187683\n",
      "Epoch 8 -- Batch 380/ 842, training loss 0.43144622445106506\n",
      "Epoch 8 -- Batch 381/ 842, training loss 0.42949581146240234\n",
      "Epoch 8 -- Batch 382/ 842, training loss 0.42653992772102356\n",
      "Epoch 8 -- Batch 383/ 842, training loss 0.43843644857406616\n",
      "Epoch 8 -- Batch 384/ 842, training loss 0.4450806975364685\n",
      "Epoch 8 -- Batch 385/ 842, training loss 0.44617581367492676\n",
      "Epoch 8 -- Batch 386/ 842, training loss 0.4531850218772888\n",
      "Epoch 8 -- Batch 387/ 842, training loss 0.4491330087184906\n",
      "Epoch 8 -- Batch 388/ 842, training loss 0.441823810338974\n",
      "Epoch 8 -- Batch 389/ 842, training loss 0.44844838976860046\n",
      "Epoch 8 -- Batch 390/ 842, training loss 0.4654284417629242\n",
      "Epoch 8 -- Batch 391/ 842, training loss 0.42849138379096985\n",
      "Epoch 8 -- Batch 392/ 842, training loss 0.4503193199634552\n",
      "Epoch 8 -- Batch 393/ 842, training loss 0.4374174475669861\n",
      "Epoch 8 -- Batch 394/ 842, training loss 0.45518696308135986\n",
      "Epoch 8 -- Batch 395/ 842, training loss 0.43228626251220703\n",
      "Epoch 8 -- Batch 396/ 842, training loss 0.44547662138938904\n",
      "Epoch 8 -- Batch 397/ 842, training loss 0.43970000743865967\n",
      "Epoch 8 -- Batch 398/ 842, training loss 0.433307021856308\n",
      "Epoch 8 -- Batch 399/ 842, training loss 0.45568355917930603\n",
      "Epoch 8 -- Batch 400/ 842, training loss 0.43133917450904846\n",
      "Epoch 8 -- Batch 401/ 842, training loss 0.43131452798843384\n",
      "Epoch 8 -- Batch 402/ 842, training loss 0.44574055075645447\n",
      "Epoch 8 -- Batch 403/ 842, training loss 0.4397547245025635\n",
      "Epoch 8 -- Batch 404/ 842, training loss 0.45279181003570557\n",
      "Epoch 8 -- Batch 405/ 842, training loss 0.4235542416572571\n",
      "Epoch 8 -- Batch 406/ 842, training loss 0.4434274733066559\n",
      "Epoch 8 -- Batch 407/ 842, training loss 0.43934884667396545\n",
      "Epoch 8 -- Batch 408/ 842, training loss 0.4269014000892639\n",
      "Epoch 8 -- Batch 409/ 842, training loss 0.42329004406929016\n",
      "Epoch 8 -- Batch 410/ 842, training loss 0.43729326128959656\n",
      "Epoch 8 -- Batch 411/ 842, training loss 0.4309506118297577\n",
      "Epoch 8 -- Batch 412/ 842, training loss 0.4289695918560028\n",
      "Epoch 8 -- Batch 413/ 842, training loss 0.4386416971683502\n",
      "Epoch 8 -- Batch 414/ 842, training loss 0.44579118490219116\n",
      "Epoch 8 -- Batch 415/ 842, training loss 0.4457787871360779\n",
      "Epoch 8 -- Batch 416/ 842, training loss 0.42754554748535156\n",
      "Epoch 8 -- Batch 417/ 842, training loss 0.44188210368156433\n",
      "Epoch 8 -- Batch 418/ 842, training loss 0.4464316964149475\n",
      "Epoch 8 -- Batch 419/ 842, training loss 0.43304502964019775\n",
      "Epoch 8 -- Batch 420/ 842, training loss 0.45504042506217957\n",
      "Epoch 8 -- Batch 421/ 842, training loss 0.4348284900188446\n",
      "Epoch 8 -- Batch 422/ 842, training loss 0.43192237615585327\n",
      "Epoch 8 -- Batch 423/ 842, training loss 0.4279552698135376\n",
      "Epoch 8 -- Batch 424/ 842, training loss 0.4453696310520172\n",
      "Epoch 8 -- Batch 425/ 842, training loss 0.4279696047306061\n",
      "Epoch 8 -- Batch 426/ 842, training loss 0.43086522817611694\n",
      "Epoch 8 -- Batch 427/ 842, training loss 0.4403138756752014\n",
      "Epoch 8 -- Batch 428/ 842, training loss 0.4469597041606903\n",
      "Epoch 8 -- Batch 429/ 842, training loss 0.4349486231803894\n",
      "Epoch 8 -- Batch 430/ 842, training loss 0.4435999393463135\n",
      "Epoch 8 -- Batch 431/ 842, training loss 0.43400052189826965\n",
      "Epoch 8 -- Batch 432/ 842, training loss 0.43851327896118164\n",
      "Epoch 8 -- Batch 433/ 842, training loss 0.4478975236415863\n",
      "Epoch 8 -- Batch 434/ 842, training loss 0.43362972140312195\n",
      "Epoch 8 -- Batch 435/ 842, training loss 0.4509546160697937\n",
      "Epoch 8 -- Batch 436/ 842, training loss 0.43342337012290955\n",
      "Epoch 8 -- Batch 437/ 842, training loss 0.4367268681526184\n",
      "Epoch 8 -- Batch 438/ 842, training loss 0.42808082699775696\n",
      "Epoch 8 -- Batch 439/ 842, training loss 0.4452204704284668\n",
      "Epoch 8 -- Batch 440/ 842, training loss 0.44339388608932495\n",
      "Epoch 8 -- Batch 441/ 842, training loss 0.4414214789867401\n",
      "Epoch 8 -- Batch 442/ 842, training loss 0.44304177165031433\n",
      "Epoch 8 -- Batch 443/ 842, training loss 0.4476592242717743\n",
      "Epoch 8 -- Batch 444/ 842, training loss 0.41214972734451294\n",
      "Epoch 8 -- Batch 445/ 842, training loss 0.4519714415073395\n",
      "Epoch 8 -- Batch 446/ 842, training loss 0.44416651129722595\n",
      "Epoch 8 -- Batch 447/ 842, training loss 0.4191135764122009\n",
      "Epoch 8 -- Batch 448/ 842, training loss 0.4283585548400879\n",
      "Epoch 8 -- Batch 449/ 842, training loss 0.4370933175086975\n",
      "Epoch 8 -- Batch 450/ 842, training loss 0.42499589920043945\n",
      "Epoch 8 -- Batch 451/ 842, training loss 0.43463045358657837\n",
      "Epoch 8 -- Batch 452/ 842, training loss 0.43116387724876404\n",
      "Epoch 8 -- Batch 453/ 842, training loss 0.41955941915512085\n",
      "Epoch 8 -- Batch 454/ 842, training loss 0.42227986454963684\n",
      "Epoch 8 -- Batch 455/ 842, training loss 0.42714354395866394\n",
      "Epoch 8 -- Batch 456/ 842, training loss 0.4446113705635071\n",
      "Epoch 8 -- Batch 457/ 842, training loss 0.43420934677124023\n",
      "Epoch 8 -- Batch 458/ 842, training loss 0.46168768405914307\n",
      "Epoch 8 -- Batch 459/ 842, training loss 0.43494534492492676\n",
      "Epoch 8 -- Batch 460/ 842, training loss 0.44173741340637207\n",
      "Epoch 8 -- Batch 461/ 842, training loss 0.435245543718338\n",
      "Epoch 8 -- Batch 462/ 842, training loss 0.42683011293411255\n",
      "Epoch 8 -- Batch 463/ 842, training loss 0.4287554621696472\n",
      "Epoch 8 -- Batch 464/ 842, training loss 0.42352989315986633\n",
      "Epoch 8 -- Batch 465/ 842, training loss 0.44510361552238464\n",
      "Epoch 8 -- Batch 466/ 842, training loss 0.44489216804504395\n",
      "Epoch 8 -- Batch 467/ 842, training loss 0.42652493715286255\n",
      "Epoch 8 -- Batch 468/ 842, training loss 0.4572603106498718\n",
      "Epoch 8 -- Batch 469/ 842, training loss 0.443203866481781\n",
      "Epoch 8 -- Batch 470/ 842, training loss 0.42217859625816345\n",
      "Epoch 8 -- Batch 471/ 842, training loss 0.44009917974472046\n",
      "Epoch 8 -- Batch 472/ 842, training loss 0.45016273856163025\n",
      "Epoch 8 -- Batch 473/ 842, training loss 0.4308534860610962\n",
      "Epoch 8 -- Batch 474/ 842, training loss 0.4360619783401489\n",
      "Epoch 8 -- Batch 475/ 842, training loss 0.42641952633857727\n",
      "Epoch 8 -- Batch 476/ 842, training loss 0.4486929178237915\n",
      "Epoch 8 -- Batch 477/ 842, training loss 0.45165014266967773\n",
      "Epoch 8 -- Batch 478/ 842, training loss 0.4209069311618805\n",
      "Epoch 8 -- Batch 479/ 842, training loss 0.4365863800048828\n",
      "Epoch 8 -- Batch 480/ 842, training loss 0.4363163411617279\n",
      "Epoch 8 -- Batch 481/ 842, training loss 0.43643292784690857\n",
      "Epoch 8 -- Batch 482/ 842, training loss 0.4338025748729706\n",
      "Epoch 8 -- Batch 483/ 842, training loss 0.4290645718574524\n",
      "Epoch 8 -- Batch 484/ 842, training loss 0.43826740980148315\n",
      "Epoch 8 -- Batch 485/ 842, training loss 0.4260577857494354\n",
      "Epoch 8 -- Batch 486/ 842, training loss 0.43553102016448975\n",
      "Epoch 8 -- Batch 487/ 842, training loss 0.4359501302242279\n",
      "Epoch 8 -- Batch 488/ 842, training loss 0.4174608290195465\n",
      "Epoch 8 -- Batch 489/ 842, training loss 0.43911871314048767\n",
      "Epoch 8 -- Batch 490/ 842, training loss 0.42514169216156006\n",
      "Epoch 8 -- Batch 491/ 842, training loss 0.4337518811225891\n",
      "Epoch 8 -- Batch 492/ 842, training loss 0.4481238126754761\n",
      "Epoch 8 -- Batch 493/ 842, training loss 0.4270183742046356\n",
      "Epoch 8 -- Batch 494/ 842, training loss 0.4445895850658417\n",
      "Epoch 8 -- Batch 495/ 842, training loss 0.45471617579460144\n",
      "Epoch 8 -- Batch 496/ 842, training loss 0.424578994512558\n",
      "Epoch 8 -- Batch 497/ 842, training loss 0.4338637888431549\n",
      "Epoch 8 -- Batch 498/ 842, training loss 0.4301992654800415\n",
      "Epoch 8 -- Batch 499/ 842, training loss 0.449789434671402\n",
      "Epoch 8 -- Batch 500/ 842, training loss 0.4419003427028656\n",
      "Epoch 8 -- Batch 501/ 842, training loss 0.42099201679229736\n",
      "Epoch 8 -- Batch 502/ 842, training loss 0.4230366051197052\n",
      "Epoch 8 -- Batch 503/ 842, training loss 0.43231141567230225\n",
      "Epoch 8 -- Batch 504/ 842, training loss 0.4404180645942688\n",
      "Epoch 8 -- Batch 505/ 842, training loss 0.4335828125476837\n",
      "Epoch 8 -- Batch 506/ 842, training loss 0.4292562007904053\n",
      "Epoch 8 -- Batch 507/ 842, training loss 0.4357420802116394\n",
      "Epoch 8 -- Batch 508/ 842, training loss 0.4438653886318207\n",
      "Epoch 8 -- Batch 509/ 842, training loss 0.4358495771884918\n",
      "Epoch 8 -- Batch 510/ 842, training loss 0.42692089080810547\n",
      "Epoch 8 -- Batch 511/ 842, training loss 0.42160993814468384\n",
      "Epoch 8 -- Batch 512/ 842, training loss 0.44496622681617737\n",
      "Epoch 8 -- Batch 513/ 842, training loss 0.43690863251686096\n",
      "Epoch 8 -- Batch 514/ 842, training loss 0.4309979975223541\n",
      "Epoch 8 -- Batch 515/ 842, training loss 0.45986828207969666\n",
      "Epoch 8 -- Batch 516/ 842, training loss 0.4294227063655853\n",
      "Epoch 8 -- Batch 517/ 842, training loss 0.44199007749557495\n",
      "Epoch 8 -- Batch 518/ 842, training loss 0.4367838501930237\n",
      "Epoch 8 -- Batch 519/ 842, training loss 0.4368842542171478\n",
      "Epoch 8 -- Batch 520/ 842, training loss 0.443688303232193\n",
      "Epoch 8 -- Batch 521/ 842, training loss 0.44089779257774353\n",
      "Epoch 8 -- Batch 522/ 842, training loss 0.42856550216674805\n",
      "Epoch 8 -- Batch 523/ 842, training loss 0.42074671387672424\n",
      "Epoch 8 -- Batch 524/ 842, training loss 0.4448590874671936\n",
      "Epoch 8 -- Batch 525/ 842, training loss 0.44316592812538147\n",
      "Epoch 8 -- Batch 526/ 842, training loss 0.4285679757595062\n",
      "Epoch 8 -- Batch 527/ 842, training loss 0.4335762560367584\n",
      "Epoch 8 -- Batch 528/ 842, training loss 0.429829478263855\n",
      "Epoch 8 -- Batch 529/ 842, training loss 0.42561447620391846\n",
      "Epoch 8 -- Batch 530/ 842, training loss 0.42416372895240784\n",
      "Epoch 8 -- Batch 531/ 842, training loss 0.43050387501716614\n",
      "Epoch 8 -- Batch 532/ 842, training loss 0.43005096912384033\n",
      "Epoch 8 -- Batch 533/ 842, training loss 0.4446704387664795\n",
      "Epoch 8 -- Batch 534/ 842, training loss 0.4340830147266388\n",
      "Epoch 8 -- Batch 535/ 842, training loss 0.43087175488471985\n",
      "Epoch 8 -- Batch 536/ 842, training loss 0.4200810194015503\n",
      "Epoch 8 -- Batch 537/ 842, training loss 0.439821720123291\n",
      "Epoch 8 -- Batch 538/ 842, training loss 0.44212979078292847\n",
      "Epoch 8 -- Batch 539/ 842, training loss 0.4371746778488159\n",
      "Epoch 8 -- Batch 540/ 842, training loss 0.42582955956459045\n",
      "Epoch 8 -- Batch 541/ 842, training loss 0.4420364201068878\n",
      "Epoch 8 -- Batch 542/ 842, training loss 0.42412421107292175\n",
      "Epoch 8 -- Batch 543/ 842, training loss 0.4289969205856323\n",
      "Epoch 8 -- Batch 544/ 842, training loss 0.44052693247795105\n",
      "Epoch 8 -- Batch 545/ 842, training loss 0.4347333014011383\n",
      "Epoch 8 -- Batch 546/ 842, training loss 0.4151705205440521\n",
      "Epoch 8 -- Batch 547/ 842, training loss 0.4339653551578522\n",
      "Epoch 8 -- Batch 548/ 842, training loss 0.42016249895095825\n",
      "Epoch 8 -- Batch 549/ 842, training loss 0.4328703284263611\n",
      "Epoch 8 -- Batch 550/ 842, training loss 0.4418192505836487\n",
      "Epoch 8 -- Batch 551/ 842, training loss 0.4311586618423462\n",
      "Epoch 8 -- Batch 552/ 842, training loss 0.44428154826164246\n",
      "Epoch 8 -- Batch 553/ 842, training loss 0.43381795287132263\n",
      "Epoch 8 -- Batch 554/ 842, training loss 0.43709665536880493\n",
      "Epoch 8 -- Batch 555/ 842, training loss 0.42849698662757874\n",
      "Epoch 8 -- Batch 556/ 842, training loss 0.42385587096214294\n",
      "Epoch 8 -- Batch 557/ 842, training loss 0.422281950712204\n",
      "Epoch 8 -- Batch 558/ 842, training loss 0.42916515469551086\n",
      "Epoch 8 -- Batch 559/ 842, training loss 0.4230056703090668\n",
      "Epoch 8 -- Batch 560/ 842, training loss 0.43053779006004333\n",
      "Epoch 8 -- Batch 561/ 842, training loss 0.444181889295578\n",
      "Epoch 8 -- Batch 562/ 842, training loss 0.45009076595306396\n",
      "Epoch 8 -- Batch 563/ 842, training loss 0.4386749267578125\n",
      "Epoch 8 -- Batch 564/ 842, training loss 0.4361543655395508\n",
      "Epoch 8 -- Batch 565/ 842, training loss 0.4489996135234833\n",
      "Epoch 8 -- Batch 566/ 842, training loss 0.4272821843624115\n",
      "Epoch 8 -- Batch 567/ 842, training loss 0.43773767352104187\n",
      "Epoch 8 -- Batch 568/ 842, training loss 0.45776885747909546\n",
      "Epoch 8 -- Batch 569/ 842, training loss 0.43759316205978394\n",
      "Epoch 8 -- Batch 570/ 842, training loss 0.4531814157962799\n",
      "Epoch 8 -- Batch 571/ 842, training loss 0.4462548494338989\n",
      "Epoch 8 -- Batch 572/ 842, training loss 0.4429665803909302\n",
      "Epoch 8 -- Batch 573/ 842, training loss 0.4337754249572754\n",
      "Epoch 8 -- Batch 574/ 842, training loss 0.4173862338066101\n",
      "Epoch 8 -- Batch 575/ 842, training loss 0.4343601167201996\n",
      "Epoch 8 -- Batch 576/ 842, training loss 0.44382375478744507\n",
      "Epoch 8 -- Batch 577/ 842, training loss 0.432532399892807\n",
      "Epoch 8 -- Batch 578/ 842, training loss 0.4506617784500122\n",
      "Epoch 8 -- Batch 579/ 842, training loss 0.45383134484291077\n",
      "Epoch 8 -- Batch 580/ 842, training loss 0.4407673180103302\n",
      "Epoch 8 -- Batch 581/ 842, training loss 0.43810170888900757\n",
      "Epoch 8 -- Batch 582/ 842, training loss 0.4517176151275635\n",
      "Epoch 8 -- Batch 583/ 842, training loss 0.4262907803058624\n",
      "Epoch 8 -- Batch 584/ 842, training loss 0.43457889556884766\n",
      "Epoch 8 -- Batch 585/ 842, training loss 0.4258073568344116\n",
      "Epoch 8 -- Batch 586/ 842, training loss 0.444580078125\n",
      "Epoch 8 -- Batch 587/ 842, training loss 0.4307129383087158\n",
      "Epoch 8 -- Batch 588/ 842, training loss 0.4385792016983032\n",
      "Epoch 8 -- Batch 589/ 842, training loss 0.4469710886478424\n",
      "Epoch 8 -- Batch 590/ 842, training loss 0.43259987235069275\n",
      "Epoch 8 -- Batch 591/ 842, training loss 0.44426995515823364\n",
      "Epoch 8 -- Batch 592/ 842, training loss 0.44568631052970886\n",
      "Epoch 8 -- Batch 593/ 842, training loss 0.4172907769680023\n",
      "Epoch 8 -- Batch 594/ 842, training loss 0.44241103529930115\n",
      "Epoch 8 -- Batch 595/ 842, training loss 0.43497881293296814\n",
      "Epoch 8 -- Batch 596/ 842, training loss 0.4284495413303375\n",
      "Epoch 8 -- Batch 597/ 842, training loss 0.4387710690498352\n",
      "Epoch 8 -- Batch 598/ 842, training loss 0.43235835433006287\n",
      "Epoch 8 -- Batch 599/ 842, training loss 0.45166751742362976\n",
      "Epoch 8 -- Batch 600/ 842, training loss 0.4298270642757416\n",
      "Epoch 8 -- Batch 601/ 842, training loss 0.4323571026325226\n",
      "Epoch 8 -- Batch 602/ 842, training loss 0.4389081597328186\n",
      "Epoch 8 -- Batch 603/ 842, training loss 0.44808903336524963\n",
      "Epoch 8 -- Batch 604/ 842, training loss 0.43160587549209595\n",
      "Epoch 8 -- Batch 605/ 842, training loss 0.4182369112968445\n",
      "Epoch 8 -- Batch 606/ 842, training loss 0.43805262446403503\n",
      "Epoch 8 -- Batch 607/ 842, training loss 0.4220229387283325\n",
      "Epoch 8 -- Batch 608/ 842, training loss 0.43369585275650024\n",
      "Epoch 8 -- Batch 609/ 842, training loss 0.45150551199913025\n",
      "Epoch 8 -- Batch 610/ 842, training loss 0.4303526282310486\n",
      "Epoch 8 -- Batch 611/ 842, training loss 0.44212695956230164\n",
      "Epoch 8 -- Batch 612/ 842, training loss 0.42580318450927734\n",
      "Epoch 8 -- Batch 613/ 842, training loss 0.43073806166648865\n",
      "Epoch 8 -- Batch 614/ 842, training loss 0.42791998386383057\n",
      "Epoch 8 -- Batch 615/ 842, training loss 0.4393504858016968\n",
      "Epoch 8 -- Batch 616/ 842, training loss 0.4280502200126648\n",
      "Epoch 8 -- Batch 617/ 842, training loss 0.4415995180606842\n",
      "Epoch 8 -- Batch 618/ 842, training loss 0.4316439628601074\n",
      "Epoch 8 -- Batch 619/ 842, training loss 0.43094897270202637\n",
      "Epoch 8 -- Batch 620/ 842, training loss 0.4504183232784271\n",
      "Epoch 8 -- Batch 621/ 842, training loss 0.4385288953781128\n",
      "Epoch 8 -- Batch 622/ 842, training loss 0.4329766035079956\n",
      "Epoch 8 -- Batch 623/ 842, training loss 0.4216463267803192\n",
      "Epoch 8 -- Batch 624/ 842, training loss 0.4278959631919861\n",
      "Epoch 8 -- Batch 625/ 842, training loss 0.4297269880771637\n",
      "Epoch 8 -- Batch 626/ 842, training loss 0.428049772977829\n",
      "Epoch 8 -- Batch 627/ 842, training loss 0.42916518449783325\n",
      "Epoch 8 -- Batch 628/ 842, training loss 0.4418964684009552\n",
      "Epoch 8 -- Batch 629/ 842, training loss 0.4451170563697815\n",
      "Epoch 8 -- Batch 630/ 842, training loss 0.43621811270713806\n",
      "Epoch 8 -- Batch 631/ 842, training loss 0.42729446291923523\n",
      "Epoch 8 -- Batch 632/ 842, training loss 0.45398449897766113\n",
      "Epoch 8 -- Batch 633/ 842, training loss 0.43394526839256287\n",
      "Epoch 8 -- Batch 634/ 842, training loss 0.45298340916633606\n",
      "Epoch 8 -- Batch 635/ 842, training loss 0.4288298785686493\n",
      "Epoch 8 -- Batch 636/ 842, training loss 0.43729931116104126\n",
      "Epoch 8 -- Batch 637/ 842, training loss 0.4381505250930786\n",
      "Epoch 8 -- Batch 638/ 842, training loss 0.43291497230529785\n",
      "Epoch 8 -- Batch 639/ 842, training loss 0.4244961142539978\n",
      "Epoch 8 -- Batch 640/ 842, training loss 0.4342724084854126\n",
      "Epoch 8 -- Batch 641/ 842, training loss 0.43910667300224304\n",
      "Epoch 8 -- Batch 642/ 842, training loss 0.4408808648586273\n",
      "Epoch 8 -- Batch 643/ 842, training loss 0.4332217574119568\n",
      "Epoch 8 -- Batch 644/ 842, training loss 0.42566409707069397\n",
      "Epoch 8 -- Batch 645/ 842, training loss 0.4428515136241913\n",
      "Epoch 8 -- Batch 646/ 842, training loss 0.4393099844455719\n",
      "Epoch 8 -- Batch 647/ 842, training loss 0.4359752833843231\n",
      "Epoch 8 -- Batch 648/ 842, training loss 0.45888417959213257\n",
      "Epoch 8 -- Batch 649/ 842, training loss 0.42851918935775757\n",
      "Epoch 8 -- Batch 650/ 842, training loss 0.43016090989112854\n",
      "Epoch 8 -- Batch 651/ 842, training loss 0.4144786298274994\n",
      "Epoch 8 -- Batch 652/ 842, training loss 0.42670077085494995\n",
      "Epoch 8 -- Batch 653/ 842, training loss 0.44750523567199707\n",
      "Epoch 8 -- Batch 654/ 842, training loss 0.4387492537498474\n",
      "Epoch 8 -- Batch 655/ 842, training loss 0.43407711386680603\n",
      "Epoch 8 -- Batch 656/ 842, training loss 0.4407776892185211\n",
      "Epoch 8 -- Batch 657/ 842, training loss 0.434477299451828\n",
      "Epoch 8 -- Batch 658/ 842, training loss 0.4268587827682495\n",
      "Epoch 8 -- Batch 659/ 842, training loss 0.4476398527622223\n",
      "Epoch 8 -- Batch 660/ 842, training loss 0.42081618309020996\n",
      "Epoch 8 -- Batch 661/ 842, training loss 0.43186354637145996\n",
      "Epoch 8 -- Batch 662/ 842, training loss 0.4267333447933197\n",
      "Epoch 8 -- Batch 663/ 842, training loss 0.4329563081264496\n",
      "Epoch 8 -- Batch 664/ 842, training loss 0.43622761964797974\n",
      "Epoch 8 -- Batch 665/ 842, training loss 0.4446065425872803\n",
      "Epoch 8 -- Batch 666/ 842, training loss 0.42099255323410034\n",
      "Epoch 8 -- Batch 667/ 842, training loss 0.452086478471756\n",
      "Epoch 8 -- Batch 668/ 842, training loss 0.42685604095458984\n",
      "Epoch 8 -- Batch 669/ 842, training loss 0.45396658778190613\n",
      "Epoch 8 -- Batch 670/ 842, training loss 0.4315919280052185\n",
      "Epoch 8 -- Batch 671/ 842, training loss 0.4308626055717468\n",
      "Epoch 8 -- Batch 672/ 842, training loss 0.44485044479370117\n",
      "Epoch 8 -- Batch 673/ 842, training loss 0.435397207736969\n",
      "Epoch 8 -- Batch 674/ 842, training loss 0.45694607496261597\n",
      "Epoch 8 -- Batch 675/ 842, training loss 0.4342719316482544\n",
      "Epoch 8 -- Batch 676/ 842, training loss 0.44114431738853455\n",
      "Epoch 8 -- Batch 677/ 842, training loss 0.43500247597694397\n",
      "Epoch 8 -- Batch 678/ 842, training loss 0.4373903274536133\n",
      "Epoch 8 -- Batch 679/ 842, training loss 0.42998480796813965\n",
      "Epoch 8 -- Batch 680/ 842, training loss 0.4299353063106537\n",
      "Epoch 8 -- Batch 681/ 842, training loss 0.44144752621650696\n",
      "Epoch 8 -- Batch 682/ 842, training loss 0.45132318139076233\n",
      "Epoch 8 -- Batch 683/ 842, training loss 0.4324256181716919\n",
      "Epoch 8 -- Batch 684/ 842, training loss 0.427369087934494\n",
      "Epoch 8 -- Batch 685/ 842, training loss 0.4362794756889343\n",
      "Epoch 8 -- Batch 686/ 842, training loss 0.43456149101257324\n",
      "Epoch 8 -- Batch 687/ 842, training loss 0.436650812625885\n",
      "Epoch 8 -- Batch 688/ 842, training loss 0.42042163014411926\n",
      "Epoch 8 -- Batch 689/ 842, training loss 0.4349387288093567\n",
      "Epoch 8 -- Batch 690/ 842, training loss 0.4322516918182373\n",
      "Epoch 8 -- Batch 691/ 842, training loss 0.44308042526245117\n",
      "Epoch 8 -- Batch 692/ 842, training loss 0.4203677475452423\n",
      "Epoch 8 -- Batch 693/ 842, training loss 0.43198516964912415\n",
      "Epoch 8 -- Batch 694/ 842, training loss 0.4378284811973572\n",
      "Epoch 8 -- Batch 695/ 842, training loss 0.428700715303421\n",
      "Epoch 8 -- Batch 696/ 842, training loss 0.4387566149234772\n",
      "Epoch 8 -- Batch 697/ 842, training loss 0.43250414729118347\n",
      "Epoch 8 -- Batch 698/ 842, training loss 0.43053850531578064\n",
      "Epoch 8 -- Batch 699/ 842, training loss 0.4274735152721405\n",
      "Epoch 8 -- Batch 700/ 842, training loss 0.42584604024887085\n",
      "Epoch 8 -- Batch 701/ 842, training loss 0.43557968735694885\n",
      "Epoch 8 -- Batch 702/ 842, training loss 0.42489364743232727\n",
      "Epoch 8 -- Batch 703/ 842, training loss 0.4379337728023529\n",
      "Epoch 8 -- Batch 704/ 842, training loss 0.4313035011291504\n",
      "Epoch 8 -- Batch 705/ 842, training loss 0.42487964034080505\n",
      "Epoch 8 -- Batch 706/ 842, training loss 0.4320599138736725\n",
      "Epoch 8 -- Batch 707/ 842, training loss 0.42980390787124634\n",
      "Epoch 8 -- Batch 708/ 842, training loss 0.4412827491760254\n",
      "Epoch 8 -- Batch 709/ 842, training loss 0.43149533867836\n",
      "Epoch 8 -- Batch 710/ 842, training loss 0.4491227865219116\n",
      "Epoch 8 -- Batch 711/ 842, training loss 0.4389030933380127\n",
      "Epoch 8 -- Batch 712/ 842, training loss 0.4342022240161896\n",
      "Epoch 8 -- Batch 713/ 842, training loss 0.4329877495765686\n",
      "Epoch 8 -- Batch 714/ 842, training loss 0.4283853769302368\n",
      "Epoch 8 -- Batch 715/ 842, training loss 0.4406155049800873\n",
      "Epoch 8 -- Batch 716/ 842, training loss 0.4426123797893524\n",
      "Epoch 8 -- Batch 717/ 842, training loss 0.43158239126205444\n",
      "Epoch 8 -- Batch 718/ 842, training loss 0.42144468426704407\n",
      "Epoch 8 -- Batch 719/ 842, training loss 0.43493229150772095\n",
      "Epoch 8 -- Batch 720/ 842, training loss 0.44796666502952576\n",
      "Epoch 8 -- Batch 721/ 842, training loss 0.41928279399871826\n",
      "Epoch 8 -- Batch 722/ 842, training loss 0.419529527425766\n",
      "Epoch 8 -- Batch 723/ 842, training loss 0.42699098587036133\n",
      "Epoch 8 -- Batch 724/ 842, training loss 0.44049835205078125\n",
      "Epoch 8 -- Batch 725/ 842, training loss 0.43551427125930786\n",
      "Epoch 8 -- Batch 726/ 842, training loss 0.43292099237442017\n",
      "Epoch 8 -- Batch 727/ 842, training loss 0.4277079105377197\n",
      "Epoch 8 -- Batch 728/ 842, training loss 0.44774821400642395\n",
      "Epoch 8 -- Batch 729/ 842, training loss 0.4396250545978546\n",
      "Epoch 8 -- Batch 730/ 842, training loss 0.43010976910591125\n",
      "Epoch 8 -- Batch 731/ 842, training loss 0.43414056301116943\n",
      "Epoch 8 -- Batch 732/ 842, training loss 0.42733287811279297\n",
      "Epoch 8 -- Batch 733/ 842, training loss 0.4469039738178253\n",
      "Epoch 8 -- Batch 734/ 842, training loss 0.4216654598712921\n",
      "Epoch 8 -- Batch 735/ 842, training loss 0.43912264704704285\n",
      "Epoch 8 -- Batch 736/ 842, training loss 0.4383838176727295\n",
      "Epoch 8 -- Batch 737/ 842, training loss 0.4377919137477875\n",
      "Epoch 8 -- Batch 738/ 842, training loss 0.44275984168052673\n",
      "Epoch 8 -- Batch 739/ 842, training loss 0.43999654054641724\n",
      "Epoch 8 -- Batch 740/ 842, training loss 0.44821494817733765\n",
      "Epoch 8 -- Batch 741/ 842, training loss 0.4168426990509033\n",
      "Epoch 8 -- Batch 742/ 842, training loss 0.4271094799041748\n",
      "Epoch 8 -- Batch 743/ 842, training loss 0.4195457100868225\n",
      "Epoch 8 -- Batch 744/ 842, training loss 0.44304823875427246\n",
      "Epoch 8 -- Batch 745/ 842, training loss 0.4231027364730835\n",
      "Epoch 8 -- Batch 746/ 842, training loss 0.4401881694793701\n",
      "Epoch 8 -- Batch 747/ 842, training loss 0.4215303659439087\n",
      "Epoch 8 -- Batch 748/ 842, training loss 0.4460119605064392\n",
      "Epoch 8 -- Batch 749/ 842, training loss 0.4467196464538574\n",
      "Epoch 8 -- Batch 750/ 842, training loss 0.4400297701358795\n",
      "Epoch 8 -- Batch 751/ 842, training loss 0.41898542642593384\n",
      "Epoch 8 -- Batch 752/ 842, training loss 0.436752051115036\n",
      "Epoch 8 -- Batch 753/ 842, training loss 0.4280337989330292\n",
      "Epoch 8 -- Batch 754/ 842, training loss 0.42589354515075684\n",
      "Epoch 8 -- Batch 755/ 842, training loss 0.44584551453590393\n",
      "Epoch 8 -- Batch 756/ 842, training loss 0.43818122148513794\n",
      "Epoch 8 -- Batch 757/ 842, training loss 0.43061622977256775\n",
      "Epoch 8 -- Batch 758/ 842, training loss 0.4369199573993683\n",
      "Epoch 8 -- Batch 759/ 842, training loss 0.4338156580924988\n",
      "Epoch 8 -- Batch 760/ 842, training loss 0.4300576150417328\n",
      "Epoch 8 -- Batch 761/ 842, training loss 0.4323437213897705\n",
      "Epoch 8 -- Batch 762/ 842, training loss 0.44885820150375366\n",
      "Epoch 8 -- Batch 763/ 842, training loss 0.4262551963329315\n",
      "Epoch 8 -- Batch 764/ 842, training loss 0.43325644731521606\n",
      "Epoch 8 -- Batch 765/ 842, training loss 0.4401369094848633\n",
      "Epoch 8 -- Batch 766/ 842, training loss 0.4379783868789673\n",
      "Epoch 8 -- Batch 767/ 842, training loss 0.43386515974998474\n",
      "Epoch 8 -- Batch 768/ 842, training loss 0.43052938580513\n",
      "Epoch 8 -- Batch 769/ 842, training loss 0.41695913672447205\n",
      "Epoch 8 -- Batch 770/ 842, training loss 0.42825374007225037\n",
      "Epoch 8 -- Batch 771/ 842, training loss 0.41471952199935913\n",
      "Epoch 8 -- Batch 772/ 842, training loss 0.4424741268157959\n",
      "Epoch 8 -- Batch 773/ 842, training loss 0.43518441915512085\n",
      "Epoch 8 -- Batch 774/ 842, training loss 0.4411078989505768\n",
      "Epoch 8 -- Batch 775/ 842, training loss 0.44438549876213074\n",
      "Epoch 8 -- Batch 776/ 842, training loss 0.4248311221599579\n",
      "Epoch 8 -- Batch 777/ 842, training loss 0.4215913414955139\n",
      "Epoch 8 -- Batch 778/ 842, training loss 0.4503506124019623\n",
      "Epoch 8 -- Batch 779/ 842, training loss 0.4194487929344177\n",
      "Epoch 8 -- Batch 780/ 842, training loss 0.44693702459335327\n",
      "Epoch 8 -- Batch 781/ 842, training loss 0.4606650769710541\n",
      "Epoch 8 -- Batch 782/ 842, training loss 0.4222012460231781\n",
      "Epoch 8 -- Batch 783/ 842, training loss 0.45428988337516785\n",
      "Epoch 8 -- Batch 784/ 842, training loss 0.43877846002578735\n",
      "Epoch 8 -- Batch 785/ 842, training loss 0.4388365149497986\n",
      "Epoch 8 -- Batch 786/ 842, training loss 0.4308590590953827\n",
      "Epoch 8 -- Batch 787/ 842, training loss 0.421850323677063\n",
      "Epoch 8 -- Batch 788/ 842, training loss 0.42538851499557495\n",
      "Epoch 8 -- Batch 789/ 842, training loss 0.4367428123950958\n",
      "Epoch 8 -- Batch 790/ 842, training loss 0.4228174388408661\n",
      "Epoch 8 -- Batch 791/ 842, training loss 0.4291189908981323\n",
      "Epoch 8 -- Batch 792/ 842, training loss 0.4342833161354065\n",
      "Epoch 8 -- Batch 793/ 842, training loss 0.42678555846214294\n",
      "Epoch 8 -- Batch 794/ 842, training loss 0.4421093761920929\n",
      "Epoch 8 -- Batch 795/ 842, training loss 0.43679365515708923\n",
      "Epoch 8 -- Batch 796/ 842, training loss 0.44888052344322205\n",
      "Epoch 8 -- Batch 797/ 842, training loss 0.4323083758354187\n",
      "Epoch 8 -- Batch 798/ 842, training loss 0.42614254355430603\n",
      "Epoch 8 -- Batch 799/ 842, training loss 0.4262102544307709\n",
      "Epoch 8 -- Batch 800/ 842, training loss 0.43110916018486023\n",
      "Epoch 8 -- Batch 801/ 842, training loss 0.43638870120048523\n",
      "Epoch 8 -- Batch 802/ 842, training loss 0.4457201063632965\n",
      "Epoch 8 -- Batch 803/ 842, training loss 0.44337183237075806\n",
      "Epoch 8 -- Batch 804/ 842, training loss 0.4337978959083557\n",
      "Epoch 8 -- Batch 805/ 842, training loss 0.4413314163684845\n",
      "Epoch 8 -- Batch 806/ 842, training loss 0.4320392906665802\n",
      "Epoch 8 -- Batch 807/ 842, training loss 0.41685912013053894\n",
      "Epoch 8 -- Batch 808/ 842, training loss 0.4354111850261688\n",
      "Epoch 8 -- Batch 809/ 842, training loss 0.44507965445518494\n",
      "Epoch 8 -- Batch 810/ 842, training loss 0.4271588623523712\n",
      "Epoch 8 -- Batch 811/ 842, training loss 0.4286597669124603\n",
      "Epoch 8 -- Batch 812/ 842, training loss 0.4398501217365265\n",
      "Epoch 8 -- Batch 813/ 842, training loss 0.4414447546005249\n",
      "Epoch 8 -- Batch 814/ 842, training loss 0.44077134132385254\n",
      "Epoch 8 -- Batch 815/ 842, training loss 0.43347716331481934\n",
      "Epoch 8 -- Batch 816/ 842, training loss 0.41980743408203125\n",
      "Epoch 8 -- Batch 817/ 842, training loss 0.45005688071250916\n",
      "Epoch 8 -- Batch 818/ 842, training loss 0.41794121265411377\n",
      "Epoch 8 -- Batch 819/ 842, training loss 0.44074440002441406\n",
      "Epoch 8 -- Batch 820/ 842, training loss 0.43672022223472595\n",
      "Epoch 8 -- Batch 821/ 842, training loss 0.4119347929954529\n",
      "Epoch 8 -- Batch 822/ 842, training loss 0.4522990584373474\n",
      "Epoch 8 -- Batch 823/ 842, training loss 0.44085434079170227\n",
      "Epoch 8 -- Batch 824/ 842, training loss 0.43889686465263367\n",
      "Epoch 8 -- Batch 825/ 842, training loss 0.4385661482810974\n",
      "Epoch 8 -- Batch 826/ 842, training loss 0.4386233985424042\n",
      "Epoch 8 -- Batch 827/ 842, training loss 0.4235275685787201\n",
      "Epoch 8 -- Batch 828/ 842, training loss 0.4232204258441925\n",
      "Epoch 8 -- Batch 829/ 842, training loss 0.4198196232318878\n",
      "Epoch 8 -- Batch 830/ 842, training loss 0.4348162114620209\n",
      "Epoch 8 -- Batch 831/ 842, training loss 0.4353065490722656\n",
      "Epoch 8 -- Batch 832/ 842, training loss 0.44624921679496765\n",
      "Epoch 8 -- Batch 833/ 842, training loss 0.43215498328208923\n",
      "Epoch 8 -- Batch 834/ 842, training loss 0.42156359553337097\n",
      "Epoch 8 -- Batch 835/ 842, training loss 0.42523476481437683\n",
      "Epoch 8 -- Batch 836/ 842, training loss 0.4315280616283417\n",
      "Epoch 8 -- Batch 837/ 842, training loss 0.43257808685302734\n",
      "Epoch 8 -- Batch 838/ 842, training loss 0.43248793482780457\n",
      "Epoch 8 -- Batch 839/ 842, training loss 0.44216597080230713\n",
      "Epoch 8 -- Batch 840/ 842, training loss 0.4150596857070923\n",
      "Epoch 8 -- Batch 841/ 842, training loss 0.41938790678977966\n",
      "Epoch 8 -- Batch 842/ 842, training loss 0.4099437892436981\n",
      "----------------------------------------------------------------------\n",
      "Epoch 8 -- Batch 1/ 94, validation loss 0.42100703716278076\n",
      "Epoch 8 -- Batch 2/ 94, validation loss 0.43917542695999146\n",
      "Epoch 8 -- Batch 3/ 94, validation loss 0.43052229285240173\n",
      "Epoch 8 -- Batch 4/ 94, validation loss 0.4286356568336487\n",
      "Epoch 8 -- Batch 5/ 94, validation loss 0.44544166326522827\n",
      "Epoch 8 -- Batch 6/ 94, validation loss 0.42073580622673035\n",
      "Epoch 8 -- Batch 7/ 94, validation loss 0.4267500340938568\n",
      "Epoch 8 -- Batch 8/ 94, validation loss 0.4324587285518646\n",
      "Epoch 8 -- Batch 9/ 94, validation loss 0.4387969672679901\n",
      "Epoch 8 -- Batch 10/ 94, validation loss 0.43144387006759644\n",
      "Epoch 8 -- Batch 11/ 94, validation loss 0.43014204502105713\n",
      "Epoch 8 -- Batch 12/ 94, validation loss 0.4246240258216858\n",
      "Epoch 8 -- Batch 13/ 94, validation loss 0.41883614659309387\n",
      "Epoch 8 -- Batch 14/ 94, validation loss 0.4315619170665741\n",
      "Epoch 8 -- Batch 15/ 94, validation loss 0.4359700679779053\n",
      "Epoch 8 -- Batch 16/ 94, validation loss 0.44512590765953064\n",
      "Epoch 8 -- Batch 17/ 94, validation loss 0.4213000535964966\n",
      "Epoch 8 -- Batch 18/ 94, validation loss 0.4238891303539276\n",
      "Epoch 8 -- Batch 19/ 94, validation loss 0.4282781183719635\n",
      "Epoch 8 -- Batch 20/ 94, validation loss 0.42964351177215576\n",
      "Epoch 8 -- Batch 21/ 94, validation loss 0.41964486241340637\n",
      "Epoch 8 -- Batch 22/ 94, validation loss 0.4244832694530487\n",
      "Epoch 8 -- Batch 23/ 94, validation loss 0.42227691411972046\n",
      "Epoch 8 -- Batch 24/ 94, validation loss 0.4224264621734619\n",
      "Epoch 8 -- Batch 25/ 94, validation loss 0.42080825567245483\n",
      "Epoch 8 -- Batch 26/ 94, validation loss 0.4031963646411896\n",
      "Epoch 8 -- Batch 27/ 94, validation loss 0.4194961190223694\n",
      "Epoch 8 -- Batch 28/ 94, validation loss 0.4136333465576172\n",
      "Epoch 8 -- Batch 29/ 94, validation loss 0.44201481342315674\n",
      "Epoch 8 -- Batch 30/ 94, validation loss 0.4153934121131897\n",
      "Epoch 8 -- Batch 31/ 94, validation loss 0.4514176845550537\n",
      "Epoch 8 -- Batch 32/ 94, validation loss 0.42206525802612305\n",
      "Epoch 8 -- Batch 33/ 94, validation loss 0.4137299060821533\n",
      "Epoch 8 -- Batch 34/ 94, validation loss 0.42924097180366516\n",
      "Epoch 8 -- Batch 35/ 94, validation loss 0.43525975942611694\n",
      "Epoch 8 -- Batch 36/ 94, validation loss 0.42849695682525635\n",
      "Epoch 8 -- Batch 37/ 94, validation loss 0.44552376866340637\n",
      "Epoch 8 -- Batch 38/ 94, validation loss 0.42813706398010254\n",
      "Epoch 8 -- Batch 39/ 94, validation loss 0.4283519685268402\n",
      "Epoch 8 -- Batch 40/ 94, validation loss 0.44124847650527954\n",
      "Epoch 8 -- Batch 41/ 94, validation loss 0.43993768095970154\n",
      "Epoch 8 -- Batch 42/ 94, validation loss 0.4371725022792816\n",
      "Epoch 8 -- Batch 43/ 94, validation loss 0.40352311730384827\n",
      "Epoch 8 -- Batch 44/ 94, validation loss 0.4204177260398865\n",
      "Epoch 8 -- Batch 45/ 94, validation loss 0.40975451469421387\n",
      "Epoch 8 -- Batch 46/ 94, validation loss 0.42870795726776123\n",
      "Epoch 8 -- Batch 47/ 94, validation loss 0.4434516727924347\n",
      "Epoch 8 -- Batch 48/ 94, validation loss 0.43536293506622314\n",
      "Epoch 8 -- Batch 49/ 94, validation loss 0.4184947907924652\n",
      "Epoch 8 -- Batch 50/ 94, validation loss 0.4367225170135498\n",
      "Epoch 8 -- Batch 51/ 94, validation loss 0.40643149614334106\n",
      "Epoch 8 -- Batch 52/ 94, validation loss 0.43438011407852173\n",
      "Epoch 8 -- Batch 53/ 94, validation loss 0.4245089888572693\n",
      "Epoch 8 -- Batch 54/ 94, validation loss 0.42098063230514526\n",
      "Epoch 8 -- Batch 55/ 94, validation loss 0.43626853823661804\n",
      "Epoch 8 -- Batch 56/ 94, validation loss 0.4485817849636078\n",
      "Epoch 8 -- Batch 57/ 94, validation loss 0.4131753742694855\n",
      "Epoch 8 -- Batch 58/ 94, validation loss 0.435724139213562\n",
      "Epoch 8 -- Batch 59/ 94, validation loss 0.42508354783058167\n",
      "Epoch 8 -- Batch 60/ 94, validation loss 0.4110631048679352\n",
      "Epoch 8 -- Batch 61/ 94, validation loss 0.4393114149570465\n",
      "Epoch 8 -- Batch 62/ 94, validation loss 0.43596601486206055\n",
      "Epoch 8 -- Batch 63/ 94, validation loss 0.4213455021381378\n",
      "Epoch 8 -- Batch 64/ 94, validation loss 0.4219176769256592\n",
      "Epoch 8 -- Batch 65/ 94, validation loss 0.4230794310569763\n",
      "Epoch 8 -- Batch 66/ 94, validation loss 0.4234989881515503\n",
      "Epoch 8 -- Batch 67/ 94, validation loss 0.42094478011131287\n",
      "Epoch 8 -- Batch 68/ 94, validation loss 0.4175088405609131\n",
      "Epoch 8 -- Batch 69/ 94, validation loss 0.43181663751602173\n",
      "Epoch 8 -- Batch 70/ 94, validation loss 0.4114258289337158\n",
      "Epoch 8 -- Batch 71/ 94, validation loss 0.44595980644226074\n",
      "Epoch 8 -- Batch 72/ 94, validation loss 0.43224194645881653\n",
      "Epoch 8 -- Batch 73/ 94, validation loss 0.41661128401756287\n",
      "Epoch 8 -- Batch 74/ 94, validation loss 0.4255462884902954\n",
      "Epoch 8 -- Batch 75/ 94, validation loss 0.42103248834609985\n",
      "Epoch 8 -- Batch 76/ 94, validation loss 0.4302944242954254\n",
      "Epoch 8 -- Batch 77/ 94, validation loss 0.4295719563961029\n",
      "Epoch 8 -- Batch 78/ 94, validation loss 0.44218650460243225\n",
      "Epoch 8 -- Batch 79/ 94, validation loss 0.4186982810497284\n",
      "Epoch 8 -- Batch 80/ 94, validation loss 0.43375346064567566\n",
      "Epoch 8 -- Batch 81/ 94, validation loss 0.4219322204589844\n",
      "Epoch 8 -- Batch 82/ 94, validation loss 0.43635880947113037\n",
      "Epoch 8 -- Batch 83/ 94, validation loss 0.4304267168045044\n",
      "Epoch 8 -- Batch 84/ 94, validation loss 0.43526503443717957\n",
      "Epoch 8 -- Batch 85/ 94, validation loss 0.40724554657936096\n",
      "Epoch 8 -- Batch 86/ 94, validation loss 0.4396228492259979\n",
      "Epoch 8 -- Batch 87/ 94, validation loss 0.432704359292984\n",
      "Epoch 8 -- Batch 88/ 94, validation loss 0.4415874183177948\n",
      "Epoch 8 -- Batch 89/ 94, validation loss 0.43171611428260803\n",
      "Epoch 8 -- Batch 90/ 94, validation loss 0.4143967926502228\n",
      "Epoch 8 -- Batch 91/ 94, validation loss 0.44110772013664246\n",
      "Epoch 8 -- Batch 92/ 94, validation loss 0.42756372690200806\n",
      "Epoch 8 -- Batch 93/ 94, validation loss 0.44633060693740845\n",
      "Epoch 8 -- Batch 94/ 94, validation loss 0.4587579071521759\n",
      "----------------------------------------------------------------------\n",
      "Epoch 8 loss: Training 0.4368181526660919, Validation 0.4587579071521759\n",
      "----------------------------------------------------------------------\n",
      "Epoch 9/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 12 13\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 5 6 23\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-c1ccc2c(c1)-c1c(cnn2CC(=O)NCc1ccc2c(c1)OCCO2)C2'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 16 17 18 19 20 23\n",
      "[19:05:07] Explicit valence for atom # 15 S, 7, is greater than permitted\n",
      "[19:05:07] non-ring atom 9 marked aromatic\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 19 20\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 22 23\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 10\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 12 16 17 18 19 20 21\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 21 22\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 6 7 8 13 14 15\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 12 13 15 17 19\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 32\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'C[C@]12CCCCC1CC1C(N3CCN(C(=O)OCC(F)(F)F)CC3)OC(O)(C2CC2)C1O'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 11 12 17\n",
      "[19:05:07] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC(=O)N(Cc2ccc(OS(=O)(=O)c2cccc(C(F)(F)F)c2)cc1)Cc1cccs1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 3 15 16 17 18 19 20\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1cc(C2C3=C(CC(C)(C)CC3=O)Oc3ccccc3C2=C2C(C)(C)CCC3=O)cc1OC(=O)C1CC1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 24 25 27\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Fc1cc(Cl)c2c(c1)-c1ccc(CN1CCCC(O)CC1)OCc1ccccc1Cl'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 17 18 19 21 22\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C#Cc2ccc3c(c2)C(c2cccs2)=NN2C(=O)CC2CCCC2)cc1'\n",
      "[19:05:07] SMILES Parse Error: extra close parentheses while parsing: Cc1[nH]nc2c1C(c1cc(Br)ccc1F)C(C#N)=C(N)O2)C1(C#N)C#N\n",
      "[19:05:07] SMILES Parse Error: Failed parsing SMILES 'Cc1[nH]nc2c1C(c1cc(Br)ccc1F)C(C#N)=C(N)O2)C1(C#N)C#N' for input: 'Cc1[nH]nc2c1C(c1cc(Br)ccc1F)C(C#N)=C(N)O2)C1(C#N)C#N'\n",
      "[19:05:07] SMILES Parse Error: extra open parentheses for input: 'CS(=O)(=O)N1[C@H](CO)[C@@H](c2ccccc2)[C@@H]1CNC(C(=O)NCC1CC1'\n",
      "[19:05:07] SMILES Parse Error: ring closure 2 duplicates bond between atom 13 and atom 14 for input: 'CCOC(=O)N1CCC(NCCCC2c2ccc3c(c2)OCO3)CC1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 9\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 14 15 17 18 20 31\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 16 18 25 26 27 28\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)N2[C@H]3CC=CC(CC(=O)N3CCOCC3)C2)cc1'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CCCCCCCn1c(O)c(=[N+]2[nH]c2ccc(OCC)cc21)\\C(=N/Nc1nc(C)cc(C)n1)C(=O)O'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 14 18 19\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 7 24\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC1=CC(C)(C)N(N2C(=O)C=C/c3ccc(Cl)cc3)C2C=CC1C3'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7 8 16 17 18\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 15 16 17 19 21 23 24\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 16 17 18 19\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 4 6 26\n",
      "[19:05:07] SMILES Parse Error: extra close parentheses while parsing: CCC(=O)N1c2ccc(S(=O)(=O)N3CCCC3)cc2C2CCCN(Cc3ncnc4sccc34)C2)C1\n",
      "[19:05:07] SMILES Parse Error: Failed parsing SMILES 'CCC(=O)N1c2ccc(S(=O)(=O)N3CCCC3)cc2C2CCCN(Cc3ncnc4sccc34)C2)C1' for input: 'CCC(=O)N1c2ccc(S(=O)(=O)N3CCCC3)cc2C2CCCN(Cc3ncnc4sccc34)C2)C1'\n",
      "[19:05:07] SMILES Parse Error: extra open parentheses for input: 'CC(C)NC(=O)Nc1ccc2c(c1)CC(=O)N([C@H](C)CO)C[C@@H](C)[C@H](CN(C)C'\n",
      "[19:05:07] SMILES Parse Error: extra close parentheses while parsing: CCc1ccccc1NC(=O)CN(C)S(=O)(=O)c1ccc2c(c1)CCN2C(=O)CC)c1ccccc1\n",
      "[19:05:07] SMILES Parse Error: Failed parsing SMILES 'CCc1ccccc1NC(=O)CN(C)S(=O)(=O)c1ccc2c(c1)CCN2C(=O)CC)c1ccccc1' for input: 'CCc1ccccc1NC(=O)CN(C)S(=O)(=O)c1ccc2c(c1)CCN2C(=O)CC)c1ccccc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 3 7 27 28\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 35\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7 21 22\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 22 24 25 26\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC1(C)C2CC=C(C3)C(=N)C(N)C(=O)C12C(=O)N1CCOCC1'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Cc1nn(C)c(C)c1-c1ncccc1CN1CCOC(CC1(C)C(=O)Nc2ccc(Cl)c(C(F)(F)F)c2)C1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 3 4 5 12 14\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)CN(=c1sc2nc3c(cc1Cl)OCO3)S(=O)(=O)c1ccc(Cl)cc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 4 5 27\n",
      "[19:05:07] Explicit valence for atom # 2 Br, 2, is greater than permitted\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 12 13 32\n",
      "[19:05:07] SMILES Parse Error: extra close parentheses while parsing: CN(C)CCO)c1ccc(CN(Cc2ccccc2)S(=O)(=O)c2ccc(Br)cc2)cc1\n",
      "[19:05:07] SMILES Parse Error: Failed parsing SMILES 'CN(C)CCO)c1ccc(CN(Cc2ccccc2)S(=O)(=O)c2ccc(Br)cc2)cc1' for input: 'CN(C)CCO)c1ccc(CN(Cc2ccccc2)S(=O)(=O)c2ccc(Br)cc2)cc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:05:07] SMILES Parse Error: extra close parentheses while parsing: CN(C)/C=C1\\CC(Oc2ccccc2)O1)C(=O)OCc1ccccc1\n",
      "[19:05:07] SMILES Parse Error: Failed parsing SMILES 'CN(C)/C=C1\\CC(Oc2ccccc2)O1)C(=O)OCc1ccccc1' for input: 'CN(C)/C=C1\\CC(Oc2ccccc2)O1)C(=O)OCc1ccccc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 13 14 16 17 19 21 22\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 8 9 12 27 28 29 30\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17 18 19 21 22\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c2nc(N3C(=O)C4C5C=CC(C5)C3C3=O)sc2c1'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)N2CCN3C(=O)[C@H](Cc4c[nH]c5cccc(O)c44)C(=O)N3C2)cc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 16 17 19\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Cn2ccc3cc4c(ncn3-c3ccc(OC)cc3)c2=O)cc1'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC1CCN(C(=O)C2CCN(C(=O)C3CCN(S(C)(=O)=O)CC2)CC2)cc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 19 20\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 3 4 5 24 25\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1cccc(O)c1=C1'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC(C)CC(C)(C)C1CCC2(CC1)CC2C(=O)O1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 10 11 20 21 22\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 12 13 14 24 26 29\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC(C)CN(Cc1cc(NCc2cccc(-n3cccn3)cc2)c2nonc2C1CCOCC1)C(=O)NCC1CCCO1'\n",
      "[19:05:07] Explicit valence for atom # 19 Br, 2, is greater than permitted\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 4\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1nc2c3c(s1)CCCCC2)C(=O)NC1CCCCC1'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1cccc(CN2CC3(CN(Cc4ccccc4)C3)c3c(n(C)c3cc(OC)ccc23)[C@@H]2CO)c1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[19:05:07] SMILES Parse Error: extra open parentheses for input: 'CCC(C(=O)N(CC(=O)N(CCc1cccnc1)Cc1cccs1)C(=O)N(CCC)CC'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(CN3CCC4(CC3)NC(=O)c3ccncc3)sc2c1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 11 12 21\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'C[C@]12CCC3(CC[C@H]4C[C@H](OC(=O)OC5CCCC53C)(C2=O)C4C2=O)[C@@H]1CO'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'O=C1C2C(c3ccccc3)C=CC(=NN2CCOCC2)=C(O)N1c1ccc(Oc2ccccc2)cc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 6 8 9 10 21 22 23\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2oc3c(c(=O)c1-c1ccccc1)C(c1ccc(O)cc1)C3C#N'\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CCCCN1C(=O)C2C(c3ccc(OC)cc3)NN(CC(=O)OCc3ccccc3)C2=C1C(=O)CCC2'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 24\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 4 5 26\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 15\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)N2CCN(Cc3nc(-c4ccc5ccccc5c4)oc4C)CC2)cc1'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 15 16\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'O=c1oc2ccc(Br)cc2cc1-c1nc2sc3nc(N4CCOCC4)c4c(cc3c2[nH]CCC5)nn12'\n",
      "[19:05:07] Can't kekulize mol.  Unkekulized atoms: 12 13 21\n",
      "[19:05:07] SMILES Parse Error: unclosed ring for input: 'CC(Sc1nc2cc(Cc3ccccc3F)ccc1OC#N)C(=O)O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 -- Batch 1/ 842, training loss 0.4141328036785126\n",
      "Epoch 9 -- Batch 2/ 842, training loss 0.405374675989151\n",
      "Epoch 9 -- Batch 3/ 842, training loss 0.44037681818008423\n",
      "Epoch 9 -- Batch 4/ 842, training loss 0.4240405559539795\n",
      "Epoch 9 -- Batch 5/ 842, training loss 0.43104496598243713\n",
      "Epoch 9 -- Batch 6/ 842, training loss 0.4211340546607971\n",
      "Epoch 9 -- Batch 7/ 842, training loss 0.4258345067501068\n",
      "Epoch 9 -- Batch 8/ 842, training loss 0.41820770502090454\n",
      "Epoch 9 -- Batch 9/ 842, training loss 0.41410553455352783\n",
      "Epoch 9 -- Batch 10/ 842, training loss 0.4191223978996277\n",
      "Epoch 9 -- Batch 11/ 842, training loss 0.4063754379749298\n",
      "Epoch 9 -- Batch 12/ 842, training loss 0.4177328050136566\n",
      "Epoch 9 -- Batch 13/ 842, training loss 0.42494046688079834\n",
      "Epoch 9 -- Batch 14/ 842, training loss 0.4191119074821472\n",
      "Epoch 9 -- Batch 15/ 842, training loss 0.42055267095565796\n",
      "Epoch 9 -- Batch 16/ 842, training loss 0.41784876585006714\n",
      "Epoch 9 -- Batch 17/ 842, training loss 0.42059874534606934\n",
      "Epoch 9 -- Batch 18/ 842, training loss 0.41303977370262146\n",
      "Epoch 9 -- Batch 19/ 842, training loss 0.43797966837882996\n",
      "Epoch 9 -- Batch 20/ 842, training loss 0.41808247566223145\n",
      "Epoch 9 -- Batch 21/ 842, training loss 0.4478490352630615\n",
      "Epoch 9 -- Batch 22/ 842, training loss 0.43308523297309875\n",
      "Epoch 9 -- Batch 23/ 842, training loss 0.41920149326324463\n",
      "Epoch 9 -- Batch 24/ 842, training loss 0.4389067590236664\n",
      "Epoch 9 -- Batch 25/ 842, training loss 0.4225281774997711\n",
      "Epoch 9 -- Batch 26/ 842, training loss 0.4189850986003876\n",
      "Epoch 9 -- Batch 27/ 842, training loss 0.4245838522911072\n",
      "Epoch 9 -- Batch 28/ 842, training loss 0.42322611808776855\n",
      "Epoch 9 -- Batch 29/ 842, training loss 0.4213676154613495\n",
      "Epoch 9 -- Batch 30/ 842, training loss 0.4049449563026428\n",
      "Epoch 9 -- Batch 31/ 842, training loss 0.42952796816825867\n",
      "Epoch 9 -- Batch 32/ 842, training loss 0.41298580169677734\n",
      "Epoch 9 -- Batch 33/ 842, training loss 0.4160802364349365\n",
      "Epoch 9 -- Batch 34/ 842, training loss 0.4245505630970001\n",
      "Epoch 9 -- Batch 35/ 842, training loss 0.41708841919898987\n",
      "Epoch 9 -- Batch 36/ 842, training loss 0.40982672572135925\n",
      "Epoch 9 -- Batch 37/ 842, training loss 0.4203704297542572\n",
      "Epoch 9 -- Batch 38/ 842, training loss 0.39446142315864563\n",
      "Epoch 9 -- Batch 39/ 842, training loss 0.40224671363830566\n",
      "Epoch 9 -- Batch 40/ 842, training loss 0.4272195100784302\n",
      "Epoch 9 -- Batch 41/ 842, training loss 0.3967835307121277\n",
      "Epoch 9 -- Batch 42/ 842, training loss 0.419644832611084\n",
      "Epoch 9 -- Batch 43/ 842, training loss 0.4304359555244446\n",
      "Epoch 9 -- Batch 44/ 842, training loss 0.42172959446907043\n",
      "Epoch 9 -- Batch 45/ 842, training loss 0.4247691333293915\n",
      "Epoch 9 -- Batch 46/ 842, training loss 0.41937193274497986\n",
      "Epoch 9 -- Batch 47/ 842, training loss 0.4283026158809662\n",
      "Epoch 9 -- Batch 48/ 842, training loss 0.41825050115585327\n",
      "Epoch 9 -- Batch 49/ 842, training loss 0.4378562271595001\n",
      "Epoch 9 -- Batch 50/ 842, training loss 0.4050629734992981\n",
      "Epoch 9 -- Batch 51/ 842, training loss 0.42768651247024536\n",
      "Epoch 9 -- Batch 52/ 842, training loss 0.4065166413784027\n",
      "Epoch 9 -- Batch 53/ 842, training loss 0.42860516905784607\n",
      "Epoch 9 -- Batch 54/ 842, training loss 0.4124912917613983\n",
      "Epoch 9 -- Batch 55/ 842, training loss 0.4095454216003418\n",
      "Epoch 9 -- Batch 56/ 842, training loss 0.42904746532440186\n",
      "Epoch 9 -- Batch 57/ 842, training loss 0.4214349389076233\n",
      "Epoch 9 -- Batch 58/ 842, training loss 0.41824817657470703\n",
      "Epoch 9 -- Batch 59/ 842, training loss 0.4314497709274292\n",
      "Epoch 9 -- Batch 60/ 842, training loss 0.4288802444934845\n",
      "Epoch 9 -- Batch 61/ 842, training loss 0.406554639339447\n",
      "Epoch 9 -- Batch 62/ 842, training loss 0.4364352822303772\n",
      "Epoch 9 -- Batch 63/ 842, training loss 0.4105014503002167\n",
      "Epoch 9 -- Batch 64/ 842, training loss 0.4271668791770935\n",
      "Epoch 9 -- Batch 65/ 842, training loss 0.429097056388855\n",
      "Epoch 9 -- Batch 66/ 842, training loss 0.41922909021377563\n",
      "Epoch 9 -- Batch 67/ 842, training loss 0.42120862007141113\n",
      "Epoch 9 -- Batch 68/ 842, training loss 0.41484132409095764\n",
      "Epoch 9 -- Batch 69/ 842, training loss 0.41455715894699097\n",
      "Epoch 9 -- Batch 70/ 842, training loss 0.43028467893600464\n",
      "Epoch 9 -- Batch 71/ 842, training loss 0.4283333718776703\n",
      "Epoch 9 -- Batch 72/ 842, training loss 0.41932663321495056\n",
      "Epoch 9 -- Batch 73/ 842, training loss 0.4137742519378662\n",
      "Epoch 9 -- Batch 74/ 842, training loss 0.43891996145248413\n",
      "Epoch 9 -- Batch 75/ 842, training loss 0.4106273651123047\n",
      "Epoch 9 -- Batch 76/ 842, training loss 0.4273556172847748\n",
      "Epoch 9 -- Batch 77/ 842, training loss 0.4047388434410095\n",
      "Epoch 9 -- Batch 78/ 842, training loss 0.42341721057891846\n",
      "Epoch 9 -- Batch 79/ 842, training loss 0.4246334135532379\n",
      "Epoch 9 -- Batch 80/ 842, training loss 0.4276280403137207\n",
      "Epoch 9 -- Batch 81/ 842, training loss 0.40843531489372253\n",
      "Epoch 9 -- Batch 82/ 842, training loss 0.41582342982292175\n",
      "Epoch 9 -- Batch 83/ 842, training loss 0.42678046226501465\n",
      "Epoch 9 -- Batch 84/ 842, training loss 0.4137211740016937\n",
      "Epoch 9 -- Batch 85/ 842, training loss 0.4195650517940521\n",
      "Epoch 9 -- Batch 86/ 842, training loss 0.41264739632606506\n",
      "Epoch 9 -- Batch 87/ 842, training loss 0.41742175817489624\n",
      "Epoch 9 -- Batch 88/ 842, training loss 0.4042504131793976\n",
      "Epoch 9 -- Batch 89/ 842, training loss 0.43865853548049927\n",
      "Epoch 9 -- Batch 90/ 842, training loss 0.4179838001728058\n",
      "Epoch 9 -- Batch 91/ 842, training loss 0.41265785694122314\n",
      "Epoch 9 -- Batch 92/ 842, training loss 0.42371705174446106\n",
      "Epoch 9 -- Batch 93/ 842, training loss 0.4331803023815155\n",
      "Epoch 9 -- Batch 94/ 842, training loss 0.41710835695266724\n",
      "Epoch 9 -- Batch 95/ 842, training loss 0.4132216274738312\n",
      "Epoch 9 -- Batch 96/ 842, training loss 0.40997564792633057\n",
      "Epoch 9 -- Batch 97/ 842, training loss 0.41585788130760193\n",
      "Epoch 9 -- Batch 98/ 842, training loss 0.40868234634399414\n",
      "Epoch 9 -- Batch 99/ 842, training loss 0.41737717390060425\n",
      "Epoch 9 -- Batch 100/ 842, training loss 0.42395076155662537\n",
      "Epoch 9 -- Batch 101/ 842, training loss 0.43220722675323486\n",
      "Epoch 9 -- Batch 102/ 842, training loss 0.4435492157936096\n",
      "Epoch 9 -- Batch 103/ 842, training loss 0.40609583258628845\n",
      "Epoch 9 -- Batch 104/ 842, training loss 0.4109801650047302\n",
      "Epoch 9 -- Batch 105/ 842, training loss 0.42643609642982483\n",
      "Epoch 9 -- Batch 106/ 842, training loss 0.4287187457084656\n",
      "Epoch 9 -- Batch 107/ 842, training loss 0.4324754774570465\n",
      "Epoch 9 -- Batch 108/ 842, training loss 0.43084725737571716\n",
      "Epoch 9 -- Batch 109/ 842, training loss 0.4208865463733673\n",
      "Epoch 9 -- Batch 110/ 842, training loss 0.40648460388183594\n",
      "Epoch 9 -- Batch 111/ 842, training loss 0.4111845791339874\n",
      "Epoch 9 -- Batch 112/ 842, training loss 0.419810950756073\n",
      "Epoch 9 -- Batch 113/ 842, training loss 0.4193008244037628\n",
      "Epoch 9 -- Batch 114/ 842, training loss 0.40889984369277954\n",
      "Epoch 9 -- Batch 115/ 842, training loss 0.39352768659591675\n",
      "Epoch 9 -- Batch 116/ 842, training loss 0.4141673743724823\n",
      "Epoch 9 -- Batch 117/ 842, training loss 0.42726394534111023\n",
      "Epoch 9 -- Batch 118/ 842, training loss 0.42910417914390564\n",
      "Epoch 9 -- Batch 119/ 842, training loss 0.402900755405426\n",
      "Epoch 9 -- Batch 120/ 842, training loss 0.4193710684776306\n",
      "Epoch 9 -- Batch 121/ 842, training loss 0.42658257484436035\n",
      "Epoch 9 -- Batch 122/ 842, training loss 0.42309844493865967\n",
      "Epoch 9 -- Batch 123/ 842, training loss 0.40469151735305786\n",
      "Epoch 9 -- Batch 124/ 842, training loss 0.4322582185268402\n",
      "Epoch 9 -- Batch 125/ 842, training loss 0.41729220747947693\n",
      "Epoch 9 -- Batch 126/ 842, training loss 0.4191986620426178\n",
      "Epoch 9 -- Batch 127/ 842, training loss 0.41337159276008606\n",
      "Epoch 9 -- Batch 128/ 842, training loss 0.4162764549255371\n",
      "Epoch 9 -- Batch 129/ 842, training loss 0.42379361391067505\n",
      "Epoch 9 -- Batch 130/ 842, training loss 0.4375184178352356\n",
      "Epoch 9 -- Batch 131/ 842, training loss 0.42258933186531067\n",
      "Epoch 9 -- Batch 132/ 842, training loss 0.423167884349823\n",
      "Epoch 9 -- Batch 133/ 842, training loss 0.4246676564216614\n",
      "Epoch 9 -- Batch 134/ 842, training loss 0.41646823287010193\n",
      "Epoch 9 -- Batch 135/ 842, training loss 0.4133003354072571\n",
      "Epoch 9 -- Batch 136/ 842, training loss 0.4126811921596527\n",
      "Epoch 9 -- Batch 137/ 842, training loss 0.41708749532699585\n",
      "Epoch 9 -- Batch 138/ 842, training loss 0.42340028285980225\n",
      "Epoch 9 -- Batch 139/ 842, training loss 0.4437396824359894\n",
      "Epoch 9 -- Batch 140/ 842, training loss 0.42771369218826294\n",
      "Epoch 9 -- Batch 141/ 842, training loss 0.42136046290397644\n",
      "Epoch 9 -- Batch 142/ 842, training loss 0.4378971457481384\n",
      "Epoch 9 -- Batch 143/ 842, training loss 0.4121445119380951\n",
      "Epoch 9 -- Batch 144/ 842, training loss 0.4203876554965973\n",
      "Epoch 9 -- Batch 145/ 842, training loss 0.4045405983924866\n",
      "Epoch 9 -- Batch 146/ 842, training loss 0.4242973327636719\n",
      "Epoch 9 -- Batch 147/ 842, training loss 0.4225122332572937\n",
      "Epoch 9 -- Batch 148/ 842, training loss 0.4152456521987915\n",
      "Epoch 9 -- Batch 149/ 842, training loss 0.406970351934433\n",
      "Epoch 9 -- Batch 150/ 842, training loss 0.4100329875946045\n",
      "Epoch 9 -- Batch 151/ 842, training loss 0.42050233483314514\n",
      "Epoch 9 -- Batch 152/ 842, training loss 0.427082359790802\n",
      "Epoch 9 -- Batch 153/ 842, training loss 0.41662079095840454\n",
      "Epoch 9 -- Batch 154/ 842, training loss 0.4081197679042816\n",
      "Epoch 9 -- Batch 155/ 842, training loss 0.4209904968738556\n",
      "Epoch 9 -- Batch 156/ 842, training loss 0.42525213956832886\n",
      "Epoch 9 -- Batch 157/ 842, training loss 0.42021793127059937\n",
      "Epoch 9 -- Batch 158/ 842, training loss 0.4297822415828705\n",
      "Epoch 9 -- Batch 159/ 842, training loss 0.42398130893707275\n",
      "Epoch 9 -- Batch 160/ 842, training loss 0.4206896722316742\n",
      "Epoch 9 -- Batch 161/ 842, training loss 0.41371598839759827\n",
      "Epoch 9 -- Batch 162/ 842, training loss 0.42411643266677856\n",
      "Epoch 9 -- Batch 163/ 842, training loss 0.3953942358493805\n",
      "Epoch 9 -- Batch 164/ 842, training loss 0.43025296926498413\n",
      "Epoch 9 -- Batch 165/ 842, training loss 0.42028141021728516\n",
      "Epoch 9 -- Batch 166/ 842, training loss 0.4320479929447174\n",
      "Epoch 9 -- Batch 167/ 842, training loss 0.4215925335884094\n",
      "Epoch 9 -- Batch 168/ 842, training loss 0.4188092350959778\n",
      "Epoch 9 -- Batch 169/ 842, training loss 0.42440086603164673\n",
      "Epoch 9 -- Batch 170/ 842, training loss 0.43915536999702454\n",
      "Epoch 9 -- Batch 171/ 842, training loss 0.42414534091949463\n",
      "Epoch 9 -- Batch 172/ 842, training loss 0.41173940896987915\n",
      "Epoch 9 -- Batch 173/ 842, training loss 0.4013795554637909\n",
      "Epoch 9 -- Batch 174/ 842, training loss 0.421576589345932\n",
      "Epoch 9 -- Batch 175/ 842, training loss 0.4281535744667053\n",
      "Epoch 9 -- Batch 176/ 842, training loss 0.4316888451576233\n",
      "Epoch 9 -- Batch 177/ 842, training loss 0.4156475365161896\n",
      "Epoch 9 -- Batch 178/ 842, training loss 0.42424216866493225\n",
      "Epoch 9 -- Batch 179/ 842, training loss 0.4021267294883728\n",
      "Epoch 9 -- Batch 180/ 842, training loss 0.4427613317966461\n",
      "Epoch 9 -- Batch 181/ 842, training loss 0.4119190275669098\n",
      "Epoch 9 -- Batch 182/ 842, training loss 0.431601881980896\n",
      "Epoch 9 -- Batch 183/ 842, training loss 0.41498515009880066\n",
      "Epoch 9 -- Batch 184/ 842, training loss 0.43037036061286926\n",
      "Epoch 9 -- Batch 185/ 842, training loss 0.4274670481681824\n",
      "Epoch 9 -- Batch 186/ 842, training loss 0.4149923324584961\n",
      "Epoch 9 -- Batch 187/ 842, training loss 0.4243229925632477\n",
      "Epoch 9 -- Batch 188/ 842, training loss 0.44299644231796265\n",
      "Epoch 9 -- Batch 189/ 842, training loss 0.4444737136363983\n",
      "Epoch 9 -- Batch 190/ 842, training loss 0.4230392277240753\n",
      "Epoch 9 -- Batch 191/ 842, training loss 0.42522674798965454\n",
      "Epoch 9 -- Batch 192/ 842, training loss 0.42602309584617615\n",
      "Epoch 9 -- Batch 193/ 842, training loss 0.4125564396381378\n",
      "Epoch 9 -- Batch 194/ 842, training loss 0.4205726385116577\n",
      "Epoch 9 -- Batch 195/ 842, training loss 0.4244481027126312\n",
      "Epoch 9 -- Batch 196/ 842, training loss 0.4016689360141754\n",
      "Epoch 9 -- Batch 197/ 842, training loss 0.41899386048316956\n",
      "Epoch 9 -- Batch 198/ 842, training loss 0.41282469034194946\n",
      "Epoch 9 -- Batch 199/ 842, training loss 0.4044921398162842\n",
      "Epoch 9 -- Batch 200/ 842, training loss 0.4191287159919739\n",
      "Epoch 9 -- Batch 201/ 842, training loss 0.43180450797080994\n",
      "Epoch 9 -- Batch 202/ 842, training loss 0.425376296043396\n",
      "Epoch 9 -- Batch 203/ 842, training loss 0.4153594970703125\n",
      "Epoch 9 -- Batch 204/ 842, training loss 0.41450035572052\n",
      "Epoch 9 -- Batch 205/ 842, training loss 0.40479162335395813\n",
      "Epoch 9 -- Batch 206/ 842, training loss 0.4240821897983551\n",
      "Epoch 9 -- Batch 207/ 842, training loss 0.4233330190181732\n",
      "Epoch 9 -- Batch 208/ 842, training loss 0.4260796308517456\n",
      "Epoch 9 -- Batch 209/ 842, training loss 0.4088101387023926\n",
      "Epoch 9 -- Batch 210/ 842, training loss 0.43150565028190613\n",
      "Epoch 9 -- Batch 211/ 842, training loss 0.4287654757499695\n",
      "Epoch 9 -- Batch 212/ 842, training loss 0.41163700819015503\n",
      "Epoch 9 -- Batch 213/ 842, training loss 0.4069928526878357\n",
      "Epoch 9 -- Batch 214/ 842, training loss 0.4244999587535858\n",
      "Epoch 9 -- Batch 215/ 842, training loss 0.41315388679504395\n",
      "Epoch 9 -- Batch 216/ 842, training loss 0.42675232887268066\n",
      "Epoch 9 -- Batch 217/ 842, training loss 0.4424000680446625\n",
      "Epoch 9 -- Batch 218/ 842, training loss 0.41537582874298096\n",
      "Epoch 9 -- Batch 219/ 842, training loss 0.43373069167137146\n",
      "Epoch 9 -- Batch 220/ 842, training loss 0.4311390519142151\n",
      "Epoch 9 -- Batch 221/ 842, training loss 0.4173837900161743\n",
      "Epoch 9 -- Batch 222/ 842, training loss 0.4011024534702301\n",
      "Epoch 9 -- Batch 223/ 842, training loss 0.41150808334350586\n",
      "Epoch 9 -- Batch 224/ 842, training loss 0.4224044382572174\n",
      "Epoch 9 -- Batch 225/ 842, training loss 0.4156738221645355\n",
      "Epoch 9 -- Batch 226/ 842, training loss 0.43198683857917786\n",
      "Epoch 9 -- Batch 227/ 842, training loss 0.42465144395828247\n",
      "Epoch 9 -- Batch 228/ 842, training loss 0.44006434082984924\n",
      "Epoch 9 -- Batch 229/ 842, training loss 0.4070572257041931\n",
      "Epoch 9 -- Batch 230/ 842, training loss 0.4345192313194275\n",
      "Epoch 9 -- Batch 231/ 842, training loss 0.4312587380409241\n",
      "Epoch 9 -- Batch 232/ 842, training loss 0.4238075017929077\n",
      "Epoch 9 -- Batch 233/ 842, training loss 0.4273747205734253\n",
      "Epoch 9 -- Batch 234/ 842, training loss 0.4107360541820526\n",
      "Epoch 9 -- Batch 235/ 842, training loss 0.4236237108707428\n",
      "Epoch 9 -- Batch 236/ 842, training loss 0.4272550940513611\n",
      "Epoch 9 -- Batch 237/ 842, training loss 0.4158594608306885\n",
      "Epoch 9 -- Batch 238/ 842, training loss 0.42503637075424194\n",
      "Epoch 9 -- Batch 239/ 842, training loss 0.4033432602882385\n",
      "Epoch 9 -- Batch 240/ 842, training loss 0.4246969521045685\n",
      "Epoch 9 -- Batch 241/ 842, training loss 0.4170197546482086\n",
      "Epoch 9 -- Batch 242/ 842, training loss 0.4173693358898163\n",
      "Epoch 9 -- Batch 243/ 842, training loss 0.43141940236091614\n",
      "Epoch 9 -- Batch 244/ 842, training loss 0.4203243851661682\n",
      "Epoch 9 -- Batch 245/ 842, training loss 0.41620808839797974\n",
      "Epoch 9 -- Batch 246/ 842, training loss 0.42506229877471924\n",
      "Epoch 9 -- Batch 247/ 842, training loss 0.41952675580978394\n",
      "Epoch 9 -- Batch 248/ 842, training loss 0.4185609817504883\n",
      "Epoch 9 -- Batch 249/ 842, training loss 0.4335661232471466\n",
      "Epoch 9 -- Batch 250/ 842, training loss 0.4220842123031616\n",
      "Epoch 9 -- Batch 251/ 842, training loss 0.4149736166000366\n",
      "Epoch 9 -- Batch 252/ 842, training loss 0.4187130033969879\n",
      "Epoch 9 -- Batch 253/ 842, training loss 0.43449848890304565\n",
      "Epoch 9 -- Batch 254/ 842, training loss 0.4120539724826813\n",
      "Epoch 9 -- Batch 255/ 842, training loss 0.42349615693092346\n",
      "Epoch 9 -- Batch 256/ 842, training loss 0.4185645282268524\n",
      "Epoch 9 -- Batch 257/ 842, training loss 0.4086770713329315\n",
      "Epoch 9 -- Batch 258/ 842, training loss 0.4148723781108856\n",
      "Epoch 9 -- Batch 259/ 842, training loss 0.43318313360214233\n",
      "Epoch 9 -- Batch 260/ 842, training loss 0.4217689633369446\n",
      "Epoch 9 -- Batch 261/ 842, training loss 0.40513473749160767\n",
      "Epoch 9 -- Batch 262/ 842, training loss 0.41709715127944946\n",
      "Epoch 9 -- Batch 263/ 842, training loss 0.3985716700553894\n",
      "Epoch 9 -- Batch 264/ 842, training loss 0.418484091758728\n",
      "Epoch 9 -- Batch 265/ 842, training loss 0.43012839555740356\n",
      "Epoch 9 -- Batch 266/ 842, training loss 0.41200605034828186\n",
      "Epoch 9 -- Batch 267/ 842, training loss 0.42281684279441833\n",
      "Epoch 9 -- Batch 268/ 842, training loss 0.4356129467487335\n",
      "Epoch 9 -- Batch 269/ 842, training loss 0.4301981031894684\n",
      "Epoch 9 -- Batch 270/ 842, training loss 0.41197264194488525\n",
      "Epoch 9 -- Batch 271/ 842, training loss 0.4277985692024231\n",
      "Epoch 9 -- Batch 272/ 842, training loss 0.4310120642185211\n",
      "Epoch 9 -- Batch 273/ 842, training loss 0.4343577027320862\n",
      "Epoch 9 -- Batch 274/ 842, training loss 0.41124749183654785\n",
      "Epoch 9 -- Batch 275/ 842, training loss 0.40630829334259033\n",
      "Epoch 9 -- Batch 276/ 842, training loss 0.41924458742141724\n",
      "Epoch 9 -- Batch 277/ 842, training loss 0.4187510013580322\n",
      "Epoch 9 -- Batch 278/ 842, training loss 0.4168012738227844\n",
      "Epoch 9 -- Batch 279/ 842, training loss 0.4303182363510132\n",
      "Epoch 9 -- Batch 280/ 842, training loss 0.4135000705718994\n",
      "Epoch 9 -- Batch 281/ 842, training loss 0.40448153018951416\n",
      "Epoch 9 -- Batch 282/ 842, training loss 0.40389883518218994\n",
      "Epoch 9 -- Batch 283/ 842, training loss 0.4162175953388214\n",
      "Epoch 9 -- Batch 284/ 842, training loss 0.4053875505924225\n",
      "Epoch 9 -- Batch 285/ 842, training loss 0.4193553924560547\n",
      "Epoch 9 -- Batch 286/ 842, training loss 0.43340715765953064\n",
      "Epoch 9 -- Batch 287/ 842, training loss 0.4151063561439514\n",
      "Epoch 9 -- Batch 288/ 842, training loss 0.43218597769737244\n",
      "Epoch 9 -- Batch 289/ 842, training loss 0.4071006774902344\n",
      "Epoch 9 -- Batch 290/ 842, training loss 0.4371633231639862\n",
      "Epoch 9 -- Batch 291/ 842, training loss 0.41216838359832764\n",
      "Epoch 9 -- Batch 292/ 842, training loss 0.39121124148368835\n",
      "Epoch 9 -- Batch 293/ 842, training loss 0.4249139428138733\n",
      "Epoch 9 -- Batch 294/ 842, training loss 0.4101153612136841\n",
      "Epoch 9 -- Batch 295/ 842, training loss 0.4337957501411438\n",
      "Epoch 9 -- Batch 296/ 842, training loss 0.4104965925216675\n",
      "Epoch 9 -- Batch 297/ 842, training loss 0.4440627992153168\n",
      "Epoch 9 -- Batch 298/ 842, training loss 0.4251345992088318\n",
      "Epoch 9 -- Batch 299/ 842, training loss 0.41282257437705994\n",
      "Epoch 9 -- Batch 300/ 842, training loss 0.4155957102775574\n",
      "Epoch 9 -- Batch 301/ 842, training loss 0.4036087393760681\n",
      "Epoch 9 -- Batch 302/ 842, training loss 0.4365280270576477\n",
      "Epoch 9 -- Batch 303/ 842, training loss 0.4091688394546509\n",
      "Epoch 9 -- Batch 304/ 842, training loss 0.4111185073852539\n",
      "Epoch 9 -- Batch 305/ 842, training loss 0.43123728036880493\n",
      "Epoch 9 -- Batch 306/ 842, training loss 0.42231786251068115\n",
      "Epoch 9 -- Batch 307/ 842, training loss 0.41952410340309143\n",
      "Epoch 9 -- Batch 308/ 842, training loss 0.4315696656703949\n",
      "Epoch 9 -- Batch 309/ 842, training loss 0.41321244835853577\n",
      "Epoch 9 -- Batch 310/ 842, training loss 0.4414096176624298\n",
      "Epoch 9 -- Batch 311/ 842, training loss 0.41435226798057556\n",
      "Epoch 9 -- Batch 312/ 842, training loss 0.42303037643432617\n",
      "Epoch 9 -- Batch 313/ 842, training loss 0.41850799322128296\n",
      "Epoch 9 -- Batch 314/ 842, training loss 0.4322393238544464\n",
      "Epoch 9 -- Batch 315/ 842, training loss 0.4221855103969574\n",
      "Epoch 9 -- Batch 316/ 842, training loss 0.42203453183174133\n",
      "Epoch 9 -- Batch 317/ 842, training loss 0.4176361560821533\n",
      "Epoch 9 -- Batch 318/ 842, training loss 0.424428254365921\n",
      "Epoch 9 -- Batch 319/ 842, training loss 0.42928987741470337\n",
      "Epoch 9 -- Batch 320/ 842, training loss 0.4133850038051605\n",
      "Epoch 9 -- Batch 321/ 842, training loss 0.43332117795944214\n",
      "Epoch 9 -- Batch 322/ 842, training loss 0.39661362767219543\n",
      "Epoch 9 -- Batch 323/ 842, training loss 0.42883095145225525\n",
      "Epoch 9 -- Batch 324/ 842, training loss 0.4343973398208618\n",
      "Epoch 9 -- Batch 325/ 842, training loss 0.4079475700855255\n",
      "Epoch 9 -- Batch 326/ 842, training loss 0.4212464988231659\n",
      "Epoch 9 -- Batch 327/ 842, training loss 0.4188957214355469\n",
      "Epoch 9 -- Batch 328/ 842, training loss 0.4361456632614136\n",
      "Epoch 9 -- Batch 329/ 842, training loss 0.43477436900138855\n",
      "Epoch 9 -- Batch 330/ 842, training loss 0.41122281551361084\n",
      "Epoch 9 -- Batch 331/ 842, training loss 0.4294532239437103\n",
      "Epoch 9 -- Batch 332/ 842, training loss 0.43773767352104187\n",
      "Epoch 9 -- Batch 333/ 842, training loss 0.42818260192871094\n",
      "Epoch 9 -- Batch 334/ 842, training loss 0.42389795184135437\n",
      "Epoch 9 -- Batch 335/ 842, training loss 0.4263274073600769\n",
      "Epoch 9 -- Batch 336/ 842, training loss 0.4238679111003876\n",
      "Epoch 9 -- Batch 337/ 842, training loss 0.4299507737159729\n",
      "Epoch 9 -- Batch 338/ 842, training loss 0.4246194362640381\n",
      "Epoch 9 -- Batch 339/ 842, training loss 0.4264204204082489\n",
      "Epoch 9 -- Batch 340/ 842, training loss 0.42159250378608704\n",
      "Epoch 9 -- Batch 341/ 842, training loss 0.4174656271934509\n",
      "Epoch 9 -- Batch 342/ 842, training loss 0.41886207461357117\n",
      "Epoch 9 -- Batch 343/ 842, training loss 0.421307235956192\n",
      "Epoch 9 -- Batch 344/ 842, training loss 0.40606412291526794\n",
      "Epoch 9 -- Batch 345/ 842, training loss 0.4153597056865692\n",
      "Epoch 9 -- Batch 346/ 842, training loss 0.408456027507782\n",
      "Epoch 9 -- Batch 347/ 842, training loss 0.4053287208080292\n",
      "Epoch 9 -- Batch 348/ 842, training loss 0.4035765826702118\n",
      "Epoch 9 -- Batch 349/ 842, training loss 0.417135089635849\n",
      "Epoch 9 -- Batch 350/ 842, training loss 0.429487943649292\n",
      "Epoch 9 -- Batch 351/ 842, training loss 0.4075199067592621\n",
      "Epoch 9 -- Batch 352/ 842, training loss 0.41451653838157654\n",
      "Epoch 9 -- Batch 353/ 842, training loss 0.41106554865837097\n",
      "Epoch 9 -- Batch 354/ 842, training loss 0.429911732673645\n",
      "Epoch 9 -- Batch 355/ 842, training loss 0.4332475960254669\n",
      "Epoch 9 -- Batch 356/ 842, training loss 0.4148619771003723\n",
      "Epoch 9 -- Batch 357/ 842, training loss 0.4215239882469177\n",
      "Epoch 9 -- Batch 358/ 842, training loss 0.4280455410480499\n",
      "Epoch 9 -- Batch 359/ 842, training loss 0.41860082745552063\n",
      "Epoch 9 -- Batch 360/ 842, training loss 0.4100581407546997\n",
      "Epoch 9 -- Batch 361/ 842, training loss 0.42492902278900146\n",
      "Epoch 9 -- Batch 362/ 842, training loss 0.43023085594177246\n",
      "Epoch 9 -- Batch 363/ 842, training loss 0.407321572303772\n",
      "Epoch 9 -- Batch 364/ 842, training loss 0.4221440851688385\n",
      "Epoch 9 -- Batch 365/ 842, training loss 0.4160044193267822\n",
      "Epoch 9 -- Batch 366/ 842, training loss 0.42459481954574585\n",
      "Epoch 9 -- Batch 367/ 842, training loss 0.4220341145992279\n",
      "Epoch 9 -- Batch 368/ 842, training loss 0.4166814684867859\n",
      "Epoch 9 -- Batch 369/ 842, training loss 0.4139556884765625\n",
      "Epoch 9 -- Batch 370/ 842, training loss 0.4173712730407715\n",
      "Epoch 9 -- Batch 371/ 842, training loss 0.4320363998413086\n",
      "Epoch 9 -- Batch 372/ 842, training loss 0.4063528776168823\n",
      "Epoch 9 -- Batch 373/ 842, training loss 0.41441845893859863\n",
      "Epoch 9 -- Batch 374/ 842, training loss 0.4142853021621704\n",
      "Epoch 9 -- Batch 375/ 842, training loss 0.42638155817985535\n",
      "Epoch 9 -- Batch 376/ 842, training loss 0.41987863183021545\n",
      "Epoch 9 -- Batch 377/ 842, training loss 0.4212816059589386\n",
      "Epoch 9 -- Batch 378/ 842, training loss 0.42363086342811584\n",
      "Epoch 9 -- Batch 379/ 842, training loss 0.4247159957885742\n",
      "Epoch 9 -- Batch 380/ 842, training loss 0.4322085976600647\n",
      "Epoch 9 -- Batch 381/ 842, training loss 0.4261989891529083\n",
      "Epoch 9 -- Batch 382/ 842, training loss 0.4273904263973236\n",
      "Epoch 9 -- Batch 383/ 842, training loss 0.41221779584884644\n",
      "Epoch 9 -- Batch 384/ 842, training loss 0.4220375120639801\n",
      "Epoch 9 -- Batch 385/ 842, training loss 0.4265114665031433\n",
      "Epoch 9 -- Batch 386/ 842, training loss 0.4265940487384796\n",
      "Epoch 9 -- Batch 387/ 842, training loss 0.4170474112033844\n",
      "Epoch 9 -- Batch 388/ 842, training loss 0.4160167872905731\n",
      "Epoch 9 -- Batch 389/ 842, training loss 0.42482900619506836\n",
      "Epoch 9 -- Batch 390/ 842, training loss 0.3984437882900238\n",
      "Epoch 9 -- Batch 391/ 842, training loss 0.4314916729927063\n",
      "Epoch 9 -- Batch 392/ 842, training loss 0.4373638331890106\n",
      "Epoch 9 -- Batch 393/ 842, training loss 0.4070870280265808\n",
      "Epoch 9 -- Batch 394/ 842, training loss 0.4123210608959198\n",
      "Epoch 9 -- Batch 395/ 842, training loss 0.41499629616737366\n",
      "Epoch 9 -- Batch 396/ 842, training loss 0.40704062581062317\n",
      "Epoch 9 -- Batch 397/ 842, training loss 0.42509663105010986\n",
      "Epoch 9 -- Batch 398/ 842, training loss 0.41061288118362427\n",
      "Epoch 9 -- Batch 399/ 842, training loss 0.4139309525489807\n",
      "Epoch 9 -- Batch 400/ 842, training loss 0.43709424138069153\n",
      "Epoch 9 -- Batch 401/ 842, training loss 0.4332493841648102\n",
      "Epoch 9 -- Batch 402/ 842, training loss 0.43402570486068726\n",
      "Epoch 9 -- Batch 403/ 842, training loss 0.42379429936408997\n",
      "Epoch 9 -- Batch 404/ 842, training loss 0.42422187328338623\n",
      "Epoch 9 -- Batch 405/ 842, training loss 0.4313284456729889\n",
      "Epoch 9 -- Batch 406/ 842, training loss 0.42190927267074585\n",
      "Epoch 9 -- Batch 407/ 842, training loss 0.40933555364608765\n",
      "Epoch 9 -- Batch 408/ 842, training loss 0.4196176826953888\n",
      "Epoch 9 -- Batch 409/ 842, training loss 0.41963255405426025\n",
      "Epoch 9 -- Batch 410/ 842, training loss 0.40432989597320557\n",
      "Epoch 9 -- Batch 411/ 842, training loss 0.4275602102279663\n",
      "Epoch 9 -- Batch 412/ 842, training loss 0.4226808249950409\n",
      "Epoch 9 -- Batch 413/ 842, training loss 0.4157690703868866\n",
      "Epoch 9 -- Batch 414/ 842, training loss 0.4092221260070801\n",
      "Epoch 9 -- Batch 415/ 842, training loss 0.4351309835910797\n",
      "Epoch 9 -- Batch 416/ 842, training loss 0.4186873137950897\n",
      "Epoch 9 -- Batch 417/ 842, training loss 0.41330814361572266\n",
      "Epoch 9 -- Batch 418/ 842, training loss 0.4349748492240906\n",
      "Epoch 9 -- Batch 419/ 842, training loss 0.4162343144416809\n",
      "Epoch 9 -- Batch 420/ 842, training loss 0.4148060083389282\n",
      "Epoch 9 -- Batch 421/ 842, training loss 0.4089800715446472\n",
      "Epoch 9 -- Batch 422/ 842, training loss 0.4123167097568512\n",
      "Epoch 9 -- Batch 423/ 842, training loss 0.4102799892425537\n",
      "Epoch 9 -- Batch 424/ 842, training loss 0.42192986607551575\n",
      "Epoch 9 -- Batch 425/ 842, training loss 0.4149405360221863\n",
      "Epoch 9 -- Batch 426/ 842, training loss 0.41653966903686523\n",
      "Epoch 9 -- Batch 427/ 842, training loss 0.42647460103034973\n",
      "Epoch 9 -- Batch 428/ 842, training loss 0.41972771286964417\n",
      "Epoch 9 -- Batch 429/ 842, training loss 0.40715351700782776\n",
      "Epoch 9 -- Batch 430/ 842, training loss 0.42951706051826477\n",
      "Epoch 9 -- Batch 431/ 842, training loss 0.4112705886363983\n",
      "Epoch 9 -- Batch 432/ 842, training loss 0.41116130352020264\n",
      "Epoch 9 -- Batch 433/ 842, training loss 0.4167052209377289\n",
      "Epoch 9 -- Batch 434/ 842, training loss 0.4294147789478302\n",
      "Epoch 9 -- Batch 435/ 842, training loss 0.42375490069389343\n",
      "Epoch 9 -- Batch 436/ 842, training loss 0.42218559980392456\n",
      "Epoch 9 -- Batch 437/ 842, training loss 0.4027925729751587\n",
      "Epoch 9 -- Batch 438/ 842, training loss 0.41342881321907043\n",
      "Epoch 9 -- Batch 439/ 842, training loss 0.41832032799720764\n",
      "Epoch 9 -- Batch 440/ 842, training loss 0.41101330518722534\n",
      "Epoch 9 -- Batch 441/ 842, training loss 0.42130228877067566\n",
      "Epoch 9 -- Batch 442/ 842, training loss 0.4234614074230194\n",
      "Epoch 9 -- Batch 443/ 842, training loss 0.43978753685951233\n",
      "Epoch 9 -- Batch 444/ 842, training loss 0.4136672019958496\n",
      "Epoch 9 -- Batch 445/ 842, training loss 0.43001341819763184\n",
      "Epoch 9 -- Batch 446/ 842, training loss 0.41809192299842834\n",
      "Epoch 9 -- Batch 447/ 842, training loss 0.4285835921764374\n",
      "Epoch 9 -- Batch 448/ 842, training loss 0.4259011149406433\n",
      "Epoch 9 -- Batch 449/ 842, training loss 0.4231279194355011\n",
      "Epoch 9 -- Batch 450/ 842, training loss 0.42555445432662964\n",
      "Epoch 9 -- Batch 451/ 842, training loss 0.40639805793762207\n",
      "Epoch 9 -- Batch 452/ 842, training loss 0.42471668124198914\n",
      "Epoch 9 -- Batch 453/ 842, training loss 0.41822585463523865\n",
      "Epoch 9 -- Batch 454/ 842, training loss 0.4308091104030609\n",
      "Epoch 9 -- Batch 455/ 842, training loss 0.4149838089942932\n",
      "Epoch 9 -- Batch 456/ 842, training loss 0.4055633842945099\n",
      "Epoch 9 -- Batch 457/ 842, training loss 0.4064818322658539\n",
      "Epoch 9 -- Batch 458/ 842, training loss 0.4138982594013214\n",
      "Epoch 9 -- Batch 459/ 842, training loss 0.4305412471294403\n",
      "Epoch 9 -- Batch 460/ 842, training loss 0.4058873653411865\n",
      "Epoch 9 -- Batch 461/ 842, training loss 0.4336956739425659\n",
      "Epoch 9 -- Batch 462/ 842, training loss 0.41506054997444153\n",
      "Epoch 9 -- Batch 463/ 842, training loss 0.4322962462902069\n",
      "Epoch 9 -- Batch 464/ 842, training loss 0.4271714389324188\n",
      "Epoch 9 -- Batch 465/ 842, training loss 0.4236220121383667\n",
      "Epoch 9 -- Batch 466/ 842, training loss 0.4256608188152313\n",
      "Epoch 9 -- Batch 467/ 842, training loss 0.4309091866016388\n",
      "Epoch 9 -- Batch 468/ 842, training loss 0.4257156550884247\n",
      "Epoch 9 -- Batch 469/ 842, training loss 0.43423691391944885\n",
      "Epoch 9 -- Batch 470/ 842, training loss 0.4233660399913788\n",
      "Epoch 9 -- Batch 471/ 842, training loss 0.41013410687446594\n",
      "Epoch 9 -- Batch 472/ 842, training loss 0.42157667875289917\n",
      "Epoch 9 -- Batch 473/ 842, training loss 0.42514264583587646\n",
      "Epoch 9 -- Batch 474/ 842, training loss 0.41215869784355164\n",
      "Epoch 9 -- Batch 475/ 842, training loss 0.43455833196640015\n",
      "Epoch 9 -- Batch 476/ 842, training loss 0.41856735944747925\n",
      "Epoch 9 -- Batch 477/ 842, training loss 0.41978421807289124\n",
      "Epoch 9 -- Batch 478/ 842, training loss 0.41819122433662415\n",
      "Epoch 9 -- Batch 479/ 842, training loss 0.4127098619937897\n",
      "Epoch 9 -- Batch 480/ 842, training loss 0.4216056764125824\n",
      "Epoch 9 -- Batch 481/ 842, training loss 0.41649359464645386\n",
      "Epoch 9 -- Batch 482/ 842, training loss 0.38901299238204956\n",
      "Epoch 9 -- Batch 483/ 842, training loss 0.4204392731189728\n",
      "Epoch 9 -- Batch 484/ 842, training loss 0.43685632944107056\n",
      "Epoch 9 -- Batch 485/ 842, training loss 0.4227116107940674\n",
      "Epoch 9 -- Batch 486/ 842, training loss 0.43446019291877747\n",
      "Epoch 9 -- Batch 487/ 842, training loss 0.4195201098918915\n",
      "Epoch 9 -- Batch 488/ 842, training loss 0.422859787940979\n",
      "Epoch 9 -- Batch 489/ 842, training loss 0.4270184636116028\n",
      "Epoch 9 -- Batch 490/ 842, training loss 0.4136926531791687\n",
      "Epoch 9 -- Batch 491/ 842, training loss 0.41098740696907043\n",
      "Epoch 9 -- Batch 492/ 842, training loss 0.4232860207557678\n",
      "Epoch 9 -- Batch 493/ 842, training loss 0.42002230882644653\n",
      "Epoch 9 -- Batch 494/ 842, training loss 0.4141305685043335\n",
      "Epoch 9 -- Batch 495/ 842, training loss 0.41136878728866577\n",
      "Epoch 9 -- Batch 496/ 842, training loss 0.4309918284416199\n",
      "Epoch 9 -- Batch 497/ 842, training loss 0.43197083473205566\n",
      "Epoch 9 -- Batch 498/ 842, training loss 0.4239144027233124\n",
      "Epoch 9 -- Batch 499/ 842, training loss 0.40520599484443665\n",
      "Epoch 9 -- Batch 500/ 842, training loss 0.43640705943107605\n",
      "Epoch 9 -- Batch 501/ 842, training loss 0.4473068416118622\n",
      "Epoch 9 -- Batch 502/ 842, training loss 0.41375991702079773\n",
      "Epoch 9 -- Batch 503/ 842, training loss 0.4189031422138214\n",
      "Epoch 9 -- Batch 504/ 842, training loss 0.41122809052467346\n",
      "Epoch 9 -- Batch 505/ 842, training loss 0.4234718382358551\n",
      "Epoch 9 -- Batch 506/ 842, training loss 0.4099637567996979\n",
      "Epoch 9 -- Batch 507/ 842, training loss 0.4362335801124573\n",
      "Epoch 9 -- Batch 508/ 842, training loss 0.43892917037010193\n",
      "Epoch 9 -- Batch 509/ 842, training loss 0.4266393482685089\n",
      "Epoch 9 -- Batch 510/ 842, training loss 0.426503449678421\n",
      "Epoch 9 -- Batch 511/ 842, training loss 0.419394314289093\n",
      "Epoch 9 -- Batch 512/ 842, training loss 0.3981103003025055\n",
      "Epoch 9 -- Batch 513/ 842, training loss 0.422065407037735\n",
      "Epoch 9 -- Batch 514/ 842, training loss 0.41702163219451904\n",
      "Epoch 9 -- Batch 515/ 842, training loss 0.40594005584716797\n",
      "Epoch 9 -- Batch 516/ 842, training loss 0.42522284388542175\n",
      "Epoch 9 -- Batch 517/ 842, training loss 0.43632081151008606\n",
      "Epoch 9 -- Batch 518/ 842, training loss 0.4285626709461212\n",
      "Epoch 9 -- Batch 519/ 842, training loss 0.421592652797699\n",
      "Epoch 9 -- Batch 520/ 842, training loss 0.43238136172294617\n",
      "Epoch 9 -- Batch 521/ 842, training loss 0.4218299686908722\n",
      "Epoch 9 -- Batch 522/ 842, training loss 0.41579949855804443\n",
      "Epoch 9 -- Batch 523/ 842, training loss 0.41134268045425415\n",
      "Epoch 9 -- Batch 524/ 842, training loss 0.41504597663879395\n",
      "Epoch 9 -- Batch 525/ 842, training loss 0.43378958106040955\n",
      "Epoch 9 -- Batch 526/ 842, training loss 0.4172573983669281\n",
      "Epoch 9 -- Batch 527/ 842, training loss 0.43278563022613525\n",
      "Epoch 9 -- Batch 528/ 842, training loss 0.4214417338371277\n",
      "Epoch 9 -- Batch 529/ 842, training loss 0.4281689524650574\n",
      "Epoch 9 -- Batch 530/ 842, training loss 0.4199717938899994\n",
      "Epoch 9 -- Batch 531/ 842, training loss 0.43232861161231995\n",
      "Epoch 9 -- Batch 532/ 842, training loss 0.4116572439670563\n",
      "Epoch 9 -- Batch 533/ 842, training loss 0.4153134524822235\n",
      "Epoch 9 -- Batch 534/ 842, training loss 0.4281528890132904\n",
      "Epoch 9 -- Batch 535/ 842, training loss 0.4138461649417877\n",
      "Epoch 9 -- Batch 536/ 842, training loss 0.43148165941238403\n",
      "Epoch 9 -- Batch 537/ 842, training loss 0.42554178833961487\n",
      "Epoch 9 -- Batch 538/ 842, training loss 0.40686121582984924\n",
      "Epoch 9 -- Batch 539/ 842, training loss 0.40130606293678284\n",
      "Epoch 9 -- Batch 540/ 842, training loss 0.40639448165893555\n",
      "Epoch 9 -- Batch 541/ 842, training loss 0.4082487225532532\n",
      "Epoch 9 -- Batch 542/ 842, training loss 0.43589332699775696\n",
      "Epoch 9 -- Batch 543/ 842, training loss 0.4199694097042084\n",
      "Epoch 9 -- Batch 544/ 842, training loss 0.4266950190067291\n",
      "Epoch 9 -- Batch 545/ 842, training loss 0.4134480655193329\n",
      "Epoch 9 -- Batch 546/ 842, training loss 0.4167829155921936\n",
      "Epoch 9 -- Batch 547/ 842, training loss 0.43699535727500916\n",
      "Epoch 9 -- Batch 548/ 842, training loss 0.43592411279678345\n",
      "Epoch 9 -- Batch 549/ 842, training loss 0.45036616921424866\n",
      "Epoch 9 -- Batch 550/ 842, training loss 0.4325519800186157\n",
      "Epoch 9 -- Batch 551/ 842, training loss 0.4249303936958313\n",
      "Epoch 9 -- Batch 552/ 842, training loss 0.4149220585823059\n",
      "Epoch 9 -- Batch 553/ 842, training loss 0.4196111261844635\n",
      "Epoch 9 -- Batch 554/ 842, training loss 0.41445186734199524\n",
      "Epoch 9 -- Batch 555/ 842, training loss 0.42371484637260437\n",
      "Epoch 9 -- Batch 556/ 842, training loss 0.4187891483306885\n",
      "Epoch 9 -- Batch 557/ 842, training loss 0.4213410019874573\n",
      "Epoch 9 -- Batch 558/ 842, training loss 0.40144357085227966\n",
      "Epoch 9 -- Batch 559/ 842, training loss 0.4315493106842041\n",
      "Epoch 9 -- Batch 560/ 842, training loss 0.40745416283607483\n",
      "Epoch 9 -- Batch 561/ 842, training loss 0.44296789169311523\n",
      "Epoch 9 -- Batch 562/ 842, training loss 0.4233841598033905\n",
      "Epoch 9 -- Batch 563/ 842, training loss 0.39859718084335327\n",
      "Epoch 9 -- Batch 564/ 842, training loss 0.42776718735694885\n",
      "Epoch 9 -- Batch 565/ 842, training loss 0.41367629170417786\n",
      "Epoch 9 -- Batch 566/ 842, training loss 0.414141982793808\n",
      "Epoch 9 -- Batch 567/ 842, training loss 0.4093334376811981\n",
      "Epoch 9 -- Batch 568/ 842, training loss 0.4279206097126007\n",
      "Epoch 9 -- Batch 569/ 842, training loss 0.43127381801605225\n",
      "Epoch 9 -- Batch 570/ 842, training loss 0.42168769240379333\n",
      "Epoch 9 -- Batch 571/ 842, training loss 0.43396416306495667\n",
      "Epoch 9 -- Batch 572/ 842, training loss 0.44861337542533875\n",
      "Epoch 9 -- Batch 573/ 842, training loss 0.41149938106536865\n",
      "Epoch 9 -- Batch 574/ 842, training loss 0.4360215365886688\n",
      "Epoch 9 -- Batch 575/ 842, training loss 0.41017135977745056\n",
      "Epoch 9 -- Batch 576/ 842, training loss 0.41109129786491394\n",
      "Epoch 9 -- Batch 577/ 842, training loss 0.43319249153137207\n",
      "Epoch 9 -- Batch 578/ 842, training loss 0.4008953273296356\n",
      "Epoch 9 -- Batch 579/ 842, training loss 0.4287413954734802\n",
      "Epoch 9 -- Batch 580/ 842, training loss 0.4303737282752991\n",
      "Epoch 9 -- Batch 581/ 842, training loss 0.41903001070022583\n",
      "Epoch 9 -- Batch 582/ 842, training loss 0.4193018078804016\n",
      "Epoch 9 -- Batch 583/ 842, training loss 0.4326014816761017\n",
      "Epoch 9 -- Batch 584/ 842, training loss 0.409178227186203\n",
      "Epoch 9 -- Batch 585/ 842, training loss 0.4174368977546692\n",
      "Epoch 9 -- Batch 586/ 842, training loss 0.4154418706893921\n",
      "Epoch 9 -- Batch 587/ 842, training loss 0.4073767066001892\n",
      "Epoch 9 -- Batch 588/ 842, training loss 0.4357960522174835\n",
      "Epoch 9 -- Batch 589/ 842, training loss 0.4105212688446045\n",
      "Epoch 9 -- Batch 590/ 842, training loss 0.43335068225860596\n",
      "Epoch 9 -- Batch 591/ 842, training loss 0.4139516353607178\n",
      "Epoch 9 -- Batch 592/ 842, training loss 0.4241320788860321\n",
      "Epoch 9 -- Batch 593/ 842, training loss 0.41317373514175415\n",
      "Epoch 9 -- Batch 594/ 842, training loss 0.3876183032989502\n",
      "Epoch 9 -- Batch 595/ 842, training loss 0.4041958451271057\n",
      "Epoch 9 -- Batch 596/ 842, training loss 0.40147864818573\n",
      "Epoch 9 -- Batch 597/ 842, training loss 0.42688965797424316\n",
      "Epoch 9 -- Batch 598/ 842, training loss 0.4385356903076172\n",
      "Epoch 9 -- Batch 599/ 842, training loss 0.4419197738170624\n",
      "Epoch 9 -- Batch 600/ 842, training loss 0.4250025451183319\n",
      "Epoch 9 -- Batch 601/ 842, training loss 0.4100646376609802\n",
      "Epoch 9 -- Batch 602/ 842, training loss 0.4001464545726776\n",
      "Epoch 9 -- Batch 603/ 842, training loss 0.4311889410018921\n",
      "Epoch 9 -- Batch 604/ 842, training loss 0.4145995080471039\n",
      "Epoch 9 -- Batch 605/ 842, training loss 0.43620699644088745\n",
      "Epoch 9 -- Batch 606/ 842, training loss 0.4175144135951996\n",
      "Epoch 9 -- Batch 607/ 842, training loss 0.42274153232574463\n",
      "Epoch 9 -- Batch 608/ 842, training loss 0.42979392409324646\n",
      "Epoch 9 -- Batch 609/ 842, training loss 0.41287124156951904\n",
      "Epoch 9 -- Batch 610/ 842, training loss 0.4172319769859314\n",
      "Epoch 9 -- Batch 611/ 842, training loss 0.4297504127025604\n",
      "Epoch 9 -- Batch 612/ 842, training loss 0.409758597612381\n",
      "Epoch 9 -- Batch 613/ 842, training loss 0.40981972217559814\n",
      "Epoch 9 -- Batch 614/ 842, training loss 0.42383354902267456\n",
      "Epoch 9 -- Batch 615/ 842, training loss 0.42462626099586487\n",
      "Epoch 9 -- Batch 616/ 842, training loss 0.41171616315841675\n",
      "Epoch 9 -- Batch 617/ 842, training loss 0.4362558424472809\n",
      "Epoch 9 -- Batch 618/ 842, training loss 0.42013490200042725\n",
      "Epoch 9 -- Batch 619/ 842, training loss 0.42855605483055115\n",
      "Epoch 9 -- Batch 620/ 842, training loss 0.41004762053489685\n",
      "Epoch 9 -- Batch 621/ 842, training loss 0.41016730666160583\n",
      "Epoch 9 -- Batch 622/ 842, training loss 0.43217897415161133\n",
      "Epoch 9 -- Batch 623/ 842, training loss 0.41873398423194885\n",
      "Epoch 9 -- Batch 624/ 842, training loss 0.43593478202819824\n",
      "Epoch 9 -- Batch 625/ 842, training loss 0.43515709042549133\n",
      "Epoch 9 -- Batch 626/ 842, training loss 0.414991557598114\n",
      "Epoch 9 -- Batch 627/ 842, training loss 0.41415292024612427\n",
      "Epoch 9 -- Batch 628/ 842, training loss 0.4124622941017151\n",
      "Epoch 9 -- Batch 629/ 842, training loss 0.4136852025985718\n",
      "Epoch 9 -- Batch 630/ 842, training loss 0.4151620864868164\n",
      "Epoch 9 -- Batch 631/ 842, training loss 0.42142951488494873\n",
      "Epoch 9 -- Batch 632/ 842, training loss 0.4131746292114258\n",
      "Epoch 9 -- Batch 633/ 842, training loss 0.43051061034202576\n",
      "Epoch 9 -- Batch 634/ 842, training loss 0.4160985052585602\n",
      "Epoch 9 -- Batch 635/ 842, training loss 0.41993609070777893\n",
      "Epoch 9 -- Batch 636/ 842, training loss 0.4107953608036041\n",
      "Epoch 9 -- Batch 637/ 842, training loss 0.42312172055244446\n",
      "Epoch 9 -- Batch 638/ 842, training loss 0.42250996828079224\n",
      "Epoch 9 -- Batch 639/ 842, training loss 0.411663681268692\n",
      "Epoch 9 -- Batch 640/ 842, training loss 0.41143742203712463\n",
      "Epoch 9 -- Batch 641/ 842, training loss 0.4190993905067444\n",
      "Epoch 9 -- Batch 642/ 842, training loss 0.42120838165283203\n",
      "Epoch 9 -- Batch 643/ 842, training loss 0.41394466161727905\n",
      "Epoch 9 -- Batch 644/ 842, training loss 0.4347326457500458\n",
      "Epoch 9 -- Batch 645/ 842, training loss 0.43007826805114746\n",
      "Epoch 9 -- Batch 646/ 842, training loss 0.40519440174102783\n",
      "Epoch 9 -- Batch 647/ 842, training loss 0.4114777743816376\n",
      "Epoch 9 -- Batch 648/ 842, training loss 0.4279523193836212\n",
      "Epoch 9 -- Batch 649/ 842, training loss 0.4110959768295288\n",
      "Epoch 9 -- Batch 650/ 842, training loss 0.41882839798927307\n",
      "Epoch 9 -- Batch 651/ 842, training loss 0.40897098183631897\n",
      "Epoch 9 -- Batch 652/ 842, training loss 0.42520663142204285\n",
      "Epoch 9 -- Batch 653/ 842, training loss 0.4199795424938202\n",
      "Epoch 9 -- Batch 654/ 842, training loss 0.42317065596580505\n",
      "Epoch 9 -- Batch 655/ 842, training loss 0.4286120533943176\n",
      "Epoch 9 -- Batch 656/ 842, training loss 0.4103449285030365\n",
      "Epoch 9 -- Batch 657/ 842, training loss 0.4250086843967438\n",
      "Epoch 9 -- Batch 658/ 842, training loss 0.4077838957309723\n",
      "Epoch 9 -- Batch 659/ 842, training loss 0.41373467445373535\n",
      "Epoch 9 -- Batch 660/ 842, training loss 0.4247560203075409\n",
      "Epoch 9 -- Batch 661/ 842, training loss 0.4095909297466278\n",
      "Epoch 9 -- Batch 662/ 842, training loss 0.40766897797584534\n",
      "Epoch 9 -- Batch 663/ 842, training loss 0.4177820384502411\n",
      "Epoch 9 -- Batch 664/ 842, training loss 0.40218469500541687\n",
      "Epoch 9 -- Batch 665/ 842, training loss 0.42332807183265686\n",
      "Epoch 9 -- Batch 666/ 842, training loss 0.4164057970046997\n",
      "Epoch 9 -- Batch 667/ 842, training loss 0.40702173113822937\n",
      "Epoch 9 -- Batch 668/ 842, training loss 0.4060472846031189\n",
      "Epoch 9 -- Batch 669/ 842, training loss 0.41413164138793945\n",
      "Epoch 9 -- Batch 670/ 842, training loss 0.41998669505119324\n",
      "Epoch 9 -- Batch 671/ 842, training loss 0.4244272708892822\n",
      "Epoch 9 -- Batch 672/ 842, training loss 0.417132169008255\n",
      "Epoch 9 -- Batch 673/ 842, training loss 0.40838634967803955\n",
      "Epoch 9 -- Batch 674/ 842, training loss 0.4203537106513977\n",
      "Epoch 9 -- Batch 675/ 842, training loss 0.41482484340667725\n",
      "Epoch 9 -- Batch 676/ 842, training loss 0.427327960729599\n",
      "Epoch 9 -- Batch 677/ 842, training loss 0.42296668887138367\n",
      "Epoch 9 -- Batch 678/ 842, training loss 0.4131941497325897\n",
      "Epoch 9 -- Batch 679/ 842, training loss 0.42449018359184265\n",
      "Epoch 9 -- Batch 680/ 842, training loss 0.4222876727581024\n",
      "Epoch 9 -- Batch 681/ 842, training loss 0.4287295341491699\n",
      "Epoch 9 -- Batch 682/ 842, training loss 0.4199846088886261\n",
      "Epoch 9 -- Batch 683/ 842, training loss 0.41038379073143005\n",
      "Epoch 9 -- Batch 684/ 842, training loss 0.42450380325317383\n",
      "Epoch 9 -- Batch 685/ 842, training loss 0.4315834939479828\n",
      "Epoch 9 -- Batch 686/ 842, training loss 0.4325813055038452\n",
      "Epoch 9 -- Batch 687/ 842, training loss 0.4070916473865509\n",
      "Epoch 9 -- Batch 688/ 842, training loss 0.42198237776756287\n",
      "Epoch 9 -- Batch 689/ 842, training loss 0.4371948838233948\n",
      "Epoch 9 -- Batch 690/ 842, training loss 0.42225727438926697\n",
      "Epoch 9 -- Batch 691/ 842, training loss 0.4144926071166992\n",
      "Epoch 9 -- Batch 692/ 842, training loss 0.44372183084487915\n",
      "Epoch 9 -- Batch 693/ 842, training loss 0.4321371018886566\n",
      "Epoch 9 -- Batch 694/ 842, training loss 0.43390870094299316\n",
      "Epoch 9 -- Batch 695/ 842, training loss 0.4297485947608948\n",
      "Epoch 9 -- Batch 696/ 842, training loss 0.4001981317996979\n",
      "Epoch 9 -- Batch 697/ 842, training loss 0.4282326400279999\n",
      "Epoch 9 -- Batch 698/ 842, training loss 0.42915812134742737\n",
      "Epoch 9 -- Batch 699/ 842, training loss 0.40825027227401733\n",
      "Epoch 9 -- Batch 700/ 842, training loss 0.4236668348312378\n",
      "Epoch 9 -- Batch 701/ 842, training loss 0.41819190979003906\n",
      "Epoch 9 -- Batch 702/ 842, training loss 0.41990530490875244\n",
      "Epoch 9 -- Batch 703/ 842, training loss 0.4323282241821289\n",
      "Epoch 9 -- Batch 704/ 842, training loss 0.42075011134147644\n",
      "Epoch 9 -- Batch 705/ 842, training loss 0.4150441288948059\n",
      "Epoch 9 -- Batch 706/ 842, training loss 0.425900936126709\n",
      "Epoch 9 -- Batch 707/ 842, training loss 0.4074741005897522\n",
      "Epoch 9 -- Batch 708/ 842, training loss 0.4037671983242035\n",
      "Epoch 9 -- Batch 709/ 842, training loss 0.4292927384376526\n",
      "Epoch 9 -- Batch 710/ 842, training loss 0.40783146023750305\n",
      "Epoch 9 -- Batch 711/ 842, training loss 0.4035910964012146\n",
      "Epoch 9 -- Batch 712/ 842, training loss 0.4115363657474518\n",
      "Epoch 9 -- Batch 713/ 842, training loss 0.4340634346008301\n",
      "Epoch 9 -- Batch 714/ 842, training loss 0.3961769640445709\n",
      "Epoch 9 -- Batch 715/ 842, training loss 0.42052292823791504\n",
      "Epoch 9 -- Batch 716/ 842, training loss 0.41941866278648376\n",
      "Epoch 9 -- Batch 717/ 842, training loss 0.41163283586502075\n",
      "Epoch 9 -- Batch 718/ 842, training loss 0.4060748815536499\n",
      "Epoch 9 -- Batch 719/ 842, training loss 0.4174838960170746\n",
      "Epoch 9 -- Batch 720/ 842, training loss 0.4152887761592865\n",
      "Epoch 9 -- Batch 721/ 842, training loss 0.41330644488334656\n",
      "Epoch 9 -- Batch 722/ 842, training loss 0.41632020473480225\n",
      "Epoch 9 -- Batch 723/ 842, training loss 0.42892128229141235\n",
      "Epoch 9 -- Batch 724/ 842, training loss 0.4050242602825165\n",
      "Epoch 9 -- Batch 725/ 842, training loss 0.41596856713294983\n",
      "Epoch 9 -- Batch 726/ 842, training loss 0.42493849992752075\n",
      "Epoch 9 -- Batch 727/ 842, training loss 0.4210086464881897\n",
      "Epoch 9 -- Batch 728/ 842, training loss 0.4217696487903595\n",
      "Epoch 9 -- Batch 729/ 842, training loss 0.4110000729560852\n",
      "Epoch 9 -- Batch 730/ 842, training loss 0.40252989530563354\n",
      "Epoch 9 -- Batch 731/ 842, training loss 0.4290289878845215\n",
      "Epoch 9 -- Batch 732/ 842, training loss 0.42477574944496155\n",
      "Epoch 9 -- Batch 733/ 842, training loss 0.424189954996109\n",
      "Epoch 9 -- Batch 734/ 842, training loss 0.43588757514953613\n",
      "Epoch 9 -- Batch 735/ 842, training loss 0.40027475357055664\n",
      "Epoch 9 -- Batch 736/ 842, training loss 0.41548585891723633\n",
      "Epoch 9 -- Batch 737/ 842, training loss 0.42634421586990356\n",
      "Epoch 9 -- Batch 738/ 842, training loss 0.40337035059928894\n",
      "Epoch 9 -- Batch 739/ 842, training loss 0.42493173480033875\n",
      "Epoch 9 -- Batch 740/ 842, training loss 0.4052334427833557\n",
      "Epoch 9 -- Batch 741/ 842, training loss 0.42277994751930237\n",
      "Epoch 9 -- Batch 742/ 842, training loss 0.41989144682884216\n",
      "Epoch 9 -- Batch 743/ 842, training loss 0.40879446268081665\n",
      "Epoch 9 -- Batch 744/ 842, training loss 0.4195130467414856\n",
      "Epoch 9 -- Batch 745/ 842, training loss 0.41075262427330017\n",
      "Epoch 9 -- Batch 746/ 842, training loss 0.4278278052806854\n",
      "Epoch 9 -- Batch 747/ 842, training loss 0.39765653014183044\n",
      "Epoch 9 -- Batch 748/ 842, training loss 0.42474785447120667\n",
      "Epoch 9 -- Batch 749/ 842, training loss 0.4084315896034241\n",
      "Epoch 9 -- Batch 750/ 842, training loss 0.4186849594116211\n",
      "Epoch 9 -- Batch 751/ 842, training loss 0.4224706292152405\n",
      "Epoch 9 -- Batch 752/ 842, training loss 0.4089761972427368\n",
      "Epoch 9 -- Batch 753/ 842, training loss 0.4283563196659088\n",
      "Epoch 9 -- Batch 754/ 842, training loss 0.43260475993156433\n",
      "Epoch 9 -- Batch 755/ 842, training loss 0.4100213646888733\n",
      "Epoch 9 -- Batch 756/ 842, training loss 0.4183371365070343\n",
      "Epoch 9 -- Batch 757/ 842, training loss 0.4186064302921295\n",
      "Epoch 9 -- Batch 758/ 842, training loss 0.42518866062164307\n",
      "Epoch 9 -- Batch 759/ 842, training loss 0.4050810933113098\n",
      "Epoch 9 -- Batch 760/ 842, training loss 0.4114820063114166\n",
      "Epoch 9 -- Batch 761/ 842, training loss 0.42046990990638733\n",
      "Epoch 9 -- Batch 762/ 842, training loss 0.42997434735298157\n",
      "Epoch 9 -- Batch 763/ 842, training loss 0.39642879366874695\n",
      "Epoch 9 -- Batch 764/ 842, training loss 0.43313559889793396\n",
      "Epoch 9 -- Batch 765/ 842, training loss 0.4250258207321167\n",
      "Epoch 9 -- Batch 766/ 842, training loss 0.4138961136341095\n",
      "Epoch 9 -- Batch 767/ 842, training loss 0.4052545726299286\n",
      "Epoch 9 -- Batch 768/ 842, training loss 0.4239312410354614\n",
      "Epoch 9 -- Batch 769/ 842, training loss 0.4307621717453003\n",
      "Epoch 9 -- Batch 770/ 842, training loss 0.39609819650650024\n",
      "Epoch 9 -- Batch 771/ 842, training loss 0.4162874221801758\n",
      "Epoch 9 -- Batch 772/ 842, training loss 0.4260365068912506\n",
      "Epoch 9 -- Batch 773/ 842, training loss 0.4249360263347626\n",
      "Epoch 9 -- Batch 774/ 842, training loss 0.4127204716205597\n",
      "Epoch 9 -- Batch 775/ 842, training loss 0.4011290669441223\n",
      "Epoch 9 -- Batch 776/ 842, training loss 0.434617817401886\n",
      "Epoch 9 -- Batch 777/ 842, training loss 0.4146761894226074\n",
      "Epoch 9 -- Batch 778/ 842, training loss 0.4176020622253418\n",
      "Epoch 9 -- Batch 779/ 842, training loss 0.4212704002857208\n",
      "Epoch 9 -- Batch 780/ 842, training loss 0.4206410348415375\n",
      "Epoch 9 -- Batch 781/ 842, training loss 0.417172372341156\n",
      "Epoch 9 -- Batch 782/ 842, training loss 0.398617148399353\n",
      "Epoch 9 -- Batch 783/ 842, training loss 0.40962889790534973\n",
      "Epoch 9 -- Batch 784/ 842, training loss 0.42786580324172974\n",
      "Epoch 9 -- Batch 785/ 842, training loss 0.41289809346199036\n",
      "Epoch 9 -- Batch 786/ 842, training loss 0.40930354595184326\n",
      "Epoch 9 -- Batch 787/ 842, training loss 0.4275429844856262\n",
      "Epoch 9 -- Batch 788/ 842, training loss 0.43557503819465637\n",
      "Epoch 9 -- Batch 789/ 842, training loss 0.42767414450645447\n",
      "Epoch 9 -- Batch 790/ 842, training loss 0.4008341133594513\n",
      "Epoch 9 -- Batch 791/ 842, training loss 0.42974406480789185\n",
      "Epoch 9 -- Batch 792/ 842, training loss 0.4132108688354492\n",
      "Epoch 9 -- Batch 793/ 842, training loss 0.42730915546417236\n",
      "Epoch 9 -- Batch 794/ 842, training loss 0.41801029443740845\n",
      "Epoch 9 -- Batch 795/ 842, training loss 0.4111179709434509\n",
      "Epoch 9 -- Batch 796/ 842, training loss 0.41662248969078064\n",
      "Epoch 9 -- Batch 797/ 842, training loss 0.4113319516181946\n",
      "Epoch 9 -- Batch 798/ 842, training loss 0.4128173589706421\n",
      "Epoch 9 -- Batch 799/ 842, training loss 0.41195714473724365\n",
      "Epoch 9 -- Batch 800/ 842, training loss 0.4053959846496582\n",
      "Epoch 9 -- Batch 801/ 842, training loss 0.4185650050640106\n",
      "Epoch 9 -- Batch 802/ 842, training loss 0.42051953077316284\n",
      "Epoch 9 -- Batch 803/ 842, training loss 0.4202090799808502\n",
      "Epoch 9 -- Batch 804/ 842, training loss 0.42660510540008545\n",
      "Epoch 9 -- Batch 805/ 842, training loss 0.43269139528274536\n",
      "Epoch 9 -- Batch 806/ 842, training loss 0.4190802276134491\n",
      "Epoch 9 -- Batch 807/ 842, training loss 0.4243590235710144\n",
      "Epoch 9 -- Batch 808/ 842, training loss 0.42302265763282776\n",
      "Epoch 9 -- Batch 809/ 842, training loss 0.4278809130191803\n",
      "Epoch 9 -- Batch 810/ 842, training loss 0.4200349748134613\n",
      "Epoch 9 -- Batch 811/ 842, training loss 0.42671144008636475\n",
      "Epoch 9 -- Batch 812/ 842, training loss 0.42087554931640625\n",
      "Epoch 9 -- Batch 813/ 842, training loss 0.40309953689575195\n",
      "Epoch 9 -- Batch 814/ 842, training loss 0.41600415110588074\n",
      "Epoch 9 -- Batch 815/ 842, training loss 0.392581969499588\n",
      "Epoch 9 -- Batch 816/ 842, training loss 0.41696983575820923\n",
      "Epoch 9 -- Batch 817/ 842, training loss 0.4325966238975525\n",
      "Epoch 9 -- Batch 818/ 842, training loss 0.40198349952697754\n",
      "Epoch 9 -- Batch 819/ 842, training loss 0.40669822692871094\n",
      "Epoch 9 -- Batch 820/ 842, training loss 0.4153502285480499\n",
      "Epoch 9 -- Batch 821/ 842, training loss 0.4272001385688782\n",
      "Epoch 9 -- Batch 822/ 842, training loss 0.42588913440704346\n",
      "Epoch 9 -- Batch 823/ 842, training loss 0.4132707417011261\n",
      "Epoch 9 -- Batch 824/ 842, training loss 0.413984090089798\n",
      "Epoch 9 -- Batch 825/ 842, training loss 0.418925404548645\n",
      "Epoch 9 -- Batch 826/ 842, training loss 0.4271512031555176\n",
      "Epoch 9 -- Batch 827/ 842, training loss 0.4206514358520508\n",
      "Epoch 9 -- Batch 828/ 842, training loss 0.4186229109764099\n",
      "Epoch 9 -- Batch 829/ 842, training loss 0.4186166822910309\n",
      "Epoch 9 -- Batch 830/ 842, training loss 0.41913503408432007\n",
      "Epoch 9 -- Batch 831/ 842, training loss 0.42244580388069153\n",
      "Epoch 9 -- Batch 832/ 842, training loss 0.4053882658481598\n",
      "Epoch 9 -- Batch 833/ 842, training loss 0.4138924479484558\n",
      "Epoch 9 -- Batch 834/ 842, training loss 0.4307873845100403\n",
      "Epoch 9 -- Batch 835/ 842, training loss 0.4209572970867157\n",
      "Epoch 9 -- Batch 836/ 842, training loss 0.4199489653110504\n",
      "Epoch 9 -- Batch 837/ 842, training loss 0.39845114946365356\n",
      "Epoch 9 -- Batch 838/ 842, training loss 0.42901235818862915\n",
      "Epoch 9 -- Batch 839/ 842, training loss 0.411675363779068\n",
      "Epoch 9 -- Batch 840/ 842, training loss 0.4142930209636688\n",
      "Epoch 9 -- Batch 841/ 842, training loss 0.4164140224456787\n",
      "Epoch 9 -- Batch 842/ 842, training loss 0.40902572870254517\n",
      "----------------------------------------------------------------------\n",
      "Epoch 9 -- Batch 1/ 94, validation loss 0.4019232392311096\n",
      "Epoch 9 -- Batch 2/ 94, validation loss 0.4039902985095978\n",
      "Epoch 9 -- Batch 3/ 94, validation loss 0.4080307185649872\n",
      "Epoch 9 -- Batch 4/ 94, validation loss 0.42326438426971436\n",
      "Epoch 9 -- Batch 5/ 94, validation loss 0.42196351289749146\n",
      "Epoch 9 -- Batch 6/ 94, validation loss 0.4141474962234497\n",
      "Epoch 9 -- Batch 7/ 94, validation loss 0.4033091068267822\n",
      "Epoch 9 -- Batch 8/ 94, validation loss 0.408962607383728\n",
      "Epoch 9 -- Batch 9/ 94, validation loss 0.4022977352142334\n",
      "Epoch 9 -- Batch 10/ 94, validation loss 0.4222986400127411\n",
      "Epoch 9 -- Batch 11/ 94, validation loss 0.42050641775131226\n",
      "Epoch 9 -- Batch 12/ 94, validation loss 0.40709492564201355\n",
      "Epoch 9 -- Batch 13/ 94, validation loss 0.43000128865242004\n",
      "Epoch 9 -- Batch 14/ 94, validation loss 0.414358526468277\n",
      "Epoch 9 -- Batch 15/ 94, validation loss 0.4039403796195984\n",
      "Epoch 9 -- Batch 16/ 94, validation loss 0.4289819300174713\n",
      "Epoch 9 -- Batch 17/ 94, validation loss 0.4323226511478424\n",
      "Epoch 9 -- Batch 18/ 94, validation loss 0.4105331599712372\n",
      "Epoch 9 -- Batch 19/ 94, validation loss 0.3959197998046875\n",
      "Epoch 9 -- Batch 20/ 94, validation loss 0.4329051375389099\n",
      "Epoch 9 -- Batch 21/ 94, validation loss 0.41424834728240967\n",
      "Epoch 9 -- Batch 22/ 94, validation loss 0.4290209710597992\n",
      "Epoch 9 -- Batch 23/ 94, validation loss 0.41401875019073486\n",
      "Epoch 9 -- Batch 24/ 94, validation loss 0.41024190187454224\n",
      "Epoch 9 -- Batch 25/ 94, validation loss 0.43014293909072876\n",
      "Epoch 9 -- Batch 26/ 94, validation loss 0.40216606855392456\n",
      "Epoch 9 -- Batch 27/ 94, validation loss 0.4099356532096863\n",
      "Epoch 9 -- Batch 28/ 94, validation loss 0.40229010581970215\n",
      "Epoch 9 -- Batch 29/ 94, validation loss 0.4192357659339905\n",
      "Epoch 9 -- Batch 30/ 94, validation loss 0.414539635181427\n",
      "Epoch 9 -- Batch 31/ 94, validation loss 0.43389517068862915\n",
      "Epoch 9 -- Batch 32/ 94, validation loss 0.4060404598712921\n",
      "Epoch 9 -- Batch 33/ 94, validation loss 0.42845746874809265\n",
      "Epoch 9 -- Batch 34/ 94, validation loss 0.3950081765651703\n",
      "Epoch 9 -- Batch 35/ 94, validation loss 0.39408591389656067\n",
      "Epoch 9 -- Batch 36/ 94, validation loss 0.4210980534553528\n",
      "Epoch 9 -- Batch 37/ 94, validation loss 0.40127885341644287\n",
      "Epoch 9 -- Batch 38/ 94, validation loss 0.41538006067276\n",
      "Epoch 9 -- Batch 39/ 94, validation loss 0.4150521755218506\n",
      "Epoch 9 -- Batch 40/ 94, validation loss 0.4205823540687561\n",
      "Epoch 9 -- Batch 41/ 94, validation loss 0.4234265387058258\n",
      "Epoch 9 -- Batch 42/ 94, validation loss 0.41947999596595764\n",
      "Epoch 9 -- Batch 43/ 94, validation loss 0.40569207072257996\n",
      "Epoch 9 -- Batch 44/ 94, validation loss 0.4009968638420105\n",
      "Epoch 9 -- Batch 45/ 94, validation loss 0.4167080223560333\n",
      "Epoch 9 -- Batch 46/ 94, validation loss 0.4106106162071228\n",
      "Epoch 9 -- Batch 47/ 94, validation loss 0.41594967246055603\n",
      "Epoch 9 -- Batch 48/ 94, validation loss 0.4122622013092041\n",
      "Epoch 9 -- Batch 49/ 94, validation loss 0.4136773645877838\n",
      "Epoch 9 -- Batch 50/ 94, validation loss 0.4039212763309479\n",
      "Epoch 9 -- Batch 51/ 94, validation loss 0.39562666416168213\n",
      "Epoch 9 -- Batch 52/ 94, validation loss 0.41778260469436646\n",
      "Epoch 9 -- Batch 53/ 94, validation loss 0.42767539620399475\n",
      "Epoch 9 -- Batch 54/ 94, validation loss 0.4055969715118408\n",
      "Epoch 9 -- Batch 55/ 94, validation loss 0.40850451588630676\n",
      "Epoch 9 -- Batch 56/ 94, validation loss 0.4030250310897827\n",
      "Epoch 9 -- Batch 57/ 94, validation loss 0.4017329812049866\n",
      "Epoch 9 -- Batch 58/ 94, validation loss 0.42369386553764343\n",
      "Epoch 9 -- Batch 59/ 94, validation loss 0.4133859872817993\n",
      "Epoch 9 -- Batch 60/ 94, validation loss 0.4160248041152954\n",
      "Epoch 9 -- Batch 61/ 94, validation loss 0.42347413301467896\n",
      "Epoch 9 -- Batch 62/ 94, validation loss 0.40685024857521057\n",
      "Epoch 9 -- Batch 63/ 94, validation loss 0.41780757904052734\n",
      "Epoch 9 -- Batch 64/ 94, validation loss 0.41596511006355286\n",
      "Epoch 9 -- Batch 65/ 94, validation loss 0.41158753633499146\n",
      "Epoch 9 -- Batch 66/ 94, validation loss 0.42400768399238586\n",
      "Epoch 9 -- Batch 67/ 94, validation loss 0.4178254008293152\n",
      "Epoch 9 -- Batch 68/ 94, validation loss 0.432838499546051\n",
      "Epoch 9 -- Batch 69/ 94, validation loss 0.42418172955513\n",
      "Epoch 9 -- Batch 70/ 94, validation loss 0.4174254536628723\n",
      "Epoch 9 -- Batch 71/ 94, validation loss 0.42977699637413025\n",
      "Epoch 9 -- Batch 72/ 94, validation loss 0.4256690442562103\n",
      "Epoch 9 -- Batch 73/ 94, validation loss 0.4221402406692505\n",
      "Epoch 9 -- Batch 74/ 94, validation loss 0.401764452457428\n",
      "Epoch 9 -- Batch 75/ 94, validation loss 0.422986775636673\n",
      "Epoch 9 -- Batch 76/ 94, validation loss 0.3995315432548523\n",
      "Epoch 9 -- Batch 77/ 94, validation loss 0.420985609292984\n",
      "Epoch 9 -- Batch 78/ 94, validation loss 0.42719000577926636\n",
      "Epoch 9 -- Batch 79/ 94, validation loss 0.39677971601486206\n",
      "Epoch 9 -- Batch 80/ 94, validation loss 0.4030159115791321\n",
      "Epoch 9 -- Batch 81/ 94, validation loss 0.4293724596500397\n",
      "Epoch 9 -- Batch 82/ 94, validation loss 0.3939255475997925\n",
      "Epoch 9 -- Batch 83/ 94, validation loss 0.41170886158943176\n",
      "Epoch 9 -- Batch 84/ 94, validation loss 0.4134471118450165\n",
      "Epoch 9 -- Batch 85/ 94, validation loss 0.4156537353992462\n",
      "Epoch 9 -- Batch 86/ 94, validation loss 0.42017877101898193\n",
      "Epoch 9 -- Batch 87/ 94, validation loss 0.40215441584587097\n",
      "Epoch 9 -- Batch 88/ 94, validation loss 0.4086036682128906\n",
      "Epoch 9 -- Batch 89/ 94, validation loss 0.39327573776245117\n",
      "Epoch 9 -- Batch 90/ 94, validation loss 0.4049205482006073\n",
      "Epoch 9 -- Batch 91/ 94, validation loss 0.41620877385139465\n",
      "Epoch 9 -- Batch 92/ 94, validation loss 0.4131474494934082\n",
      "Epoch 9 -- Batch 93/ 94, validation loss 0.40724512934684753\n",
      "Epoch 9 -- Batch 94/ 94, validation loss 0.43153613805770874\n",
      "----------------------------------------------------------------------\n",
      "Epoch 9 loss: Training 0.41993606090545654, Validation 0.43153613805770874\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: CNC(=O)C1(C)CCN(C(=O)c2cc3c(c(C)nn4-c3ccc(F)cc3)c2)CCCC3)C1\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'CNC(=O)C1(C)CCN(C(=O)c2cc3c(c(C)nn4-c3ccc(F)cc3)c2)CCCC3)C1' for input: 'CNC(=O)C1(C)CCN(C(=O)c2cc3c(c(C)nn4-c3ccc(F)cc3)c2)CCCC3)C1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(C=NNc3nnnn33)csc2cc1OC(C)=O'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 5 6 8 12 13\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(NC(=O)C(NC(=O)CCc2cc(C(=O)OC)c3c(C)cc(C)cc34)n2)c1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC(C(=O)O)C1C2CC3CC(C3)CC2CC41CC[C@@]2(C)C(=O)CCC2'\n",
      "[19:05:17] Explicit valence for atom # 11 Br, 2, is greater than permitted\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1c(S(=O)(=O)NCc2ccc(C#N)cc2)cccc12'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 17\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 13 14 15 21 22\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CCc1nc(SCc2cc(=O)[nH]3c4c(sc2n2)CCC4)c2c3c(sc2n1)CCCC3'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 11 12 20\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'c1csc2cc(OCC(=O)c3c[nH]c4ccccc35)ccc12'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 9\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'N#CC[C@@H]1CC2C[C@H]1CC[C@H](N2CC=CC1)C2(C)C'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 5 6 23 28 29\n",
      "[19:05:17] SMILES Parse Error: duplicated ring closure 3 bonds atom 24 to itself for input: 'Cc1cc2c(s1)c1cnn(CC(=O)NCCc3c[nH]c3ccccc33)c(=O)c1n2C'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 8 9 24 25 26 27 28 29 30\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC12CCC(=O)N1C(=N/C=C1\\CCCCC1)C2=C(CCCC1=O)O2'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 13\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 7 8 9 11 21 22 27 28 29\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC1CCN(S(=O)(=O)c2ccc3c(c2)nc(SCc2ccccc2Cl)n2Cc2ccccc2)CC1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)c2oc(=S)n3c2cc(C(F)(F)F)cc(C(F)(F)F)c2)c(C)c1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 2 3 5 6 9 20\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(NC(=O)c2sc(=S)n3Cc3ccco3)cc2c1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1(N(CC(C)C)C(=O)Nc2ccccc23)CCCCC1'\n",
      "[19:05:17] Explicit valence for atom # 1 F, 2, is greater than permitted\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1cccc(C[C@@H]2NC(=O)[C@H]3CNCCN22)c1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 6 7 9 10 11 24 25 32 33\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 3 4 21 22 23\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)c1cc(C2CCCN2S(=O)(=O)CC2Nn2ccc(C(F)(F)F)c2)oc1=O'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC1CC1(C(=O)OCC(=O)NCc2ccco2)c(=O)n(-c2ccccc2)c1C'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 28 29 30\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'O=C(NC1CC2c3ccccc3C2)c(=O)c2c1nc1nonc13'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 5 18 19 20 21 22 32 33 34\n",
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: O=C1OC(c2ccc(F)cc2)=N/C1=C\\c1ccccc1)OC1CCCCC1=O\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'O=C1OC(c2ccc(F)cc2)=N/C1=C\\c1ccccc1)OC1CCCCC1=O' for input: 'O=C1OC(c2ccc(F)cc2)=N/C1=C\\c1ccccc1)OC1CCCCC1=O'\n",
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: CC1(C)CC2(CCOC(=O)c3ccccc3)CC(c3ccccc3)C2)=C(O)C1\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'CC1(C)CC2(CCOC(=O)c3ccccc3)CC(c3ccccc3)C2)=C(O)C1' for input: 'CC1(C)CC2(CCOC(=O)c3ccccc3)CC(c3ccccc3)C2)=C(O)C1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 6 7 11\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1cccc(-c2cccc(-n3cc(Nc4nccc(C(C)C)c5)nn3)c2)c1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1cc(NC(=O)CSc2nc3nc4cc([N+](=O)[O-])ccc4nc3c2=O)nc2cc(C(C)(C)C)nn22'\n",
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: N#Cc1c2n(c(=O)n(-c3ccc(Cl)cc3)c2=O)C=Cc2ccccc12)c1cc([N+](=O)[O-])ccc1Cl\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'N#Cc1c2n(c(=O)n(-c3ccc(Cl)cc3)c2=O)C=Cc2ccccc12)c1cc([N+](=O)[O-])ccc1Cl' for input: 'N#Cc1c2n(c(=O)n(-c3ccc(Cl)cc3)c2=O)C=Cc2ccccc12)c1cc([N+](=O)[O-])ccc1Cl'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 15 16 17 25 26\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CCCCCCN1C(=O)N(Cc2cccc(OC)c2)C(=O)C12C'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 6 7 8 22 23 24 26\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2[nH]nc(C(=O)N3CCC5(CCCO4)OC3)c2c1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'S=C(c1ccccc1)N1CCn2c(nnc2C2c2ccccc2-n2cccc21)C1'\n",
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(C(C)=O)cc1COC(=O)c1ccc(N2C(=O)C3C4C=CC(C5)C3C3=O)cc2[N+](=O)[O-])c1\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(C(C)=O)cc1COC(=O)c1ccc(N2C(=O)C3C4C=CC(C5)C3C3=O)cc2[N+](=O)[O-])c1' for input: 'COc1ccc(C(C)=O)cc1COC(=O)c1ccc(N2C(=O)C3C4C=CC(C5)C3C3=O)cc2[N+](=O)[O-])c1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 0 1 2 21 23 24 25\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 16 17 18\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COC(CN1CCC(NC(=O)C2=N)CC1)c1ccccc1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2(C(=O)NCCc3ccc4(C(C)C)OCCO4)CCCO2)cc1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 2\n",
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1C1C=C(N(C)C2CCCCC2)C(=O)CCl)cc1\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1C1C=C(N(C)C2CCCCC2)C(=O)CCl)cc1' for input: 'COc1ccccc1C1C=C(N(C)C2CCCCC2)C(=O)CCl)cc1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 1 3 4 5 6 7 14 15 17 18 19\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 13 14 17 18 20 21 25\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 11 13 14 15 24 25 27\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 17 18 24\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19 20 21\n",
      "[19:05:17] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 7 9 10 14 15 16 17 18 19 20 21\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 3 4 11\n",
      "[19:05:17] SMILES Parse Error: ring closure 1 duplicates bond between atom 20 and atom 21 for input: 'O=C(NC1(C(F)(F)F)N=C2C=CCC2O2)c1ccccc1-c1ccccc1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC(C)C1CN(CCNC(=O)C2c3ccccc3C(=O)N3Cc3ccccc32)CCC1=O'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 20 21 22\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'C=CCN(Cc1ccccc1OC)Cc1cc(=O)n2nc(C4CCCCC3)oc2n1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 19 20 27\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 20 22 23 24\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 15\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CCOc1ccccc1N1C(=O)CC(SC(=NCC)NC2=S)C1c1ccc(O)c(OC)c1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1ON(C(=O)CCc2ccccc2)[C@H](CC(=O)N2CCc3c([nH]c4ccccc44)C23)[C@H]1C(=O)OC'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1cc(N=NC(=O)CSc2nnc(-c3ccc(OC)cc3)n3-c3ccccc3)c2ccccc21'\n",
      "[19:05:17] SMILES Parse Error: ring closure 2 duplicates bond between atom 21 and atom 22 for input: 'Cc1nn(-c2ccc(S(=O)(=O)N3CCCCCC3)cc2)c(C2c2ccccc2O)c1C(=O)Nc1cccc(F)c1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 12 13 15\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 12 13 18 20 21 23 24\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'CC(C)C#Cc1cnc([C@@H]2[C@H]3CO[C@H]2CN4CC2CCCC2)nn1'\n",
      "[19:05:17] non-ring atom 19 marked aromatic\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1cccc(Cn2cccc2CN(C)C(=O)CCc2nnc3n2CCCCC2)c1O'\n",
      "[19:05:17] SMILES Parse Error: duplicated ring closure 2 bonds atom 5 to itself for input: 'N#Cc1c(NC22CC3CC(CC(C3)C2)C3=O)c(-c2ccccc2)n1-c1ccc(Cl)cc1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'O=c1[nH]c(N2CCCCC2)nc2c1C(c1ccccc1)CC(N3CCOCC1)S2'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 17 18 23\n",
      "[19:05:17] Explicit valence for atom # 16 C, 5, is greater than permitted\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 11 12 25\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1C1CC(=O)N(C(C)C)C2=O'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 4 5 6 17 21\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'O=C1N=c2ccccc2=C1c1nc(SCc1ccccc1Cl)n1cc(-c2ccccc2)[nH]1'\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1ccs2c(=O)c3c(nc22)CNC(c1ccc(Cl)cc1)(C3)C3'\n",
      "[19:05:17] SMILES Parse Error: extra close parentheses while parsing: CCCCN1CC23C=CC(C4)CC2C4)c(=O)n1Cc1ccccc1\n",
      "[19:05:17] SMILES Parse Error: Failed parsing SMILES 'CCCCN1CC23C=CC(C4)CC2C4)c(=O)n1Cc1ccccc1' for input: 'CCCCN1CC23C=CC(C4)CC2C4)c(=O)n1Cc1ccccc1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 2 3 19\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 3 4 5 16 19\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CS(=O)(=O)Cc2ccc(-c3csc(N4CCC(C)CC5)nn3)cc2)c1'\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[19:05:17] Can't kekulize mol.  Unkekulized atoms: 12 13 18 19 20\n",
      "[19:05:17] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2n[nH]c(C)c2C(=O)N2CCc3c(cnc4C3CC3)O2)cc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 -- Batch 1/ 842, training loss 0.3968014419078827\n",
      "Epoch 10 -- Batch 2/ 842, training loss 0.4055347144603729\n",
      "Epoch 10 -- Batch 3/ 842, training loss 0.3834630250930786\n",
      "Epoch 10 -- Batch 4/ 842, training loss 0.40947389602661133\n",
      "Epoch 10 -- Batch 5/ 842, training loss 0.39625853300094604\n",
      "Epoch 10 -- Batch 6/ 842, training loss 0.4041198790073395\n",
      "Epoch 10 -- Batch 7/ 842, training loss 0.4093148112297058\n",
      "Epoch 10 -- Batch 8/ 842, training loss 0.4101628065109253\n",
      "Epoch 10 -- Batch 9/ 842, training loss 0.4296288788318634\n",
      "Epoch 10 -- Batch 10/ 842, training loss 0.4103587865829468\n",
      "Epoch 10 -- Batch 11/ 842, training loss 0.39165323972702026\n",
      "Epoch 10 -- Batch 12/ 842, training loss 0.4026673436164856\n",
      "Epoch 10 -- Batch 13/ 842, training loss 0.3870570957660675\n",
      "Epoch 10 -- Batch 14/ 842, training loss 0.3966355621814728\n",
      "Epoch 10 -- Batch 15/ 842, training loss 0.39804041385650635\n",
      "Epoch 10 -- Batch 16/ 842, training loss 0.3929382860660553\n",
      "Epoch 10 -- Batch 17/ 842, training loss 0.39885035157203674\n",
      "Epoch 10 -- Batch 18/ 842, training loss 0.40358030796051025\n",
      "Epoch 10 -- Batch 19/ 842, training loss 0.40531429648399353\n",
      "Epoch 10 -- Batch 20/ 842, training loss 0.3966107666492462\n",
      "Epoch 10 -- Batch 21/ 842, training loss 0.398598313331604\n",
      "Epoch 10 -- Batch 22/ 842, training loss 0.3889203369617462\n",
      "Epoch 10 -- Batch 23/ 842, training loss 0.41820600628852844\n",
      "Epoch 10 -- Batch 24/ 842, training loss 0.4030178189277649\n",
      "Epoch 10 -- Batch 25/ 842, training loss 0.4032055139541626\n",
      "Epoch 10 -- Batch 26/ 842, training loss 0.4325057864189148\n",
      "Epoch 10 -- Batch 27/ 842, training loss 0.41852450370788574\n",
      "Epoch 10 -- Batch 28/ 842, training loss 0.41752001643180847\n",
      "Epoch 10 -- Batch 29/ 842, training loss 0.3931601941585541\n",
      "Epoch 10 -- Batch 30/ 842, training loss 0.4103100001811981\n",
      "Epoch 10 -- Batch 31/ 842, training loss 0.39464884996414185\n",
      "Epoch 10 -- Batch 32/ 842, training loss 0.4095926880836487\n",
      "Epoch 10 -- Batch 33/ 842, training loss 0.4047865867614746\n",
      "Epoch 10 -- Batch 34/ 842, training loss 0.40328508615493774\n",
      "Epoch 10 -- Batch 35/ 842, training loss 0.395623117685318\n",
      "Epoch 10 -- Batch 36/ 842, training loss 0.401395708322525\n",
      "Epoch 10 -- Batch 37/ 842, training loss 0.39401090145111084\n",
      "Epoch 10 -- Batch 38/ 842, training loss 0.38438931107521057\n",
      "Epoch 10 -- Batch 39/ 842, training loss 0.3919770419597626\n",
      "Epoch 10 -- Batch 40/ 842, training loss 0.4045761525630951\n",
      "Epoch 10 -- Batch 41/ 842, training loss 0.39719247817993164\n",
      "Epoch 10 -- Batch 42/ 842, training loss 0.4007217586040497\n",
      "Epoch 10 -- Batch 43/ 842, training loss 0.39353039860725403\n",
      "Epoch 10 -- Batch 44/ 842, training loss 0.40628546476364136\n",
      "Epoch 10 -- Batch 45/ 842, training loss 0.4096314013004303\n",
      "Epoch 10 -- Batch 46/ 842, training loss 0.405730664730072\n",
      "Epoch 10 -- Batch 47/ 842, training loss 0.3973069190979004\n",
      "Epoch 10 -- Batch 48/ 842, training loss 0.40247705578804016\n",
      "Epoch 10 -- Batch 49/ 842, training loss 0.404422402381897\n",
      "Epoch 10 -- Batch 50/ 842, training loss 0.4103614389896393\n",
      "Epoch 10 -- Batch 51/ 842, training loss 0.41368669271469116\n",
      "Epoch 10 -- Batch 52/ 842, training loss 0.3909430503845215\n",
      "Epoch 10 -- Batch 53/ 842, training loss 0.39004871249198914\n",
      "Epoch 10 -- Batch 54/ 842, training loss 0.40521240234375\n",
      "Epoch 10 -- Batch 55/ 842, training loss 0.3870086073875427\n",
      "Epoch 10 -- Batch 56/ 842, training loss 0.4050653576850891\n",
      "Epoch 10 -- Batch 57/ 842, training loss 0.39361631870269775\n",
      "Epoch 10 -- Batch 58/ 842, training loss 0.40097907185554504\n",
      "Epoch 10 -- Batch 59/ 842, training loss 0.4103248715400696\n",
      "Epoch 10 -- Batch 60/ 842, training loss 0.4124341309070587\n",
      "Epoch 10 -- Batch 61/ 842, training loss 0.4155885875225067\n",
      "Epoch 10 -- Batch 62/ 842, training loss 0.40306150913238525\n",
      "Epoch 10 -- Batch 63/ 842, training loss 0.40941041707992554\n",
      "Epoch 10 -- Batch 64/ 842, training loss 0.40620043873786926\n",
      "Epoch 10 -- Batch 65/ 842, training loss 0.3982282876968384\n",
      "Epoch 10 -- Batch 66/ 842, training loss 0.4004957675933838\n",
      "Epoch 10 -- Batch 67/ 842, training loss 0.3990394175052643\n",
      "Epoch 10 -- Batch 68/ 842, training loss 0.3962321877479553\n",
      "Epoch 10 -- Batch 69/ 842, training loss 0.4133138954639435\n",
      "Epoch 10 -- Batch 70/ 842, training loss 0.4090650677680969\n",
      "Epoch 10 -- Batch 71/ 842, training loss 0.4084964990615845\n",
      "Epoch 10 -- Batch 72/ 842, training loss 0.4076893627643585\n",
      "Epoch 10 -- Batch 73/ 842, training loss 0.40063685178756714\n",
      "Epoch 10 -- Batch 74/ 842, training loss 0.4024770259857178\n",
      "Epoch 10 -- Batch 75/ 842, training loss 0.39201048016548157\n",
      "Epoch 10 -- Batch 76/ 842, training loss 0.40165233612060547\n",
      "Epoch 10 -- Batch 77/ 842, training loss 0.40875324606895447\n",
      "Epoch 10 -- Batch 78/ 842, training loss 0.3989071249961853\n",
      "Epoch 10 -- Batch 79/ 842, training loss 0.4086158275604248\n",
      "Epoch 10 -- Batch 80/ 842, training loss 0.41554543375968933\n",
      "Epoch 10 -- Batch 81/ 842, training loss 0.40976986289024353\n",
      "Epoch 10 -- Batch 82/ 842, training loss 0.4012869596481323\n",
      "Epoch 10 -- Batch 83/ 842, training loss 0.4075055420398712\n",
      "Epoch 10 -- Batch 84/ 842, training loss 0.414049357175827\n",
      "Epoch 10 -- Batch 85/ 842, training loss 0.41326195001602173\n",
      "Epoch 10 -- Batch 86/ 842, training loss 0.4003745913505554\n",
      "Epoch 10 -- Batch 87/ 842, training loss 0.4061730206012726\n",
      "Epoch 10 -- Batch 88/ 842, training loss 0.4016738831996918\n",
      "Epoch 10 -- Batch 89/ 842, training loss 0.40568771958351135\n",
      "Epoch 10 -- Batch 90/ 842, training loss 0.3927364945411682\n",
      "Epoch 10 -- Batch 91/ 842, training loss 0.4072378873825073\n",
      "Epoch 10 -- Batch 92/ 842, training loss 0.4224255383014679\n",
      "Epoch 10 -- Batch 93/ 842, training loss 0.40424400568008423\n",
      "Epoch 10 -- Batch 94/ 842, training loss 0.3895401954650879\n",
      "Epoch 10 -- Batch 95/ 842, training loss 0.400470495223999\n",
      "Epoch 10 -- Batch 96/ 842, training loss 0.41206425428390503\n",
      "Epoch 10 -- Batch 97/ 842, training loss 0.41619160771369934\n",
      "Epoch 10 -- Batch 98/ 842, training loss 0.4173925817012787\n",
      "Epoch 10 -- Batch 99/ 842, training loss 0.4037490487098694\n",
      "Epoch 10 -- Batch 100/ 842, training loss 0.39131057262420654\n",
      "Epoch 10 -- Batch 101/ 842, training loss 0.39811769127845764\n",
      "Epoch 10 -- Batch 102/ 842, training loss 0.39041778445243835\n",
      "Epoch 10 -- Batch 103/ 842, training loss 0.4117911458015442\n",
      "Epoch 10 -- Batch 104/ 842, training loss 0.4094490706920624\n",
      "Epoch 10 -- Batch 105/ 842, training loss 0.3931385278701782\n",
      "Epoch 10 -- Batch 106/ 842, training loss 0.4004824757575989\n",
      "Epoch 10 -- Batch 107/ 842, training loss 0.4075447618961334\n",
      "Epoch 10 -- Batch 108/ 842, training loss 0.4044066369533539\n",
      "Epoch 10 -- Batch 109/ 842, training loss 0.4069712162017822\n",
      "Epoch 10 -- Batch 110/ 842, training loss 0.396057665348053\n",
      "Epoch 10 -- Batch 111/ 842, training loss 0.3924371600151062\n",
      "Epoch 10 -- Batch 112/ 842, training loss 0.40273240208625793\n",
      "Epoch 10 -- Batch 113/ 842, training loss 0.4007805585861206\n",
      "Epoch 10 -- Batch 114/ 842, training loss 0.4133027493953705\n",
      "Epoch 10 -- Batch 115/ 842, training loss 0.4042784571647644\n",
      "Epoch 10 -- Batch 116/ 842, training loss 0.40614789724349976\n",
      "Epoch 10 -- Batch 117/ 842, training loss 0.4053069055080414\n",
      "Epoch 10 -- Batch 118/ 842, training loss 0.4138105511665344\n",
      "Epoch 10 -- Batch 119/ 842, training loss 0.4083489179611206\n",
      "Epoch 10 -- Batch 120/ 842, training loss 0.4007501006126404\n",
      "Epoch 10 -- Batch 121/ 842, training loss 0.40152767300605774\n",
      "Epoch 10 -- Batch 122/ 842, training loss 0.4124909043312073\n",
      "Epoch 10 -- Batch 123/ 842, training loss 0.39500537514686584\n",
      "Epoch 10 -- Batch 124/ 842, training loss 0.4190220832824707\n",
      "Epoch 10 -- Batch 125/ 842, training loss 0.41005223989486694\n",
      "Epoch 10 -- Batch 126/ 842, training loss 0.4132339060306549\n",
      "Epoch 10 -- Batch 127/ 842, training loss 0.39459943771362305\n",
      "Epoch 10 -- Batch 128/ 842, training loss 0.40840667486190796\n",
      "Epoch 10 -- Batch 129/ 842, training loss 0.40626174211502075\n",
      "Epoch 10 -- Batch 130/ 842, training loss 0.4073381721973419\n",
      "Epoch 10 -- Batch 131/ 842, training loss 0.3879427909851074\n",
      "Epoch 10 -- Batch 132/ 842, training loss 0.3987807333469391\n",
      "Epoch 10 -- Batch 133/ 842, training loss 0.3994987905025482\n",
      "Epoch 10 -- Batch 134/ 842, training loss 0.40876641869544983\n",
      "Epoch 10 -- Batch 135/ 842, training loss 0.3979324400424957\n",
      "Epoch 10 -- Batch 136/ 842, training loss 0.3910808861255646\n",
      "Epoch 10 -- Batch 137/ 842, training loss 0.4195006489753723\n",
      "Epoch 10 -- Batch 138/ 842, training loss 0.39706823229789734\n",
      "Epoch 10 -- Batch 139/ 842, training loss 0.4083450734615326\n",
      "Epoch 10 -- Batch 140/ 842, training loss 0.40746137499809265\n",
      "Epoch 10 -- Batch 141/ 842, training loss 0.3909778594970703\n",
      "Epoch 10 -- Batch 142/ 842, training loss 0.40584486722946167\n",
      "Epoch 10 -- Batch 143/ 842, training loss 0.413156121969223\n",
      "Epoch 10 -- Batch 144/ 842, training loss 0.4030541777610779\n",
      "Epoch 10 -- Batch 145/ 842, training loss 0.4100533127784729\n",
      "Epoch 10 -- Batch 146/ 842, training loss 0.4164949059486389\n",
      "Epoch 10 -- Batch 147/ 842, training loss 0.3986654579639435\n",
      "Epoch 10 -- Batch 148/ 842, training loss 0.39548054337501526\n",
      "Epoch 10 -- Batch 149/ 842, training loss 0.41338449716567993\n",
      "Epoch 10 -- Batch 150/ 842, training loss 0.40154772996902466\n",
      "Epoch 10 -- Batch 151/ 842, training loss 0.413991779088974\n",
      "Epoch 10 -- Batch 152/ 842, training loss 0.41089802980422974\n",
      "Epoch 10 -- Batch 153/ 842, training loss 0.4128940999507904\n",
      "Epoch 10 -- Batch 154/ 842, training loss 0.40831202268600464\n",
      "Epoch 10 -- Batch 155/ 842, training loss 0.3941795825958252\n",
      "Epoch 10 -- Batch 156/ 842, training loss 0.41337499022483826\n",
      "Epoch 10 -- Batch 157/ 842, training loss 0.3915879726409912\n",
      "Epoch 10 -- Batch 158/ 842, training loss 0.41407886147499084\n",
      "Epoch 10 -- Batch 159/ 842, training loss 0.404840350151062\n",
      "Epoch 10 -- Batch 160/ 842, training loss 0.39334776997566223\n",
      "Epoch 10 -- Batch 161/ 842, training loss 0.39230623841285706\n",
      "Epoch 10 -- Batch 162/ 842, training loss 0.3904600143432617\n",
      "Epoch 10 -- Batch 163/ 842, training loss 0.3918093144893646\n",
      "Epoch 10 -- Batch 164/ 842, training loss 0.42216193675994873\n",
      "Epoch 10 -- Batch 165/ 842, training loss 0.4091481864452362\n",
      "Epoch 10 -- Batch 166/ 842, training loss 0.41152140498161316\n",
      "Epoch 10 -- Batch 167/ 842, training loss 0.4192964434623718\n",
      "Epoch 10 -- Batch 168/ 842, training loss 0.39980629086494446\n",
      "Epoch 10 -- Batch 169/ 842, training loss 0.4026683270931244\n",
      "Epoch 10 -- Batch 170/ 842, training loss 0.4187450706958771\n",
      "Epoch 10 -- Batch 171/ 842, training loss 0.40715909004211426\n",
      "Epoch 10 -- Batch 172/ 842, training loss 0.43108466267585754\n",
      "Epoch 10 -- Batch 173/ 842, training loss 0.4073995053768158\n",
      "Epoch 10 -- Batch 174/ 842, training loss 0.3893391788005829\n",
      "Epoch 10 -- Batch 175/ 842, training loss 0.4150797426700592\n",
      "Epoch 10 -- Batch 176/ 842, training loss 0.41541925072669983\n",
      "Epoch 10 -- Batch 177/ 842, training loss 0.40981683135032654\n",
      "Epoch 10 -- Batch 178/ 842, training loss 0.3975115418434143\n",
      "Epoch 10 -- Batch 179/ 842, training loss 0.3909698724746704\n",
      "Epoch 10 -- Batch 180/ 842, training loss 0.3860991597175598\n",
      "Epoch 10 -- Batch 181/ 842, training loss 0.39736121892929077\n",
      "Epoch 10 -- Batch 182/ 842, training loss 0.41243642568588257\n",
      "Epoch 10 -- Batch 183/ 842, training loss 0.396287202835083\n",
      "Epoch 10 -- Batch 184/ 842, training loss 0.41428905725479126\n",
      "Epoch 10 -- Batch 185/ 842, training loss 0.3888719081878662\n",
      "Epoch 10 -- Batch 186/ 842, training loss 0.3967534899711609\n",
      "Epoch 10 -- Batch 187/ 842, training loss 0.40180647373199463\n",
      "Epoch 10 -- Batch 188/ 842, training loss 0.4045216143131256\n",
      "Epoch 10 -- Batch 189/ 842, training loss 0.4047984778881073\n",
      "Epoch 10 -- Batch 190/ 842, training loss 0.40885525941848755\n",
      "Epoch 10 -- Batch 191/ 842, training loss 0.39910832047462463\n",
      "Epoch 10 -- Batch 192/ 842, training loss 0.406765878200531\n",
      "Epoch 10 -- Batch 193/ 842, training loss 0.39545372128486633\n",
      "Epoch 10 -- Batch 194/ 842, training loss 0.41400423645973206\n",
      "Epoch 10 -- Batch 195/ 842, training loss 0.4055764377117157\n",
      "Epoch 10 -- Batch 196/ 842, training loss 0.41526225209236145\n",
      "Epoch 10 -- Batch 197/ 842, training loss 0.41058531403541565\n",
      "Epoch 10 -- Batch 198/ 842, training loss 0.41332533955574036\n",
      "Epoch 10 -- Batch 199/ 842, training loss 0.38541385531425476\n",
      "Epoch 10 -- Batch 200/ 842, training loss 0.402575820684433\n",
      "Epoch 10 -- Batch 201/ 842, training loss 0.40474987030029297\n",
      "Epoch 10 -- Batch 202/ 842, training loss 0.41849929094314575\n",
      "Epoch 10 -- Batch 203/ 842, training loss 0.39676085114479065\n",
      "Epoch 10 -- Batch 204/ 842, training loss 0.41821110248565674\n",
      "Epoch 10 -- Batch 205/ 842, training loss 0.41448912024497986\n",
      "Epoch 10 -- Batch 206/ 842, training loss 0.4000810384750366\n",
      "Epoch 10 -- Batch 207/ 842, training loss 0.4034709632396698\n",
      "Epoch 10 -- Batch 208/ 842, training loss 0.39865967631340027\n",
      "Epoch 10 -- Batch 209/ 842, training loss 0.39897820353507996\n",
      "Epoch 10 -- Batch 210/ 842, training loss 0.3989037275314331\n",
      "Epoch 10 -- Batch 211/ 842, training loss 0.41437822580337524\n",
      "Epoch 10 -- Batch 212/ 842, training loss 0.4191558361053467\n",
      "Epoch 10 -- Batch 213/ 842, training loss 0.3965325355529785\n",
      "Epoch 10 -- Batch 214/ 842, training loss 0.4061897099018097\n",
      "Epoch 10 -- Batch 215/ 842, training loss 0.3994397819042206\n",
      "Epoch 10 -- Batch 216/ 842, training loss 0.4140273928642273\n",
      "Epoch 10 -- Batch 217/ 842, training loss 0.406455934047699\n",
      "Epoch 10 -- Batch 218/ 842, training loss 0.40175217390060425\n",
      "Epoch 10 -- Batch 219/ 842, training loss 0.4077717661857605\n",
      "Epoch 10 -- Batch 220/ 842, training loss 0.397439569234848\n",
      "Epoch 10 -- Batch 221/ 842, training loss 0.4175688624382019\n",
      "Epoch 10 -- Batch 222/ 842, training loss 0.39770448207855225\n",
      "Epoch 10 -- Batch 223/ 842, training loss 0.4130798578262329\n",
      "Epoch 10 -- Batch 224/ 842, training loss 0.4163360297679901\n",
      "Epoch 10 -- Batch 225/ 842, training loss 0.3941446840763092\n",
      "Epoch 10 -- Batch 226/ 842, training loss 0.4219237267971039\n",
      "Epoch 10 -- Batch 227/ 842, training loss 0.4038262367248535\n",
      "Epoch 10 -- Batch 228/ 842, training loss 0.4222155809402466\n",
      "Epoch 10 -- Batch 229/ 842, training loss 0.38770726323127747\n",
      "Epoch 10 -- Batch 230/ 842, training loss 0.3997122645378113\n",
      "Epoch 10 -- Batch 231/ 842, training loss 0.3883727788925171\n",
      "Epoch 10 -- Batch 232/ 842, training loss 0.3996553122997284\n",
      "Epoch 10 -- Batch 233/ 842, training loss 0.39633798599243164\n",
      "Epoch 10 -- Batch 234/ 842, training loss 0.3947261571884155\n",
      "Epoch 10 -- Batch 235/ 842, training loss 0.4008885622024536\n",
      "Epoch 10 -- Batch 236/ 842, training loss 0.40679121017456055\n",
      "Epoch 10 -- Batch 237/ 842, training loss 0.4106001853942871\n",
      "Epoch 10 -- Batch 238/ 842, training loss 0.4104863107204437\n",
      "Epoch 10 -- Batch 239/ 842, training loss 0.4075389504432678\n",
      "Epoch 10 -- Batch 240/ 842, training loss 0.3940831124782562\n",
      "Epoch 10 -- Batch 241/ 842, training loss 0.4079091250896454\n",
      "Epoch 10 -- Batch 242/ 842, training loss 0.3973816931247711\n",
      "Epoch 10 -- Batch 243/ 842, training loss 0.41617852449417114\n",
      "Epoch 10 -- Batch 244/ 842, training loss 0.3915672302246094\n",
      "Epoch 10 -- Batch 245/ 842, training loss 0.41967037320137024\n",
      "Epoch 10 -- Batch 246/ 842, training loss 0.40510985255241394\n",
      "Epoch 10 -- Batch 247/ 842, training loss 0.3873935341835022\n",
      "Epoch 10 -- Batch 248/ 842, training loss 0.39828574657440186\n",
      "Epoch 10 -- Batch 249/ 842, training loss 0.39752650260925293\n",
      "Epoch 10 -- Batch 250/ 842, training loss 0.41239169239997864\n",
      "Epoch 10 -- Batch 251/ 842, training loss 0.39253130555152893\n",
      "Epoch 10 -- Batch 252/ 842, training loss 0.4085143804550171\n",
      "Epoch 10 -- Batch 253/ 842, training loss 0.4204541742801666\n",
      "Epoch 10 -- Batch 254/ 842, training loss 0.40962880849838257\n",
      "Epoch 10 -- Batch 255/ 842, training loss 0.3992137312889099\n",
      "Epoch 10 -- Batch 256/ 842, training loss 0.40142378211021423\n",
      "Epoch 10 -- Batch 257/ 842, training loss 0.4027257263660431\n",
      "Epoch 10 -- Batch 258/ 842, training loss 0.4167783558368683\n",
      "Epoch 10 -- Batch 259/ 842, training loss 0.4040553867816925\n",
      "Epoch 10 -- Batch 260/ 842, training loss 0.41269874572753906\n",
      "Epoch 10 -- Batch 261/ 842, training loss 0.3989320993423462\n",
      "Epoch 10 -- Batch 262/ 842, training loss 0.4160460829734802\n",
      "Epoch 10 -- Batch 263/ 842, training loss 0.40702030062675476\n",
      "Epoch 10 -- Batch 264/ 842, training loss 0.419953852891922\n",
      "Epoch 10 -- Batch 265/ 842, training loss 0.4211553633213043\n",
      "Epoch 10 -- Batch 266/ 842, training loss 0.40133315324783325\n",
      "Epoch 10 -- Batch 267/ 842, training loss 0.3968300223350525\n",
      "Epoch 10 -- Batch 268/ 842, training loss 0.40775832533836365\n",
      "Epoch 10 -- Batch 269/ 842, training loss 0.4139338731765747\n",
      "Epoch 10 -- Batch 270/ 842, training loss 0.4274924099445343\n",
      "Epoch 10 -- Batch 271/ 842, training loss 0.3908884823322296\n",
      "Epoch 10 -- Batch 272/ 842, training loss 0.405775249004364\n",
      "Epoch 10 -- Batch 273/ 842, training loss 0.4119306206703186\n",
      "Epoch 10 -- Batch 274/ 842, training loss 0.39432603120803833\n",
      "Epoch 10 -- Batch 275/ 842, training loss 0.39547818899154663\n",
      "Epoch 10 -- Batch 276/ 842, training loss 0.40121427178382874\n",
      "Epoch 10 -- Batch 277/ 842, training loss 0.40186426043510437\n",
      "Epoch 10 -- Batch 278/ 842, training loss 0.403527170419693\n",
      "Epoch 10 -- Batch 279/ 842, training loss 0.40312594175338745\n",
      "Epoch 10 -- Batch 280/ 842, training loss 0.41596367955207825\n",
      "Epoch 10 -- Batch 281/ 842, training loss 0.39903292059898376\n",
      "Epoch 10 -- Batch 282/ 842, training loss 0.4155377149581909\n",
      "Epoch 10 -- Batch 283/ 842, training loss 0.41595858335494995\n",
      "Epoch 10 -- Batch 284/ 842, training loss 0.4115913510322571\n",
      "Epoch 10 -- Batch 285/ 842, training loss 0.39833956956863403\n",
      "Epoch 10 -- Batch 286/ 842, training loss 0.4143694341182709\n",
      "Epoch 10 -- Batch 287/ 842, training loss 0.4120666980743408\n",
      "Epoch 10 -- Batch 288/ 842, training loss 0.4018259644508362\n",
      "Epoch 10 -- Batch 289/ 842, training loss 0.41244733333587646\n",
      "Epoch 10 -- Batch 290/ 842, training loss 0.4083665907382965\n",
      "Epoch 10 -- Batch 291/ 842, training loss 0.4090709090232849\n",
      "Epoch 10 -- Batch 292/ 842, training loss 0.4089139401912689\n",
      "Epoch 10 -- Batch 293/ 842, training loss 0.4057280123233795\n",
      "Epoch 10 -- Batch 294/ 842, training loss 0.4079742431640625\n",
      "Epoch 10 -- Batch 295/ 842, training loss 0.40750619769096375\n",
      "Epoch 10 -- Batch 296/ 842, training loss 0.4173109531402588\n",
      "Epoch 10 -- Batch 297/ 842, training loss 0.41333335638046265\n",
      "Epoch 10 -- Batch 298/ 842, training loss 0.4110269844532013\n",
      "Epoch 10 -- Batch 299/ 842, training loss 0.41716110706329346\n",
      "Epoch 10 -- Batch 300/ 842, training loss 0.405693918466568\n",
      "Epoch 10 -- Batch 301/ 842, training loss 0.4019491672515869\n",
      "Epoch 10 -- Batch 302/ 842, training loss 0.3842959403991699\n",
      "Epoch 10 -- Batch 303/ 842, training loss 0.4083748459815979\n",
      "Epoch 10 -- Batch 304/ 842, training loss 0.39580923318862915\n",
      "Epoch 10 -- Batch 305/ 842, training loss 0.4144807457923889\n",
      "Epoch 10 -- Batch 306/ 842, training loss 0.4127855896949768\n",
      "Epoch 10 -- Batch 307/ 842, training loss 0.3929350674152374\n",
      "Epoch 10 -- Batch 308/ 842, training loss 0.41054198145866394\n",
      "Epoch 10 -- Batch 309/ 842, training loss 0.3916384279727936\n",
      "Epoch 10 -- Batch 310/ 842, training loss 0.414788156747818\n",
      "Epoch 10 -- Batch 311/ 842, training loss 0.39660418033599854\n",
      "Epoch 10 -- Batch 312/ 842, training loss 0.42529401183128357\n",
      "Epoch 10 -- Batch 313/ 842, training loss 0.42505234479904175\n",
      "Epoch 10 -- Batch 314/ 842, training loss 0.4065801203250885\n",
      "Epoch 10 -- Batch 315/ 842, training loss 0.39276570081710815\n",
      "Epoch 10 -- Batch 316/ 842, training loss 0.4096823036670685\n",
      "Epoch 10 -- Batch 317/ 842, training loss 0.39727795124053955\n",
      "Epoch 10 -- Batch 318/ 842, training loss 0.4075375199317932\n",
      "Epoch 10 -- Batch 319/ 842, training loss 0.3935015797615051\n",
      "Epoch 10 -- Batch 320/ 842, training loss 0.4142979085445404\n",
      "Epoch 10 -- Batch 321/ 842, training loss 0.3882240355014801\n",
      "Epoch 10 -- Batch 322/ 842, training loss 0.3983284533023834\n",
      "Epoch 10 -- Batch 323/ 842, training loss 0.3984554409980774\n",
      "Epoch 10 -- Batch 324/ 842, training loss 0.3936285376548767\n",
      "Epoch 10 -- Batch 325/ 842, training loss 0.42782682180404663\n",
      "Epoch 10 -- Batch 326/ 842, training loss 0.39800894260406494\n",
      "Epoch 10 -- Batch 327/ 842, training loss 0.40717384219169617\n",
      "Epoch 10 -- Batch 328/ 842, training loss 0.39922472834587097\n",
      "Epoch 10 -- Batch 329/ 842, training loss 0.39658454060554504\n",
      "Epoch 10 -- Batch 330/ 842, training loss 0.4031657874584198\n",
      "Epoch 10 -- Batch 331/ 842, training loss 0.4193422198295593\n",
      "Epoch 10 -- Batch 332/ 842, training loss 0.4060002863407135\n",
      "Epoch 10 -- Batch 333/ 842, training loss 0.4009663462638855\n",
      "Epoch 10 -- Batch 334/ 842, training loss 0.4003891050815582\n",
      "Epoch 10 -- Batch 335/ 842, training loss 0.40349072217941284\n",
      "Epoch 10 -- Batch 336/ 842, training loss 0.4055946469306946\n",
      "Epoch 10 -- Batch 337/ 842, training loss 0.42235758900642395\n",
      "Epoch 10 -- Batch 338/ 842, training loss 0.4052388668060303\n",
      "Epoch 10 -- Batch 339/ 842, training loss 0.4067940413951874\n",
      "Epoch 10 -- Batch 340/ 842, training loss 0.411295622587204\n",
      "Epoch 10 -- Batch 341/ 842, training loss 0.39632663130760193\n",
      "Epoch 10 -- Batch 342/ 842, training loss 0.39942336082458496\n",
      "Epoch 10 -- Batch 343/ 842, training loss 0.39983299374580383\n",
      "Epoch 10 -- Batch 344/ 842, training loss 0.4056781530380249\n",
      "Epoch 10 -- Batch 345/ 842, training loss 0.4061785936355591\n",
      "Epoch 10 -- Batch 346/ 842, training loss 0.40676960349082947\n",
      "Epoch 10 -- Batch 347/ 842, training loss 0.4056898057460785\n",
      "Epoch 10 -- Batch 348/ 842, training loss 0.39793452620506287\n",
      "Epoch 10 -- Batch 349/ 842, training loss 0.4127112627029419\n",
      "Epoch 10 -- Batch 350/ 842, training loss 0.3955564796924591\n",
      "Epoch 10 -- Batch 351/ 842, training loss 0.3916245698928833\n",
      "Epoch 10 -- Batch 352/ 842, training loss 0.41343221068382263\n",
      "Epoch 10 -- Batch 353/ 842, training loss 0.4118528962135315\n",
      "Epoch 10 -- Batch 354/ 842, training loss 0.40063363313674927\n",
      "Epoch 10 -- Batch 355/ 842, training loss 0.39616915583610535\n",
      "Epoch 10 -- Batch 356/ 842, training loss 0.40773719549179077\n",
      "Epoch 10 -- Batch 357/ 842, training loss 0.39528700709342957\n",
      "Epoch 10 -- Batch 358/ 842, training loss 0.3928583860397339\n",
      "Epoch 10 -- Batch 359/ 842, training loss 0.40534716844558716\n",
      "Epoch 10 -- Batch 360/ 842, training loss 0.3963226079940796\n",
      "Epoch 10 -- Batch 361/ 842, training loss 0.3989024758338928\n",
      "Epoch 10 -- Batch 362/ 842, training loss 0.4066028594970703\n",
      "Epoch 10 -- Batch 363/ 842, training loss 0.4073341488838196\n",
      "Epoch 10 -- Batch 364/ 842, training loss 0.4077449142932892\n",
      "Epoch 10 -- Batch 365/ 842, training loss 0.3938915729522705\n",
      "Epoch 10 -- Batch 366/ 842, training loss 0.4066163897514343\n",
      "Epoch 10 -- Batch 367/ 842, training loss 0.40357092022895813\n",
      "Epoch 10 -- Batch 368/ 842, training loss 0.3925831913948059\n",
      "Epoch 10 -- Batch 369/ 842, training loss 0.3987371027469635\n",
      "Epoch 10 -- Batch 370/ 842, training loss 0.3915342688560486\n",
      "Epoch 10 -- Batch 371/ 842, training loss 0.4122656583786011\n",
      "Epoch 10 -- Batch 372/ 842, training loss 0.4133513867855072\n",
      "Epoch 10 -- Batch 373/ 842, training loss 0.3963417708873749\n",
      "Epoch 10 -- Batch 374/ 842, training loss 0.3975051939487457\n",
      "Epoch 10 -- Batch 375/ 842, training loss 0.42120683193206787\n",
      "Epoch 10 -- Batch 376/ 842, training loss 0.40448319911956787\n",
      "Epoch 10 -- Batch 377/ 842, training loss 0.41891220211982727\n",
      "Epoch 10 -- Batch 378/ 842, training loss 0.39768633246421814\n",
      "Epoch 10 -- Batch 379/ 842, training loss 0.40864530205726624\n",
      "Epoch 10 -- Batch 380/ 842, training loss 0.39288467168807983\n",
      "Epoch 10 -- Batch 381/ 842, training loss 0.42541053891181946\n",
      "Epoch 10 -- Batch 382/ 842, training loss 0.41480082273483276\n",
      "Epoch 10 -- Batch 383/ 842, training loss 0.4176178574562073\n",
      "Epoch 10 -- Batch 384/ 842, training loss 0.40553098917007446\n",
      "Epoch 10 -- Batch 385/ 842, training loss 0.418917179107666\n",
      "Epoch 10 -- Batch 386/ 842, training loss 0.40496692061424255\n",
      "Epoch 10 -- Batch 387/ 842, training loss 0.4146648347377777\n",
      "Epoch 10 -- Batch 388/ 842, training loss 0.41122952103614807\n",
      "Epoch 10 -- Batch 389/ 842, training loss 0.418900728225708\n",
      "Epoch 10 -- Batch 390/ 842, training loss 0.4210783839225769\n",
      "Epoch 10 -- Batch 391/ 842, training loss 0.41283848881721497\n",
      "Epoch 10 -- Batch 392/ 842, training loss 0.4192934036254883\n",
      "Epoch 10 -- Batch 393/ 842, training loss 0.3939194083213806\n",
      "Epoch 10 -- Batch 394/ 842, training loss 0.4198218882083893\n",
      "Epoch 10 -- Batch 395/ 842, training loss 0.41147580742836\n",
      "Epoch 10 -- Batch 396/ 842, training loss 0.4181846082210541\n",
      "Epoch 10 -- Batch 397/ 842, training loss 0.40633270144462585\n",
      "Epoch 10 -- Batch 398/ 842, training loss 0.4166707694530487\n",
      "Epoch 10 -- Batch 399/ 842, training loss 0.4099764823913574\n",
      "Epoch 10 -- Batch 400/ 842, training loss 0.4074859321117401\n",
      "Epoch 10 -- Batch 401/ 842, training loss 0.41744527220726013\n",
      "Epoch 10 -- Batch 402/ 842, training loss 0.4120802879333496\n",
      "Epoch 10 -- Batch 403/ 842, training loss 0.3951716721057892\n",
      "Epoch 10 -- Batch 404/ 842, training loss 0.4154120087623596\n",
      "Epoch 10 -- Batch 405/ 842, training loss 0.40025168657302856\n",
      "Epoch 10 -- Batch 406/ 842, training loss 0.4014095664024353\n",
      "Epoch 10 -- Batch 407/ 842, training loss 0.3979850709438324\n",
      "Epoch 10 -- Batch 408/ 842, training loss 0.4126252233982086\n",
      "Epoch 10 -- Batch 409/ 842, training loss 0.4229658246040344\n",
      "Epoch 10 -- Batch 410/ 842, training loss 0.4029393196105957\n",
      "Epoch 10 -- Batch 411/ 842, training loss 0.4084276556968689\n",
      "Epoch 10 -- Batch 412/ 842, training loss 0.4094192385673523\n",
      "Epoch 10 -- Batch 413/ 842, training loss 0.40657174587249756\n",
      "Epoch 10 -- Batch 414/ 842, training loss 0.40093573927879333\n",
      "Epoch 10 -- Batch 415/ 842, training loss 0.4096004068851471\n",
      "Epoch 10 -- Batch 416/ 842, training loss 0.4084114134311676\n",
      "Epoch 10 -- Batch 417/ 842, training loss 0.3923189043998718\n",
      "Epoch 10 -- Batch 418/ 842, training loss 0.40473616123199463\n",
      "Epoch 10 -- Batch 419/ 842, training loss 0.41685375571250916\n",
      "Epoch 10 -- Batch 420/ 842, training loss 0.39895492792129517\n",
      "Epoch 10 -- Batch 421/ 842, training loss 0.42623192071914673\n",
      "Epoch 10 -- Batch 422/ 842, training loss 0.4085472524166107\n",
      "Epoch 10 -- Batch 423/ 842, training loss 0.41335952281951904\n",
      "Epoch 10 -- Batch 424/ 842, training loss 0.41248950362205505\n",
      "Epoch 10 -- Batch 425/ 842, training loss 0.38541242480278015\n",
      "Epoch 10 -- Batch 426/ 842, training loss 0.402709424495697\n",
      "Epoch 10 -- Batch 427/ 842, training loss 0.39560574293136597\n",
      "Epoch 10 -- Batch 428/ 842, training loss 0.4130691587924957\n",
      "Epoch 10 -- Batch 429/ 842, training loss 0.40832751989364624\n",
      "Epoch 10 -- Batch 430/ 842, training loss 0.4078408479690552\n",
      "Epoch 10 -- Batch 431/ 842, training loss 0.4134979248046875\n",
      "Epoch 10 -- Batch 432/ 842, training loss 0.4153095781803131\n",
      "Epoch 10 -- Batch 433/ 842, training loss 0.4128192365169525\n",
      "Epoch 10 -- Batch 434/ 842, training loss 0.404350608587265\n",
      "Epoch 10 -- Batch 435/ 842, training loss 0.4130263030529022\n",
      "Epoch 10 -- Batch 436/ 842, training loss 0.4096143841743469\n",
      "Epoch 10 -- Batch 437/ 842, training loss 0.3944857716560364\n",
      "Epoch 10 -- Batch 438/ 842, training loss 0.4246615767478943\n",
      "Epoch 10 -- Batch 439/ 842, training loss 0.3914768397808075\n",
      "Epoch 10 -- Batch 440/ 842, training loss 0.4020930230617523\n",
      "Epoch 10 -- Batch 441/ 842, training loss 0.39967623353004456\n",
      "Epoch 10 -- Batch 442/ 842, training loss 0.39747655391693115\n",
      "Epoch 10 -- Batch 443/ 842, training loss 0.39730092883110046\n",
      "Epoch 10 -- Batch 444/ 842, training loss 0.4158087372779846\n",
      "Epoch 10 -- Batch 445/ 842, training loss 0.4046139121055603\n",
      "Epoch 10 -- Batch 446/ 842, training loss 0.3973683714866638\n",
      "Epoch 10 -- Batch 447/ 842, training loss 0.3980054557323456\n",
      "Epoch 10 -- Batch 448/ 842, training loss 0.40091565251350403\n",
      "Epoch 10 -- Batch 449/ 842, training loss 0.40643319487571716\n",
      "Epoch 10 -- Batch 450/ 842, training loss 0.4048256576061249\n",
      "Epoch 10 -- Batch 451/ 842, training loss 0.40304163098335266\n",
      "Epoch 10 -- Batch 452/ 842, training loss 0.41840413212776184\n",
      "Epoch 10 -- Batch 453/ 842, training loss 0.39863282442092896\n",
      "Epoch 10 -- Batch 454/ 842, training loss 0.3935069739818573\n",
      "Epoch 10 -- Batch 455/ 842, training loss 0.3942320942878723\n",
      "Epoch 10 -- Batch 456/ 842, training loss 0.4005972146987915\n",
      "Epoch 10 -- Batch 457/ 842, training loss 0.40356189012527466\n",
      "Epoch 10 -- Batch 458/ 842, training loss 0.3950788676738739\n",
      "Epoch 10 -- Batch 459/ 842, training loss 0.4059126675128937\n",
      "Epoch 10 -- Batch 460/ 842, training loss 0.40874361991882324\n",
      "Epoch 10 -- Batch 461/ 842, training loss 0.3893077075481415\n",
      "Epoch 10 -- Batch 462/ 842, training loss 0.40002626180648804\n",
      "Epoch 10 -- Batch 463/ 842, training loss 0.39162319898605347\n",
      "Epoch 10 -- Batch 464/ 842, training loss 0.41411152482032776\n",
      "Epoch 10 -- Batch 465/ 842, training loss 0.41510507464408875\n",
      "Epoch 10 -- Batch 466/ 842, training loss 0.4144352972507477\n",
      "Epoch 10 -- Batch 467/ 842, training loss 0.4144475758075714\n",
      "Epoch 10 -- Batch 468/ 842, training loss 0.3942926228046417\n",
      "Epoch 10 -- Batch 469/ 842, training loss 0.414015531539917\n",
      "Epoch 10 -- Batch 470/ 842, training loss 0.4012107253074646\n",
      "Epoch 10 -- Batch 471/ 842, training loss 0.405170738697052\n",
      "Epoch 10 -- Batch 472/ 842, training loss 0.40597692131996155\n",
      "Epoch 10 -- Batch 473/ 842, training loss 0.39671790599823\n",
      "Epoch 10 -- Batch 474/ 842, training loss 0.39255037903785706\n",
      "Epoch 10 -- Batch 475/ 842, training loss 0.41377946734428406\n",
      "Epoch 10 -- Batch 476/ 842, training loss 0.4103010594844818\n",
      "Epoch 10 -- Batch 477/ 842, training loss 0.4071325659751892\n",
      "Epoch 10 -- Batch 478/ 842, training loss 0.40951651334762573\n",
      "Epoch 10 -- Batch 479/ 842, training loss 0.4083944857120514\n",
      "Epoch 10 -- Batch 480/ 842, training loss 0.4141250550746918\n",
      "Epoch 10 -- Batch 481/ 842, training loss 0.3924851715564728\n",
      "Epoch 10 -- Batch 482/ 842, training loss 0.4244941473007202\n",
      "Epoch 10 -- Batch 483/ 842, training loss 0.39220574498176575\n",
      "Epoch 10 -- Batch 484/ 842, training loss 0.4198254346847534\n",
      "Epoch 10 -- Batch 485/ 842, training loss 0.38585489988327026\n",
      "Epoch 10 -- Batch 486/ 842, training loss 0.41112497448921204\n",
      "Epoch 10 -- Batch 487/ 842, training loss 0.41873183846473694\n",
      "Epoch 10 -- Batch 488/ 842, training loss 0.4181060194969177\n",
      "Epoch 10 -- Batch 489/ 842, training loss 0.4079177677631378\n",
      "Epoch 10 -- Batch 490/ 842, training loss 0.41036486625671387\n",
      "Epoch 10 -- Batch 491/ 842, training loss 0.3993929922580719\n",
      "Epoch 10 -- Batch 492/ 842, training loss 0.39524179697036743\n",
      "Epoch 10 -- Batch 493/ 842, training loss 0.4067513942718506\n",
      "Epoch 10 -- Batch 494/ 842, training loss 0.3939801752567291\n",
      "Epoch 10 -- Batch 495/ 842, training loss 0.40669795870780945\n",
      "Epoch 10 -- Batch 496/ 842, training loss 0.40632060170173645\n",
      "Epoch 10 -- Batch 497/ 842, training loss 0.412609726190567\n",
      "Epoch 10 -- Batch 498/ 842, training loss 0.40555110573768616\n",
      "Epoch 10 -- Batch 499/ 842, training loss 0.4004213213920593\n",
      "Epoch 10 -- Batch 500/ 842, training loss 0.4065980315208435\n",
      "Epoch 10 -- Batch 501/ 842, training loss 0.4026188254356384\n",
      "Epoch 10 -- Batch 502/ 842, training loss 0.42741602659225464\n",
      "Epoch 10 -- Batch 503/ 842, training loss 0.3963058888912201\n",
      "Epoch 10 -- Batch 504/ 842, training loss 0.39633187651634216\n",
      "Epoch 10 -- Batch 505/ 842, training loss 0.3966253697872162\n",
      "Epoch 10 -- Batch 506/ 842, training loss 0.39901259541511536\n",
      "Epoch 10 -- Batch 507/ 842, training loss 0.4018021821975708\n",
      "Epoch 10 -- Batch 508/ 842, training loss 0.4186721742153168\n",
      "Epoch 10 -- Batch 509/ 842, training loss 0.3963680863380432\n",
      "Epoch 10 -- Batch 510/ 842, training loss 0.3908350169658661\n",
      "Epoch 10 -- Batch 511/ 842, training loss 0.4142308235168457\n",
      "Epoch 10 -- Batch 512/ 842, training loss 0.40265825390815735\n",
      "Epoch 10 -- Batch 513/ 842, training loss 0.4054839313030243\n",
      "Epoch 10 -- Batch 514/ 842, training loss 0.3960704803466797\n",
      "Epoch 10 -- Batch 515/ 842, training loss 0.42617931962013245\n",
      "Epoch 10 -- Batch 516/ 842, training loss 0.39995288848876953\n",
      "Epoch 10 -- Batch 517/ 842, training loss 0.4028112292289734\n",
      "Epoch 10 -- Batch 518/ 842, training loss 0.4070189297199249\n",
      "Epoch 10 -- Batch 519/ 842, training loss 0.40466994047164917\n",
      "Epoch 10 -- Batch 520/ 842, training loss 0.40779295563697815\n",
      "Epoch 10 -- Batch 521/ 842, training loss 0.3882960081100464\n",
      "Epoch 10 -- Batch 522/ 842, training loss 0.40085455775260925\n",
      "Epoch 10 -- Batch 523/ 842, training loss 0.3840964734554291\n",
      "Epoch 10 -- Batch 524/ 842, training loss 0.3975561559200287\n",
      "Epoch 10 -- Batch 525/ 842, training loss 0.41263237595558167\n",
      "Epoch 10 -- Batch 526/ 842, training loss 0.39440926909446716\n",
      "Epoch 10 -- Batch 527/ 842, training loss 0.417201429605484\n",
      "Epoch 10 -- Batch 528/ 842, training loss 0.4230213165283203\n",
      "Epoch 10 -- Batch 529/ 842, training loss 0.40270060300827026\n",
      "Epoch 10 -- Batch 530/ 842, training loss 0.4201471209526062\n",
      "Epoch 10 -- Batch 531/ 842, training loss 0.4102051854133606\n",
      "Epoch 10 -- Batch 532/ 842, training loss 0.41769009828567505\n",
      "Epoch 10 -- Batch 533/ 842, training loss 0.38517269492149353\n",
      "Epoch 10 -- Batch 534/ 842, training loss 0.39855822920799255\n",
      "Epoch 10 -- Batch 535/ 842, training loss 0.40049511194229126\n",
      "Epoch 10 -- Batch 536/ 842, training loss 0.41181913018226624\n",
      "Epoch 10 -- Batch 537/ 842, training loss 0.4062286615371704\n",
      "Epoch 10 -- Batch 538/ 842, training loss 0.40982088446617126\n",
      "Epoch 10 -- Batch 539/ 842, training loss 0.41865476965904236\n",
      "Epoch 10 -- Batch 540/ 842, training loss 0.4045645296573639\n",
      "Epoch 10 -- Batch 541/ 842, training loss 0.40115901827812195\n",
      "Epoch 10 -- Batch 542/ 842, training loss 0.40840375423431396\n",
      "Epoch 10 -- Batch 543/ 842, training loss 0.3945913314819336\n",
      "Epoch 10 -- Batch 544/ 842, training loss 0.3892447054386139\n",
      "Epoch 10 -- Batch 545/ 842, training loss 0.40545058250427246\n",
      "Epoch 10 -- Batch 546/ 842, training loss 0.4038093686103821\n",
      "Epoch 10 -- Batch 547/ 842, training loss 0.4093048870563507\n",
      "Epoch 10 -- Batch 548/ 842, training loss 0.41630086302757263\n",
      "Epoch 10 -- Batch 549/ 842, training loss 0.38502639532089233\n",
      "Epoch 10 -- Batch 550/ 842, training loss 0.4138317108154297\n",
      "Epoch 10 -- Batch 551/ 842, training loss 0.4056030213832855\n",
      "Epoch 10 -- Batch 552/ 842, training loss 0.419999897480011\n",
      "Epoch 10 -- Batch 553/ 842, training loss 0.40134480595588684\n",
      "Epoch 10 -- Batch 554/ 842, training loss 0.4058731198310852\n",
      "Epoch 10 -- Batch 555/ 842, training loss 0.4008787274360657\n",
      "Epoch 10 -- Batch 556/ 842, training loss 0.40292808413505554\n",
      "Epoch 10 -- Batch 557/ 842, training loss 0.4110044240951538\n",
      "Epoch 10 -- Batch 558/ 842, training loss 0.4048454165458679\n",
      "Epoch 10 -- Batch 559/ 842, training loss 0.39941638708114624\n",
      "Epoch 10 -- Batch 560/ 842, training loss 0.4001813530921936\n",
      "Epoch 10 -- Batch 561/ 842, training loss 0.4146125316619873\n",
      "Epoch 10 -- Batch 562/ 842, training loss 0.4073593020439148\n",
      "Epoch 10 -- Batch 563/ 842, training loss 0.39710673689842224\n",
      "Epoch 10 -- Batch 564/ 842, training loss 0.3885318338871002\n",
      "Epoch 10 -- Batch 565/ 842, training loss 0.4141899645328522\n",
      "Epoch 10 -- Batch 566/ 842, training loss 0.4106679856777191\n",
      "Epoch 10 -- Batch 567/ 842, training loss 0.4090195596218109\n",
      "Epoch 10 -- Batch 568/ 842, training loss 0.41172584891319275\n",
      "Epoch 10 -- Batch 569/ 842, training loss 0.38615742325782776\n",
      "Epoch 10 -- Batch 570/ 842, training loss 0.4013751149177551\n",
      "Epoch 10 -- Batch 571/ 842, training loss 0.4054761528968811\n",
      "Epoch 10 -- Batch 572/ 842, training loss 0.41068077087402344\n",
      "Epoch 10 -- Batch 573/ 842, training loss 0.40087801218032837\n",
      "Epoch 10 -- Batch 574/ 842, training loss 0.3896024227142334\n",
      "Epoch 10 -- Batch 575/ 842, training loss 0.3953039348125458\n",
      "Epoch 10 -- Batch 576/ 842, training loss 0.4041634500026703\n",
      "Epoch 10 -- Batch 577/ 842, training loss 0.40312567353248596\n",
      "Epoch 10 -- Batch 578/ 842, training loss 0.3965708315372467\n",
      "Epoch 10 -- Batch 579/ 842, training loss 0.40383949875831604\n",
      "Epoch 10 -- Batch 580/ 842, training loss 0.4157770872116089\n",
      "Epoch 10 -- Batch 581/ 842, training loss 0.40795212984085083\n",
      "Epoch 10 -- Batch 582/ 842, training loss 0.41583096981048584\n",
      "Epoch 10 -- Batch 583/ 842, training loss 0.4177000820636749\n",
      "Epoch 10 -- Batch 584/ 842, training loss 0.4196990430355072\n",
      "Epoch 10 -- Batch 585/ 842, training loss 0.4096229374408722\n",
      "Epoch 10 -- Batch 586/ 842, training loss 0.38224610686302185\n",
      "Epoch 10 -- Batch 587/ 842, training loss 0.4036790430545807\n",
      "Epoch 10 -- Batch 588/ 842, training loss 0.4129558503627777\n",
      "Epoch 10 -- Batch 589/ 842, training loss 0.4014453887939453\n",
      "Epoch 10 -- Batch 590/ 842, training loss 0.4058382213115692\n",
      "Epoch 10 -- Batch 591/ 842, training loss 0.3949970006942749\n",
      "Epoch 10 -- Batch 592/ 842, training loss 0.3976908326148987\n",
      "Epoch 10 -- Batch 593/ 842, training loss 0.3869529068470001\n",
      "Epoch 10 -- Batch 594/ 842, training loss 0.39429980516433716\n",
      "Epoch 10 -- Batch 595/ 842, training loss 0.41861093044281006\n",
      "Epoch 10 -- Batch 596/ 842, training loss 0.39862504601478577\n",
      "Epoch 10 -- Batch 597/ 842, training loss 0.4223804175853729\n",
      "Epoch 10 -- Batch 598/ 842, training loss 0.4248809516429901\n",
      "Epoch 10 -- Batch 599/ 842, training loss 0.3969055414199829\n",
      "Epoch 10 -- Batch 600/ 842, training loss 0.4093964099884033\n",
      "Epoch 10 -- Batch 601/ 842, training loss 0.4108346700668335\n",
      "Epoch 10 -- Batch 602/ 842, training loss 0.4035935401916504\n",
      "Epoch 10 -- Batch 603/ 842, training loss 0.4058908522129059\n",
      "Epoch 10 -- Batch 604/ 842, training loss 0.39296144247055054\n",
      "Epoch 10 -- Batch 605/ 842, training loss 0.4121977984905243\n",
      "Epoch 10 -- Batch 606/ 842, training loss 0.40905430912971497\n",
      "Epoch 10 -- Batch 607/ 842, training loss 0.4130784273147583\n",
      "Epoch 10 -- Batch 608/ 842, training loss 0.38340288400650024\n",
      "Epoch 10 -- Batch 609/ 842, training loss 0.3982073664665222\n",
      "Epoch 10 -- Batch 610/ 842, training loss 0.3938937485218048\n",
      "Epoch 10 -- Batch 611/ 842, training loss 0.41461148858070374\n",
      "Epoch 10 -- Batch 612/ 842, training loss 0.40642672777175903\n",
      "Epoch 10 -- Batch 613/ 842, training loss 0.42104804515838623\n",
      "Epoch 10 -- Batch 614/ 842, training loss 0.4009743630886078\n",
      "Epoch 10 -- Batch 615/ 842, training loss 0.3983272612094879\n",
      "Epoch 10 -- Batch 616/ 842, training loss 0.3864661157131195\n",
      "Epoch 10 -- Batch 617/ 842, training loss 0.404279500246048\n",
      "Epoch 10 -- Batch 618/ 842, training loss 0.40850189328193665\n",
      "Epoch 10 -- Batch 619/ 842, training loss 0.4127901494503021\n",
      "Epoch 10 -- Batch 620/ 842, training loss 0.4062252640724182\n",
      "Epoch 10 -- Batch 621/ 842, training loss 0.39421162009239197\n",
      "Epoch 10 -- Batch 622/ 842, training loss 0.40659061074256897\n",
      "Epoch 10 -- Batch 623/ 842, training loss 0.3996936082839966\n",
      "Epoch 10 -- Batch 624/ 842, training loss 0.41723793745040894\n",
      "Epoch 10 -- Batch 625/ 842, training loss 0.3975159227848053\n",
      "Epoch 10 -- Batch 626/ 842, training loss 0.41105544567108154\n",
      "Epoch 10 -- Batch 627/ 842, training loss 0.39520013332366943\n",
      "Epoch 10 -- Batch 628/ 842, training loss 0.3896615505218506\n",
      "Epoch 10 -- Batch 629/ 842, training loss 0.4089359939098358\n",
      "Epoch 10 -- Batch 630/ 842, training loss 0.40312659740448\n",
      "Epoch 10 -- Batch 631/ 842, training loss 0.4100264608860016\n",
      "Epoch 10 -- Batch 632/ 842, training loss 0.41609546542167664\n",
      "Epoch 10 -- Batch 633/ 842, training loss 0.4144205152988434\n",
      "Epoch 10 -- Batch 634/ 842, training loss 0.41575926542282104\n",
      "Epoch 10 -- Batch 635/ 842, training loss 0.39941540360450745\n",
      "Epoch 10 -- Batch 636/ 842, training loss 0.40581998229026794\n",
      "Epoch 10 -- Batch 637/ 842, training loss 0.40650084614753723\n",
      "Epoch 10 -- Batch 638/ 842, training loss 0.41196030378341675\n",
      "Epoch 10 -- Batch 639/ 842, training loss 0.3999721109867096\n",
      "Epoch 10 -- Batch 640/ 842, training loss 0.41041260957717896\n",
      "Epoch 10 -- Batch 641/ 842, training loss 0.40255865454673767\n",
      "Epoch 10 -- Batch 642/ 842, training loss 0.38159385323524475\n",
      "Epoch 10 -- Batch 643/ 842, training loss 0.4045815169811249\n",
      "Epoch 10 -- Batch 644/ 842, training loss 0.4022139310836792\n",
      "Epoch 10 -- Batch 645/ 842, training loss 0.4013794958591461\n",
      "Epoch 10 -- Batch 646/ 842, training loss 0.388680100440979\n",
      "Epoch 10 -- Batch 647/ 842, training loss 0.4016418159008026\n",
      "Epoch 10 -- Batch 648/ 842, training loss 0.40612542629241943\n",
      "Epoch 10 -- Batch 649/ 842, training loss 0.3934687674045563\n",
      "Epoch 10 -- Batch 650/ 842, training loss 0.39524126052856445\n",
      "Epoch 10 -- Batch 651/ 842, training loss 0.4131070673465729\n",
      "Epoch 10 -- Batch 652/ 842, training loss 0.40997710824012756\n",
      "Epoch 10 -- Batch 653/ 842, training loss 0.40253064036369324\n",
      "Epoch 10 -- Batch 654/ 842, training loss 0.3933524489402771\n",
      "Epoch 10 -- Batch 655/ 842, training loss 0.39467474818229675\n",
      "Epoch 10 -- Batch 656/ 842, training loss 0.40428850054740906\n",
      "Epoch 10 -- Batch 657/ 842, training loss 0.3981368839740753\n",
      "Epoch 10 -- Batch 658/ 842, training loss 0.4045218229293823\n",
      "Epoch 10 -- Batch 659/ 842, training loss 0.40094080567359924\n",
      "Epoch 10 -- Batch 660/ 842, training loss 0.4009782373905182\n",
      "Epoch 10 -- Batch 661/ 842, training loss 0.3992488384246826\n",
      "Epoch 10 -- Batch 662/ 842, training loss 0.41698768734931946\n",
      "Epoch 10 -- Batch 663/ 842, training loss 0.4098967909812927\n",
      "Epoch 10 -- Batch 664/ 842, training loss 0.4096546769142151\n",
      "Epoch 10 -- Batch 665/ 842, training loss 0.39544805884361267\n",
      "Epoch 10 -- Batch 666/ 842, training loss 0.40655791759490967\n",
      "Epoch 10 -- Batch 667/ 842, training loss 0.41670089960098267\n",
      "Epoch 10 -- Batch 668/ 842, training loss 0.41188526153564453\n",
      "Epoch 10 -- Batch 669/ 842, training loss 0.41991516947746277\n",
      "Epoch 10 -- Batch 670/ 842, training loss 0.3892803192138672\n",
      "Epoch 10 -- Batch 671/ 842, training loss 0.3978330194950104\n",
      "Epoch 10 -- Batch 672/ 842, training loss 0.40216806530952454\n",
      "Epoch 10 -- Batch 673/ 842, training loss 0.4152868986129761\n",
      "Epoch 10 -- Batch 674/ 842, training loss 0.398171067237854\n",
      "Epoch 10 -- Batch 675/ 842, training loss 0.39307644963264465\n",
      "Epoch 10 -- Batch 676/ 842, training loss 0.39642632007598877\n",
      "Epoch 10 -- Batch 677/ 842, training loss 0.40176406502723694\n",
      "Epoch 10 -- Batch 678/ 842, training loss 0.40354618430137634\n",
      "Epoch 10 -- Batch 679/ 842, training loss 0.407625675201416\n",
      "Epoch 10 -- Batch 680/ 842, training loss 0.3943774998188019\n",
      "Epoch 10 -- Batch 681/ 842, training loss 0.39752164483070374\n",
      "Epoch 10 -- Batch 682/ 842, training loss 0.4018624722957611\n",
      "Epoch 10 -- Batch 683/ 842, training loss 0.4145345389842987\n",
      "Epoch 10 -- Batch 684/ 842, training loss 0.40927353501319885\n",
      "Epoch 10 -- Batch 685/ 842, training loss 0.4112790822982788\n",
      "Epoch 10 -- Batch 686/ 842, training loss 0.39792829751968384\n",
      "Epoch 10 -- Batch 687/ 842, training loss 0.4159374535083771\n",
      "Epoch 10 -- Batch 688/ 842, training loss 0.41192230582237244\n",
      "Epoch 10 -- Batch 689/ 842, training loss 0.4045056104660034\n",
      "Epoch 10 -- Batch 690/ 842, training loss 0.40371188521385193\n",
      "Epoch 10 -- Batch 691/ 842, training loss 0.40097156167030334\n",
      "Epoch 10 -- Batch 692/ 842, training loss 0.394853413105011\n",
      "Epoch 10 -- Batch 693/ 842, training loss 0.40908434987068176\n",
      "Epoch 10 -- Batch 694/ 842, training loss 0.41177162528038025\n",
      "Epoch 10 -- Batch 695/ 842, training loss 0.42282524704933167\n",
      "Epoch 10 -- Batch 696/ 842, training loss 0.40162166953086853\n",
      "Epoch 10 -- Batch 697/ 842, training loss 0.4071670472621918\n",
      "Epoch 10 -- Batch 698/ 842, training loss 0.406538724899292\n",
      "Epoch 10 -- Batch 699/ 842, training loss 0.40871649980545044\n",
      "Epoch 10 -- Batch 700/ 842, training loss 0.4029349982738495\n",
      "Epoch 10 -- Batch 701/ 842, training loss 0.39257553219795227\n",
      "Epoch 10 -- Batch 702/ 842, training loss 0.41146472096443176\n",
      "Epoch 10 -- Batch 703/ 842, training loss 0.40241652727127075\n",
      "Epoch 10 -- Batch 704/ 842, training loss 0.39513134956359863\n",
      "Epoch 10 -- Batch 705/ 842, training loss 0.4249049723148346\n",
      "Epoch 10 -- Batch 706/ 842, training loss 0.404824823141098\n",
      "Epoch 10 -- Batch 707/ 842, training loss 0.4062608480453491\n",
      "Epoch 10 -- Batch 708/ 842, training loss 0.41331085562705994\n",
      "Epoch 10 -- Batch 709/ 842, training loss 0.4043227732181549\n",
      "Epoch 10 -- Batch 710/ 842, training loss 0.396208792924881\n",
      "Epoch 10 -- Batch 711/ 842, training loss 0.4068453907966614\n",
      "Epoch 10 -- Batch 712/ 842, training loss 0.41064077615737915\n",
      "Epoch 10 -- Batch 713/ 842, training loss 0.41222628951072693\n",
      "Epoch 10 -- Batch 714/ 842, training loss 0.396398663520813\n",
      "Epoch 10 -- Batch 715/ 842, training loss 0.40946412086486816\n",
      "Epoch 10 -- Batch 716/ 842, training loss 0.3911168575286865\n",
      "Epoch 10 -- Batch 717/ 842, training loss 0.408761203289032\n",
      "Epoch 10 -- Batch 718/ 842, training loss 0.4168073534965515\n",
      "Epoch 10 -- Batch 719/ 842, training loss 0.41886305809020996\n",
      "Epoch 10 -- Batch 720/ 842, training loss 0.3898041844367981\n",
      "Epoch 10 -- Batch 721/ 842, training loss 0.4097125232219696\n",
      "Epoch 10 -- Batch 722/ 842, training loss 0.41940951347351074\n",
      "Epoch 10 -- Batch 723/ 842, training loss 0.40284088253974915\n",
      "Epoch 10 -- Batch 724/ 842, training loss 0.4045431911945343\n",
      "Epoch 10 -- Batch 725/ 842, training loss 0.3883422017097473\n",
      "Epoch 10 -- Batch 726/ 842, training loss 0.3993435204029083\n",
      "Epoch 10 -- Batch 727/ 842, training loss 0.39662280678749084\n",
      "Epoch 10 -- Batch 728/ 842, training loss 0.39579853415489197\n",
      "Epoch 10 -- Batch 729/ 842, training loss 0.3746248781681061\n",
      "Epoch 10 -- Batch 730/ 842, training loss 0.40179842710494995\n",
      "Epoch 10 -- Batch 731/ 842, training loss 0.40424227714538574\n",
      "Epoch 10 -- Batch 732/ 842, training loss 0.40116560459136963\n",
      "Epoch 10 -- Batch 733/ 842, training loss 0.4150637686252594\n",
      "Epoch 10 -- Batch 734/ 842, training loss 0.4262109696865082\n",
      "Epoch 10 -- Batch 735/ 842, training loss 0.4182175099849701\n",
      "Epoch 10 -- Batch 736/ 842, training loss 0.4154340624809265\n",
      "Epoch 10 -- Batch 737/ 842, training loss 0.3930511474609375\n",
      "Epoch 10 -- Batch 738/ 842, training loss 0.4096258580684662\n",
      "Epoch 10 -- Batch 739/ 842, training loss 0.41681039333343506\n",
      "Epoch 10 -- Batch 740/ 842, training loss 0.40461060404777527\n",
      "Epoch 10 -- Batch 741/ 842, training loss 0.39804670214653015\n",
      "Epoch 10 -- Batch 742/ 842, training loss 0.4008655250072479\n",
      "Epoch 10 -- Batch 743/ 842, training loss 0.4001443684101105\n",
      "Epoch 10 -- Batch 744/ 842, training loss 0.39860954880714417\n",
      "Epoch 10 -- Batch 745/ 842, training loss 0.392630398273468\n",
      "Epoch 10 -- Batch 746/ 842, training loss 0.4015122056007385\n",
      "Epoch 10 -- Batch 747/ 842, training loss 0.4073297083377838\n",
      "Epoch 10 -- Batch 748/ 842, training loss 0.39519503712654114\n",
      "Epoch 10 -- Batch 749/ 842, training loss 0.41362082958221436\n",
      "Epoch 10 -- Batch 750/ 842, training loss 0.4147847592830658\n",
      "Epoch 10 -- Batch 751/ 842, training loss 0.4081960916519165\n",
      "Epoch 10 -- Batch 752/ 842, training loss 0.39983630180358887\n",
      "Epoch 10 -- Batch 753/ 842, training loss 0.404763787984848\n",
      "Epoch 10 -- Batch 754/ 842, training loss 0.4078670144081116\n",
      "Epoch 10 -- Batch 755/ 842, training loss 0.4105365574359894\n",
      "Epoch 10 -- Batch 756/ 842, training loss 0.3993549346923828\n",
      "Epoch 10 -- Batch 757/ 842, training loss 0.41524940729141235\n",
      "Epoch 10 -- Batch 758/ 842, training loss 0.39617136120796204\n",
      "Epoch 10 -- Batch 759/ 842, training loss 0.39448726177215576\n",
      "Epoch 10 -- Batch 760/ 842, training loss 0.4232340157032013\n",
      "Epoch 10 -- Batch 761/ 842, training loss 0.4048926830291748\n",
      "Epoch 10 -- Batch 762/ 842, training loss 0.4070950448513031\n",
      "Epoch 10 -- Batch 763/ 842, training loss 0.4008677899837494\n",
      "Epoch 10 -- Batch 764/ 842, training loss 0.38997140526771545\n",
      "Epoch 10 -- Batch 765/ 842, training loss 0.41148361563682556\n",
      "Epoch 10 -- Batch 766/ 842, training loss 0.41104739904403687\n",
      "Epoch 10 -- Batch 767/ 842, training loss 0.3897269666194916\n",
      "Epoch 10 -- Batch 768/ 842, training loss 0.40531790256500244\n",
      "Epoch 10 -- Batch 769/ 842, training loss 0.4150826632976532\n",
      "Epoch 10 -- Batch 770/ 842, training loss 0.3998088240623474\n",
      "Epoch 10 -- Batch 771/ 842, training loss 0.40063565969467163\n",
      "Epoch 10 -- Batch 772/ 842, training loss 0.4081168472766876\n",
      "Epoch 10 -- Batch 773/ 842, training loss 0.40673592686653137\n",
      "Epoch 10 -- Batch 774/ 842, training loss 0.39811256527900696\n",
      "Epoch 10 -- Batch 775/ 842, training loss 0.3945959806442261\n",
      "Epoch 10 -- Batch 776/ 842, training loss 0.39569321274757385\n",
      "Epoch 10 -- Batch 777/ 842, training loss 0.4022815525531769\n",
      "Epoch 10 -- Batch 778/ 842, training loss 0.4214998781681061\n",
      "Epoch 10 -- Batch 779/ 842, training loss 0.4176070988178253\n",
      "Epoch 10 -- Batch 780/ 842, training loss 0.40322211384773254\n",
      "Epoch 10 -- Batch 781/ 842, training loss 0.3874579071998596\n",
      "Epoch 10 -- Batch 782/ 842, training loss 0.41084590554237366\n",
      "Epoch 10 -- Batch 783/ 842, training loss 0.40922996401786804\n",
      "Epoch 10 -- Batch 784/ 842, training loss 0.3980310559272766\n",
      "Epoch 10 -- Batch 785/ 842, training loss 0.4044174253940582\n",
      "Epoch 10 -- Batch 786/ 842, training loss 0.40096479654312134\n",
      "Epoch 10 -- Batch 787/ 842, training loss 0.40308699011802673\n",
      "Epoch 10 -- Batch 788/ 842, training loss 0.40208327770233154\n",
      "Epoch 10 -- Batch 789/ 842, training loss 0.4219050705432892\n",
      "Epoch 10 -- Batch 790/ 842, training loss 0.3993068337440491\n",
      "Epoch 10 -- Batch 791/ 842, training loss 0.406551331281662\n",
      "Epoch 10 -- Batch 792/ 842, training loss 0.4013965129852295\n",
      "Epoch 10 -- Batch 793/ 842, training loss 0.39984405040740967\n",
      "Epoch 10 -- Batch 794/ 842, training loss 0.4064856171607971\n",
      "Epoch 10 -- Batch 795/ 842, training loss 0.40195342898368835\n",
      "Epoch 10 -- Batch 796/ 842, training loss 0.4077235460281372\n",
      "Epoch 10 -- Batch 797/ 842, training loss 0.4104781150817871\n",
      "Epoch 10 -- Batch 798/ 842, training loss 0.40044280886650085\n",
      "Epoch 10 -- Batch 799/ 842, training loss 0.4048066735267639\n",
      "Epoch 10 -- Batch 800/ 842, training loss 0.4010472595691681\n",
      "Epoch 10 -- Batch 801/ 842, training loss 0.3898340165615082\n",
      "Epoch 10 -- Batch 802/ 842, training loss 0.41097480058670044\n",
      "Epoch 10 -- Batch 803/ 842, training loss 0.4129687547683716\n",
      "Epoch 10 -- Batch 804/ 842, training loss 0.3957841992378235\n",
      "Epoch 10 -- Batch 805/ 842, training loss 0.40696877241134644\n",
      "Epoch 10 -- Batch 806/ 842, training loss 0.3998769521713257\n",
      "Epoch 10 -- Batch 807/ 842, training loss 0.4060212969779968\n",
      "Epoch 10 -- Batch 808/ 842, training loss 0.4183056950569153\n",
      "Epoch 10 -- Batch 809/ 842, training loss 0.40241214632987976\n",
      "Epoch 10 -- Batch 810/ 842, training loss 0.38766247034072876\n",
      "Epoch 10 -- Batch 811/ 842, training loss 0.42077386379241943\n",
      "Epoch 10 -- Batch 812/ 842, training loss 0.4075607657432556\n",
      "Epoch 10 -- Batch 813/ 842, training loss 0.3992517590522766\n",
      "Epoch 10 -- Batch 814/ 842, training loss 0.39850977063179016\n",
      "Epoch 10 -- Batch 815/ 842, training loss 0.41077300906181335\n",
      "Epoch 10 -- Batch 816/ 842, training loss 0.40431153774261475\n",
      "Epoch 10 -- Batch 817/ 842, training loss 0.40895265340805054\n",
      "Epoch 10 -- Batch 818/ 842, training loss 0.3978269100189209\n",
      "Epoch 10 -- Batch 819/ 842, training loss 0.4091050922870636\n",
      "Epoch 10 -- Batch 820/ 842, training loss 0.4009174108505249\n",
      "Epoch 10 -- Batch 821/ 842, training loss 0.40423768758773804\n",
      "Epoch 10 -- Batch 822/ 842, training loss 0.39878854155540466\n",
      "Epoch 10 -- Batch 823/ 842, training loss 0.4108624756336212\n",
      "Epoch 10 -- Batch 824/ 842, training loss 0.40344297885894775\n",
      "Epoch 10 -- Batch 825/ 842, training loss 0.3965156078338623\n",
      "Epoch 10 -- Batch 826/ 842, training loss 0.4127004146575928\n",
      "Epoch 10 -- Batch 827/ 842, training loss 0.4049149751663208\n",
      "Epoch 10 -- Batch 828/ 842, training loss 0.4124693274497986\n",
      "Epoch 10 -- Batch 829/ 842, training loss 0.40292197465896606\n",
      "Epoch 10 -- Batch 830/ 842, training loss 0.4013980031013489\n",
      "Epoch 10 -- Batch 831/ 842, training loss 0.41489043831825256\n",
      "Epoch 10 -- Batch 832/ 842, training loss 0.39958491921424866\n",
      "Epoch 10 -- Batch 833/ 842, training loss 0.40716856718063354\n",
      "Epoch 10 -- Batch 834/ 842, training loss 0.41114237904548645\n",
      "Epoch 10 -- Batch 835/ 842, training loss 0.40639543533325195\n",
      "Epoch 10 -- Batch 836/ 842, training loss 0.4052964746952057\n",
      "Epoch 10 -- Batch 837/ 842, training loss 0.4048839509487152\n",
      "Epoch 10 -- Batch 838/ 842, training loss 0.4003457725048065\n",
      "Epoch 10 -- Batch 839/ 842, training loss 0.39828747510910034\n",
      "Epoch 10 -- Batch 840/ 842, training loss 0.41295626759529114\n",
      "Epoch 10 -- Batch 841/ 842, training loss 0.4078618288040161\n",
      "Epoch 10 -- Batch 842/ 842, training loss 0.3491625189781189\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10 -- Batch 1/ 94, validation loss 0.4025760293006897\n",
      "Epoch 10 -- Batch 2/ 94, validation loss 0.386311411857605\n",
      "Epoch 10 -- Batch 3/ 94, validation loss 0.4113761782646179\n",
      "Epoch 10 -- Batch 4/ 94, validation loss 0.3993632197380066\n",
      "Epoch 10 -- Batch 5/ 94, validation loss 0.3909606337547302\n",
      "Epoch 10 -- Batch 6/ 94, validation loss 0.40305018424987793\n",
      "Epoch 10 -- Batch 7/ 94, validation loss 0.401406854391098\n",
      "Epoch 10 -- Batch 8/ 94, validation loss 0.38719239830970764\n",
      "Epoch 10 -- Batch 9/ 94, validation loss 0.40778443217277527\n",
      "Epoch 10 -- Batch 10/ 94, validation loss 0.3823889493942261\n",
      "Epoch 10 -- Batch 11/ 94, validation loss 0.3926425278186798\n",
      "Epoch 10 -- Batch 12/ 94, validation loss 0.38725709915161133\n",
      "Epoch 10 -- Batch 13/ 94, validation loss 0.39749622344970703\n",
      "Epoch 10 -- Batch 14/ 94, validation loss 0.4034215211868286\n",
      "Epoch 10 -- Batch 15/ 94, validation loss 0.39511802792549133\n",
      "Epoch 10 -- Batch 16/ 94, validation loss 0.38407102227211\n",
      "Epoch 10 -- Batch 17/ 94, validation loss 0.40451133251190186\n",
      "Epoch 10 -- Batch 18/ 94, validation loss 0.40274694561958313\n",
      "Epoch 10 -- Batch 19/ 94, validation loss 0.3934914767742157\n",
      "Epoch 10 -- Batch 20/ 94, validation loss 0.39200690388679504\n",
      "Epoch 10 -- Batch 21/ 94, validation loss 0.3880147933959961\n",
      "Epoch 10 -- Batch 22/ 94, validation loss 0.41244766116142273\n",
      "Epoch 10 -- Batch 23/ 94, validation loss 0.3963940739631653\n",
      "Epoch 10 -- Batch 24/ 94, validation loss 0.3967157304286957\n",
      "Epoch 10 -- Batch 25/ 94, validation loss 0.40477731823921204\n",
      "Epoch 10 -- Batch 26/ 94, validation loss 0.40684589743614197\n",
      "Epoch 10 -- Batch 27/ 94, validation loss 0.3986007571220398\n",
      "Epoch 10 -- Batch 28/ 94, validation loss 0.38758546113967896\n",
      "Epoch 10 -- Batch 29/ 94, validation loss 0.4002259373664856\n",
      "Epoch 10 -- Batch 30/ 94, validation loss 0.4032033681869507\n",
      "Epoch 10 -- Batch 31/ 94, validation loss 0.3978741466999054\n",
      "Epoch 10 -- Batch 32/ 94, validation loss 0.3937515318393707\n",
      "Epoch 10 -- Batch 33/ 94, validation loss 0.40656086802482605\n",
      "Epoch 10 -- Batch 34/ 94, validation loss 0.4129371643066406\n",
      "Epoch 10 -- Batch 35/ 94, validation loss 0.3886939287185669\n",
      "Epoch 10 -- Batch 36/ 94, validation loss 0.3856312036514282\n",
      "Epoch 10 -- Batch 37/ 94, validation loss 0.3851238787174225\n",
      "Epoch 10 -- Batch 38/ 94, validation loss 0.40585237741470337\n",
      "Epoch 10 -- Batch 39/ 94, validation loss 0.41631796956062317\n",
      "Epoch 10 -- Batch 40/ 94, validation loss 0.3896879255771637\n",
      "Epoch 10 -- Batch 41/ 94, validation loss 0.39225587248802185\n",
      "Epoch 10 -- Batch 42/ 94, validation loss 0.38806942105293274\n",
      "Epoch 10 -- Batch 43/ 94, validation loss 0.39181187748908997\n",
      "Epoch 10 -- Batch 44/ 94, validation loss 0.41238659620285034\n",
      "Epoch 10 -- Batch 45/ 94, validation loss 0.39970138669013977\n",
      "Epoch 10 -- Batch 46/ 94, validation loss 0.4058377146720886\n",
      "Epoch 10 -- Batch 47/ 94, validation loss 0.39779216051101685\n",
      "Epoch 10 -- Batch 48/ 94, validation loss 0.40927553176879883\n",
      "Epoch 10 -- Batch 49/ 94, validation loss 0.41161468625068665\n",
      "Epoch 10 -- Batch 50/ 94, validation loss 0.3927074074745178\n",
      "Epoch 10 -- Batch 51/ 94, validation loss 0.40255898237228394\n",
      "Epoch 10 -- Batch 52/ 94, validation loss 0.3854498267173767\n",
      "Epoch 10 -- Batch 53/ 94, validation loss 0.4022042453289032\n",
      "Epoch 10 -- Batch 54/ 94, validation loss 0.39182448387145996\n",
      "Epoch 10 -- Batch 55/ 94, validation loss 0.4119822680950165\n",
      "Epoch 10 -- Batch 56/ 94, validation loss 0.3986944854259491\n",
      "Epoch 10 -- Batch 57/ 94, validation loss 0.40904510021209717\n",
      "Epoch 10 -- Batch 58/ 94, validation loss 0.4027169644832611\n",
      "Epoch 10 -- Batch 59/ 94, validation loss 0.39499717950820923\n",
      "Epoch 10 -- Batch 60/ 94, validation loss 0.4099131226539612\n",
      "Epoch 10 -- Batch 61/ 94, validation loss 0.39426663517951965\n",
      "Epoch 10 -- Batch 62/ 94, validation loss 0.4117506742477417\n",
      "Epoch 10 -- Batch 63/ 94, validation loss 0.4118993282318115\n",
      "Epoch 10 -- Batch 64/ 94, validation loss 0.39624258875846863\n",
      "Epoch 10 -- Batch 65/ 94, validation loss 0.38729560375213623\n",
      "Epoch 10 -- Batch 66/ 94, validation loss 0.3806309103965759\n",
      "Epoch 10 -- Batch 67/ 94, validation loss 0.3753017783164978\n",
      "Epoch 10 -- Batch 68/ 94, validation loss 0.40624263882637024\n",
      "Epoch 10 -- Batch 69/ 94, validation loss 0.40066787600517273\n",
      "Epoch 10 -- Batch 70/ 94, validation loss 0.43545037508010864\n",
      "Epoch 10 -- Batch 71/ 94, validation loss 0.4108440577983856\n",
      "Epoch 10 -- Batch 72/ 94, validation loss 0.40922605991363525\n",
      "Epoch 10 -- Batch 73/ 94, validation loss 0.4204200804233551\n",
      "Epoch 10 -- Batch 74/ 94, validation loss 0.39858219027519226\n",
      "Epoch 10 -- Batch 75/ 94, validation loss 0.39328470826148987\n",
      "Epoch 10 -- Batch 76/ 94, validation loss 0.3826914131641388\n",
      "Epoch 10 -- Batch 77/ 94, validation loss 0.4105665683746338\n",
      "Epoch 10 -- Batch 78/ 94, validation loss 0.3809013366699219\n",
      "Epoch 10 -- Batch 79/ 94, validation loss 0.40318408608436584\n",
      "Epoch 10 -- Batch 80/ 94, validation loss 0.3888869881629944\n",
      "Epoch 10 -- Batch 81/ 94, validation loss 0.3849162459373474\n",
      "Epoch 10 -- Batch 82/ 94, validation loss 0.40155982971191406\n",
      "Epoch 10 -- Batch 83/ 94, validation loss 0.40491145849227905\n",
      "Epoch 10 -- Batch 84/ 94, validation loss 0.3879714608192444\n",
      "Epoch 10 -- Batch 85/ 94, validation loss 0.4019908607006073\n",
      "Epoch 10 -- Batch 86/ 94, validation loss 0.3995295763015747\n",
      "Epoch 10 -- Batch 87/ 94, validation loss 0.39847326278686523\n",
      "Epoch 10 -- Batch 88/ 94, validation loss 0.3892161548137665\n",
      "Epoch 10 -- Batch 89/ 94, validation loss 0.3944915533065796\n",
      "Epoch 10 -- Batch 90/ 94, validation loss 0.40764743089675903\n",
      "Epoch 10 -- Batch 91/ 94, validation loss 0.37450072169303894\n",
      "Epoch 10 -- Batch 92/ 94, validation loss 0.39989322423934937\n",
      "Epoch 10 -- Batch 93/ 94, validation loss 0.4051787853240967\n",
      "Epoch 10 -- Batch 94/ 94, validation loss 0.4102345108985901\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10 loss: Training 0.40484634041786194, Validation 0.4102345108985901\n",
      "----------------------------------------------------------------------\n",
      "Epoch 11/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 20 21 22 23 24\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 9 10 11\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 13 14 15\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2SCC3(O)CCN2Cc2ccc(Cl)cc2)cc1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 11 12 22 23 33 34 35\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2CCc3nc4ccccc4c(C(=O)N3CCN(C)CC4)c23)CC1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 3 4 5 9 13 14\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 16 17 18 19 20 23 24 25\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'C[C@]12CCC3C(CC[C@H]4C[C@@H](O)CC[C@]44C)C1C[C@@H](O5)[C@H]2O'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'N#CC1=C(N)Oc2c1c(=O)oc3ccccc3c21'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 3 4 5 15 16 17 18 19 20 21 22\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 4 5 17 18 22 23 24 25 26 27 28\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 5\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 28\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 31\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 3 4 6\n",
      "[19:05:27] SMILES Parse Error: extra open parentheses for input: 'O=C1CCCC2=C1C(c1ccccc1O)C1=C(CCCC1'\n",
      "[19:05:27] SMILES Parse Error: duplicated ring closure 3 bonds atom 20 to itself for input: 'CN1C(=O)c2cc(NC(=O)NC3CCCc3ccccc33)ccc2OC1F'\n",
      "[19:05:27] SMILES Parse Error: extra close parentheses while parsing: CC(C)CC1=C(C(=O)OCC)C2(c3ccc(OC)cc3)CCN(CC(=O)Nc3cccc(Cl)c3)C2=O)c1OC\n",
      "[19:05:27] SMILES Parse Error: Failed parsing SMILES 'CC(C)CC1=C(C(=O)OCC)C2(c3ccc(OC)cc3)CCN(CC(=O)Nc3cccc(Cl)c3)C2=O)c1OC' for input: 'CC(C)CC1=C(C(=O)OCC)C2(c3ccc(OC)cc3)CCN(CC(=O)Nc3cccc(Cl)c3)C2=O)c1OC'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 1 2 6\n",
      "[19:05:27] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 23 24 31\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2c(C)c[n+]3c4N=Nc4ncnc4cc23)cc1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 21\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'c1ccc(-c2cc(C34CCCNCC4=O)n[nH]2)cc1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'O=C1CCC2(C(=O)O)Cc3c(ccc3ccccc33)N1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CN(C)S(=O)(=O)c1ccc(CN(c2ccc(NC(=O)c3ccccc3Cl)c(Cl)c2)C3CC2)cc1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'O=c1sc2c(s1S(=O)(=O)c1ccccc1)CCN1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 19 21 22 23\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CC(C)(OC=O)C1C2C=CC3(CN(c4cccc5c(c4ccccc5)C3)C(=O)C13)O2'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 15 17\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 18 19 20 21 22 23 24 25 26 27 28\n",
      "[19:05:27] SMILES Parse Error: extra close parentheses while parsing: Cn1c2c(c(=O)n(C)c1=O)C(c1ccc(Cl)c(Cl)c1)C1=C(N2)c3ccccc3C1=O)C2\n",
      "[19:05:27] SMILES Parse Error: Failed parsing SMILES 'Cn1c2c(c(=O)n(C)c1=O)C(c1ccc(Cl)c(Cl)c1)C1=C(N2)c3ccccc3C1=O)C2' for input: 'Cn1c2c(c(=O)n(C)c1=O)C(c1ccc(Cl)c(Cl)c1)C1=C(N2)c3ccccc3C1=O)C2'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 1 2 6 7 17 18 19\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 29 31\n",
      "[19:05:27] SMILES Parse Error: extra open parentheses for input: 'O=C(O)c1ccc(CN2CCOc3ccc(CN4CCN(C4CCCCC5)cc4C3)cc2C1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1ccc2c(c1)C(=O)N(c1ccc(OC(C)=O)cc1)C1=O'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CCn1c(C2CCN(CC3CC=O)CC3)n(C)c2ccccc21'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)[C@H]2CN(C(=O)c3ccc(F)cc3)CCN2C(=O)[C@]12CCCCC1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 6 7 8 19 20 21\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CCn1c2ccccc2c2nnc(SCC(=O)N3CCCC4=O)nc21'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CC1(C)C2CC[C@@]1(CS(=O)(=O)NCCC(F)(F)F)C1(C)Oc1ccccc12'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 5 6 7 10 12\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CN2CC3(CN(C)C3)c3c([nH]c4cc(OC)ccc34)C23)c1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1=C(C)N=c2s/c(=C\\c3ccsc3)c(=O)n2-c1ccc(Cl)cc1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'O=C(c1cccc(CN2CCC3(CC2)NCCN2C(=O)CCCCc2ccccc2)c1)NC(=O)c1ccccc1'\n",
      "[19:05:27] SMILES Parse Error: syntax error while parsing: COCCCn1c(C)c(C)n2c3c(=O)n(CCCN4CCCCC4=)c(=O)n(C)c3nc12\n",
      "[19:05:27] SMILES Parse Error: Failed parsing SMILES 'COCCCn1c(C)c(C)n2c3c(=O)n(CCCN4CCCCC4=)c(=O)n(C)c3nc12' for input: 'COCCCn1c(C)c(C)n2c3c(=O)n(CCCN4CCCCC4=)c(=O)n(C)c3nc12'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'Cc1c(C(=O)O)sn2Cc1ccccc1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 17 18 31 32\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)n(N(CCC(=O)NCCc2ccc(S(N)(=O)=O)cc3)c2=O)c1C#N'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1=C(C(=O)OCC)SC(=C2C(=S)C(C)(C)N(C(=O)c3ccccc4)c3ccccc32)S1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 6 7 10 12 30\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 15 16 17 18 19\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'Nc1ncccc1COc1ccc(C2N3CC4CC(C3)CC3C(=O)N4)cc1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)nnn1Cc1ccccc1OC'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CCCCNc1ccc(C)cc1C1CN2CCc3c(F)cccc31'\n",
      "[19:05:27] SMILES Parse Error: extra close parentheses while parsing: Cc1nc(C)c(C(=O)N2CCN3C(C(=O)Nc4ccc(C)cc4C)CC3)c(=O)[nH]1)c1ccccc1\n",
      "[19:05:27] SMILES Parse Error: Failed parsing SMILES 'Cc1nc(C)c(C(=O)N2CCN3C(C(=O)Nc4ccc(C)cc4C)CC3)c(=O)[nH]1)c1ccccc1' for input: 'Cc1nc(C)c(C(=O)N2CCN3C(C(=O)Nc4ccc(C)cc4C)CC3)c(=O)[nH]1)c1ccccc1'\n",
      "[19:05:27] SMILES Parse Error: extra close parentheses while parsing: CCOc1ccc(N2C(=O)C3C4c5ccccc5C(C(=O)N5CC(C)C)(C(=O)OC(C)C)C4C3=O)c2)cc1\n",
      "[19:05:27] SMILES Parse Error: Failed parsing SMILES 'CCOc1ccc(N2C(=O)C3C4c5ccccc5C(C(=O)N5CC(C)C)(C(=O)OC(C)C)C4C3=O)c2)cc1' for input: 'CCOc1ccc(N2C(=O)C3C4c5ccccc5C(C(=O)N5CC(C)C)(C(=O)OC(C)C)C4C3=O)c2)cc1'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COC1CN(C(=O)CCCl)sc2C1c1ccc(F)cc1'\n",
      "[19:05:27] SMILES Parse Error: syntax error while parsing: C=CCn1c(=O)c(C(=O)NCc2ccc3c(c2)OCO3)cn2c3c(sc2=)SCC3\n",
      "[19:05:27] SMILES Parse Error: Failed parsing SMILES 'C=CCn1c(=O)c(C(=O)NCc2ccc3c(c2)OCO3)cn2c3c(sc2=)SCC3' for input: 'C=CCn1c(=O)c(C(=O)NCc2ccc3c(c2)OCO3)cn2c3c(sc2=)SCC3'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2CC(c3cccs3)=NN3C(=O)CCC2=O)cc1'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 20 21 23 24\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'O=C1Nc2ccc(N3C(=O)CCCC3C3=O)cc2C1=O'\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CN(C)CCCN(C(=O)c1cccc2ccccc12)c1nc2nc3c(c(=O)[nH]1)CCC(C)C3'\n",
      "[19:05:27] SMILES Parse Error: extra open parentheses for input: 'CCN1C(=O)CC(CC(=O)N2CCN(C(C(=O)Nc3ccc(F)cc3)CC2)c2ccccc21'\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 7 25\n",
      "[19:05:27] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:05:27] Can't kekulize mol.  Unkekulized atoms: 3 12 13 14 15 16 17\n",
      "[19:05:27] SMILES Parse Error: unclosed ring for input: 'CC1c2cccn2CCN1CC(=O)N(C#Cc1ccccc1)C1(C)C'\n",
      "[19:05:28] SMILES Parse Error: unclosed ring for input: 'COc1cc(OC)cc(C(=O)N2C[C@H]3C[C@@H](C2)c3ccc(-c4ccccc4F)cc3[C@H]2CO)c1'\n",
      "[19:05:28] SMILES Parse Error: unclosed ring for input: 'CC(C)C1C2NC(=S)N(c3ccccc3)C1NC(=O)Cc1ccccc1'\n",
      "[19:05:28] Can't kekulize mol.  Unkekulized atoms: 1 3 4 5 23 24 25\n",
      "[19:05:28] Can't kekulize mol.  Unkekulized atoms: 2 4 5 6 7 8 9 10 11 12\n",
      "[19:05:28] Can't kekulize mol.  Unkekulized atoms: 4 11 12 13 14 15 16\n",
      "[19:05:28] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 25\n",
      "[19:05:28] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 -- Batch 1/ 842, training loss 0.386670857667923\n",
      "Epoch 11 -- Batch 2/ 842, training loss 0.3914146423339844\n",
      "Epoch 11 -- Batch 3/ 842, training loss 0.3789967894554138\n",
      "Epoch 11 -- Batch 4/ 842, training loss 0.37827327847480774\n",
      "Epoch 11 -- Batch 5/ 842, training loss 0.39302995800971985\n",
      "Epoch 11 -- Batch 6/ 842, training loss 0.37669143080711365\n",
      "Epoch 11 -- Batch 7/ 842, training loss 0.40123122930526733\n",
      "Epoch 11 -- Batch 8/ 842, training loss 0.3964408040046692\n",
      "Epoch 11 -- Batch 9/ 842, training loss 0.39984405040740967\n",
      "Epoch 11 -- Batch 10/ 842, training loss 0.3935472071170807\n",
      "Epoch 11 -- Batch 11/ 842, training loss 0.3677078187465668\n",
      "Epoch 11 -- Batch 12/ 842, training loss 0.3954195976257324\n",
      "Epoch 11 -- Batch 13/ 842, training loss 0.3994562327861786\n",
      "Epoch 11 -- Batch 14/ 842, training loss 0.39097753167152405\n",
      "Epoch 11 -- Batch 15/ 842, training loss 0.4138634502887726\n",
      "Epoch 11 -- Batch 16/ 842, training loss 0.39848795533180237\n",
      "Epoch 11 -- Batch 17/ 842, training loss 0.38917866349220276\n",
      "Epoch 11 -- Batch 18/ 842, training loss 0.3901419937610626\n",
      "Epoch 11 -- Batch 19/ 842, training loss 0.3916662037372589\n",
      "Epoch 11 -- Batch 20/ 842, training loss 0.3941923975944519\n",
      "Epoch 11 -- Batch 21/ 842, training loss 0.3795046806335449\n",
      "Epoch 11 -- Batch 22/ 842, training loss 0.4063294231891632\n",
      "Epoch 11 -- Batch 23/ 842, training loss 0.38678428530693054\n",
      "Epoch 11 -- Batch 24/ 842, training loss 0.3855888247489929\n",
      "Epoch 11 -- Batch 25/ 842, training loss 0.39171525835990906\n",
      "Epoch 11 -- Batch 26/ 842, training loss 0.413783460855484\n",
      "Epoch 11 -- Batch 27/ 842, training loss 0.3844994306564331\n",
      "Epoch 11 -- Batch 28/ 842, training loss 0.3760455250740051\n",
      "Epoch 11 -- Batch 29/ 842, training loss 0.37985318899154663\n",
      "Epoch 11 -- Batch 30/ 842, training loss 0.38957715034484863\n",
      "Epoch 11 -- Batch 31/ 842, training loss 0.4034007489681244\n",
      "Epoch 11 -- Batch 32/ 842, training loss 0.375564306974411\n",
      "Epoch 11 -- Batch 33/ 842, training loss 0.3992752730846405\n",
      "Epoch 11 -- Batch 34/ 842, training loss 0.3738156259059906\n",
      "Epoch 11 -- Batch 35/ 842, training loss 0.3884410560131073\n",
      "Epoch 11 -- Batch 36/ 842, training loss 0.3941432237625122\n",
      "Epoch 11 -- Batch 37/ 842, training loss 0.3749157786369324\n",
      "Epoch 11 -- Batch 38/ 842, training loss 0.3918840289115906\n",
      "Epoch 11 -- Batch 39/ 842, training loss 0.39973652362823486\n",
      "Epoch 11 -- Batch 40/ 842, training loss 0.39043447375297546\n",
      "Epoch 11 -- Batch 41/ 842, training loss 0.3920154869556427\n",
      "Epoch 11 -- Batch 42/ 842, training loss 0.3901073634624481\n",
      "Epoch 11 -- Batch 43/ 842, training loss 0.4006110727787018\n",
      "Epoch 11 -- Batch 44/ 842, training loss 0.3889942169189453\n",
      "Epoch 11 -- Batch 45/ 842, training loss 0.3868654668331146\n",
      "Epoch 11 -- Batch 46/ 842, training loss 0.4016166925430298\n",
      "Epoch 11 -- Batch 47/ 842, training loss 0.3905276656150818\n",
      "Epoch 11 -- Batch 48/ 842, training loss 0.3863219618797302\n",
      "Epoch 11 -- Batch 49/ 842, training loss 0.38338595628738403\n",
      "Epoch 11 -- Batch 50/ 842, training loss 0.37884777784347534\n",
      "Epoch 11 -- Batch 51/ 842, training loss 0.3714388906955719\n",
      "Epoch 11 -- Batch 52/ 842, training loss 0.39181631803512573\n",
      "Epoch 11 -- Batch 53/ 842, training loss 0.39186984300613403\n",
      "Epoch 11 -- Batch 54/ 842, training loss 0.37901774048805237\n",
      "Epoch 11 -- Batch 55/ 842, training loss 0.3955000042915344\n",
      "Epoch 11 -- Batch 56/ 842, training loss 0.3783309757709503\n",
      "Epoch 11 -- Batch 57/ 842, training loss 0.3880211114883423\n",
      "Epoch 11 -- Batch 58/ 842, training loss 0.3916454613208771\n",
      "Epoch 11 -- Batch 59/ 842, training loss 0.3897605240345001\n",
      "Epoch 11 -- Batch 60/ 842, training loss 0.3810652494430542\n",
      "Epoch 11 -- Batch 61/ 842, training loss 0.3776099383831024\n",
      "Epoch 11 -- Batch 62/ 842, training loss 0.39190563559532166\n",
      "Epoch 11 -- Batch 63/ 842, training loss 0.38593822717666626\n",
      "Epoch 11 -- Batch 64/ 842, training loss 0.3926311135292053\n",
      "Epoch 11 -- Batch 65/ 842, training loss 0.3815988004207611\n",
      "Epoch 11 -- Batch 66/ 842, training loss 0.38460278511047363\n",
      "Epoch 11 -- Batch 67/ 842, training loss 0.3826506435871124\n",
      "Epoch 11 -- Batch 68/ 842, training loss 0.3932141661643982\n",
      "Epoch 11 -- Batch 69/ 842, training loss 0.40257349610328674\n",
      "Epoch 11 -- Batch 70/ 842, training loss 0.39438506960868835\n",
      "Epoch 11 -- Batch 71/ 842, training loss 0.38210779428482056\n",
      "Epoch 11 -- Batch 72/ 842, training loss 0.39444369077682495\n",
      "Epoch 11 -- Batch 73/ 842, training loss 0.3851035237312317\n",
      "Epoch 11 -- Batch 74/ 842, training loss 0.38565146923065186\n",
      "Epoch 11 -- Batch 75/ 842, training loss 0.3941861093044281\n",
      "Epoch 11 -- Batch 76/ 842, training loss 0.377485066652298\n",
      "Epoch 11 -- Batch 77/ 842, training loss 0.3824675977230072\n",
      "Epoch 11 -- Batch 78/ 842, training loss 0.3937198221683502\n",
      "Epoch 11 -- Batch 79/ 842, training loss 0.40275368094444275\n",
      "Epoch 11 -- Batch 80/ 842, training loss 0.3994722068309784\n",
      "Epoch 11 -- Batch 81/ 842, training loss 0.40620943903923035\n",
      "Epoch 11 -- Batch 82/ 842, training loss 0.38705623149871826\n",
      "Epoch 11 -- Batch 83/ 842, training loss 0.38897785544395447\n",
      "Epoch 11 -- Batch 84/ 842, training loss 0.3991174101829529\n",
      "Epoch 11 -- Batch 85/ 842, training loss 0.4031352400779724\n",
      "Epoch 11 -- Batch 86/ 842, training loss 0.39699023962020874\n",
      "Epoch 11 -- Batch 87/ 842, training loss 0.38525390625\n",
      "Epoch 11 -- Batch 88/ 842, training loss 0.388406366109848\n",
      "Epoch 11 -- Batch 89/ 842, training loss 0.3956855833530426\n",
      "Epoch 11 -- Batch 90/ 842, training loss 0.4059181213378906\n",
      "Epoch 11 -- Batch 91/ 842, training loss 0.3935294449329376\n",
      "Epoch 11 -- Batch 92/ 842, training loss 0.39676326513290405\n",
      "Epoch 11 -- Batch 93/ 842, training loss 0.39539098739624023\n",
      "Epoch 11 -- Batch 94/ 842, training loss 0.39861443638801575\n",
      "Epoch 11 -- Batch 95/ 842, training loss 0.39401915669441223\n",
      "Epoch 11 -- Batch 96/ 842, training loss 0.3948574364185333\n",
      "Epoch 11 -- Batch 97/ 842, training loss 0.3970252275466919\n",
      "Epoch 11 -- Batch 98/ 842, training loss 0.38657423853874207\n",
      "Epoch 11 -- Batch 99/ 842, training loss 0.3867461383342743\n",
      "Epoch 11 -- Batch 100/ 842, training loss 0.38439005613327026\n",
      "Epoch 11 -- Batch 101/ 842, training loss 0.3976100981235504\n",
      "Epoch 11 -- Batch 102/ 842, training loss 0.3881641924381256\n",
      "Epoch 11 -- Batch 103/ 842, training loss 0.39159202575683594\n",
      "Epoch 11 -- Batch 104/ 842, training loss 0.4030775725841522\n",
      "Epoch 11 -- Batch 105/ 842, training loss 0.40194037556648254\n",
      "Epoch 11 -- Batch 106/ 842, training loss 0.40631815791130066\n",
      "Epoch 11 -- Batch 107/ 842, training loss 0.3967965245246887\n",
      "Epoch 11 -- Batch 108/ 842, training loss 0.4010656177997589\n",
      "Epoch 11 -- Batch 109/ 842, training loss 0.3839872181415558\n",
      "Epoch 11 -- Batch 110/ 842, training loss 0.3897046446800232\n",
      "Epoch 11 -- Batch 111/ 842, training loss 0.38545355200767517\n",
      "Epoch 11 -- Batch 112/ 842, training loss 0.38652172684669495\n",
      "Epoch 11 -- Batch 113/ 842, training loss 0.4047204256057739\n",
      "Epoch 11 -- Batch 114/ 842, training loss 0.39259201288223267\n",
      "Epoch 11 -- Batch 115/ 842, training loss 0.4118747115135193\n",
      "Epoch 11 -- Batch 116/ 842, training loss 0.38312670588493347\n",
      "Epoch 11 -- Batch 117/ 842, training loss 0.4037688374519348\n",
      "Epoch 11 -- Batch 118/ 842, training loss 0.38679805397987366\n",
      "Epoch 11 -- Batch 119/ 842, training loss 0.3809237778186798\n",
      "Epoch 11 -- Batch 120/ 842, training loss 0.3939912021160126\n",
      "Epoch 11 -- Batch 121/ 842, training loss 0.3954222500324249\n",
      "Epoch 11 -- Batch 122/ 842, training loss 0.3991629183292389\n",
      "Epoch 11 -- Batch 123/ 842, training loss 0.40431705117225647\n",
      "Epoch 11 -- Batch 124/ 842, training loss 0.4078604578971863\n",
      "Epoch 11 -- Batch 125/ 842, training loss 0.4008512794971466\n",
      "Epoch 11 -- Batch 126/ 842, training loss 0.4004538953304291\n",
      "Epoch 11 -- Batch 127/ 842, training loss 0.39049819111824036\n",
      "Epoch 11 -- Batch 128/ 842, training loss 0.4030173122882843\n",
      "Epoch 11 -- Batch 129/ 842, training loss 0.39061400294303894\n",
      "Epoch 11 -- Batch 130/ 842, training loss 0.389778196811676\n",
      "Epoch 11 -- Batch 131/ 842, training loss 0.39780157804489136\n",
      "Epoch 11 -- Batch 132/ 842, training loss 0.38950052857398987\n",
      "Epoch 11 -- Batch 133/ 842, training loss 0.39394059777259827\n",
      "Epoch 11 -- Batch 134/ 842, training loss 0.3881102502346039\n",
      "Epoch 11 -- Batch 135/ 842, training loss 0.39220526814460754\n",
      "Epoch 11 -- Batch 136/ 842, training loss 0.39535561203956604\n",
      "Epoch 11 -- Batch 137/ 842, training loss 0.3973217010498047\n",
      "Epoch 11 -- Batch 138/ 842, training loss 0.39511632919311523\n",
      "Epoch 11 -- Batch 139/ 842, training loss 0.4005163311958313\n",
      "Epoch 11 -- Batch 140/ 842, training loss 0.3924542963504791\n",
      "Epoch 11 -- Batch 141/ 842, training loss 0.37866663932800293\n",
      "Epoch 11 -- Batch 142/ 842, training loss 0.3673662543296814\n",
      "Epoch 11 -- Batch 143/ 842, training loss 0.38362035155296326\n",
      "Epoch 11 -- Batch 144/ 842, training loss 0.37669041752815247\n",
      "Epoch 11 -- Batch 145/ 842, training loss 0.38123446702957153\n",
      "Epoch 11 -- Batch 146/ 842, training loss 0.4115581214427948\n",
      "Epoch 11 -- Batch 147/ 842, training loss 0.3903866410255432\n",
      "Epoch 11 -- Batch 148/ 842, training loss 0.39663243293762207\n",
      "Epoch 11 -- Batch 149/ 842, training loss 0.3933654725551605\n",
      "Epoch 11 -- Batch 150/ 842, training loss 0.38857585191726685\n",
      "Epoch 11 -- Batch 151/ 842, training loss 0.38428565859794617\n",
      "Epoch 11 -- Batch 152/ 842, training loss 0.39952775835990906\n",
      "Epoch 11 -- Batch 153/ 842, training loss 0.3815268576145172\n",
      "Epoch 11 -- Batch 154/ 842, training loss 0.3976730704307556\n",
      "Epoch 11 -- Batch 155/ 842, training loss 0.38540419936180115\n",
      "Epoch 11 -- Batch 156/ 842, training loss 0.408363938331604\n",
      "Epoch 11 -- Batch 157/ 842, training loss 0.3974515199661255\n",
      "Epoch 11 -- Batch 158/ 842, training loss 0.39174360036849976\n",
      "Epoch 11 -- Batch 159/ 842, training loss 0.38872092962265015\n",
      "Epoch 11 -- Batch 160/ 842, training loss 0.40326833724975586\n",
      "Epoch 11 -- Batch 161/ 842, training loss 0.39159831404685974\n",
      "Epoch 11 -- Batch 162/ 842, training loss 0.39248335361480713\n",
      "Epoch 11 -- Batch 163/ 842, training loss 0.3969341814517975\n",
      "Epoch 11 -- Batch 164/ 842, training loss 0.3893527686595917\n",
      "Epoch 11 -- Batch 165/ 842, training loss 0.39051344990730286\n",
      "Epoch 11 -- Batch 166/ 842, training loss 0.3879295885562897\n",
      "Epoch 11 -- Batch 167/ 842, training loss 0.3781116008758545\n",
      "Epoch 11 -- Batch 168/ 842, training loss 0.3925151228904724\n",
      "Epoch 11 -- Batch 169/ 842, training loss 0.3983929753303528\n",
      "Epoch 11 -- Batch 170/ 842, training loss 0.4015893340110779\n",
      "Epoch 11 -- Batch 171/ 842, training loss 0.38342735171318054\n",
      "Epoch 11 -- Batch 172/ 842, training loss 0.39116016030311584\n",
      "Epoch 11 -- Batch 173/ 842, training loss 0.3881530165672302\n",
      "Epoch 11 -- Batch 174/ 842, training loss 0.3840816617012024\n",
      "Epoch 11 -- Batch 175/ 842, training loss 0.3835289180278778\n",
      "Epoch 11 -- Batch 176/ 842, training loss 0.3975558280944824\n",
      "Epoch 11 -- Batch 177/ 842, training loss 0.37419530749320984\n",
      "Epoch 11 -- Batch 178/ 842, training loss 0.38132956624031067\n",
      "Epoch 11 -- Batch 179/ 842, training loss 0.3767412602901459\n",
      "Epoch 11 -- Batch 180/ 842, training loss 0.4026978611946106\n",
      "Epoch 11 -- Batch 181/ 842, training loss 0.3988756835460663\n",
      "Epoch 11 -- Batch 182/ 842, training loss 0.40083593130111694\n",
      "Epoch 11 -- Batch 183/ 842, training loss 0.3807469606399536\n",
      "Epoch 11 -- Batch 184/ 842, training loss 0.3952024579048157\n",
      "Epoch 11 -- Batch 185/ 842, training loss 0.39616653323173523\n",
      "Epoch 11 -- Batch 186/ 842, training loss 0.3751310110092163\n",
      "Epoch 11 -- Batch 187/ 842, training loss 0.3815046548843384\n",
      "Epoch 11 -- Batch 188/ 842, training loss 0.39493048191070557\n",
      "Epoch 11 -- Batch 189/ 842, training loss 0.40273523330688477\n",
      "Epoch 11 -- Batch 190/ 842, training loss 0.38248780369758606\n",
      "Epoch 11 -- Batch 191/ 842, training loss 0.3927183747291565\n",
      "Epoch 11 -- Batch 192/ 842, training loss 0.39130961894989014\n",
      "Epoch 11 -- Batch 193/ 842, training loss 0.39464274048805237\n",
      "Epoch 11 -- Batch 194/ 842, training loss 0.38700684905052185\n",
      "Epoch 11 -- Batch 195/ 842, training loss 0.3893655240535736\n",
      "Epoch 11 -- Batch 196/ 842, training loss 0.3962332010269165\n",
      "Epoch 11 -- Batch 197/ 842, training loss 0.4003679156303406\n",
      "Epoch 11 -- Batch 198/ 842, training loss 0.39849111437797546\n",
      "Epoch 11 -- Batch 199/ 842, training loss 0.37929072976112366\n",
      "Epoch 11 -- Batch 200/ 842, training loss 0.40347355604171753\n",
      "Epoch 11 -- Batch 201/ 842, training loss 0.3957829475402832\n",
      "Epoch 11 -- Batch 202/ 842, training loss 0.393825888633728\n",
      "Epoch 11 -- Batch 203/ 842, training loss 0.38516825437545776\n",
      "Epoch 11 -- Batch 204/ 842, training loss 0.4188726842403412\n",
      "Epoch 11 -- Batch 205/ 842, training loss 0.41425180435180664\n",
      "Epoch 11 -- Batch 206/ 842, training loss 0.3936922252178192\n",
      "Epoch 11 -- Batch 207/ 842, training loss 0.38363853096961975\n",
      "Epoch 11 -- Batch 208/ 842, training loss 0.38668450713157654\n",
      "Epoch 11 -- Batch 209/ 842, training loss 0.40372228622436523\n",
      "Epoch 11 -- Batch 210/ 842, training loss 0.39719095826148987\n",
      "Epoch 11 -- Batch 211/ 842, training loss 0.37494590878486633\n",
      "Epoch 11 -- Batch 212/ 842, training loss 0.3865194022655487\n",
      "Epoch 11 -- Batch 213/ 842, training loss 0.3890637457370758\n",
      "Epoch 11 -- Batch 214/ 842, training loss 0.3871305286884308\n",
      "Epoch 11 -- Batch 215/ 842, training loss 0.3812854588031769\n",
      "Epoch 11 -- Batch 216/ 842, training loss 0.3884386718273163\n",
      "Epoch 11 -- Batch 217/ 842, training loss 0.3858078122138977\n",
      "Epoch 11 -- Batch 218/ 842, training loss 0.3871195316314697\n",
      "Epoch 11 -- Batch 219/ 842, training loss 0.3895910084247589\n",
      "Epoch 11 -- Batch 220/ 842, training loss 0.3947966694831848\n",
      "Epoch 11 -- Batch 221/ 842, training loss 0.38833463191986084\n",
      "Epoch 11 -- Batch 222/ 842, training loss 0.4061364531517029\n",
      "Epoch 11 -- Batch 223/ 842, training loss 0.3761170208454132\n",
      "Epoch 11 -- Batch 224/ 842, training loss 0.38208916783332825\n",
      "Epoch 11 -- Batch 225/ 842, training loss 0.393962025642395\n",
      "Epoch 11 -- Batch 226/ 842, training loss 0.39648956060409546\n",
      "Epoch 11 -- Batch 227/ 842, training loss 0.39502590894699097\n",
      "Epoch 11 -- Batch 228/ 842, training loss 0.40720874071121216\n",
      "Epoch 11 -- Batch 229/ 842, training loss 0.39493799209594727\n",
      "Epoch 11 -- Batch 230/ 842, training loss 0.3979083299636841\n",
      "Epoch 11 -- Batch 231/ 842, training loss 0.39784255623817444\n",
      "Epoch 11 -- Batch 232/ 842, training loss 0.4054903984069824\n",
      "Epoch 11 -- Batch 233/ 842, training loss 0.3909379243850708\n",
      "Epoch 11 -- Batch 234/ 842, training loss 0.3914705216884613\n",
      "Epoch 11 -- Batch 235/ 842, training loss 0.40007948875427246\n",
      "Epoch 11 -- Batch 236/ 842, training loss 0.3930833041667938\n",
      "Epoch 11 -- Batch 237/ 842, training loss 0.38575777411460876\n",
      "Epoch 11 -- Batch 238/ 842, training loss 0.3780527114868164\n",
      "Epoch 11 -- Batch 239/ 842, training loss 0.39111757278442383\n",
      "Epoch 11 -- Batch 240/ 842, training loss 0.3912907838821411\n",
      "Epoch 11 -- Batch 241/ 842, training loss 0.4122491180896759\n",
      "Epoch 11 -- Batch 242/ 842, training loss 0.38689011335372925\n",
      "Epoch 11 -- Batch 243/ 842, training loss 0.4010502099990845\n",
      "Epoch 11 -- Batch 244/ 842, training loss 0.41001811623573303\n",
      "Epoch 11 -- Batch 245/ 842, training loss 0.38751792907714844\n",
      "Epoch 11 -- Batch 246/ 842, training loss 0.3857380151748657\n",
      "Epoch 11 -- Batch 247/ 842, training loss 0.38034531474113464\n",
      "Epoch 11 -- Batch 248/ 842, training loss 0.3961349427700043\n",
      "Epoch 11 -- Batch 249/ 842, training loss 0.3921010196208954\n",
      "Epoch 11 -- Batch 250/ 842, training loss 0.37317705154418945\n",
      "Epoch 11 -- Batch 251/ 842, training loss 0.3934690058231354\n",
      "Epoch 11 -- Batch 252/ 842, training loss 0.40481841564178467\n",
      "Epoch 11 -- Batch 253/ 842, training loss 0.39592859148979187\n",
      "Epoch 11 -- Batch 254/ 842, training loss 0.4024696350097656\n",
      "Epoch 11 -- Batch 255/ 842, training loss 0.3873530924320221\n",
      "Epoch 11 -- Batch 256/ 842, training loss 0.3960561454296112\n",
      "Epoch 11 -- Batch 257/ 842, training loss 0.3975810408592224\n",
      "Epoch 11 -- Batch 258/ 842, training loss 0.3849569261074066\n",
      "Epoch 11 -- Batch 259/ 842, training loss 0.3807685375213623\n",
      "Epoch 11 -- Batch 260/ 842, training loss 0.40666842460632324\n",
      "Epoch 11 -- Batch 261/ 842, training loss 0.39439767599105835\n",
      "Epoch 11 -- Batch 262/ 842, training loss 0.40076348185539246\n",
      "Epoch 11 -- Batch 263/ 842, training loss 0.3983200490474701\n",
      "Epoch 11 -- Batch 264/ 842, training loss 0.3919060528278351\n",
      "Epoch 11 -- Batch 265/ 842, training loss 0.3857738673686981\n",
      "Epoch 11 -- Batch 266/ 842, training loss 0.3943381905555725\n",
      "Epoch 11 -- Batch 267/ 842, training loss 0.4054824411869049\n",
      "Epoch 11 -- Batch 268/ 842, training loss 0.38937363028526306\n",
      "Epoch 11 -- Batch 269/ 842, training loss 0.3873416781425476\n",
      "Epoch 11 -- Batch 270/ 842, training loss 0.39084580540657043\n",
      "Epoch 11 -- Batch 271/ 842, training loss 0.39739537239074707\n",
      "Epoch 11 -- Batch 272/ 842, training loss 0.3903464078903198\n",
      "Epoch 11 -- Batch 273/ 842, training loss 0.4045649468898773\n",
      "Epoch 11 -- Batch 274/ 842, training loss 0.4036064147949219\n",
      "Epoch 11 -- Batch 275/ 842, training loss 0.37951359152793884\n",
      "Epoch 11 -- Batch 276/ 842, training loss 0.4033017158508301\n",
      "Epoch 11 -- Batch 277/ 842, training loss 0.3899807631969452\n",
      "Epoch 11 -- Batch 278/ 842, training loss 0.39479517936706543\n",
      "Epoch 11 -- Batch 279/ 842, training loss 0.3828525245189667\n",
      "Epoch 11 -- Batch 280/ 842, training loss 0.37827208638191223\n",
      "Epoch 11 -- Batch 281/ 842, training loss 0.3845859467983246\n",
      "Epoch 11 -- Batch 282/ 842, training loss 0.4142110347747803\n",
      "Epoch 11 -- Batch 283/ 842, training loss 0.39495015144348145\n",
      "Epoch 11 -- Batch 284/ 842, training loss 0.3692062199115753\n",
      "Epoch 11 -- Batch 285/ 842, training loss 0.3947334587574005\n",
      "Epoch 11 -- Batch 286/ 842, training loss 0.39677509665489197\n",
      "Epoch 11 -- Batch 287/ 842, training loss 0.39311155676841736\n",
      "Epoch 11 -- Batch 288/ 842, training loss 0.40130606293678284\n",
      "Epoch 11 -- Batch 289/ 842, training loss 0.3959656059741974\n",
      "Epoch 11 -- Batch 290/ 842, training loss 0.38886183500289917\n",
      "Epoch 11 -- Batch 291/ 842, training loss 0.3881256580352783\n",
      "Epoch 11 -- Batch 292/ 842, training loss 0.37783706188201904\n",
      "Epoch 11 -- Batch 293/ 842, training loss 0.39712759852409363\n",
      "Epoch 11 -- Batch 294/ 842, training loss 0.39163658022880554\n",
      "Epoch 11 -- Batch 295/ 842, training loss 0.37806928157806396\n",
      "Epoch 11 -- Batch 296/ 842, training loss 0.39722615480422974\n",
      "Epoch 11 -- Batch 297/ 842, training loss 0.37417396903038025\n",
      "Epoch 11 -- Batch 298/ 842, training loss 0.37872666120529175\n",
      "Epoch 11 -- Batch 299/ 842, training loss 0.398898720741272\n",
      "Epoch 11 -- Batch 300/ 842, training loss 0.39372265338897705\n",
      "Epoch 11 -- Batch 301/ 842, training loss 0.41017210483551025\n",
      "Epoch 11 -- Batch 302/ 842, training loss 0.4077521562576294\n",
      "Epoch 11 -- Batch 303/ 842, training loss 0.40115785598754883\n",
      "Epoch 11 -- Batch 304/ 842, training loss 0.39330050349235535\n",
      "Epoch 11 -- Batch 305/ 842, training loss 0.40320226550102234\n",
      "Epoch 11 -- Batch 306/ 842, training loss 0.396395742893219\n",
      "Epoch 11 -- Batch 307/ 842, training loss 0.40154966711997986\n",
      "Epoch 11 -- Batch 308/ 842, training loss 0.3825606107711792\n",
      "Epoch 11 -- Batch 309/ 842, training loss 0.3779350221157074\n",
      "Epoch 11 -- Batch 310/ 842, training loss 0.39039933681488037\n",
      "Epoch 11 -- Batch 311/ 842, training loss 0.38156118988990784\n",
      "Epoch 11 -- Batch 312/ 842, training loss 0.4048526883125305\n",
      "Epoch 11 -- Batch 313/ 842, training loss 0.39624646306037903\n",
      "Epoch 11 -- Batch 314/ 842, training loss 0.39147284626960754\n",
      "Epoch 11 -- Batch 315/ 842, training loss 0.3746020495891571\n",
      "Epoch 11 -- Batch 316/ 842, training loss 0.3915833532810211\n",
      "Epoch 11 -- Batch 317/ 842, training loss 0.39192917943000793\n",
      "Epoch 11 -- Batch 318/ 842, training loss 0.39319995045661926\n",
      "Epoch 11 -- Batch 319/ 842, training loss 0.39278483390808105\n",
      "Epoch 11 -- Batch 320/ 842, training loss 0.3941817581653595\n",
      "Epoch 11 -- Batch 321/ 842, training loss 0.39437463879585266\n",
      "Epoch 11 -- Batch 322/ 842, training loss 0.39532414078712463\n",
      "Epoch 11 -- Batch 323/ 842, training loss 0.39074134826660156\n",
      "Epoch 11 -- Batch 324/ 842, training loss 0.4067234992980957\n",
      "Epoch 11 -- Batch 325/ 842, training loss 0.3925837278366089\n",
      "Epoch 11 -- Batch 326/ 842, training loss 0.373717337846756\n",
      "Epoch 11 -- Batch 327/ 842, training loss 0.39239904284477234\n",
      "Epoch 11 -- Batch 328/ 842, training loss 0.4087373614311218\n",
      "Epoch 11 -- Batch 329/ 842, training loss 0.4129984974861145\n",
      "Epoch 11 -- Batch 330/ 842, training loss 0.4110371768474579\n",
      "Epoch 11 -- Batch 331/ 842, training loss 0.400890588760376\n",
      "Epoch 11 -- Batch 332/ 842, training loss 0.3903496265411377\n",
      "Epoch 11 -- Batch 333/ 842, training loss 0.38772574067115784\n",
      "Epoch 11 -- Batch 334/ 842, training loss 0.3949268162250519\n",
      "Epoch 11 -- Batch 335/ 842, training loss 0.40717262029647827\n",
      "Epoch 11 -- Batch 336/ 842, training loss 0.3987426459789276\n",
      "Epoch 11 -- Batch 337/ 842, training loss 0.40344658493995667\n",
      "Epoch 11 -- Batch 338/ 842, training loss 0.3955423831939697\n",
      "Epoch 11 -- Batch 339/ 842, training loss 0.3931737244129181\n",
      "Epoch 11 -- Batch 340/ 842, training loss 0.3786296248435974\n",
      "Epoch 11 -- Batch 341/ 842, training loss 0.39343133568763733\n",
      "Epoch 11 -- Batch 342/ 842, training loss 0.38541024923324585\n",
      "Epoch 11 -- Batch 343/ 842, training loss 0.38585665822029114\n",
      "Epoch 11 -- Batch 344/ 842, training loss 0.4022562801837921\n",
      "Epoch 11 -- Batch 345/ 842, training loss 0.4081743359565735\n",
      "Epoch 11 -- Batch 346/ 842, training loss 0.38494259119033813\n",
      "Epoch 11 -- Batch 347/ 842, training loss 0.42183858156204224\n",
      "Epoch 11 -- Batch 348/ 842, training loss 0.40761494636535645\n",
      "Epoch 11 -- Batch 349/ 842, training loss 0.39448028802871704\n",
      "Epoch 11 -- Batch 350/ 842, training loss 0.39917075634002686\n",
      "Epoch 11 -- Batch 351/ 842, training loss 0.3870098292827606\n",
      "Epoch 11 -- Batch 352/ 842, training loss 0.38552069664001465\n",
      "Epoch 11 -- Batch 353/ 842, training loss 0.40285682678222656\n",
      "Epoch 11 -- Batch 354/ 842, training loss 0.39959949254989624\n",
      "Epoch 11 -- Batch 355/ 842, training loss 0.39819660782814026\n",
      "Epoch 11 -- Batch 356/ 842, training loss 0.4018963575363159\n",
      "Epoch 11 -- Batch 357/ 842, training loss 0.3923649191856384\n",
      "Epoch 11 -- Batch 358/ 842, training loss 0.3928144574165344\n",
      "Epoch 11 -- Batch 359/ 842, training loss 0.383448988199234\n",
      "Epoch 11 -- Batch 360/ 842, training loss 0.38057851791381836\n",
      "Epoch 11 -- Batch 361/ 842, training loss 0.38515806198120117\n",
      "Epoch 11 -- Batch 362/ 842, training loss 0.39614924788475037\n",
      "Epoch 11 -- Batch 363/ 842, training loss 0.3804789185523987\n",
      "Epoch 11 -- Batch 364/ 842, training loss 0.39975762367248535\n",
      "Epoch 11 -- Batch 365/ 842, training loss 0.38972944021224976\n",
      "Epoch 11 -- Batch 366/ 842, training loss 0.3862362205982208\n",
      "Epoch 11 -- Batch 367/ 842, training loss 0.39721882343292236\n",
      "Epoch 11 -- Batch 368/ 842, training loss 0.3919537663459778\n",
      "Epoch 11 -- Batch 369/ 842, training loss 0.39806967973709106\n",
      "Epoch 11 -- Batch 370/ 842, training loss 0.3920324742794037\n",
      "Epoch 11 -- Batch 371/ 842, training loss 0.3961407542228699\n",
      "Epoch 11 -- Batch 372/ 842, training loss 0.39229074120521545\n",
      "Epoch 11 -- Batch 373/ 842, training loss 0.3899347186088562\n",
      "Epoch 11 -- Batch 374/ 842, training loss 0.39458122849464417\n",
      "Epoch 11 -- Batch 375/ 842, training loss 0.3867836594581604\n",
      "Epoch 11 -- Batch 376/ 842, training loss 0.402652382850647\n",
      "Epoch 11 -- Batch 377/ 842, training loss 0.3892032206058502\n",
      "Epoch 11 -- Batch 378/ 842, training loss 0.3882412314414978\n",
      "Epoch 11 -- Batch 379/ 842, training loss 0.40227842330932617\n",
      "Epoch 11 -- Batch 380/ 842, training loss 0.38995805382728577\n",
      "Epoch 11 -- Batch 381/ 842, training loss 0.39613303542137146\n",
      "Epoch 11 -- Batch 382/ 842, training loss 0.3789522349834442\n",
      "Epoch 11 -- Batch 383/ 842, training loss 0.3946112096309662\n",
      "Epoch 11 -- Batch 384/ 842, training loss 0.3995998501777649\n",
      "Epoch 11 -- Batch 385/ 842, training loss 0.38220787048339844\n",
      "Epoch 11 -- Batch 386/ 842, training loss 0.38305363059043884\n",
      "Epoch 11 -- Batch 387/ 842, training loss 0.394470751285553\n",
      "Epoch 11 -- Batch 388/ 842, training loss 0.406829297542572\n",
      "Epoch 11 -- Batch 389/ 842, training loss 0.39958953857421875\n",
      "Epoch 11 -- Batch 390/ 842, training loss 0.4005473852157593\n",
      "Epoch 11 -- Batch 391/ 842, training loss 0.4033091962337494\n",
      "Epoch 11 -- Batch 392/ 842, training loss 0.3965129852294922\n",
      "Epoch 11 -- Batch 393/ 842, training loss 0.3726012110710144\n",
      "Epoch 11 -- Batch 394/ 842, training loss 0.3990458846092224\n",
      "Epoch 11 -- Batch 395/ 842, training loss 0.3916485607624054\n",
      "Epoch 11 -- Batch 396/ 842, training loss 0.3928002119064331\n",
      "Epoch 11 -- Batch 397/ 842, training loss 0.39229607582092285\n",
      "Epoch 11 -- Batch 398/ 842, training loss 0.3934682607650757\n",
      "Epoch 11 -- Batch 399/ 842, training loss 0.379883736371994\n",
      "Epoch 11 -- Batch 400/ 842, training loss 0.38779330253601074\n",
      "Epoch 11 -- Batch 401/ 842, training loss 0.3879237473011017\n",
      "Epoch 11 -- Batch 402/ 842, training loss 0.3936469256877899\n",
      "Epoch 11 -- Batch 403/ 842, training loss 0.3885829448699951\n",
      "Epoch 11 -- Batch 404/ 842, training loss 0.39872652292251587\n",
      "Epoch 11 -- Batch 405/ 842, training loss 0.4045351445674896\n",
      "Epoch 11 -- Batch 406/ 842, training loss 0.39166009426116943\n",
      "Epoch 11 -- Batch 407/ 842, training loss 0.3863293528556824\n",
      "Epoch 11 -- Batch 408/ 842, training loss 0.39612066745758057\n",
      "Epoch 11 -- Batch 409/ 842, training loss 0.3952710032463074\n",
      "Epoch 11 -- Batch 410/ 842, training loss 0.3875386118888855\n",
      "Epoch 11 -- Batch 411/ 842, training loss 0.38892173767089844\n",
      "Epoch 11 -- Batch 412/ 842, training loss 0.39898061752319336\n",
      "Epoch 11 -- Batch 413/ 842, training loss 0.3819091022014618\n",
      "Epoch 11 -- Batch 414/ 842, training loss 0.38825371861457825\n",
      "Epoch 11 -- Batch 415/ 842, training loss 0.40526464581489563\n",
      "Epoch 11 -- Batch 416/ 842, training loss 0.38609570264816284\n",
      "Epoch 11 -- Batch 417/ 842, training loss 0.401475727558136\n",
      "Epoch 11 -- Batch 418/ 842, training loss 0.41804590821266174\n",
      "Epoch 11 -- Batch 419/ 842, training loss 0.39374151825904846\n",
      "Epoch 11 -- Batch 420/ 842, training loss 0.40240076184272766\n",
      "Epoch 11 -- Batch 421/ 842, training loss 0.40385618805885315\n",
      "Epoch 11 -- Batch 422/ 842, training loss 0.3984910547733307\n",
      "Epoch 11 -- Batch 423/ 842, training loss 0.38960668444633484\n",
      "Epoch 11 -- Batch 424/ 842, training loss 0.39067980647087097\n",
      "Epoch 11 -- Batch 425/ 842, training loss 0.390431672334671\n",
      "Epoch 11 -- Batch 426/ 842, training loss 0.3904355466365814\n",
      "Epoch 11 -- Batch 427/ 842, training loss 0.38427796959877014\n",
      "Epoch 11 -- Batch 428/ 842, training loss 0.373921275138855\n",
      "Epoch 11 -- Batch 429/ 842, training loss 0.39836281538009644\n",
      "Epoch 11 -- Batch 430/ 842, training loss 0.38648226857185364\n",
      "Epoch 11 -- Batch 431/ 842, training loss 0.3851260542869568\n",
      "Epoch 11 -- Batch 432/ 842, training loss 0.39376816153526306\n",
      "Epoch 11 -- Batch 433/ 842, training loss 0.3870972692966461\n",
      "Epoch 11 -- Batch 434/ 842, training loss 0.4057162404060364\n",
      "Epoch 11 -- Batch 435/ 842, training loss 0.4049097001552582\n",
      "Epoch 11 -- Batch 436/ 842, training loss 0.38756635785102844\n",
      "Epoch 11 -- Batch 437/ 842, training loss 0.4118514657020569\n",
      "Epoch 11 -- Batch 438/ 842, training loss 0.41684651374816895\n",
      "Epoch 11 -- Batch 439/ 842, training loss 0.3862268924713135\n",
      "Epoch 11 -- Batch 440/ 842, training loss 0.39127758145332336\n",
      "Epoch 11 -- Batch 441/ 842, training loss 0.394255131483078\n",
      "Epoch 11 -- Batch 442/ 842, training loss 0.38937830924987793\n",
      "Epoch 11 -- Batch 443/ 842, training loss 0.386412650346756\n",
      "Epoch 11 -- Batch 444/ 842, training loss 0.39589864015579224\n",
      "Epoch 11 -- Batch 445/ 842, training loss 0.3940007984638214\n",
      "Epoch 11 -- Batch 446/ 842, training loss 0.39155688881874084\n",
      "Epoch 11 -- Batch 447/ 842, training loss 0.39379656314849854\n",
      "Epoch 11 -- Batch 448/ 842, training loss 0.3921475112438202\n",
      "Epoch 11 -- Batch 449/ 842, training loss 0.38480350375175476\n",
      "Epoch 11 -- Batch 450/ 842, training loss 0.38832995295524597\n",
      "Epoch 11 -- Batch 451/ 842, training loss 0.40227770805358887\n",
      "Epoch 11 -- Batch 452/ 842, training loss 0.39033862948417664\n",
      "Epoch 11 -- Batch 453/ 842, training loss 0.3986496329307556\n",
      "Epoch 11 -- Batch 454/ 842, training loss 0.38565513491630554\n",
      "Epoch 11 -- Batch 455/ 842, training loss 0.39690133929252625\n",
      "Epoch 11 -- Batch 456/ 842, training loss 0.3927314877510071\n",
      "Epoch 11 -- Batch 457/ 842, training loss 0.3982956111431122\n",
      "Epoch 11 -- Batch 458/ 842, training loss 0.3829958140850067\n",
      "Epoch 11 -- Batch 459/ 842, training loss 0.4033395051956177\n",
      "Epoch 11 -- Batch 460/ 842, training loss 0.3823763132095337\n",
      "Epoch 11 -- Batch 461/ 842, training loss 0.39776894450187683\n",
      "Epoch 11 -- Batch 462/ 842, training loss 0.4018993377685547\n",
      "Epoch 11 -- Batch 463/ 842, training loss 0.39645296335220337\n",
      "Epoch 11 -- Batch 464/ 842, training loss 0.39050108194351196\n",
      "Epoch 11 -- Batch 465/ 842, training loss 0.3990187346935272\n",
      "Epoch 11 -- Batch 466/ 842, training loss 0.382312148809433\n",
      "Epoch 11 -- Batch 467/ 842, training loss 0.40464845299720764\n",
      "Epoch 11 -- Batch 468/ 842, training loss 0.39690282940864563\n",
      "Epoch 11 -- Batch 469/ 842, training loss 0.39783138036727905\n",
      "Epoch 11 -- Batch 470/ 842, training loss 0.4009822607040405\n",
      "Epoch 11 -- Batch 471/ 842, training loss 0.39491474628448486\n",
      "Epoch 11 -- Batch 472/ 842, training loss 0.4025121331214905\n",
      "Epoch 11 -- Batch 473/ 842, training loss 0.378974586725235\n",
      "Epoch 11 -- Batch 474/ 842, training loss 0.3938281238079071\n",
      "Epoch 11 -- Batch 475/ 842, training loss 0.39529991149902344\n",
      "Epoch 11 -- Batch 476/ 842, training loss 0.3856644928455353\n",
      "Epoch 11 -- Batch 477/ 842, training loss 0.3881102204322815\n",
      "Epoch 11 -- Batch 478/ 842, training loss 0.397638738155365\n",
      "Epoch 11 -- Batch 479/ 842, training loss 0.39758095145225525\n",
      "Epoch 11 -- Batch 480/ 842, training loss 0.39654573798179626\n",
      "Epoch 11 -- Batch 481/ 842, training loss 0.37954461574554443\n",
      "Epoch 11 -- Batch 482/ 842, training loss 0.40185657143592834\n",
      "Epoch 11 -- Batch 483/ 842, training loss 0.39064815640449524\n",
      "Epoch 11 -- Batch 484/ 842, training loss 0.3940849006175995\n",
      "Epoch 11 -- Batch 485/ 842, training loss 0.4090479612350464\n",
      "Epoch 11 -- Batch 486/ 842, training loss 0.3837835192680359\n",
      "Epoch 11 -- Batch 487/ 842, training loss 0.3959670066833496\n",
      "Epoch 11 -- Batch 488/ 842, training loss 0.40167954564094543\n",
      "Epoch 11 -- Batch 489/ 842, training loss 0.3912786543369293\n",
      "Epoch 11 -- Batch 490/ 842, training loss 0.3992439806461334\n",
      "Epoch 11 -- Batch 491/ 842, training loss 0.38841870427131653\n",
      "Epoch 11 -- Batch 492/ 842, training loss 0.40414220094680786\n",
      "Epoch 11 -- Batch 493/ 842, training loss 0.38974401354789734\n",
      "Epoch 11 -- Batch 494/ 842, training loss 0.4084642827510834\n",
      "Epoch 11 -- Batch 495/ 842, training loss 0.40955695509910583\n",
      "Epoch 11 -- Batch 496/ 842, training loss 0.3880855441093445\n",
      "Epoch 11 -- Batch 497/ 842, training loss 0.3880712389945984\n",
      "Epoch 11 -- Batch 498/ 842, training loss 0.40254878997802734\n",
      "Epoch 11 -- Batch 499/ 842, training loss 0.3931984603404999\n",
      "Epoch 11 -- Batch 500/ 842, training loss 0.39588114619255066\n",
      "Epoch 11 -- Batch 501/ 842, training loss 0.3838198482990265\n",
      "Epoch 11 -- Batch 502/ 842, training loss 0.38793620467185974\n",
      "Epoch 11 -- Batch 503/ 842, training loss 0.38707977533340454\n",
      "Epoch 11 -- Batch 504/ 842, training loss 0.38503357768058777\n",
      "Epoch 11 -- Batch 505/ 842, training loss 0.3962555527687073\n",
      "Epoch 11 -- Batch 506/ 842, training loss 0.38299787044525146\n",
      "Epoch 11 -- Batch 507/ 842, training loss 0.39151665568351746\n",
      "Epoch 11 -- Batch 508/ 842, training loss 0.3788335919380188\n",
      "Epoch 11 -- Batch 509/ 842, training loss 0.39686787128448486\n",
      "Epoch 11 -- Batch 510/ 842, training loss 0.38287171721458435\n",
      "Epoch 11 -- Batch 511/ 842, training loss 0.38800331950187683\n",
      "Epoch 11 -- Batch 512/ 842, training loss 0.40029263496398926\n",
      "Epoch 11 -- Batch 513/ 842, training loss 0.3886960744857788\n",
      "Epoch 11 -- Batch 514/ 842, training loss 0.3877885341644287\n",
      "Epoch 11 -- Batch 515/ 842, training loss 0.3868810534477234\n",
      "Epoch 11 -- Batch 516/ 842, training loss 0.40877553820610046\n",
      "Epoch 11 -- Batch 517/ 842, training loss 0.3958756625652313\n",
      "Epoch 11 -- Batch 518/ 842, training loss 0.378574937582016\n",
      "Epoch 11 -- Batch 519/ 842, training loss 0.3792361319065094\n",
      "Epoch 11 -- Batch 520/ 842, training loss 0.38837730884552\n",
      "Epoch 11 -- Batch 521/ 842, training loss 0.39321863651275635\n",
      "Epoch 11 -- Batch 522/ 842, training loss 0.3896595239639282\n",
      "Epoch 11 -- Batch 523/ 842, training loss 0.3863573372364044\n",
      "Epoch 11 -- Batch 524/ 842, training loss 0.3727034330368042\n",
      "Epoch 11 -- Batch 525/ 842, training loss 0.3829079568386078\n",
      "Epoch 11 -- Batch 526/ 842, training loss 0.3931443691253662\n",
      "Epoch 11 -- Batch 527/ 842, training loss 0.37562528252601624\n",
      "Epoch 11 -- Batch 528/ 842, training loss 0.39886558055877686\n",
      "Epoch 11 -- Batch 529/ 842, training loss 0.39205798506736755\n",
      "Epoch 11 -- Batch 530/ 842, training loss 0.3875731825828552\n",
      "Epoch 11 -- Batch 531/ 842, training loss 0.3910093605518341\n",
      "Epoch 11 -- Batch 532/ 842, training loss 0.38950860500335693\n",
      "Epoch 11 -- Batch 533/ 842, training loss 0.40055418014526367\n",
      "Epoch 11 -- Batch 534/ 842, training loss 0.3952690660953522\n",
      "Epoch 11 -- Batch 535/ 842, training loss 0.3946162462234497\n",
      "Epoch 11 -- Batch 536/ 842, training loss 0.39023709297180176\n",
      "Epoch 11 -- Batch 537/ 842, training loss 0.38967227935791016\n",
      "Epoch 11 -- Batch 538/ 842, training loss 0.39088037610054016\n",
      "Epoch 11 -- Batch 539/ 842, training loss 0.3874794840812683\n",
      "Epoch 11 -- Batch 540/ 842, training loss 0.4167563319206238\n",
      "Epoch 11 -- Batch 541/ 842, training loss 0.3883488178253174\n",
      "Epoch 11 -- Batch 542/ 842, training loss 0.3791329562664032\n",
      "Epoch 11 -- Batch 543/ 842, training loss 0.390269011259079\n",
      "Epoch 11 -- Batch 544/ 842, training loss 0.397198885679245\n",
      "Epoch 11 -- Batch 545/ 842, training loss 0.38208460807800293\n",
      "Epoch 11 -- Batch 546/ 842, training loss 0.395975261926651\n",
      "Epoch 11 -- Batch 547/ 842, training loss 0.38392314314842224\n",
      "Epoch 11 -- Batch 548/ 842, training loss 0.38682109117507935\n",
      "Epoch 11 -- Batch 549/ 842, training loss 0.39149168133735657\n",
      "Epoch 11 -- Batch 550/ 842, training loss 0.3799839913845062\n",
      "Epoch 11 -- Batch 551/ 842, training loss 0.3864513635635376\n",
      "Epoch 11 -- Batch 552/ 842, training loss 0.41095930337905884\n",
      "Epoch 11 -- Batch 553/ 842, training loss 0.39157965779304504\n",
      "Epoch 11 -- Batch 554/ 842, training loss 0.38438278436660767\n",
      "Epoch 11 -- Batch 555/ 842, training loss 0.3715348243713379\n",
      "Epoch 11 -- Batch 556/ 842, training loss 0.3853304088115692\n",
      "Epoch 11 -- Batch 557/ 842, training loss 0.3853300213813782\n",
      "Epoch 11 -- Batch 558/ 842, training loss 0.3922836184501648\n",
      "Epoch 11 -- Batch 559/ 842, training loss 0.4032340943813324\n",
      "Epoch 11 -- Batch 560/ 842, training loss 0.3888867199420929\n",
      "Epoch 11 -- Batch 561/ 842, training loss 0.38828444480895996\n",
      "Epoch 11 -- Batch 562/ 842, training loss 0.4042532444000244\n",
      "Epoch 11 -- Batch 563/ 842, training loss 0.39887967705726624\n",
      "Epoch 11 -- Batch 564/ 842, training loss 0.3910183906555176\n",
      "Epoch 11 -- Batch 565/ 842, training loss 0.3933733403682709\n",
      "Epoch 11 -- Batch 566/ 842, training loss 0.39779970049858093\n",
      "Epoch 11 -- Batch 567/ 842, training loss 0.37985315918922424\n",
      "Epoch 11 -- Batch 568/ 842, training loss 0.39755305647850037\n",
      "Epoch 11 -- Batch 569/ 842, training loss 0.3831805884838104\n",
      "Epoch 11 -- Batch 570/ 842, training loss 0.3874988257884979\n",
      "Epoch 11 -- Batch 571/ 842, training loss 0.38787636160850525\n",
      "Epoch 11 -- Batch 572/ 842, training loss 0.3864668011665344\n",
      "Epoch 11 -- Batch 573/ 842, training loss 0.39358505606651306\n",
      "Epoch 11 -- Batch 574/ 842, training loss 0.3918212056159973\n",
      "Epoch 11 -- Batch 575/ 842, training loss 0.38729238510131836\n",
      "Epoch 11 -- Batch 576/ 842, training loss 0.37928688526153564\n",
      "Epoch 11 -- Batch 577/ 842, training loss 0.39726969599723816\n",
      "Epoch 11 -- Batch 578/ 842, training loss 0.3877239227294922\n",
      "Epoch 11 -- Batch 579/ 842, training loss 0.3861577808856964\n",
      "Epoch 11 -- Batch 580/ 842, training loss 0.38853538036346436\n",
      "Epoch 11 -- Batch 581/ 842, training loss 0.37803736329078674\n",
      "Epoch 11 -- Batch 582/ 842, training loss 0.4028141498565674\n",
      "Epoch 11 -- Batch 583/ 842, training loss 0.39854830503463745\n",
      "Epoch 11 -- Batch 584/ 842, training loss 0.398889422416687\n",
      "Epoch 11 -- Batch 585/ 842, training loss 0.38788577914237976\n",
      "Epoch 11 -- Batch 586/ 842, training loss 0.3903847634792328\n",
      "Epoch 11 -- Batch 587/ 842, training loss 0.384213924407959\n",
      "Epoch 11 -- Batch 588/ 842, training loss 0.3990922272205353\n",
      "Epoch 11 -- Batch 589/ 842, training loss 0.3873833417892456\n",
      "Epoch 11 -- Batch 590/ 842, training loss 0.3837305009365082\n",
      "Epoch 11 -- Batch 591/ 842, training loss 0.39214468002319336\n",
      "Epoch 11 -- Batch 592/ 842, training loss 0.3864905834197998\n",
      "Epoch 11 -- Batch 593/ 842, training loss 0.3831080496311188\n",
      "Epoch 11 -- Batch 594/ 842, training loss 0.38485217094421387\n",
      "Epoch 11 -- Batch 595/ 842, training loss 0.386168509721756\n",
      "Epoch 11 -- Batch 596/ 842, training loss 0.38596752285957336\n",
      "Epoch 11 -- Batch 597/ 842, training loss 0.38978490233421326\n",
      "Epoch 11 -- Batch 598/ 842, training loss 0.3977009057998657\n",
      "Epoch 11 -- Batch 599/ 842, training loss 0.3782781660556793\n",
      "Epoch 11 -- Batch 600/ 842, training loss 0.3786331117153168\n",
      "Epoch 11 -- Batch 601/ 842, training loss 0.4023515582084656\n",
      "Epoch 11 -- Batch 602/ 842, training loss 0.392171174287796\n",
      "Epoch 11 -- Batch 603/ 842, training loss 0.4053732752799988\n",
      "Epoch 11 -- Batch 604/ 842, training loss 0.3978075683116913\n",
      "Epoch 11 -- Batch 605/ 842, training loss 0.38946908712387085\n",
      "Epoch 11 -- Batch 606/ 842, training loss 0.3985196650028229\n",
      "Epoch 11 -- Batch 607/ 842, training loss 0.3742484748363495\n",
      "Epoch 11 -- Batch 608/ 842, training loss 0.38157734274864197\n",
      "Epoch 11 -- Batch 609/ 842, training loss 0.39886435866355896\n",
      "Epoch 11 -- Batch 610/ 842, training loss 0.39172446727752686\n",
      "Epoch 11 -- Batch 611/ 842, training loss 0.39578869938850403\n",
      "Epoch 11 -- Batch 612/ 842, training loss 0.3858895003795624\n",
      "Epoch 11 -- Batch 613/ 842, training loss 0.3891608715057373\n",
      "Epoch 11 -- Batch 614/ 842, training loss 0.4085795283317566\n",
      "Epoch 11 -- Batch 615/ 842, training loss 0.38062015175819397\n",
      "Epoch 11 -- Batch 616/ 842, training loss 0.39183250069618225\n",
      "Epoch 11 -- Batch 617/ 842, training loss 0.39861395955085754\n",
      "Epoch 11 -- Batch 618/ 842, training loss 0.3712729215621948\n",
      "Epoch 11 -- Batch 619/ 842, training loss 0.4012051820755005\n",
      "Epoch 11 -- Batch 620/ 842, training loss 0.40294215083122253\n",
      "Epoch 11 -- Batch 621/ 842, training loss 0.39799872040748596\n",
      "Epoch 11 -- Batch 622/ 842, training loss 0.3840271830558777\n",
      "Epoch 11 -- Batch 623/ 842, training loss 0.398945152759552\n",
      "Epoch 11 -- Batch 624/ 842, training loss 0.3944573700428009\n",
      "Epoch 11 -- Batch 625/ 842, training loss 0.3897245526313782\n",
      "Epoch 11 -- Batch 626/ 842, training loss 0.40217211842536926\n",
      "Epoch 11 -- Batch 627/ 842, training loss 0.39942294359207153\n",
      "Epoch 11 -- Batch 628/ 842, training loss 0.392720103263855\n",
      "Epoch 11 -- Batch 629/ 842, training loss 0.3884100615978241\n",
      "Epoch 11 -- Batch 630/ 842, training loss 0.3896927237510681\n",
      "Epoch 11 -- Batch 631/ 842, training loss 0.4015451967716217\n",
      "Epoch 11 -- Batch 632/ 842, training loss 0.381879985332489\n",
      "Epoch 11 -- Batch 633/ 842, training loss 0.38456034660339355\n",
      "Epoch 11 -- Batch 634/ 842, training loss 0.38626453280448914\n",
      "Epoch 11 -- Batch 635/ 842, training loss 0.3984040319919586\n",
      "Epoch 11 -- Batch 636/ 842, training loss 0.3935747742652893\n",
      "Epoch 11 -- Batch 637/ 842, training loss 0.39934003353118896\n",
      "Epoch 11 -- Batch 638/ 842, training loss 0.3898215889930725\n",
      "Epoch 11 -- Batch 639/ 842, training loss 0.3955218493938446\n",
      "Epoch 11 -- Batch 640/ 842, training loss 0.3963564932346344\n",
      "Epoch 11 -- Batch 641/ 842, training loss 0.3791705369949341\n",
      "Epoch 11 -- Batch 642/ 842, training loss 0.38023000955581665\n",
      "Epoch 11 -- Batch 643/ 842, training loss 0.3887365758419037\n",
      "Epoch 11 -- Batch 644/ 842, training loss 0.3959636092185974\n",
      "Epoch 11 -- Batch 645/ 842, training loss 0.4003296196460724\n",
      "Epoch 11 -- Batch 646/ 842, training loss 0.3826906979084015\n",
      "Epoch 11 -- Batch 647/ 842, training loss 0.38198405504226685\n",
      "Epoch 11 -- Batch 648/ 842, training loss 0.40587523579597473\n",
      "Epoch 11 -- Batch 649/ 842, training loss 0.390001118183136\n",
      "Epoch 11 -- Batch 650/ 842, training loss 0.4110029339790344\n",
      "Epoch 11 -- Batch 651/ 842, training loss 0.3932148516178131\n",
      "Epoch 11 -- Batch 652/ 842, training loss 0.3841415345668793\n",
      "Epoch 11 -- Batch 653/ 842, training loss 0.39023736119270325\n",
      "Epoch 11 -- Batch 654/ 842, training loss 0.40555426478385925\n",
      "Epoch 11 -- Batch 655/ 842, training loss 0.38625115156173706\n",
      "Epoch 11 -- Batch 656/ 842, training loss 0.3971470594406128\n",
      "Epoch 11 -- Batch 657/ 842, training loss 0.3922586441040039\n",
      "Epoch 11 -- Batch 658/ 842, training loss 0.3883076310157776\n",
      "Epoch 11 -- Batch 659/ 842, training loss 0.3992774784564972\n",
      "Epoch 11 -- Batch 660/ 842, training loss 0.384879469871521\n",
      "Epoch 11 -- Batch 661/ 842, training loss 0.37102940678596497\n",
      "Epoch 11 -- Batch 662/ 842, training loss 0.38706496357917786\n",
      "Epoch 11 -- Batch 663/ 842, training loss 0.37563955783843994\n",
      "Epoch 11 -- Batch 664/ 842, training loss 0.3820417821407318\n",
      "Epoch 11 -- Batch 665/ 842, training loss 0.4064921438694\n",
      "Epoch 11 -- Batch 666/ 842, training loss 0.3919305205345154\n",
      "Epoch 11 -- Batch 667/ 842, training loss 0.3817811906337738\n",
      "Epoch 11 -- Batch 668/ 842, training loss 0.39126425981521606\n",
      "Epoch 11 -- Batch 669/ 842, training loss 0.37378615140914917\n",
      "Epoch 11 -- Batch 670/ 842, training loss 0.3895910978317261\n",
      "Epoch 11 -- Batch 671/ 842, training loss 0.39259225130081177\n",
      "Epoch 11 -- Batch 672/ 842, training loss 0.37972044944763184\n",
      "Epoch 11 -- Batch 673/ 842, training loss 0.37307801842689514\n",
      "Epoch 11 -- Batch 674/ 842, training loss 0.38950851559638977\n",
      "Epoch 11 -- Batch 675/ 842, training loss 0.38257548213005066\n",
      "Epoch 11 -- Batch 676/ 842, training loss 0.3858676254749298\n",
      "Epoch 11 -- Batch 677/ 842, training loss 0.40292608737945557\n",
      "Epoch 11 -- Batch 678/ 842, training loss 0.37176647782325745\n",
      "Epoch 11 -- Batch 679/ 842, training loss 0.39321061968803406\n",
      "Epoch 11 -- Batch 680/ 842, training loss 0.3822908401489258\n",
      "Epoch 11 -- Batch 681/ 842, training loss 0.40290164947509766\n",
      "Epoch 11 -- Batch 682/ 842, training loss 0.3954290747642517\n",
      "Epoch 11 -- Batch 683/ 842, training loss 0.3950428366661072\n",
      "Epoch 11 -- Batch 684/ 842, training loss 0.4268910586833954\n",
      "Epoch 11 -- Batch 685/ 842, training loss 0.4014846384525299\n",
      "Epoch 11 -- Batch 686/ 842, training loss 0.3821735680103302\n",
      "Epoch 11 -- Batch 687/ 842, training loss 0.40854328870773315\n",
      "Epoch 11 -- Batch 688/ 842, training loss 0.3981456756591797\n",
      "Epoch 11 -- Batch 689/ 842, training loss 0.3892577886581421\n",
      "Epoch 11 -- Batch 690/ 842, training loss 0.4017145335674286\n",
      "Epoch 11 -- Batch 691/ 842, training loss 0.37597525119781494\n",
      "Epoch 11 -- Batch 692/ 842, training loss 0.4171653687953949\n",
      "Epoch 11 -- Batch 693/ 842, training loss 0.3763028681278229\n",
      "Epoch 11 -- Batch 694/ 842, training loss 0.4071061313152313\n",
      "Epoch 11 -- Batch 695/ 842, training loss 0.39081045985221863\n",
      "Epoch 11 -- Batch 696/ 842, training loss 0.41188815236091614\n",
      "Epoch 11 -- Batch 697/ 842, training loss 0.3953333795070648\n",
      "Epoch 11 -- Batch 698/ 842, training loss 0.377501517534256\n",
      "Epoch 11 -- Batch 699/ 842, training loss 0.39132222533226013\n",
      "Epoch 11 -- Batch 700/ 842, training loss 0.38807761669158936\n",
      "Epoch 11 -- Batch 701/ 842, training loss 0.3947621285915375\n",
      "Epoch 11 -- Batch 702/ 842, training loss 0.3945938050746918\n",
      "Epoch 11 -- Batch 703/ 842, training loss 0.40377169847488403\n",
      "Epoch 11 -- Batch 704/ 842, training loss 0.3888627886772156\n",
      "Epoch 11 -- Batch 705/ 842, training loss 0.381362646818161\n",
      "Epoch 11 -- Batch 706/ 842, training loss 0.38086941838264465\n",
      "Epoch 11 -- Batch 707/ 842, training loss 0.3832886815071106\n",
      "Epoch 11 -- Batch 708/ 842, training loss 0.38547244668006897\n",
      "Epoch 11 -- Batch 709/ 842, training loss 0.39567288756370544\n",
      "Epoch 11 -- Batch 710/ 842, training loss 0.39935240149497986\n",
      "Epoch 11 -- Batch 711/ 842, training loss 0.3754262328147888\n",
      "Epoch 11 -- Batch 712/ 842, training loss 0.39096689224243164\n",
      "Epoch 11 -- Batch 713/ 842, training loss 0.4062609374523163\n",
      "Epoch 11 -- Batch 714/ 842, training loss 0.39602234959602356\n",
      "Epoch 11 -- Batch 715/ 842, training loss 0.3881855309009552\n",
      "Epoch 11 -- Batch 716/ 842, training loss 0.37177520990371704\n",
      "Epoch 11 -- Batch 717/ 842, training loss 0.39165353775024414\n",
      "Epoch 11 -- Batch 718/ 842, training loss 0.39599618315696716\n",
      "Epoch 11 -- Batch 719/ 842, training loss 0.3920738101005554\n",
      "Epoch 11 -- Batch 720/ 842, training loss 0.3869008719921112\n",
      "Epoch 11 -- Batch 721/ 842, training loss 0.3892928957939148\n",
      "Epoch 11 -- Batch 722/ 842, training loss 0.38068997859954834\n",
      "Epoch 11 -- Batch 723/ 842, training loss 0.38363581895828247\n",
      "Epoch 11 -- Batch 724/ 842, training loss 0.40168628096580505\n",
      "Epoch 11 -- Batch 725/ 842, training loss 0.3774229884147644\n",
      "Epoch 11 -- Batch 726/ 842, training loss 0.3942553997039795\n",
      "Epoch 11 -- Batch 727/ 842, training loss 0.3801364302635193\n",
      "Epoch 11 -- Batch 728/ 842, training loss 0.3811342120170593\n",
      "Epoch 11 -- Batch 729/ 842, training loss 0.3815622925758362\n",
      "Epoch 11 -- Batch 730/ 842, training loss 0.38423946499824524\n",
      "Epoch 11 -- Batch 731/ 842, training loss 0.41431424021720886\n",
      "Epoch 11 -- Batch 732/ 842, training loss 0.3824535012245178\n",
      "Epoch 11 -- Batch 733/ 842, training loss 0.3849430978298187\n",
      "Epoch 11 -- Batch 734/ 842, training loss 0.40047088265419006\n",
      "Epoch 11 -- Batch 735/ 842, training loss 0.38069263100624084\n",
      "Epoch 11 -- Batch 736/ 842, training loss 0.38043680787086487\n",
      "Epoch 11 -- Batch 737/ 842, training loss 0.3813033103942871\n",
      "Epoch 11 -- Batch 738/ 842, training loss 0.3916400671005249\n",
      "Epoch 11 -- Batch 739/ 842, training loss 0.3914564847946167\n",
      "Epoch 11 -- Batch 740/ 842, training loss 0.38972219824790955\n",
      "Epoch 11 -- Batch 741/ 842, training loss 0.39150431752204895\n",
      "Epoch 11 -- Batch 742/ 842, training loss 0.39344388246536255\n",
      "Epoch 11 -- Batch 743/ 842, training loss 0.38719427585601807\n",
      "Epoch 11 -- Batch 744/ 842, training loss 0.38624292612075806\n",
      "Epoch 11 -- Batch 745/ 842, training loss 0.3833633363246918\n",
      "Epoch 11 -- Batch 746/ 842, training loss 0.39119672775268555\n",
      "Epoch 11 -- Batch 747/ 842, training loss 0.3896133303642273\n",
      "Epoch 11 -- Batch 748/ 842, training loss 0.40107467770576477\n",
      "Epoch 11 -- Batch 749/ 842, training loss 0.39059773087501526\n",
      "Epoch 11 -- Batch 750/ 842, training loss 0.3892586827278137\n",
      "Epoch 11 -- Batch 751/ 842, training loss 0.3973710238933563\n",
      "Epoch 11 -- Batch 752/ 842, training loss 0.38226190209388733\n",
      "Epoch 11 -- Batch 753/ 842, training loss 0.3852006196975708\n",
      "Epoch 11 -- Batch 754/ 842, training loss 0.400950163602829\n",
      "Epoch 11 -- Batch 755/ 842, training loss 0.3890809416770935\n",
      "Epoch 11 -- Batch 756/ 842, training loss 0.3797866404056549\n",
      "Epoch 11 -- Batch 757/ 842, training loss 0.38299402594566345\n",
      "Epoch 11 -- Batch 758/ 842, training loss 0.39193326234817505\n",
      "Epoch 11 -- Batch 759/ 842, training loss 0.38499584794044495\n",
      "Epoch 11 -- Batch 760/ 842, training loss 0.40944674611091614\n",
      "Epoch 11 -- Batch 761/ 842, training loss 0.38947924971580505\n",
      "Epoch 11 -- Batch 762/ 842, training loss 0.3884789049625397\n",
      "Epoch 11 -- Batch 763/ 842, training loss 0.3902689814567566\n",
      "Epoch 11 -- Batch 764/ 842, training loss 0.39842140674591064\n",
      "Epoch 11 -- Batch 765/ 842, training loss 0.3873754143714905\n",
      "Epoch 11 -- Batch 766/ 842, training loss 0.3833577334880829\n",
      "Epoch 11 -- Batch 767/ 842, training loss 0.38064539432525635\n",
      "Epoch 11 -- Batch 768/ 842, training loss 0.3805620074272156\n",
      "Epoch 11 -- Batch 769/ 842, training loss 0.3838939964771271\n",
      "Epoch 11 -- Batch 770/ 842, training loss 0.38480108976364136\n",
      "Epoch 11 -- Batch 771/ 842, training loss 0.398895800113678\n",
      "Epoch 11 -- Batch 772/ 842, training loss 0.4030452370643616\n",
      "Epoch 11 -- Batch 773/ 842, training loss 0.38850587606430054\n",
      "Epoch 11 -- Batch 774/ 842, training loss 0.3798982799053192\n",
      "Epoch 11 -- Batch 775/ 842, training loss 0.4088488221168518\n",
      "Epoch 11 -- Batch 776/ 842, training loss 0.3977058231830597\n",
      "Epoch 11 -- Batch 777/ 842, training loss 0.37848374247550964\n",
      "Epoch 11 -- Batch 778/ 842, training loss 0.3799290955066681\n",
      "Epoch 11 -- Batch 779/ 842, training loss 0.39023587107658386\n",
      "Epoch 11 -- Batch 780/ 842, training loss 0.38354092836380005\n",
      "Epoch 11 -- Batch 781/ 842, training loss 0.3885001838207245\n",
      "Epoch 11 -- Batch 782/ 842, training loss 0.39151903986930847\n",
      "Epoch 11 -- Batch 783/ 842, training loss 0.3971615433692932\n",
      "Epoch 11 -- Batch 784/ 842, training loss 0.38475537300109863\n",
      "Epoch 11 -- Batch 785/ 842, training loss 0.4127473533153534\n",
      "Epoch 11 -- Batch 786/ 842, training loss 0.39530935883522034\n",
      "Epoch 11 -- Batch 787/ 842, training loss 0.381574809551239\n",
      "Epoch 11 -- Batch 788/ 842, training loss 0.39994320273399353\n",
      "Epoch 11 -- Batch 789/ 842, training loss 0.3949655592441559\n",
      "Epoch 11 -- Batch 790/ 842, training loss 0.3799144923686981\n",
      "Epoch 11 -- Batch 791/ 842, training loss 0.39694687724113464\n",
      "Epoch 11 -- Batch 792/ 842, training loss 0.39022693037986755\n",
      "Epoch 11 -- Batch 793/ 842, training loss 0.3902606964111328\n",
      "Epoch 11 -- Batch 794/ 842, training loss 0.3867586553096771\n",
      "Epoch 11 -- Batch 795/ 842, training loss 0.3960021138191223\n",
      "Epoch 11 -- Batch 796/ 842, training loss 0.3899376392364502\n",
      "Epoch 11 -- Batch 797/ 842, training loss 0.3958585858345032\n",
      "Epoch 11 -- Batch 798/ 842, training loss 0.38941845297813416\n",
      "Epoch 11 -- Batch 799/ 842, training loss 0.4126925766468048\n",
      "Epoch 11 -- Batch 800/ 842, training loss 0.39039936661720276\n",
      "Epoch 11 -- Batch 801/ 842, training loss 0.394607275724411\n",
      "Epoch 11 -- Batch 802/ 842, training loss 0.386526495218277\n",
      "Epoch 11 -- Batch 803/ 842, training loss 0.3741738200187683\n",
      "Epoch 11 -- Batch 804/ 842, training loss 0.38184431195259094\n",
      "Epoch 11 -- Batch 805/ 842, training loss 0.39309027791023254\n",
      "Epoch 11 -- Batch 806/ 842, training loss 0.38285231590270996\n",
      "Epoch 11 -- Batch 807/ 842, training loss 0.38205036520957947\n",
      "Epoch 11 -- Batch 808/ 842, training loss 0.3779689371585846\n",
      "Epoch 11 -- Batch 809/ 842, training loss 0.39256444573402405\n",
      "Epoch 11 -- Batch 810/ 842, training loss 0.38769081234931946\n",
      "Epoch 11 -- Batch 811/ 842, training loss 0.3765999674797058\n",
      "Epoch 11 -- Batch 812/ 842, training loss 0.3779582977294922\n",
      "Epoch 11 -- Batch 813/ 842, training loss 0.3822334408760071\n",
      "Epoch 11 -- Batch 814/ 842, training loss 0.395304799079895\n",
      "Epoch 11 -- Batch 815/ 842, training loss 0.386229008436203\n",
      "Epoch 11 -- Batch 816/ 842, training loss 0.4011304974555969\n",
      "Epoch 11 -- Batch 817/ 842, training loss 0.38053300976753235\n",
      "Epoch 11 -- Batch 818/ 842, training loss 0.3695515990257263\n",
      "Epoch 11 -- Batch 819/ 842, training loss 0.386494904756546\n",
      "Epoch 11 -- Batch 820/ 842, training loss 0.38818204402923584\n",
      "Epoch 11 -- Batch 821/ 842, training loss 0.38719314336776733\n",
      "Epoch 11 -- Batch 822/ 842, training loss 0.3943640887737274\n",
      "Epoch 11 -- Batch 823/ 842, training loss 0.38996556401252747\n",
      "Epoch 11 -- Batch 824/ 842, training loss 0.37967243790626526\n",
      "Epoch 11 -- Batch 825/ 842, training loss 0.38887885212898254\n",
      "Epoch 11 -- Batch 826/ 842, training loss 0.394599586725235\n",
      "Epoch 11 -- Batch 827/ 842, training loss 0.3937113583087921\n",
      "Epoch 11 -- Batch 828/ 842, training loss 0.3846752345561981\n",
      "Epoch 11 -- Batch 829/ 842, training loss 0.3871603310108185\n",
      "Epoch 11 -- Batch 830/ 842, training loss 0.4084358811378479\n",
      "Epoch 11 -- Batch 831/ 842, training loss 0.40573668479919434\n",
      "Epoch 11 -- Batch 832/ 842, training loss 0.3887438178062439\n",
      "Epoch 11 -- Batch 833/ 842, training loss 0.3815837502479553\n",
      "Epoch 11 -- Batch 834/ 842, training loss 0.4001246988773346\n",
      "Epoch 11 -- Batch 835/ 842, training loss 0.40254631638526917\n",
      "Epoch 11 -- Batch 836/ 842, training loss 0.4123966097831726\n",
      "Epoch 11 -- Batch 837/ 842, training loss 0.3973327577114105\n",
      "Epoch 11 -- Batch 838/ 842, training loss 0.38763663172721863\n",
      "Epoch 11 -- Batch 839/ 842, training loss 0.3870089650154114\n",
      "Epoch 11 -- Batch 840/ 842, training loss 0.4028191864490509\n",
      "Epoch 11 -- Batch 841/ 842, training loss 0.37798964977264404\n",
      "Epoch 11 -- Batch 842/ 842, training loss 0.38288211822509766\n",
      "----------------------------------------------------------------------\n",
      "Epoch 11 -- Batch 1/ 94, validation loss 0.38368281722068787\n",
      "Epoch 11 -- Batch 2/ 94, validation loss 0.37946510314941406\n",
      "Epoch 11 -- Batch 3/ 94, validation loss 0.40020978450775146\n",
      "Epoch 11 -- Batch 4/ 94, validation loss 0.3810776472091675\n",
      "Epoch 11 -- Batch 5/ 94, validation loss 0.39324215054512024\n",
      "Epoch 11 -- Batch 6/ 94, validation loss 0.37263253331184387\n",
      "Epoch 11 -- Batch 7/ 94, validation loss 0.3841172754764557\n",
      "Epoch 11 -- Batch 8/ 94, validation loss 0.3903719484806061\n",
      "Epoch 11 -- Batch 9/ 94, validation loss 0.3780941069126129\n",
      "Epoch 11 -- Batch 10/ 94, validation loss 0.3681735098361969\n",
      "Epoch 11 -- Batch 11/ 94, validation loss 0.38411396741867065\n",
      "Epoch 11 -- Batch 12/ 94, validation loss 0.3911987245082855\n",
      "Epoch 11 -- Batch 13/ 94, validation loss 0.38061943650245667\n",
      "Epoch 11 -- Batch 14/ 94, validation loss 0.38640329241752625\n",
      "Epoch 11 -- Batch 15/ 94, validation loss 0.38249891996383667\n",
      "Epoch 11 -- Batch 16/ 94, validation loss 0.3770748972892761\n",
      "Epoch 11 -- Batch 17/ 94, validation loss 0.3924841284751892\n",
      "Epoch 11 -- Batch 18/ 94, validation loss 0.3856019675731659\n",
      "Epoch 11 -- Batch 19/ 94, validation loss 0.390229731798172\n",
      "Epoch 11 -- Batch 20/ 94, validation loss 0.3802545666694641\n",
      "Epoch 11 -- Batch 21/ 94, validation loss 0.36442065238952637\n",
      "Epoch 11 -- Batch 22/ 94, validation loss 0.38084179162979126\n",
      "Epoch 11 -- Batch 23/ 94, validation loss 0.3850279152393341\n",
      "Epoch 11 -- Batch 24/ 94, validation loss 0.3898515999317169\n",
      "Epoch 11 -- Batch 25/ 94, validation loss 0.39214491844177246\n",
      "Epoch 11 -- Batch 26/ 94, validation loss 0.38932502269744873\n",
      "Epoch 11 -- Batch 27/ 94, validation loss 0.3825206160545349\n",
      "Epoch 11 -- Batch 28/ 94, validation loss 0.38315853476524353\n",
      "Epoch 11 -- Batch 29/ 94, validation loss 0.3720862865447998\n",
      "Epoch 11 -- Batch 30/ 94, validation loss 0.3916606307029724\n",
      "Epoch 11 -- Batch 31/ 94, validation loss 0.38318949937820435\n",
      "Epoch 11 -- Batch 32/ 94, validation loss 0.38780486583709717\n",
      "Epoch 11 -- Batch 33/ 94, validation loss 0.40021467208862305\n",
      "Epoch 11 -- Batch 34/ 94, validation loss 0.42702504992485046\n",
      "Epoch 11 -- Batch 35/ 94, validation loss 0.4063064157962799\n",
      "Epoch 11 -- Batch 36/ 94, validation loss 0.39259782433509827\n",
      "Epoch 11 -- Batch 37/ 94, validation loss 0.37943798303604126\n",
      "Epoch 11 -- Batch 38/ 94, validation loss 0.40212443470954895\n",
      "Epoch 11 -- Batch 39/ 94, validation loss 0.42201316356658936\n",
      "Epoch 11 -- Batch 40/ 94, validation loss 0.3821101784706116\n",
      "Epoch 11 -- Batch 41/ 94, validation loss 0.38662806153297424\n",
      "Epoch 11 -- Batch 42/ 94, validation loss 0.3941866159439087\n",
      "Epoch 11 -- Batch 43/ 94, validation loss 0.3898470103740692\n",
      "Epoch 11 -- Batch 44/ 94, validation loss 0.3738478422164917\n",
      "Epoch 11 -- Batch 45/ 94, validation loss 0.3984053432941437\n",
      "Epoch 11 -- Batch 46/ 94, validation loss 0.3859318196773529\n",
      "Epoch 11 -- Batch 47/ 94, validation loss 0.41364786028862\n",
      "Epoch 11 -- Batch 48/ 94, validation loss 0.3853890597820282\n",
      "Epoch 11 -- Batch 49/ 94, validation loss 0.36968812346458435\n",
      "Epoch 11 -- Batch 50/ 94, validation loss 0.3817365765571594\n",
      "Epoch 11 -- Batch 51/ 94, validation loss 0.4111440181732178\n",
      "Epoch 11 -- Batch 52/ 94, validation loss 0.378837913274765\n",
      "Epoch 11 -- Batch 53/ 94, validation loss 0.37765756249427795\n",
      "Epoch 11 -- Batch 54/ 94, validation loss 0.38339170813560486\n",
      "Epoch 11 -- Batch 55/ 94, validation loss 0.3793970048427582\n",
      "Epoch 11 -- Batch 56/ 94, validation loss 0.3788830041885376\n",
      "Epoch 11 -- Batch 57/ 94, validation loss 0.3842802047729492\n",
      "Epoch 11 -- Batch 58/ 94, validation loss 0.3929515779018402\n",
      "Epoch 11 -- Batch 59/ 94, validation loss 0.38391077518463135\n",
      "Epoch 11 -- Batch 60/ 94, validation loss 0.3799087703227997\n",
      "Epoch 11 -- Batch 61/ 94, validation loss 0.38786402344703674\n",
      "Epoch 11 -- Batch 62/ 94, validation loss 0.3998872637748718\n",
      "Epoch 11 -- Batch 63/ 94, validation loss 0.3982325494289398\n",
      "Epoch 11 -- Batch 64/ 94, validation loss 0.3951583504676819\n",
      "Epoch 11 -- Batch 65/ 94, validation loss 0.3879633843898773\n",
      "Epoch 11 -- Batch 66/ 94, validation loss 0.40365657210350037\n",
      "Epoch 11 -- Batch 67/ 94, validation loss 0.38284236192703247\n",
      "Epoch 11 -- Batch 68/ 94, validation loss 0.3964894115924835\n",
      "Epoch 11 -- Batch 69/ 94, validation loss 0.38627997040748596\n",
      "Epoch 11 -- Batch 70/ 94, validation loss 0.3951728045940399\n",
      "Epoch 11 -- Batch 71/ 94, validation loss 0.380064457654953\n",
      "Epoch 11 -- Batch 72/ 94, validation loss 0.367672860622406\n",
      "Epoch 11 -- Batch 73/ 94, validation loss 0.38232851028442383\n",
      "Epoch 11 -- Batch 74/ 94, validation loss 0.3849795460700989\n",
      "Epoch 11 -- Batch 75/ 94, validation loss 0.3959670960903168\n",
      "Epoch 11 -- Batch 76/ 94, validation loss 0.40461015701293945\n",
      "Epoch 11 -- Batch 77/ 94, validation loss 0.3794490694999695\n",
      "Epoch 11 -- Batch 78/ 94, validation loss 0.3925146460533142\n",
      "Epoch 11 -- Batch 79/ 94, validation loss 0.38960421085357666\n",
      "Epoch 11 -- Batch 80/ 94, validation loss 0.39386776089668274\n",
      "Epoch 11 -- Batch 81/ 94, validation loss 0.3789833188056946\n",
      "Epoch 11 -- Batch 82/ 94, validation loss 0.37862667441368103\n",
      "Epoch 11 -- Batch 83/ 94, validation loss 0.3879357874393463\n",
      "Epoch 11 -- Batch 84/ 94, validation loss 0.3763716220855713\n",
      "Epoch 11 -- Batch 85/ 94, validation loss 0.39923566579818726\n",
      "Epoch 11 -- Batch 86/ 94, validation loss 0.37524494528770447\n",
      "Epoch 11 -- Batch 87/ 94, validation loss 0.39256513118743896\n",
      "Epoch 11 -- Batch 88/ 94, validation loss 0.395748108625412\n",
      "Epoch 11 -- Batch 89/ 94, validation loss 0.38785839080810547\n",
      "Epoch 11 -- Batch 90/ 94, validation loss 0.39171072840690613\n",
      "Epoch 11 -- Batch 91/ 94, validation loss 0.393352210521698\n",
      "Epoch 11 -- Batch 92/ 94, validation loss 0.3878859281539917\n",
      "Epoch 11 -- Batch 93/ 94, validation loss 0.39797165989875793\n",
      "Epoch 11 -- Batch 94/ 94, validation loss 0.39208826422691345\n",
      "----------------------------------------------------------------------\n",
      "Epoch 11 loss: Training 0.3917623460292816, Validation 0.39208826422691345\n",
      "----------------------------------------------------------------------\n",
      "Epoch 12/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CN1C[C@@H](C)[C@@H](OC)CN(C)C(=O)c2ccc(NC(=O)C3CCO)cc2OC[C@@H]1C'\n",
      "[19:05:38] SMILES Parse Error: syntax error while parsing: /C(=C\\COc1ccccc1)Nc1c(-c2ccc(F)cc2)nc2ccccn12\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES '/C(=C\\COc1ccccc1)Nc1c(-c2ccc(F)cc2)nc2ccccn12' for input: '/C(=C\\COc1ccccc1)Nc1c(-c2ccc(F)cc2)nc2ccccn12'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC2(C)CC(=O)C2=C1C(c1ccco1)C(C(=O)OC(C)C)C(C#N)=C(N)O2'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 13 14 15 16 19 20 22\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'Cc1oc2nc3c4ccc(Br)cc4nc(c3ccnc4ccccc34)c2cc1C'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(-c2csc(N3C(=O)NC(=O)/C(=C/c3cccc(F)c3)C2=O)c2ccccc2)cc1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'Cc1c(C(=O)NC(c2ccccc2)c2ncnn2C)on1c(=O)cc(C)c1ccccc12'\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: CC1(C)Oc2ccc3c(=O)c(Cl)cc(-c4ccc(F)cc4)cc3C2)c(OC)c1OC\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'CC1(C)Oc2ccc3c(=O)c(Cl)cc(-c4ccc(F)cc4)cc3C2)c(OC)c1OC' for input: 'CC1(C)Oc2ccc3c(=O)c(Cl)cc(-c4ccc(F)cc4)cc3C2)c(OC)c1OC'\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: Cc1ncn(-c2ccc(Nc3nccc(-c4ccc5nc(-c6ccnc(C)c6)[nH]4)cc4)n3c2)c1)c1ccccc1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'Cc1ncn(-c2ccc(Nc3nccc(-c4ccc5nc(-c6ccnc(C)c6)[nH]4)cc4)n3c2)c1)c1ccccc1' for input: 'Cc1ncn(-c2ccc(Nc3nccc(-c4ccc5nc(-c6ccnc(C)c6)[nH]4)cc4)n3c2)c1)c1ccccc1'\n",
      "[19:05:38] SMILES Parse Error: ring closure 1 duplicates bond between atom 5 and atom 6 for input: 'CCN(CC)C1c1ccc2cc(OCC(=O)OCC)ccc2c1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'O=C1NCCN(CCC2c3ccccc3Oc3ccccc31)c1ccc2ccccc2c1'\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(CCNC(=O)CC)Cc2ccc(S(=O)(=O)N3CCCC3)cc2)c1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(CCNC(=O)CC)Cc2ccc(S(=O)(=O)N3CCCC3)cc2)c1' for input: 'COc1ccc(CCNC(=O)CC)Cc2ccc(S(=O)(=O)N3CCCC3)cc2)c1'\n",
      "[19:05:38] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 7 8 9 10 11 12 14 17\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCc1cc2c(cc3c(=O)n4c(nc4cccc(F)c43)n(C)c(=O)n12)c1ccc(F)cc1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCc1ccc(NC(=O)C(C)OC(=O)C2C3CC4CC(Br3)CC2C(=O)N2CCc2ccccc2)cc1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 17 18 19 27 28 29 30\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: Cc1c2c2ncn[n+](SCC(=O)NCc3ccccc3)c2ccc1C)n(O)c1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'Cc1c2c2ncn[n+](SCC(=O)NCc3ccccc3)c2ccc1C)n(O)c1' for input: 'Cc1c2c2ncn[n+](SCC(=O)NCc3ccccc3)c2ccc1C)n(O)c1'\n",
      "[19:05:38] SMILES Parse Error: extra open parentheses for input: 'c1ccc(-n2nnnc2N2CCC'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'N#Cc1cc2cc3c(cc2nc1N1CCN(c4ccccc4)CC2)OCO3'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 7 9\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'Cc1cccn2c(=O)c3cc(C(=O)NC4CC(=O)Nc4ccc(C(F)(F)F)cc43)nc3n12'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 7 8 9\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 19 28\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: CCN(CC)CC(=O)N1CCCC(c2c(C(N)=O)c(=O)[nH]c3=O)C2)C1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'CCN(CC)CC(=O)N1CCCC(c2c(C(N)=O)c(=O)[nH]c3=O)C2)C1' for input: 'CCN(CC)CC(=O)N1CCCC(c2c(C(N)=O)c(=O)[nH]c3=O)C2)C1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 15 16 23\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: O=c1c2cnc3nc4ccccc4cc3cnn2n1n1NOCc2ccccc2)cc1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'O=c1c2cnc3nc4ccccc4cc3cnn2n1n1NOCc2ccccc2)cc1' for input: 'O=c1c2cnc3nc4ccccc4cc3cnn2n1n1NOCc2ccccc2)cc1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 17 18 19\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 7 8 24\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 20 21 22\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1C(=O)N2CCC1(C)C(=O)NS(=O)(=O)Cc1ccc(F)cc1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1-n1c(=S)[nH]c2[nH]c(=O)=C(Nc3ccc(F)cc3)C(=O)N2Cc2ccccc2)c1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1-n1c(=S)[nH]c2[nH]c(=O)=C(Nc3ccc(F)cc3)C(=O)N2Cc2ccccc2)c1' for input: 'COc1ccccc1-n1c(=S)[nH]c2[nH]c(=O)=C(Nc3ccc(F)cc3)C(=O)N2Cc2ccccc2)c1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 5 16 17 18 22 23 24\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 10 21 24 25\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 16 17 22\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: O=S1(=O)N=NC(N2CCCC(N3CCOCC3)=N2)c2ccccc21)c1ccccc1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'O=S1(=O)N=NC(N2CCCC(N3CCOCC3)=N2)c2ccccc21)c1ccccc1' for input: 'O=S1(=O)N=NC(N2CCCC(N3CCOCC3)=N2)c2ccccc21)c1ccccc1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'COC(=O)COc1cccc(C2C3=C(CCCC3=O)Nc3cc4c(cc4ccccc43)N2)c1'\n",
      "[19:05:38] SMILES Parse Error: extra close parentheses while parsing: CS(=O)(=O)N(CC(=O)NC(C)(C)C)c1cccc(C(F)(F)F)c1)c1ccccc1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'CS(=O)(=O)N(CC(=O)NC(C)(C)C)c1cccc(C(F)(F)F)c1)c1ccccc1' for input: 'CS(=O)(=O)N(CC(=O)NC(C)(C)C)c1cccc(C(F)(F)F)c1)c1ccccc1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'O=C(CCn1cccn1)N1C[C@@H](c2ccccc2)[C@@H]2C[C@@H](C1CNCCN1Cc1ccsc1)O2'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCC(C)(C(=O)O)C1CCC2(C)C(=CC2C(c2ccccc2)=NC2(C#N)C#N)C(=O)C1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCCCCCN1C(=O)CN(C(=O)CN2CCCCCC2)CC(C)C'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'O=C1C2C3C=CC(C3CC3Br)N2C(=O)CN1c1ccc(N(C)C)cc1'\n",
      "[19:05:38] SMILES Parse Error: syntax error while parsing: CC(C)C(=O)Nc1ccc(NC(=O)c2ccccc2)c(\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'CC(C)C(=O)Nc1ccc(NC(=O)c2ccccc2)c(' for input: 'CC(C)C(=O)Nc1ccc(NC(=O)c2ccccc2)c('\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])/N=C1\\CC3CCCCC2C1=O'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)cc(OCCCOc2cccc(/C=C3\\C(=N)N4N=C(c5ccccc5)=NC3=O)c2)c1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 4 18 22\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 17 18 26\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 7 9\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CC(Br)C1CC12CC3CC(C)(CC(=O)N(CCO)C1)C2(C)C'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'COc1cccc(CNc2nc(CN3CCC(O)(c4ncnc5C4CC4)CC3)cs2)c1'\n",
      "[19:05:38] SMILES Parse Error: syntax error while parsing: O=C(c1ccncc1)N1CC(=O)N2[C@H](C=)[C@H]C[C@@H]2C1\n",
      "[19:05:38] SMILES Parse Error: Failed parsing SMILES 'O=C(c1ccncc1)N1CC(=O)N2[C@H](C=)[C@H]C[C@@H]2C1' for input: 'O=C(c1ccncc1)N1CC(=O)N2[C@H](C=)[C@H]C[C@@H]2C1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1C1C2=C(CCC2=O)Nc2c1c(=O)n2c(C)cc(=O)n2C'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2ccc(C3NCc4ccccc4C4)nn2)cc1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CC1(C)Cc2c(sc3c4onc(SCC(N)=O)n33)C(C)C1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCc1nc2ccc(CNC(=O)Cc3c[nH]c4ccccc35)cn2c1N(C)Cc1ccc(C(F)(F)F)cc1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 24\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 15 16 17 21 22 23 24 25 26\n",
      "[19:05:38] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)C2(Cc3cc4ccccc4cc3NC3=O)CC2C1=O'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 17\n",
      "[19:05:38] Explicit valence for atom # 11 C, 5, is greater than permitted\n",
      "[19:05:38] Explicit valence for atom # 1 Cl, 2, is greater than permitted\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 23 34 35\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 4 5 6 8 9 10 17 18 19\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 26 30\n",
      "[19:05:38] SMILES Parse Error: duplicated ring closure 3 bonds atom 18 to itself for input: 'COc1cccc(C2C3=C(CC(C)(C)CC3=O)OC33C(=O)N(c4ccc(F)cc4)C(=O)C3O2)c1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'O=C(NC1CC2c3ccccc3C2)c(=O)c2c1-c1ccccc1'\n",
      "[19:05:38] Can't kekulize mol.  Unkekulized atoms: 6 8 9 10 22 23 24\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'C1=CCc2oc3c(c2cc1Cl)C(C)(C)N(C(=O)CN(C)C)C1'\n",
      "[19:05:38] SMILES Parse Error: unclosed ring for input: 'S=c1sc2c(s1)SCCN1C(=O)c1ccc(Cl)cc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 -- Batch 1/ 842, training loss 0.3802540898323059\n",
      "Epoch 12 -- Batch 2/ 842, training loss 0.3868546783924103\n",
      "Epoch 12 -- Batch 3/ 842, training loss 0.3748510777950287\n",
      "Epoch 12 -- Batch 4/ 842, training loss 0.3893398940563202\n",
      "Epoch 12 -- Batch 5/ 842, training loss 0.37522462010383606\n",
      "Epoch 12 -- Batch 6/ 842, training loss 0.3762870132923126\n",
      "Epoch 12 -- Batch 7/ 842, training loss 0.4037068784236908\n",
      "Epoch 12 -- Batch 8/ 842, training loss 0.3855622410774231\n",
      "Epoch 12 -- Batch 9/ 842, training loss 0.38504043221473694\n",
      "Epoch 12 -- Batch 10/ 842, training loss 0.38066625595092773\n",
      "Epoch 12 -- Batch 11/ 842, training loss 0.3765183389186859\n",
      "Epoch 12 -- Batch 12/ 842, training loss 0.378429114818573\n",
      "Epoch 12 -- Batch 13/ 842, training loss 0.37309572100639343\n",
      "Epoch 12 -- Batch 14/ 842, training loss 0.3816050887107849\n",
      "Epoch 12 -- Batch 15/ 842, training loss 0.3844953179359436\n",
      "Epoch 12 -- Batch 16/ 842, training loss 0.3727725148200989\n",
      "Epoch 12 -- Batch 17/ 842, training loss 0.3848768174648285\n",
      "Epoch 12 -- Batch 18/ 842, training loss 0.3761520981788635\n",
      "Epoch 12 -- Batch 19/ 842, training loss 0.38928788900375366\n",
      "Epoch 12 -- Batch 20/ 842, training loss 0.3892476558685303\n",
      "Epoch 12 -- Batch 21/ 842, training loss 0.37960943579673767\n",
      "Epoch 12 -- Batch 22/ 842, training loss 0.3752128779888153\n",
      "Epoch 12 -- Batch 23/ 842, training loss 0.385031133890152\n",
      "Epoch 12 -- Batch 24/ 842, training loss 0.36171460151672363\n",
      "Epoch 12 -- Batch 25/ 842, training loss 0.37187284231185913\n",
      "Epoch 12 -- Batch 26/ 842, training loss 0.373961865901947\n",
      "Epoch 12 -- Batch 27/ 842, training loss 0.3784825801849365\n",
      "Epoch 12 -- Batch 28/ 842, training loss 0.38617604970932007\n",
      "Epoch 12 -- Batch 29/ 842, training loss 0.38338491320610046\n",
      "Epoch 12 -- Batch 30/ 842, training loss 0.39822572469711304\n",
      "Epoch 12 -- Batch 31/ 842, training loss 0.3727535307407379\n",
      "Epoch 12 -- Batch 32/ 842, training loss 0.382980078458786\n",
      "Epoch 12 -- Batch 33/ 842, training loss 0.3819296061992645\n",
      "Epoch 12 -- Batch 34/ 842, training loss 0.38811424374580383\n",
      "Epoch 12 -- Batch 35/ 842, training loss 0.379898339509964\n",
      "Epoch 12 -- Batch 36/ 842, training loss 0.36332616209983826\n",
      "Epoch 12 -- Batch 37/ 842, training loss 0.3853139877319336\n",
      "Epoch 12 -- Batch 38/ 842, training loss 0.374832421541214\n",
      "Epoch 12 -- Batch 39/ 842, training loss 0.38033074140548706\n",
      "Epoch 12 -- Batch 40/ 842, training loss 0.3820968568325043\n",
      "Epoch 12 -- Batch 41/ 842, training loss 0.36931025981903076\n",
      "Epoch 12 -- Batch 42/ 842, training loss 0.3835199475288391\n",
      "Epoch 12 -- Batch 43/ 842, training loss 0.38868072628974915\n",
      "Epoch 12 -- Batch 44/ 842, training loss 0.36895981431007385\n",
      "Epoch 12 -- Batch 45/ 842, training loss 0.38461554050445557\n",
      "Epoch 12 -- Batch 46/ 842, training loss 0.3710813820362091\n",
      "Epoch 12 -- Batch 47/ 842, training loss 0.3679201304912567\n",
      "Epoch 12 -- Batch 48/ 842, training loss 0.3810655474662781\n",
      "Epoch 12 -- Batch 49/ 842, training loss 0.3877412676811218\n",
      "Epoch 12 -- Batch 50/ 842, training loss 0.3657374978065491\n",
      "Epoch 12 -- Batch 51/ 842, training loss 0.3723340630531311\n",
      "Epoch 12 -- Batch 52/ 842, training loss 0.37155598402023315\n",
      "Epoch 12 -- Batch 53/ 842, training loss 0.37265530228614807\n",
      "Epoch 12 -- Batch 54/ 842, training loss 0.37177038192749023\n",
      "Epoch 12 -- Batch 55/ 842, training loss 0.3915056586265564\n",
      "Epoch 12 -- Batch 56/ 842, training loss 0.3755086064338684\n",
      "Epoch 12 -- Batch 57/ 842, training loss 0.38473594188690186\n",
      "Epoch 12 -- Batch 58/ 842, training loss 0.39326247572898865\n",
      "Epoch 12 -- Batch 59/ 842, training loss 0.3716545104980469\n",
      "Epoch 12 -- Batch 60/ 842, training loss 0.3807799220085144\n",
      "Epoch 12 -- Batch 61/ 842, training loss 0.38616394996643066\n",
      "Epoch 12 -- Batch 62/ 842, training loss 0.3834536075592041\n",
      "Epoch 12 -- Batch 63/ 842, training loss 0.3769546449184418\n",
      "Epoch 12 -- Batch 64/ 842, training loss 0.3856549561023712\n",
      "Epoch 12 -- Batch 65/ 842, training loss 0.388894647359848\n",
      "Epoch 12 -- Batch 66/ 842, training loss 0.3792407512664795\n",
      "Epoch 12 -- Batch 67/ 842, training loss 0.37457871437072754\n",
      "Epoch 12 -- Batch 68/ 842, training loss 0.3581653833389282\n",
      "Epoch 12 -- Batch 69/ 842, training loss 0.3911145329475403\n",
      "Epoch 12 -- Batch 70/ 842, training loss 0.36621928215026855\n",
      "Epoch 12 -- Batch 71/ 842, training loss 0.3762987554073334\n",
      "Epoch 12 -- Batch 72/ 842, training loss 0.37807518243789673\n",
      "Epoch 12 -- Batch 73/ 842, training loss 0.3967510461807251\n",
      "Epoch 12 -- Batch 74/ 842, training loss 0.38464078307151794\n",
      "Epoch 12 -- Batch 75/ 842, training loss 0.3675413429737091\n",
      "Epoch 12 -- Batch 76/ 842, training loss 0.36603280901908875\n",
      "Epoch 12 -- Batch 77/ 842, training loss 0.3759198784828186\n",
      "Epoch 12 -- Batch 78/ 842, training loss 0.3704386055469513\n",
      "Epoch 12 -- Batch 79/ 842, training loss 0.3640598654747009\n",
      "Epoch 12 -- Batch 80/ 842, training loss 0.3891952931880951\n",
      "Epoch 12 -- Batch 81/ 842, training loss 0.385885089635849\n",
      "Epoch 12 -- Batch 82/ 842, training loss 0.37943023443222046\n",
      "Epoch 12 -- Batch 83/ 842, training loss 0.37298932671546936\n",
      "Epoch 12 -- Batch 84/ 842, training loss 0.3740653693675995\n",
      "Epoch 12 -- Batch 85/ 842, training loss 0.3816414773464203\n",
      "Epoch 12 -- Batch 86/ 842, training loss 0.36473143100738525\n",
      "Epoch 12 -- Batch 87/ 842, training loss 0.37255290150642395\n",
      "Epoch 12 -- Batch 88/ 842, training loss 0.3694731295108795\n",
      "Epoch 12 -- Batch 89/ 842, training loss 0.3767235279083252\n",
      "Epoch 12 -- Batch 90/ 842, training loss 0.38606661558151245\n",
      "Epoch 12 -- Batch 91/ 842, training loss 0.3999021649360657\n",
      "Epoch 12 -- Batch 92/ 842, training loss 0.3814640939235687\n",
      "Epoch 12 -- Batch 93/ 842, training loss 0.3781265914440155\n",
      "Epoch 12 -- Batch 94/ 842, training loss 0.3817235231399536\n",
      "Epoch 12 -- Batch 95/ 842, training loss 0.38002440333366394\n",
      "Epoch 12 -- Batch 96/ 842, training loss 0.36173874139785767\n",
      "Epoch 12 -- Batch 97/ 842, training loss 0.3748755156993866\n",
      "Epoch 12 -- Batch 98/ 842, training loss 0.37180742621421814\n",
      "Epoch 12 -- Batch 99/ 842, training loss 0.3825328052043915\n",
      "Epoch 12 -- Batch 100/ 842, training loss 0.3909309506416321\n",
      "Epoch 12 -- Batch 101/ 842, training loss 0.38103237748146057\n",
      "Epoch 12 -- Batch 102/ 842, training loss 0.37563300132751465\n",
      "Epoch 12 -- Batch 103/ 842, training loss 0.3937950134277344\n",
      "Epoch 12 -- Batch 104/ 842, training loss 0.38673990964889526\n",
      "Epoch 12 -- Batch 105/ 842, training loss 0.39263883233070374\n",
      "Epoch 12 -- Batch 106/ 842, training loss 0.37372449040412903\n",
      "Epoch 12 -- Batch 107/ 842, training loss 0.39088907837867737\n",
      "Epoch 12 -- Batch 108/ 842, training loss 0.37974193692207336\n",
      "Epoch 12 -- Batch 109/ 842, training loss 0.3791528046131134\n",
      "Epoch 12 -- Batch 110/ 842, training loss 0.3658001720905304\n",
      "Epoch 12 -- Batch 111/ 842, training loss 0.37801265716552734\n",
      "Epoch 12 -- Batch 112/ 842, training loss 0.37243226170539856\n",
      "Epoch 12 -- Batch 113/ 842, training loss 0.36456993222236633\n",
      "Epoch 12 -- Batch 114/ 842, training loss 0.3950752317905426\n",
      "Epoch 12 -- Batch 115/ 842, training loss 0.3707125782966614\n",
      "Epoch 12 -- Batch 116/ 842, training loss 0.3816562294960022\n",
      "Epoch 12 -- Batch 117/ 842, training loss 0.3852101266384125\n",
      "Epoch 12 -- Batch 118/ 842, training loss 0.38578057289123535\n",
      "Epoch 12 -- Batch 119/ 842, training loss 0.3748542368412018\n",
      "Epoch 12 -- Batch 120/ 842, training loss 0.36958688497543335\n",
      "Epoch 12 -- Batch 121/ 842, training loss 0.3822435140609741\n",
      "Epoch 12 -- Batch 122/ 842, training loss 0.37856951355934143\n",
      "Epoch 12 -- Batch 123/ 842, training loss 0.38541197776794434\n",
      "Epoch 12 -- Batch 124/ 842, training loss 0.3647047281265259\n",
      "Epoch 12 -- Batch 125/ 842, training loss 0.36751237511634827\n",
      "Epoch 12 -- Batch 126/ 842, training loss 0.38129135966300964\n",
      "Epoch 12 -- Batch 127/ 842, training loss 0.3876197636127472\n",
      "Epoch 12 -- Batch 128/ 842, training loss 0.37108251452445984\n",
      "Epoch 12 -- Batch 129/ 842, training loss 0.38552865386009216\n",
      "Epoch 12 -- Batch 130/ 842, training loss 0.38006383180618286\n",
      "Epoch 12 -- Batch 131/ 842, training loss 0.3878980875015259\n",
      "Epoch 12 -- Batch 132/ 842, training loss 0.3941478133201599\n",
      "Epoch 12 -- Batch 133/ 842, training loss 0.3674609661102295\n",
      "Epoch 12 -- Batch 134/ 842, training loss 0.3681595027446747\n",
      "Epoch 12 -- Batch 135/ 842, training loss 0.37250110507011414\n",
      "Epoch 12 -- Batch 136/ 842, training loss 0.3778623640537262\n",
      "Epoch 12 -- Batch 137/ 842, training loss 0.3898719251155853\n",
      "Epoch 12 -- Batch 138/ 842, training loss 0.36866140365600586\n",
      "Epoch 12 -- Batch 139/ 842, training loss 0.39596766233444214\n",
      "Epoch 12 -- Batch 140/ 842, training loss 0.4001246690750122\n",
      "Epoch 12 -- Batch 141/ 842, training loss 0.38017743825912476\n",
      "Epoch 12 -- Batch 142/ 842, training loss 0.3754768669605255\n",
      "Epoch 12 -- Batch 143/ 842, training loss 0.3818676471710205\n",
      "Epoch 12 -- Batch 144/ 842, training loss 0.366489976644516\n",
      "Epoch 12 -- Batch 145/ 842, training loss 0.38092178106307983\n",
      "Epoch 12 -- Batch 146/ 842, training loss 0.3887404799461365\n",
      "Epoch 12 -- Batch 147/ 842, training loss 0.37836071848869324\n",
      "Epoch 12 -- Batch 148/ 842, training loss 0.38394418358802795\n",
      "Epoch 12 -- Batch 149/ 842, training loss 0.39495888352394104\n",
      "Epoch 12 -- Batch 150/ 842, training loss 0.37703001499176025\n",
      "Epoch 12 -- Batch 151/ 842, training loss 0.3750632405281067\n",
      "Epoch 12 -- Batch 152/ 842, training loss 0.3633717894554138\n",
      "Epoch 12 -- Batch 153/ 842, training loss 0.3691216707229614\n",
      "Epoch 12 -- Batch 154/ 842, training loss 0.37335750460624695\n",
      "Epoch 12 -- Batch 155/ 842, training loss 0.3759754002094269\n",
      "Epoch 12 -- Batch 156/ 842, training loss 0.369637131690979\n",
      "Epoch 12 -- Batch 157/ 842, training loss 0.3862183690071106\n",
      "Epoch 12 -- Batch 158/ 842, training loss 0.39578232169151306\n",
      "Epoch 12 -- Batch 159/ 842, training loss 0.3788726031780243\n",
      "Epoch 12 -- Batch 160/ 842, training loss 0.37471452355384827\n",
      "Epoch 12 -- Batch 161/ 842, training loss 0.3945748507976532\n",
      "Epoch 12 -- Batch 162/ 842, training loss 0.36642882227897644\n",
      "Epoch 12 -- Batch 163/ 842, training loss 0.36683741211891174\n",
      "Epoch 12 -- Batch 164/ 842, training loss 0.3762715756893158\n",
      "Epoch 12 -- Batch 165/ 842, training loss 0.3809683620929718\n",
      "Epoch 12 -- Batch 166/ 842, training loss 0.38039031624794006\n",
      "Epoch 12 -- Batch 167/ 842, training loss 0.3776111304759979\n",
      "Epoch 12 -- Batch 168/ 842, training loss 0.37283119559288025\n",
      "Epoch 12 -- Batch 169/ 842, training loss 0.37738052010536194\n",
      "Epoch 12 -- Batch 170/ 842, training loss 0.4032810926437378\n",
      "Epoch 12 -- Batch 171/ 842, training loss 0.3695540130138397\n",
      "Epoch 12 -- Batch 172/ 842, training loss 0.3780330419540405\n",
      "Epoch 12 -- Batch 173/ 842, training loss 0.38467100262641907\n",
      "Epoch 12 -- Batch 174/ 842, training loss 0.371451199054718\n",
      "Epoch 12 -- Batch 175/ 842, training loss 0.36267614364624023\n",
      "Epoch 12 -- Batch 176/ 842, training loss 0.37139591574668884\n",
      "Epoch 12 -- Batch 177/ 842, training loss 0.3782626688480377\n",
      "Epoch 12 -- Batch 178/ 842, training loss 0.37441501021385193\n",
      "Epoch 12 -- Batch 179/ 842, training loss 0.3935948312282562\n",
      "Epoch 12 -- Batch 180/ 842, training loss 0.3657981753349304\n",
      "Epoch 12 -- Batch 181/ 842, training loss 0.3817466199398041\n",
      "Epoch 12 -- Batch 182/ 842, training loss 0.37582576274871826\n",
      "Epoch 12 -- Batch 183/ 842, training loss 0.3752193748950958\n",
      "Epoch 12 -- Batch 184/ 842, training loss 0.37187060713768005\n",
      "Epoch 12 -- Batch 185/ 842, training loss 0.37309518456459045\n",
      "Epoch 12 -- Batch 186/ 842, training loss 0.38997191190719604\n",
      "Epoch 12 -- Batch 187/ 842, training loss 0.37073373794555664\n",
      "Epoch 12 -- Batch 188/ 842, training loss 0.36999109387397766\n",
      "Epoch 12 -- Batch 189/ 842, training loss 0.3807509243488312\n",
      "Epoch 12 -- Batch 190/ 842, training loss 0.3805203437805176\n",
      "Epoch 12 -- Batch 191/ 842, training loss 0.379133403301239\n",
      "Epoch 12 -- Batch 192/ 842, training loss 0.3662550747394562\n",
      "Epoch 12 -- Batch 193/ 842, training loss 0.3769712448120117\n",
      "Epoch 12 -- Batch 194/ 842, training loss 0.38000044226646423\n",
      "Epoch 12 -- Batch 195/ 842, training loss 0.3853592574596405\n",
      "Epoch 12 -- Batch 196/ 842, training loss 0.3848137855529785\n",
      "Epoch 12 -- Batch 197/ 842, training loss 0.3649826645851135\n",
      "Epoch 12 -- Batch 198/ 842, training loss 0.3767351806163788\n",
      "Epoch 12 -- Batch 199/ 842, training loss 0.3764401078224182\n",
      "Epoch 12 -- Batch 200/ 842, training loss 0.3742949962615967\n",
      "Epoch 12 -- Batch 201/ 842, training loss 0.3787728548049927\n",
      "Epoch 12 -- Batch 202/ 842, training loss 0.38158124685287476\n",
      "Epoch 12 -- Batch 203/ 842, training loss 0.39737504720687866\n",
      "Epoch 12 -- Batch 204/ 842, training loss 0.3832325041294098\n",
      "Epoch 12 -- Batch 205/ 842, training loss 0.37331193685531616\n",
      "Epoch 12 -- Batch 206/ 842, training loss 0.38619497418403625\n",
      "Epoch 12 -- Batch 207/ 842, training loss 0.3685000240802765\n",
      "Epoch 12 -- Batch 208/ 842, training loss 0.3864375054836273\n",
      "Epoch 12 -- Batch 209/ 842, training loss 0.38954418897628784\n",
      "Epoch 12 -- Batch 210/ 842, training loss 0.37387827038764954\n",
      "Epoch 12 -- Batch 211/ 842, training loss 0.38681933283805847\n",
      "Epoch 12 -- Batch 212/ 842, training loss 0.3873891234397888\n",
      "Epoch 12 -- Batch 213/ 842, training loss 0.38508495688438416\n",
      "Epoch 12 -- Batch 214/ 842, training loss 0.39712002873420715\n",
      "Epoch 12 -- Batch 215/ 842, training loss 0.38349905610084534\n",
      "Epoch 12 -- Batch 216/ 842, training loss 0.38063475489616394\n",
      "Epoch 12 -- Batch 217/ 842, training loss 0.36826857924461365\n",
      "Epoch 12 -- Batch 218/ 842, training loss 0.38129061460494995\n",
      "Epoch 12 -- Batch 219/ 842, training loss 0.38451677560806274\n",
      "Epoch 12 -- Batch 220/ 842, training loss 0.38275960087776184\n",
      "Epoch 12 -- Batch 221/ 842, training loss 0.3647126257419586\n",
      "Epoch 12 -- Batch 222/ 842, training loss 0.3836992681026459\n",
      "Epoch 12 -- Batch 223/ 842, training loss 0.3888257145881653\n",
      "Epoch 12 -- Batch 224/ 842, training loss 0.38706091046333313\n",
      "Epoch 12 -- Batch 225/ 842, training loss 0.3892357647418976\n",
      "Epoch 12 -- Batch 226/ 842, training loss 0.37518003582954407\n",
      "Epoch 12 -- Batch 227/ 842, training loss 0.37691447138786316\n",
      "Epoch 12 -- Batch 228/ 842, training loss 0.3858363628387451\n",
      "Epoch 12 -- Batch 229/ 842, training loss 0.383896142244339\n",
      "Epoch 12 -- Batch 230/ 842, training loss 0.3804647922515869\n",
      "Epoch 12 -- Batch 231/ 842, training loss 0.3804338872432709\n",
      "Epoch 12 -- Batch 232/ 842, training loss 0.38380321860313416\n",
      "Epoch 12 -- Batch 233/ 842, training loss 0.3728023171424866\n",
      "Epoch 12 -- Batch 234/ 842, training loss 0.39262866973876953\n",
      "Epoch 12 -- Batch 235/ 842, training loss 0.36715638637542725\n",
      "Epoch 12 -- Batch 236/ 842, training loss 0.3774397373199463\n",
      "Epoch 12 -- Batch 237/ 842, training loss 0.3882755935192108\n",
      "Epoch 12 -- Batch 238/ 842, training loss 0.3869788944721222\n",
      "Epoch 12 -- Batch 239/ 842, training loss 0.3772331476211548\n",
      "Epoch 12 -- Batch 240/ 842, training loss 0.36594441533088684\n",
      "Epoch 12 -- Batch 241/ 842, training loss 0.3744770586490631\n",
      "Epoch 12 -- Batch 242/ 842, training loss 0.3889729976654053\n",
      "Epoch 12 -- Batch 243/ 842, training loss 0.3835192620754242\n",
      "Epoch 12 -- Batch 244/ 842, training loss 0.393554151058197\n",
      "Epoch 12 -- Batch 245/ 842, training loss 0.3609239459037781\n",
      "Epoch 12 -- Batch 246/ 842, training loss 0.3853360712528229\n",
      "Epoch 12 -- Batch 247/ 842, training loss 0.38224056363105774\n",
      "Epoch 12 -- Batch 248/ 842, training loss 0.3767586052417755\n",
      "Epoch 12 -- Batch 249/ 842, training loss 0.38562628626823425\n",
      "Epoch 12 -- Batch 250/ 842, training loss 0.3843579888343811\n",
      "Epoch 12 -- Batch 251/ 842, training loss 0.37328875064849854\n",
      "Epoch 12 -- Batch 252/ 842, training loss 0.3761485517024994\n",
      "Epoch 12 -- Batch 253/ 842, training loss 0.3724731206893921\n",
      "Epoch 12 -- Batch 254/ 842, training loss 0.38044416904449463\n",
      "Epoch 12 -- Batch 255/ 842, training loss 0.3841109275817871\n",
      "Epoch 12 -- Batch 256/ 842, training loss 0.3919849097728729\n",
      "Epoch 12 -- Batch 257/ 842, training loss 0.37012413144111633\n",
      "Epoch 12 -- Batch 258/ 842, training loss 0.37980934977531433\n",
      "Epoch 12 -- Batch 259/ 842, training loss 0.37298494577407837\n",
      "Epoch 12 -- Batch 260/ 842, training loss 0.3710210621356964\n",
      "Epoch 12 -- Batch 261/ 842, training loss 0.3805728554725647\n",
      "Epoch 12 -- Batch 262/ 842, training loss 0.38425153493881226\n",
      "Epoch 12 -- Batch 263/ 842, training loss 0.36719000339508057\n",
      "Epoch 12 -- Batch 264/ 842, training loss 0.38524389266967773\n",
      "Epoch 12 -- Batch 265/ 842, training loss 0.3706645965576172\n",
      "Epoch 12 -- Batch 266/ 842, training loss 0.3879759609699249\n",
      "Epoch 12 -- Batch 267/ 842, training loss 0.3845149874687195\n",
      "Epoch 12 -- Batch 268/ 842, training loss 0.3868691027164459\n",
      "Epoch 12 -- Batch 269/ 842, training loss 0.3713941276073456\n",
      "Epoch 12 -- Batch 270/ 842, training loss 0.38590261340141296\n",
      "Epoch 12 -- Batch 271/ 842, training loss 0.38615870475769043\n",
      "Epoch 12 -- Batch 272/ 842, training loss 0.40032559633255005\n",
      "Epoch 12 -- Batch 273/ 842, training loss 0.38266703486442566\n",
      "Epoch 12 -- Batch 274/ 842, training loss 0.3651731610298157\n",
      "Epoch 12 -- Batch 275/ 842, training loss 0.3796783685684204\n",
      "Epoch 12 -- Batch 276/ 842, training loss 0.37165364623069763\n",
      "Epoch 12 -- Batch 277/ 842, training loss 0.3623843193054199\n",
      "Epoch 12 -- Batch 278/ 842, training loss 0.3632909953594208\n",
      "Epoch 12 -- Batch 279/ 842, training loss 0.3685629963874817\n",
      "Epoch 12 -- Batch 280/ 842, training loss 0.3827607333660126\n",
      "Epoch 12 -- Batch 281/ 842, training loss 0.3877430856227875\n",
      "Epoch 12 -- Batch 282/ 842, training loss 0.3721417486667633\n",
      "Epoch 12 -- Batch 283/ 842, training loss 0.38748791813850403\n",
      "Epoch 12 -- Batch 284/ 842, training loss 0.3712126612663269\n",
      "Epoch 12 -- Batch 285/ 842, training loss 0.3858747184276581\n",
      "Epoch 12 -- Batch 286/ 842, training loss 0.3901180326938629\n",
      "Epoch 12 -- Batch 287/ 842, training loss 0.3797135651111603\n",
      "Epoch 12 -- Batch 288/ 842, training loss 0.37765219807624817\n",
      "Epoch 12 -- Batch 289/ 842, training loss 0.3857889771461487\n",
      "Epoch 12 -- Batch 290/ 842, training loss 0.38174423575401306\n",
      "Epoch 12 -- Batch 291/ 842, training loss 0.38685688376426697\n",
      "Epoch 12 -- Batch 292/ 842, training loss 0.3797870874404907\n",
      "Epoch 12 -- Batch 293/ 842, training loss 0.38111287355422974\n",
      "Epoch 12 -- Batch 294/ 842, training loss 0.38360291719436646\n",
      "Epoch 12 -- Batch 295/ 842, training loss 0.3879774510860443\n",
      "Epoch 12 -- Batch 296/ 842, training loss 0.3962681293487549\n",
      "Epoch 12 -- Batch 297/ 842, training loss 0.3768492043018341\n",
      "Epoch 12 -- Batch 298/ 842, training loss 0.36771172285079956\n",
      "Epoch 12 -- Batch 299/ 842, training loss 0.37818342447280884\n",
      "Epoch 12 -- Batch 300/ 842, training loss 0.3561471104621887\n",
      "Epoch 12 -- Batch 301/ 842, training loss 0.37899279594421387\n",
      "Epoch 12 -- Batch 302/ 842, training loss 0.3740806579589844\n",
      "Epoch 12 -- Batch 303/ 842, training loss 0.3752981126308441\n",
      "Epoch 12 -- Batch 304/ 842, training loss 0.38226500153541565\n",
      "Epoch 12 -- Batch 305/ 842, training loss 0.38608425855636597\n",
      "Epoch 12 -- Batch 306/ 842, training loss 0.37315094470977783\n",
      "Epoch 12 -- Batch 307/ 842, training loss 0.37547507882118225\n",
      "Epoch 12 -- Batch 308/ 842, training loss 0.3787834942340851\n",
      "Epoch 12 -- Batch 309/ 842, training loss 0.37549445033073425\n",
      "Epoch 12 -- Batch 310/ 842, training loss 0.3702945113182068\n",
      "Epoch 12 -- Batch 311/ 842, training loss 0.3759872317314148\n",
      "Epoch 12 -- Batch 312/ 842, training loss 0.3732677102088928\n",
      "Epoch 12 -- Batch 313/ 842, training loss 0.383810430765152\n",
      "Epoch 12 -- Batch 314/ 842, training loss 0.38922539353370667\n",
      "Epoch 12 -- Batch 315/ 842, training loss 0.3837977647781372\n",
      "Epoch 12 -- Batch 316/ 842, training loss 0.38185569643974304\n",
      "Epoch 12 -- Batch 317/ 842, training loss 0.3686082065105438\n",
      "Epoch 12 -- Batch 318/ 842, training loss 0.3899121582508087\n",
      "Epoch 12 -- Batch 319/ 842, training loss 0.3866077959537506\n",
      "Epoch 12 -- Batch 320/ 842, training loss 0.3758087754249573\n",
      "Epoch 12 -- Batch 321/ 842, training loss 0.37541577219963074\n",
      "Epoch 12 -- Batch 322/ 842, training loss 0.3871079087257385\n",
      "Epoch 12 -- Batch 323/ 842, training loss 0.3741549253463745\n",
      "Epoch 12 -- Batch 324/ 842, training loss 0.379899799823761\n",
      "Epoch 12 -- Batch 325/ 842, training loss 0.39060935378074646\n",
      "Epoch 12 -- Batch 326/ 842, training loss 0.382269024848938\n",
      "Epoch 12 -- Batch 327/ 842, training loss 0.37633758783340454\n",
      "Epoch 12 -- Batch 328/ 842, training loss 0.38207289576530457\n",
      "Epoch 12 -- Batch 329/ 842, training loss 0.38769441843032837\n",
      "Epoch 12 -- Batch 330/ 842, training loss 0.3968496322631836\n",
      "Epoch 12 -- Batch 331/ 842, training loss 0.37653419375419617\n",
      "Epoch 12 -- Batch 332/ 842, training loss 0.38767170906066895\n",
      "Epoch 12 -- Batch 333/ 842, training loss 0.3667721450328827\n",
      "Epoch 12 -- Batch 334/ 842, training loss 0.3682582676410675\n",
      "Epoch 12 -- Batch 335/ 842, training loss 0.37750306725502014\n",
      "Epoch 12 -- Batch 336/ 842, training loss 0.38198626041412354\n",
      "Epoch 12 -- Batch 337/ 842, training loss 0.3751342296600342\n",
      "Epoch 12 -- Batch 338/ 842, training loss 0.3808905780315399\n",
      "Epoch 12 -- Batch 339/ 842, training loss 0.37957069277763367\n",
      "Epoch 12 -- Batch 340/ 842, training loss 0.3769082725048065\n",
      "Epoch 12 -- Batch 341/ 842, training loss 0.3815031945705414\n",
      "Epoch 12 -- Batch 342/ 842, training loss 0.3934139609336853\n",
      "Epoch 12 -- Batch 343/ 842, training loss 0.3791731297969818\n",
      "Epoch 12 -- Batch 344/ 842, training loss 0.388723224401474\n",
      "Epoch 12 -- Batch 345/ 842, training loss 0.3871873915195465\n",
      "Epoch 12 -- Batch 346/ 842, training loss 0.3707225024700165\n",
      "Epoch 12 -- Batch 347/ 842, training loss 0.3738481104373932\n",
      "Epoch 12 -- Batch 348/ 842, training loss 0.39536231756210327\n",
      "Epoch 12 -- Batch 349/ 842, training loss 0.3865542411804199\n",
      "Epoch 12 -- Batch 350/ 842, training loss 0.37229788303375244\n",
      "Epoch 12 -- Batch 351/ 842, training loss 0.3602113127708435\n",
      "Epoch 12 -- Batch 352/ 842, training loss 0.3761087656021118\n",
      "Epoch 12 -- Batch 353/ 842, training loss 0.39992889761924744\n",
      "Epoch 12 -- Batch 354/ 842, training loss 0.39689135551452637\n",
      "Epoch 12 -- Batch 355/ 842, training loss 0.377718985080719\n",
      "Epoch 12 -- Batch 356/ 842, training loss 0.3974972665309906\n",
      "Epoch 12 -- Batch 357/ 842, training loss 0.3882061541080475\n",
      "Epoch 12 -- Batch 358/ 842, training loss 0.38750213384628296\n",
      "Epoch 12 -- Batch 359/ 842, training loss 0.371633917093277\n",
      "Epoch 12 -- Batch 360/ 842, training loss 0.3743101954460144\n",
      "Epoch 12 -- Batch 361/ 842, training loss 0.38365572690963745\n",
      "Epoch 12 -- Batch 362/ 842, training loss 0.3822416663169861\n",
      "Epoch 12 -- Batch 363/ 842, training loss 0.4020465314388275\n",
      "Epoch 12 -- Batch 364/ 842, training loss 0.38658735156059265\n",
      "Epoch 12 -- Batch 365/ 842, training loss 0.38319358229637146\n",
      "Epoch 12 -- Batch 366/ 842, training loss 0.3792518675327301\n",
      "Epoch 12 -- Batch 367/ 842, training loss 0.3863859176635742\n",
      "Epoch 12 -- Batch 368/ 842, training loss 0.382146418094635\n",
      "Epoch 12 -- Batch 369/ 842, training loss 0.39047619700431824\n",
      "Epoch 12 -- Batch 370/ 842, training loss 0.3907260000705719\n",
      "Epoch 12 -- Batch 371/ 842, training loss 0.3949628174304962\n",
      "Epoch 12 -- Batch 372/ 842, training loss 0.3799193203449249\n",
      "Epoch 12 -- Batch 373/ 842, training loss 0.3664046823978424\n",
      "Epoch 12 -- Batch 374/ 842, training loss 0.38595959544181824\n",
      "Epoch 12 -- Batch 375/ 842, training loss 0.37098008394241333\n",
      "Epoch 12 -- Batch 376/ 842, training loss 0.38673707842826843\n",
      "Epoch 12 -- Batch 377/ 842, training loss 0.3822392523288727\n",
      "Epoch 12 -- Batch 378/ 842, training loss 0.38011792302131653\n",
      "Epoch 12 -- Batch 379/ 842, training loss 0.374432772397995\n",
      "Epoch 12 -- Batch 380/ 842, training loss 0.38171520829200745\n",
      "Epoch 12 -- Batch 381/ 842, training loss 0.37083637714385986\n",
      "Epoch 12 -- Batch 382/ 842, training loss 0.3696993887424469\n",
      "Epoch 12 -- Batch 383/ 842, training loss 0.3765210509300232\n",
      "Epoch 12 -- Batch 384/ 842, training loss 0.37153056263923645\n",
      "Epoch 12 -- Batch 385/ 842, training loss 0.3734327256679535\n",
      "Epoch 12 -- Batch 386/ 842, training loss 0.3791169226169586\n",
      "Epoch 12 -- Batch 387/ 842, training loss 0.3729186952114105\n",
      "Epoch 12 -- Batch 388/ 842, training loss 0.37881141901016235\n",
      "Epoch 12 -- Batch 389/ 842, training loss 0.38673657178878784\n",
      "Epoch 12 -- Batch 390/ 842, training loss 0.4005894064903259\n",
      "Epoch 12 -- Batch 391/ 842, training loss 0.368379682302475\n",
      "Epoch 12 -- Batch 392/ 842, training loss 0.3849310576915741\n",
      "Epoch 12 -- Batch 393/ 842, training loss 0.37327101826667786\n",
      "Epoch 12 -- Batch 394/ 842, training loss 0.3821076452732086\n",
      "Epoch 12 -- Batch 395/ 842, training loss 0.37480589747428894\n",
      "Epoch 12 -- Batch 396/ 842, training loss 0.36885562539100647\n",
      "Epoch 12 -- Batch 397/ 842, training loss 0.3866429626941681\n",
      "Epoch 12 -- Batch 398/ 842, training loss 0.36781448125839233\n",
      "Epoch 12 -- Batch 399/ 842, training loss 0.37980300188064575\n",
      "Epoch 12 -- Batch 400/ 842, training loss 0.38424041867256165\n",
      "Epoch 12 -- Batch 401/ 842, training loss 0.3760622441768646\n",
      "Epoch 12 -- Batch 402/ 842, training loss 0.37094682455062866\n",
      "Epoch 12 -- Batch 403/ 842, training loss 0.39054661989212036\n",
      "Epoch 12 -- Batch 404/ 842, training loss 0.380874902009964\n",
      "Epoch 12 -- Batch 405/ 842, training loss 0.36213117837905884\n",
      "Epoch 12 -- Batch 406/ 842, training loss 0.3813532888889313\n",
      "Epoch 12 -- Batch 407/ 842, training loss 0.39007917046546936\n",
      "Epoch 12 -- Batch 408/ 842, training loss 0.36925819516181946\n",
      "Epoch 12 -- Batch 409/ 842, training loss 0.3744674623012543\n",
      "Epoch 12 -- Batch 410/ 842, training loss 0.3831841051578522\n",
      "Epoch 12 -- Batch 411/ 842, training loss 0.38694480061531067\n",
      "Epoch 12 -- Batch 412/ 842, training loss 0.4022299349308014\n",
      "Epoch 12 -- Batch 413/ 842, training loss 0.3699217736721039\n",
      "Epoch 12 -- Batch 414/ 842, training loss 0.3870905041694641\n",
      "Epoch 12 -- Batch 415/ 842, training loss 0.3882012963294983\n",
      "Epoch 12 -- Batch 416/ 842, training loss 0.39641308784484863\n",
      "Epoch 12 -- Batch 417/ 842, training loss 0.38106465339660645\n",
      "Epoch 12 -- Batch 418/ 842, training loss 0.36628472805023193\n",
      "Epoch 12 -- Batch 419/ 842, training loss 0.3738657534122467\n",
      "Epoch 12 -- Batch 420/ 842, training loss 0.3742501437664032\n",
      "Epoch 12 -- Batch 421/ 842, training loss 0.3903011977672577\n",
      "Epoch 12 -- Batch 422/ 842, training loss 0.3874833881855011\n",
      "Epoch 12 -- Batch 423/ 842, training loss 0.370448499917984\n",
      "Epoch 12 -- Batch 424/ 842, training loss 0.3918248414993286\n",
      "Epoch 12 -- Batch 425/ 842, training loss 0.37896251678466797\n",
      "Epoch 12 -- Batch 426/ 842, training loss 0.37281477451324463\n",
      "Epoch 12 -- Batch 427/ 842, training loss 0.3902674913406372\n",
      "Epoch 12 -- Batch 428/ 842, training loss 0.3849913775920868\n",
      "Epoch 12 -- Batch 429/ 842, training loss 0.37471824884414673\n",
      "Epoch 12 -- Batch 430/ 842, training loss 0.38141071796417236\n",
      "Epoch 12 -- Batch 431/ 842, training loss 0.3872072100639343\n",
      "Epoch 12 -- Batch 432/ 842, training loss 0.3859753906726837\n",
      "Epoch 12 -- Batch 433/ 842, training loss 0.36899957060813904\n",
      "Epoch 12 -- Batch 434/ 842, training loss 0.37859296798706055\n",
      "Epoch 12 -- Batch 435/ 842, training loss 0.37900832295417786\n",
      "Epoch 12 -- Batch 436/ 842, training loss 0.3835732936859131\n",
      "Epoch 12 -- Batch 437/ 842, training loss 0.37309956550598145\n",
      "Epoch 12 -- Batch 438/ 842, training loss 0.38837820291519165\n",
      "Epoch 12 -- Batch 439/ 842, training loss 0.3775561451911926\n",
      "Epoch 12 -- Batch 440/ 842, training loss 0.37522023916244507\n",
      "Epoch 12 -- Batch 441/ 842, training loss 0.37155452370643616\n",
      "Epoch 12 -- Batch 442/ 842, training loss 0.38918164372444153\n",
      "Epoch 12 -- Batch 443/ 842, training loss 0.3950985372066498\n",
      "Epoch 12 -- Batch 444/ 842, training loss 0.37802639603614807\n",
      "Epoch 12 -- Batch 445/ 842, training loss 0.38340166211128235\n",
      "Epoch 12 -- Batch 446/ 842, training loss 0.382371723651886\n",
      "Epoch 12 -- Batch 447/ 842, training loss 0.3909618854522705\n",
      "Epoch 12 -- Batch 448/ 842, training loss 0.3813113868236542\n",
      "Epoch 12 -- Batch 449/ 842, training loss 0.3658505380153656\n",
      "Epoch 12 -- Batch 450/ 842, training loss 0.36956098675727844\n",
      "Epoch 12 -- Batch 451/ 842, training loss 0.3712359070777893\n",
      "Epoch 12 -- Batch 452/ 842, training loss 0.3896952271461487\n",
      "Epoch 12 -- Batch 453/ 842, training loss 0.38054898381233215\n",
      "Epoch 12 -- Batch 454/ 842, training loss 0.3734824061393738\n",
      "Epoch 12 -- Batch 455/ 842, training loss 0.3700252175331116\n",
      "Epoch 12 -- Batch 456/ 842, training loss 0.38024526834487915\n",
      "Epoch 12 -- Batch 457/ 842, training loss 0.38495030999183655\n",
      "Epoch 12 -- Batch 458/ 842, training loss 0.38285359740257263\n",
      "Epoch 12 -- Batch 459/ 842, training loss 0.37432631850242615\n",
      "Epoch 12 -- Batch 460/ 842, training loss 0.37714552879333496\n",
      "Epoch 12 -- Batch 461/ 842, training loss 0.3719463348388672\n",
      "Epoch 12 -- Batch 462/ 842, training loss 0.3755113184452057\n",
      "Epoch 12 -- Batch 463/ 842, training loss 0.3728748559951782\n",
      "Epoch 12 -- Batch 464/ 842, training loss 0.3731565773487091\n",
      "Epoch 12 -- Batch 465/ 842, training loss 0.3756831884384155\n",
      "Epoch 12 -- Batch 466/ 842, training loss 0.38757118582725525\n",
      "Epoch 12 -- Batch 467/ 842, training loss 0.38792169094085693\n",
      "Epoch 12 -- Batch 468/ 842, training loss 0.388215571641922\n",
      "Epoch 12 -- Batch 469/ 842, training loss 0.3831477463245392\n",
      "Epoch 12 -- Batch 470/ 842, training loss 0.37235185503959656\n",
      "Epoch 12 -- Batch 471/ 842, training loss 0.37535881996154785\n",
      "Epoch 12 -- Batch 472/ 842, training loss 0.38210949301719666\n",
      "Epoch 12 -- Batch 473/ 842, training loss 0.38675886392593384\n",
      "Epoch 12 -- Batch 474/ 842, training loss 0.38334476947784424\n",
      "Epoch 12 -- Batch 475/ 842, training loss 0.3805609345436096\n",
      "Epoch 12 -- Batch 476/ 842, training loss 0.3793432116508484\n",
      "Epoch 12 -- Batch 477/ 842, training loss 0.37836676836013794\n",
      "Epoch 12 -- Batch 478/ 842, training loss 0.3662307560443878\n",
      "Epoch 12 -- Batch 479/ 842, training loss 0.37193194031715393\n",
      "Epoch 12 -- Batch 480/ 842, training loss 0.37596744298934937\n",
      "Epoch 12 -- Batch 481/ 842, training loss 0.37513795495033264\n",
      "Epoch 12 -- Batch 482/ 842, training loss 0.38754355907440186\n",
      "Epoch 12 -- Batch 483/ 842, training loss 0.37865325808525085\n",
      "Epoch 12 -- Batch 484/ 842, training loss 0.3777199685573578\n",
      "Epoch 12 -- Batch 485/ 842, training loss 0.3682246506214142\n",
      "Epoch 12 -- Batch 486/ 842, training loss 0.379840224981308\n",
      "Epoch 12 -- Batch 487/ 842, training loss 0.39519965648651123\n",
      "Epoch 12 -- Batch 488/ 842, training loss 0.3836187422275543\n",
      "Epoch 12 -- Batch 489/ 842, training loss 0.38919809460639954\n",
      "Epoch 12 -- Batch 490/ 842, training loss 0.3877549171447754\n",
      "Epoch 12 -- Batch 491/ 842, training loss 0.38439279794692993\n",
      "Epoch 12 -- Batch 492/ 842, training loss 0.36997756361961365\n",
      "Epoch 12 -- Batch 493/ 842, training loss 0.37303873896598816\n",
      "Epoch 12 -- Batch 494/ 842, training loss 0.3808290660381317\n",
      "Epoch 12 -- Batch 495/ 842, training loss 0.3604393005371094\n",
      "Epoch 12 -- Batch 496/ 842, training loss 0.38597285747528076\n",
      "Epoch 12 -- Batch 497/ 842, training loss 0.37616026401519775\n",
      "Epoch 12 -- Batch 498/ 842, training loss 0.3838481307029724\n",
      "Epoch 12 -- Batch 499/ 842, training loss 0.3794431686401367\n",
      "Epoch 12 -- Batch 500/ 842, training loss 0.3922066390514374\n",
      "Epoch 12 -- Batch 501/ 842, training loss 0.37035998702049255\n",
      "Epoch 12 -- Batch 502/ 842, training loss 0.3796343207359314\n",
      "Epoch 12 -- Batch 503/ 842, training loss 0.37473317980766296\n",
      "Epoch 12 -- Batch 504/ 842, training loss 0.3745606541633606\n",
      "Epoch 12 -- Batch 505/ 842, training loss 0.3785557746887207\n",
      "Epoch 12 -- Batch 506/ 842, training loss 0.3937980532646179\n",
      "Epoch 12 -- Batch 507/ 842, training loss 0.3789137303829193\n",
      "Epoch 12 -- Batch 508/ 842, training loss 0.3673875033855438\n",
      "Epoch 12 -- Batch 509/ 842, training loss 0.38223403692245483\n",
      "Epoch 12 -- Batch 510/ 842, training loss 0.3650378882884979\n",
      "Epoch 12 -- Batch 511/ 842, training loss 0.3731481730937958\n",
      "Epoch 12 -- Batch 512/ 842, training loss 0.3620500862598419\n",
      "Epoch 12 -- Batch 513/ 842, training loss 0.36623290181159973\n",
      "Epoch 12 -- Batch 514/ 842, training loss 0.3671436905860901\n",
      "Epoch 12 -- Batch 515/ 842, training loss 0.3825400769710541\n",
      "Epoch 12 -- Batch 516/ 842, training loss 0.38823533058166504\n",
      "Epoch 12 -- Batch 517/ 842, training loss 0.3714395761489868\n",
      "Epoch 12 -- Batch 518/ 842, training loss 0.36745843291282654\n",
      "Epoch 12 -- Batch 519/ 842, training loss 0.3746626675128937\n",
      "Epoch 12 -- Batch 520/ 842, training loss 0.3819716274738312\n",
      "Epoch 12 -- Batch 521/ 842, training loss 0.3901001811027527\n",
      "Epoch 12 -- Batch 522/ 842, training loss 0.3803598880767822\n",
      "Epoch 12 -- Batch 523/ 842, training loss 0.40169814229011536\n",
      "Epoch 12 -- Batch 524/ 842, training loss 0.3821815550327301\n",
      "Epoch 12 -- Batch 525/ 842, training loss 0.37830650806427\n",
      "Epoch 12 -- Batch 526/ 842, training loss 0.39222022891044617\n",
      "Epoch 12 -- Batch 527/ 842, training loss 0.3754759132862091\n",
      "Epoch 12 -- Batch 528/ 842, training loss 0.3816220462322235\n",
      "Epoch 12 -- Batch 529/ 842, training loss 0.3966069221496582\n",
      "Epoch 12 -- Batch 530/ 842, training loss 0.382234662771225\n",
      "Epoch 12 -- Batch 531/ 842, training loss 0.3820619583129883\n",
      "Epoch 12 -- Batch 532/ 842, training loss 0.3858647048473358\n",
      "Epoch 12 -- Batch 533/ 842, training loss 0.3738173544406891\n",
      "Epoch 12 -- Batch 534/ 842, training loss 0.3762800395488739\n",
      "Epoch 12 -- Batch 535/ 842, training loss 0.3694653809070587\n",
      "Epoch 12 -- Batch 536/ 842, training loss 0.3893289566040039\n",
      "Epoch 12 -- Batch 537/ 842, training loss 0.39428266882896423\n",
      "Epoch 12 -- Batch 538/ 842, training loss 0.37817421555519104\n",
      "Epoch 12 -- Batch 539/ 842, training loss 0.39259088039398193\n",
      "Epoch 12 -- Batch 540/ 842, training loss 0.39298519492149353\n",
      "Epoch 12 -- Batch 541/ 842, training loss 0.3798508942127228\n",
      "Epoch 12 -- Batch 542/ 842, training loss 0.401875764131546\n",
      "Epoch 12 -- Batch 543/ 842, training loss 0.3855232894420624\n",
      "Epoch 12 -- Batch 544/ 842, training loss 0.38614732027053833\n",
      "Epoch 12 -- Batch 545/ 842, training loss 0.3717939555644989\n",
      "Epoch 12 -- Batch 546/ 842, training loss 0.3757655620574951\n",
      "Epoch 12 -- Batch 547/ 842, training loss 0.37922608852386475\n",
      "Epoch 12 -- Batch 548/ 842, training loss 0.3811109662055969\n",
      "Epoch 12 -- Batch 549/ 842, training loss 0.3903018534183502\n",
      "Epoch 12 -- Batch 550/ 842, training loss 0.3742105960845947\n",
      "Epoch 12 -- Batch 551/ 842, training loss 0.37664055824279785\n",
      "Epoch 12 -- Batch 552/ 842, training loss 0.38273531198501587\n",
      "Epoch 12 -- Batch 553/ 842, training loss 0.37112098932266235\n",
      "Epoch 12 -- Batch 554/ 842, training loss 0.36524301767349243\n",
      "Epoch 12 -- Batch 555/ 842, training loss 0.38152626156806946\n",
      "Epoch 12 -- Batch 556/ 842, training loss 0.39170435070991516\n",
      "Epoch 12 -- Batch 557/ 842, training loss 0.37404564023017883\n",
      "Epoch 12 -- Batch 558/ 842, training loss 0.3962589204311371\n",
      "Epoch 12 -- Batch 559/ 842, training loss 0.39042702317237854\n",
      "Epoch 12 -- Batch 560/ 842, training loss 0.3838014602661133\n",
      "Epoch 12 -- Batch 561/ 842, training loss 0.3649440109729767\n",
      "Epoch 12 -- Batch 562/ 842, training loss 0.3949410021305084\n",
      "Epoch 12 -- Batch 563/ 842, training loss 0.38581517338752747\n",
      "Epoch 12 -- Batch 564/ 842, training loss 0.41097643971443176\n",
      "Epoch 12 -- Batch 565/ 842, training loss 0.37985363602638245\n",
      "Epoch 12 -- Batch 566/ 842, training loss 0.38120901584625244\n",
      "Epoch 12 -- Batch 567/ 842, training loss 0.37481018900871277\n",
      "Epoch 12 -- Batch 568/ 842, training loss 0.3913022577762604\n",
      "Epoch 12 -- Batch 569/ 842, training loss 0.37959110736846924\n",
      "Epoch 12 -- Batch 570/ 842, training loss 0.3824772238731384\n",
      "Epoch 12 -- Batch 571/ 842, training loss 0.380944162607193\n",
      "Epoch 12 -- Batch 572/ 842, training loss 0.35686519742012024\n",
      "Epoch 12 -- Batch 573/ 842, training loss 0.3788340091705322\n",
      "Epoch 12 -- Batch 574/ 842, training loss 0.38729602098464966\n",
      "Epoch 12 -- Batch 575/ 842, training loss 0.37328478693962097\n",
      "Epoch 12 -- Batch 576/ 842, training loss 0.3688829243183136\n",
      "Epoch 12 -- Batch 577/ 842, training loss 0.39415687322616577\n",
      "Epoch 12 -- Batch 578/ 842, training loss 0.3799589276313782\n",
      "Epoch 12 -- Batch 579/ 842, training loss 0.37086784839630127\n",
      "Epoch 12 -- Batch 580/ 842, training loss 0.36545687913894653\n",
      "Epoch 12 -- Batch 581/ 842, training loss 0.3861490786075592\n",
      "Epoch 12 -- Batch 582/ 842, training loss 0.38016170263290405\n",
      "Epoch 12 -- Batch 583/ 842, training loss 0.3878163993358612\n",
      "Epoch 12 -- Batch 584/ 842, training loss 0.3889486789703369\n",
      "Epoch 12 -- Batch 585/ 842, training loss 0.38221505284309387\n",
      "Epoch 12 -- Batch 586/ 842, training loss 0.3934682309627533\n",
      "Epoch 12 -- Batch 587/ 842, training loss 0.38298723101615906\n",
      "Epoch 12 -- Batch 588/ 842, training loss 0.39033403992652893\n",
      "Epoch 12 -- Batch 589/ 842, training loss 0.39328533411026\n",
      "Epoch 12 -- Batch 590/ 842, training loss 0.3748190999031067\n",
      "Epoch 12 -- Batch 591/ 842, training loss 0.3727548122406006\n",
      "Epoch 12 -- Batch 592/ 842, training loss 0.36640501022338867\n",
      "Epoch 12 -- Batch 593/ 842, training loss 0.38935229182243347\n",
      "Epoch 12 -- Batch 594/ 842, training loss 0.39221370220184326\n",
      "Epoch 12 -- Batch 595/ 842, training loss 0.37349995970726013\n",
      "Epoch 12 -- Batch 596/ 842, training loss 0.39804404973983765\n",
      "Epoch 12 -- Batch 597/ 842, training loss 0.39265507459640503\n",
      "Epoch 12 -- Batch 598/ 842, training loss 0.3832019865512848\n",
      "Epoch 12 -- Batch 599/ 842, training loss 0.39247021079063416\n",
      "Epoch 12 -- Batch 600/ 842, training loss 0.3810575306415558\n",
      "Epoch 12 -- Batch 601/ 842, training loss 0.38064202666282654\n",
      "Epoch 12 -- Batch 602/ 842, training loss 0.3928300440311432\n",
      "Epoch 12 -- Batch 603/ 842, training loss 0.3790421783924103\n",
      "Epoch 12 -- Batch 604/ 842, training loss 0.367614209651947\n",
      "Epoch 12 -- Batch 605/ 842, training loss 0.39050349593162537\n",
      "Epoch 12 -- Batch 606/ 842, training loss 0.3788832426071167\n",
      "Epoch 12 -- Batch 607/ 842, training loss 0.3808406889438629\n",
      "Epoch 12 -- Batch 608/ 842, training loss 0.37668517231941223\n",
      "Epoch 12 -- Batch 609/ 842, training loss 0.3689907193183899\n",
      "Epoch 12 -- Batch 610/ 842, training loss 0.39587485790252686\n",
      "Epoch 12 -- Batch 611/ 842, training loss 0.3852307200431824\n",
      "Epoch 12 -- Batch 612/ 842, training loss 0.3824943006038666\n",
      "Epoch 12 -- Batch 613/ 842, training loss 0.3638334572315216\n",
      "Epoch 12 -- Batch 614/ 842, training loss 0.3938726782798767\n",
      "Epoch 12 -- Batch 615/ 842, training loss 0.3855009377002716\n",
      "Epoch 12 -- Batch 616/ 842, training loss 0.3893502354621887\n",
      "Epoch 12 -- Batch 617/ 842, training loss 0.3814241290092468\n",
      "Epoch 12 -- Batch 618/ 842, training loss 0.38277679681777954\n",
      "Epoch 12 -- Batch 619/ 842, training loss 0.38432568311691284\n",
      "Epoch 12 -- Batch 620/ 842, training loss 0.377799391746521\n",
      "Epoch 12 -- Batch 621/ 842, training loss 0.3876883089542389\n",
      "Epoch 12 -- Batch 622/ 842, training loss 0.3775647282600403\n",
      "Epoch 12 -- Batch 623/ 842, training loss 0.3806999921798706\n",
      "Epoch 12 -- Batch 624/ 842, training loss 0.37388285994529724\n",
      "Epoch 12 -- Batch 625/ 842, training loss 0.3824631869792938\n",
      "Epoch 12 -- Batch 626/ 842, training loss 0.39785683155059814\n",
      "Epoch 12 -- Batch 627/ 842, training loss 0.38572466373443604\n",
      "Epoch 12 -- Batch 628/ 842, training loss 0.3750731945037842\n",
      "Epoch 12 -- Batch 629/ 842, training loss 0.38567039370536804\n",
      "Epoch 12 -- Batch 630/ 842, training loss 0.3739921748638153\n",
      "Epoch 12 -- Batch 631/ 842, training loss 0.3769310414791107\n",
      "Epoch 12 -- Batch 632/ 842, training loss 0.38596293330192566\n",
      "Epoch 12 -- Batch 633/ 842, training loss 0.38232338428497314\n",
      "Epoch 12 -- Batch 634/ 842, training loss 0.37975871562957764\n",
      "Epoch 12 -- Batch 635/ 842, training loss 0.37759360671043396\n",
      "Epoch 12 -- Batch 636/ 842, training loss 0.37218159437179565\n",
      "Epoch 12 -- Batch 637/ 842, training loss 0.38743695616722107\n",
      "Epoch 12 -- Batch 638/ 842, training loss 0.3944476246833801\n",
      "Epoch 12 -- Batch 639/ 842, training loss 0.3673970103263855\n",
      "Epoch 12 -- Batch 640/ 842, training loss 0.39120885729789734\n",
      "Epoch 12 -- Batch 641/ 842, training loss 0.3777956962585449\n",
      "Epoch 12 -- Batch 642/ 842, training loss 0.3955188989639282\n",
      "Epoch 12 -- Batch 643/ 842, training loss 0.38500136137008667\n",
      "Epoch 12 -- Batch 644/ 842, training loss 0.37898120284080505\n",
      "Epoch 12 -- Batch 645/ 842, training loss 0.3856333792209625\n",
      "Epoch 12 -- Batch 646/ 842, training loss 0.3728641867637634\n",
      "Epoch 12 -- Batch 647/ 842, training loss 0.37868958711624146\n",
      "Epoch 12 -- Batch 648/ 842, training loss 0.3803248107433319\n",
      "Epoch 12 -- Batch 649/ 842, training loss 0.39337053894996643\n",
      "Epoch 12 -- Batch 650/ 842, training loss 0.3860395550727844\n",
      "Epoch 12 -- Batch 651/ 842, training loss 0.37090834975242615\n",
      "Epoch 12 -- Batch 652/ 842, training loss 0.38810575008392334\n",
      "Epoch 12 -- Batch 653/ 842, training loss 0.3818068504333496\n",
      "Epoch 12 -- Batch 654/ 842, training loss 0.391513466835022\n",
      "Epoch 12 -- Batch 655/ 842, training loss 0.38713112473487854\n",
      "Epoch 12 -- Batch 656/ 842, training loss 0.39448311924934387\n",
      "Epoch 12 -- Batch 657/ 842, training loss 0.3980756402015686\n",
      "Epoch 12 -- Batch 658/ 842, training loss 0.39621132612228394\n",
      "Epoch 12 -- Batch 659/ 842, training loss 0.4019211530685425\n",
      "Epoch 12 -- Batch 660/ 842, training loss 0.37343713641166687\n",
      "Epoch 12 -- Batch 661/ 842, training loss 0.3831564784049988\n",
      "Epoch 12 -- Batch 662/ 842, training loss 0.3728262484073639\n",
      "Epoch 12 -- Batch 663/ 842, training loss 0.38332292437553406\n",
      "Epoch 12 -- Batch 664/ 842, training loss 0.37356677651405334\n",
      "Epoch 12 -- Batch 665/ 842, training loss 0.365133136510849\n",
      "Epoch 12 -- Batch 666/ 842, training loss 0.38495105504989624\n",
      "Epoch 12 -- Batch 667/ 842, training loss 0.3756697475910187\n",
      "Epoch 12 -- Batch 668/ 842, training loss 0.3729371130466461\n",
      "Epoch 12 -- Batch 669/ 842, training loss 0.3751846253871918\n",
      "Epoch 12 -- Batch 670/ 842, training loss 0.3695104122161865\n",
      "Epoch 12 -- Batch 671/ 842, training loss 0.38377824425697327\n",
      "Epoch 12 -- Batch 672/ 842, training loss 0.37674134969711304\n",
      "Epoch 12 -- Batch 673/ 842, training loss 0.3788306713104248\n",
      "Epoch 12 -- Batch 674/ 842, training loss 0.3844296932220459\n",
      "Epoch 12 -- Batch 675/ 842, training loss 0.3799251914024353\n",
      "Epoch 12 -- Batch 676/ 842, training loss 0.37671607732772827\n",
      "Epoch 12 -- Batch 677/ 842, training loss 0.3874785602092743\n",
      "Epoch 12 -- Batch 678/ 842, training loss 0.37904444336891174\n",
      "Epoch 12 -- Batch 679/ 842, training loss 0.3779774606227875\n",
      "Epoch 12 -- Batch 680/ 842, training loss 0.3810199797153473\n",
      "Epoch 12 -- Batch 681/ 842, training loss 0.37709155678749084\n",
      "Epoch 12 -- Batch 682/ 842, training loss 0.382723867893219\n",
      "Epoch 12 -- Batch 683/ 842, training loss 0.37121161818504333\n",
      "Epoch 12 -- Batch 684/ 842, training loss 0.3860743045806885\n",
      "Epoch 12 -- Batch 685/ 842, training loss 0.3733679950237274\n",
      "Epoch 12 -- Batch 686/ 842, training loss 0.3782604932785034\n",
      "Epoch 12 -- Batch 687/ 842, training loss 0.38605666160583496\n",
      "Epoch 12 -- Batch 688/ 842, training loss 0.3887014389038086\n",
      "Epoch 12 -- Batch 689/ 842, training loss 0.38186222314834595\n",
      "Epoch 12 -- Batch 690/ 842, training loss 0.39114075899124146\n",
      "Epoch 12 -- Batch 691/ 842, training loss 0.37119513750076294\n",
      "Epoch 12 -- Batch 692/ 842, training loss 0.3794008195400238\n",
      "Epoch 12 -- Batch 693/ 842, training loss 0.38116738200187683\n",
      "Epoch 12 -- Batch 694/ 842, training loss 0.3822780251502991\n",
      "Epoch 12 -- Batch 695/ 842, training loss 0.3938688039779663\n",
      "Epoch 12 -- Batch 696/ 842, training loss 0.39017170667648315\n",
      "Epoch 12 -- Batch 697/ 842, training loss 0.36584270000457764\n",
      "Epoch 12 -- Batch 698/ 842, training loss 0.3969500660896301\n",
      "Epoch 12 -- Batch 699/ 842, training loss 0.39260777831077576\n",
      "Epoch 12 -- Batch 700/ 842, training loss 0.3924887478351593\n",
      "Epoch 12 -- Batch 701/ 842, training loss 0.4031563997268677\n",
      "Epoch 12 -- Batch 702/ 842, training loss 0.3840489387512207\n",
      "Epoch 12 -- Batch 703/ 842, training loss 0.3900398015975952\n",
      "Epoch 12 -- Batch 704/ 842, training loss 0.3807604908943176\n",
      "Epoch 12 -- Batch 705/ 842, training loss 0.3953007757663727\n",
      "Epoch 12 -- Batch 706/ 842, training loss 0.3907265365123749\n",
      "Epoch 12 -- Batch 707/ 842, training loss 0.375399112701416\n",
      "Epoch 12 -- Batch 708/ 842, training loss 0.38259267807006836\n",
      "Epoch 12 -- Batch 709/ 842, training loss 0.3766022026538849\n",
      "Epoch 12 -- Batch 710/ 842, training loss 0.3807885944843292\n",
      "Epoch 12 -- Batch 711/ 842, training loss 0.3845968544483185\n",
      "Epoch 12 -- Batch 712/ 842, training loss 0.38960516452789307\n",
      "Epoch 12 -- Batch 713/ 842, training loss 0.38855424523353577\n",
      "Epoch 12 -- Batch 714/ 842, training loss 0.396098792552948\n",
      "Epoch 12 -- Batch 715/ 842, training loss 0.3773967921733856\n",
      "Epoch 12 -- Batch 716/ 842, training loss 0.36547785997390747\n",
      "Epoch 12 -- Batch 717/ 842, training loss 0.38318151235580444\n",
      "Epoch 12 -- Batch 718/ 842, training loss 0.3658309578895569\n",
      "Epoch 12 -- Batch 719/ 842, training loss 0.40182778239250183\n",
      "Epoch 12 -- Batch 720/ 842, training loss 0.3720009922981262\n",
      "Epoch 12 -- Batch 721/ 842, training loss 0.3911724388599396\n",
      "Epoch 12 -- Batch 722/ 842, training loss 0.3813389837741852\n",
      "Epoch 12 -- Batch 723/ 842, training loss 0.37786737084388733\n",
      "Epoch 12 -- Batch 724/ 842, training loss 0.3833172917366028\n",
      "Epoch 12 -- Batch 725/ 842, training loss 0.394731342792511\n",
      "Epoch 12 -- Batch 726/ 842, training loss 0.3787958323955536\n",
      "Epoch 12 -- Batch 727/ 842, training loss 0.387167751789093\n",
      "Epoch 12 -- Batch 728/ 842, training loss 0.3828042447566986\n",
      "Epoch 12 -- Batch 729/ 842, training loss 0.37380078434944153\n",
      "Epoch 12 -- Batch 730/ 842, training loss 0.3823259472846985\n",
      "Epoch 12 -- Batch 731/ 842, training loss 0.39385560154914856\n",
      "Epoch 12 -- Batch 732/ 842, training loss 0.3718768358230591\n",
      "Epoch 12 -- Batch 733/ 842, training loss 0.36978772282600403\n",
      "Epoch 12 -- Batch 734/ 842, training loss 0.3795356750488281\n",
      "Epoch 12 -- Batch 735/ 842, training loss 0.3769851624965668\n",
      "Epoch 12 -- Batch 736/ 842, training loss 0.38439199328422546\n",
      "Epoch 12 -- Batch 737/ 842, training loss 0.3810874819755554\n",
      "Epoch 12 -- Batch 738/ 842, training loss 0.39304715394973755\n",
      "Epoch 12 -- Batch 739/ 842, training loss 0.382489413022995\n",
      "Epoch 12 -- Batch 740/ 842, training loss 0.36448103189468384\n",
      "Epoch 12 -- Batch 741/ 842, training loss 0.3895469009876251\n",
      "Epoch 12 -- Batch 742/ 842, training loss 0.39705461263656616\n",
      "Epoch 12 -- Batch 743/ 842, training loss 0.39392808079719543\n",
      "Epoch 12 -- Batch 744/ 842, training loss 0.38118603825569153\n",
      "Epoch 12 -- Batch 745/ 842, training loss 0.38876235485076904\n",
      "Epoch 12 -- Batch 746/ 842, training loss 0.37794262170791626\n",
      "Epoch 12 -- Batch 747/ 842, training loss 0.37425661087036133\n",
      "Epoch 12 -- Batch 748/ 842, training loss 0.3692830801010132\n",
      "Epoch 12 -- Batch 749/ 842, training loss 0.3821988105773926\n",
      "Epoch 12 -- Batch 750/ 842, training loss 0.38159576058387756\n",
      "Epoch 12 -- Batch 751/ 842, training loss 0.3728233575820923\n",
      "Epoch 12 -- Batch 752/ 842, training loss 0.3707340955734253\n",
      "Epoch 12 -- Batch 753/ 842, training loss 0.3751133978366852\n",
      "Epoch 12 -- Batch 754/ 842, training loss 0.39043188095092773\n",
      "Epoch 12 -- Batch 755/ 842, training loss 0.37444356083869934\n",
      "Epoch 12 -- Batch 756/ 842, training loss 0.39649856090545654\n",
      "Epoch 12 -- Batch 757/ 842, training loss 0.38230013847351074\n",
      "Epoch 12 -- Batch 758/ 842, training loss 0.3797491788864136\n",
      "Epoch 12 -- Batch 759/ 842, training loss 0.38361096382141113\n",
      "Epoch 12 -- Batch 760/ 842, training loss 0.3945973813533783\n",
      "Epoch 12 -- Batch 761/ 842, training loss 0.38026151061058044\n",
      "Epoch 12 -- Batch 762/ 842, training loss 0.38374122977256775\n",
      "Epoch 12 -- Batch 763/ 842, training loss 0.381030797958374\n",
      "Epoch 12 -- Batch 764/ 842, training loss 0.3694484829902649\n",
      "Epoch 12 -- Batch 765/ 842, training loss 0.37857785820961\n",
      "Epoch 12 -- Batch 766/ 842, training loss 0.3872612416744232\n",
      "Epoch 12 -- Batch 767/ 842, training loss 0.38631340861320496\n",
      "Epoch 12 -- Batch 768/ 842, training loss 0.38320687413215637\n",
      "Epoch 12 -- Batch 769/ 842, training loss 0.3618524372577667\n",
      "Epoch 12 -- Batch 770/ 842, training loss 0.38672465085983276\n",
      "Epoch 12 -- Batch 771/ 842, training loss 0.3772747218608856\n",
      "Epoch 12 -- Batch 772/ 842, training loss 0.3835488259792328\n",
      "Epoch 12 -- Batch 773/ 842, training loss 0.38956692814826965\n",
      "Epoch 12 -- Batch 774/ 842, training loss 0.37209752202033997\n",
      "Epoch 12 -- Batch 775/ 842, training loss 0.3825327157974243\n",
      "Epoch 12 -- Batch 776/ 842, training loss 0.37977147102355957\n",
      "Epoch 12 -- Batch 777/ 842, training loss 0.4045467972755432\n",
      "Epoch 12 -- Batch 778/ 842, training loss 0.3801998794078827\n",
      "Epoch 12 -- Batch 779/ 842, training loss 0.37747305631637573\n",
      "Epoch 12 -- Batch 780/ 842, training loss 0.37637636065483093\n",
      "Epoch 12 -- Batch 781/ 842, training loss 0.3955555856227875\n",
      "Epoch 12 -- Batch 782/ 842, training loss 0.3741682767868042\n",
      "Epoch 12 -- Batch 783/ 842, training loss 0.3857218325138092\n",
      "Epoch 12 -- Batch 784/ 842, training loss 0.39399322867393494\n",
      "Epoch 12 -- Batch 785/ 842, training loss 0.3616522550582886\n",
      "Epoch 12 -- Batch 786/ 842, training loss 0.396867960691452\n",
      "Epoch 12 -- Batch 787/ 842, training loss 0.39503294229507446\n",
      "Epoch 12 -- Batch 788/ 842, training loss 0.3870384991168976\n",
      "Epoch 12 -- Batch 789/ 842, training loss 0.3827000856399536\n",
      "Epoch 12 -- Batch 790/ 842, training loss 0.377266526222229\n",
      "Epoch 12 -- Batch 791/ 842, training loss 0.39940306544303894\n",
      "Epoch 12 -- Batch 792/ 842, training loss 0.3769868016242981\n",
      "Epoch 12 -- Batch 793/ 842, training loss 0.3752516210079193\n",
      "Epoch 12 -- Batch 794/ 842, training loss 0.3878183960914612\n",
      "Epoch 12 -- Batch 795/ 842, training loss 0.3785557746887207\n",
      "Epoch 12 -- Batch 796/ 842, training loss 0.38972535729408264\n",
      "Epoch 12 -- Batch 797/ 842, training loss 0.3865002691745758\n",
      "Epoch 12 -- Batch 798/ 842, training loss 0.37757256627082825\n",
      "Epoch 12 -- Batch 799/ 842, training loss 0.38575485348701477\n",
      "Epoch 12 -- Batch 800/ 842, training loss 0.38119205832481384\n",
      "Epoch 12 -- Batch 801/ 842, training loss 0.3685472905635834\n",
      "Epoch 12 -- Batch 802/ 842, training loss 0.3744819164276123\n",
      "Epoch 12 -- Batch 803/ 842, training loss 0.3857322931289673\n",
      "Epoch 12 -- Batch 804/ 842, training loss 0.3863673210144043\n",
      "Epoch 12 -- Batch 805/ 842, training loss 0.3676954209804535\n",
      "Epoch 12 -- Batch 806/ 842, training loss 0.3627074062824249\n",
      "Epoch 12 -- Batch 807/ 842, training loss 0.4004806578159332\n",
      "Epoch 12 -- Batch 808/ 842, training loss 0.3828953802585602\n",
      "Epoch 12 -- Batch 809/ 842, training loss 0.3692128658294678\n",
      "Epoch 12 -- Batch 810/ 842, training loss 0.37357592582702637\n",
      "Epoch 12 -- Batch 811/ 842, training loss 0.38851305842399597\n",
      "Epoch 12 -- Batch 812/ 842, training loss 0.3789212107658386\n",
      "Epoch 12 -- Batch 813/ 842, training loss 0.3732711970806122\n",
      "Epoch 12 -- Batch 814/ 842, training loss 0.38385432958602905\n",
      "Epoch 12 -- Batch 815/ 842, training loss 0.377633661031723\n",
      "Epoch 12 -- Batch 816/ 842, training loss 0.37920355796813965\n",
      "Epoch 12 -- Batch 817/ 842, training loss 0.36968177556991577\n",
      "Epoch 12 -- Batch 818/ 842, training loss 0.3841436207294464\n",
      "Epoch 12 -- Batch 819/ 842, training loss 0.3902016282081604\n",
      "Epoch 12 -- Batch 820/ 842, training loss 0.3858819007873535\n",
      "Epoch 12 -- Batch 821/ 842, training loss 0.3900751769542694\n",
      "Epoch 12 -- Batch 822/ 842, training loss 0.3527178168296814\n",
      "Epoch 12 -- Batch 823/ 842, training loss 0.3601594865322113\n",
      "Epoch 12 -- Batch 824/ 842, training loss 0.3827001750469208\n",
      "Epoch 12 -- Batch 825/ 842, training loss 0.3783719837665558\n",
      "Epoch 12 -- Batch 826/ 842, training loss 0.3899906575679779\n",
      "Epoch 12 -- Batch 827/ 842, training loss 0.3744887709617615\n",
      "Epoch 12 -- Batch 828/ 842, training loss 0.3818144202232361\n",
      "Epoch 12 -- Batch 829/ 842, training loss 0.38804665207862854\n",
      "Epoch 12 -- Batch 830/ 842, training loss 0.3807581663131714\n",
      "Epoch 12 -- Batch 831/ 842, training loss 0.38602203130722046\n",
      "Epoch 12 -- Batch 832/ 842, training loss 0.3828642666339874\n",
      "Epoch 12 -- Batch 833/ 842, training loss 0.3746374249458313\n",
      "Epoch 12 -- Batch 834/ 842, training loss 0.3727363348007202\n",
      "Epoch 12 -- Batch 835/ 842, training loss 0.3702852129936218\n",
      "Epoch 12 -- Batch 836/ 842, training loss 0.3658570647239685\n",
      "Epoch 12 -- Batch 837/ 842, training loss 0.38169124722480774\n",
      "Epoch 12 -- Batch 838/ 842, training loss 0.3796592354774475\n",
      "Epoch 12 -- Batch 839/ 842, training loss 0.3693526089191437\n",
      "Epoch 12 -- Batch 840/ 842, training loss 0.3761007785797119\n",
      "Epoch 12 -- Batch 841/ 842, training loss 0.3974771201610565\n",
      "Epoch 12 -- Batch 842/ 842, training loss 0.30859020352363586\n",
      "----------------------------------------------------------------------\n",
      "Epoch 12 -- Batch 1/ 94, validation loss 0.3684280514717102\n",
      "Epoch 12 -- Batch 2/ 94, validation loss 0.3777104318141937\n",
      "Epoch 12 -- Batch 3/ 94, validation loss 0.3802451491355896\n",
      "Epoch 12 -- Batch 4/ 94, validation loss 0.3741110861301422\n",
      "Epoch 12 -- Batch 5/ 94, validation loss 0.377422958612442\n",
      "Epoch 12 -- Batch 6/ 94, validation loss 0.3877725899219513\n",
      "Epoch 12 -- Batch 7/ 94, validation loss 0.3679423928260803\n",
      "Epoch 12 -- Batch 8/ 94, validation loss 0.3821409046649933\n",
      "Epoch 12 -- Batch 9/ 94, validation loss 0.3788912296295166\n",
      "Epoch 12 -- Batch 10/ 94, validation loss 0.37870535254478455\n",
      "Epoch 12 -- Batch 11/ 94, validation loss 0.3748420178890228\n",
      "Epoch 12 -- Batch 12/ 94, validation loss 0.3806663751602173\n",
      "Epoch 12 -- Batch 13/ 94, validation loss 0.3827550709247589\n",
      "Epoch 12 -- Batch 14/ 94, validation loss 0.3620576560497284\n",
      "Epoch 12 -- Batch 15/ 94, validation loss 0.3683680593967438\n",
      "Epoch 12 -- Batch 16/ 94, validation loss 0.397398442029953\n",
      "Epoch 12 -- Batch 17/ 94, validation loss 0.3646036684513092\n",
      "Epoch 12 -- Batch 18/ 94, validation loss 0.36067235469818115\n",
      "Epoch 12 -- Batch 19/ 94, validation loss 0.364778995513916\n",
      "Epoch 12 -- Batch 20/ 94, validation loss 0.3656657040119171\n",
      "Epoch 12 -- Batch 21/ 94, validation loss 0.3722117841243744\n",
      "Epoch 12 -- Batch 22/ 94, validation loss 0.3736039996147156\n",
      "Epoch 12 -- Batch 23/ 94, validation loss 0.3693993389606476\n",
      "Epoch 12 -- Batch 24/ 94, validation loss 0.3728156089782715\n",
      "Epoch 12 -- Batch 25/ 94, validation loss 0.3729919195175171\n",
      "Epoch 12 -- Batch 26/ 94, validation loss 0.37769967317581177\n",
      "Epoch 12 -- Batch 27/ 94, validation loss 0.3707369565963745\n",
      "Epoch 12 -- Batch 28/ 94, validation loss 0.36085402965545654\n",
      "Epoch 12 -- Batch 29/ 94, validation loss 0.357096791267395\n",
      "Epoch 12 -- Batch 30/ 94, validation loss 0.37906503677368164\n",
      "Epoch 12 -- Batch 31/ 94, validation loss 0.36002010107040405\n",
      "Epoch 12 -- Batch 32/ 94, validation loss 0.37152957916259766\n",
      "Epoch 12 -- Batch 33/ 94, validation loss 0.38597434759140015\n",
      "Epoch 12 -- Batch 34/ 94, validation loss 0.3658739924430847\n",
      "Epoch 12 -- Batch 35/ 94, validation loss 0.38787853717803955\n",
      "Epoch 12 -- Batch 36/ 94, validation loss 0.38643404841423035\n",
      "Epoch 12 -- Batch 37/ 94, validation loss 0.3837128281593323\n",
      "Epoch 12 -- Batch 38/ 94, validation loss 0.38714614510536194\n",
      "Epoch 12 -- Batch 39/ 94, validation loss 0.3759530782699585\n",
      "Epoch 12 -- Batch 40/ 94, validation loss 0.36057910323143005\n",
      "Epoch 12 -- Batch 41/ 94, validation loss 0.37431564927101135\n",
      "Epoch 12 -- Batch 42/ 94, validation loss 0.37075093388557434\n",
      "Epoch 12 -- Batch 43/ 94, validation loss 0.38012218475341797\n",
      "Epoch 12 -- Batch 44/ 94, validation loss 0.3733282685279846\n",
      "Epoch 12 -- Batch 45/ 94, validation loss 0.3812210261821747\n",
      "Epoch 12 -- Batch 46/ 94, validation loss 0.37857675552368164\n",
      "Epoch 12 -- Batch 47/ 94, validation loss 0.36598604917526245\n",
      "Epoch 12 -- Batch 48/ 94, validation loss 0.37532946467399597\n",
      "Epoch 12 -- Batch 49/ 94, validation loss 0.38951051235198975\n",
      "Epoch 12 -- Batch 50/ 94, validation loss 0.39324259757995605\n",
      "Epoch 12 -- Batch 51/ 94, validation loss 0.39126139879226685\n",
      "Epoch 12 -- Batch 52/ 94, validation loss 0.3695753514766693\n",
      "Epoch 12 -- Batch 53/ 94, validation loss 0.3813965320587158\n",
      "Epoch 12 -- Batch 54/ 94, validation loss 0.3650197982788086\n",
      "Epoch 12 -- Batch 55/ 94, validation loss 0.39904215931892395\n",
      "Epoch 12 -- Batch 56/ 94, validation loss 0.3607160747051239\n",
      "Epoch 12 -- Batch 57/ 94, validation loss 0.3796709179878235\n",
      "Epoch 12 -- Batch 58/ 94, validation loss 0.38306987285614014\n",
      "Epoch 12 -- Batch 59/ 94, validation loss 0.3756522834300995\n",
      "Epoch 12 -- Batch 60/ 94, validation loss 0.37455835938453674\n",
      "Epoch 12 -- Batch 61/ 94, validation loss 0.3809843063354492\n",
      "Epoch 12 -- Batch 62/ 94, validation loss 0.3675614595413208\n",
      "Epoch 12 -- Batch 63/ 94, validation loss 0.3767302334308624\n",
      "Epoch 12 -- Batch 64/ 94, validation loss 0.4006041884422302\n",
      "Epoch 12 -- Batch 65/ 94, validation loss 0.3678027093410492\n",
      "Epoch 12 -- Batch 66/ 94, validation loss 0.38815584778785706\n",
      "Epoch 12 -- Batch 67/ 94, validation loss 0.3644154369831085\n",
      "Epoch 12 -- Batch 68/ 94, validation loss 0.3697177469730377\n",
      "Epoch 12 -- Batch 69/ 94, validation loss 0.37732580304145813\n",
      "Epoch 12 -- Batch 70/ 94, validation loss 0.3802017867565155\n",
      "Epoch 12 -- Batch 71/ 94, validation loss 0.37626785039901733\n",
      "Epoch 12 -- Batch 72/ 94, validation loss 0.3727096915245056\n",
      "Epoch 12 -- Batch 73/ 94, validation loss 0.3713458180427551\n",
      "Epoch 12 -- Batch 74/ 94, validation loss 0.3673779368400574\n",
      "Epoch 12 -- Batch 75/ 94, validation loss 0.36724594235420227\n",
      "Epoch 12 -- Batch 76/ 94, validation loss 0.37834542989730835\n",
      "Epoch 12 -- Batch 77/ 94, validation loss 0.36522263288497925\n",
      "Epoch 12 -- Batch 78/ 94, validation loss 0.37173372507095337\n",
      "Epoch 12 -- Batch 79/ 94, validation loss 0.35185256600379944\n",
      "Epoch 12 -- Batch 80/ 94, validation loss 0.38710665702819824\n",
      "Epoch 12 -- Batch 81/ 94, validation loss 0.3705960214138031\n",
      "Epoch 12 -- Batch 82/ 94, validation loss 0.37325847148895264\n",
      "Epoch 12 -- Batch 83/ 94, validation loss 0.3681851923465729\n",
      "Epoch 12 -- Batch 84/ 94, validation loss 0.3798239529132843\n",
      "Epoch 12 -- Batch 85/ 94, validation loss 0.3649662137031555\n",
      "Epoch 12 -- Batch 86/ 94, validation loss 0.366242378950119\n",
      "Epoch 12 -- Batch 87/ 94, validation loss 0.37169286608695984\n",
      "Epoch 12 -- Batch 88/ 94, validation loss 0.36090922355651855\n",
      "Epoch 12 -- Batch 89/ 94, validation loss 0.3663499057292938\n",
      "Epoch 12 -- Batch 90/ 94, validation loss 0.37287694215774536\n",
      "Epoch 12 -- Batch 91/ 94, validation loss 0.3869386613368988\n",
      "Epoch 12 -- Batch 92/ 94, validation loss 0.37617728114128113\n",
      "Epoch 12 -- Batch 93/ 94, validation loss 0.36736708879470825\n",
      "Epoch 12 -- Batch 94/ 94, validation loss 0.3827950954437256\n",
      "----------------------------------------------------------------------\n",
      "Epoch 12 loss: Training 0.38050729036331177, Validation 0.3827950954437256\n",
      "----------------------------------------------------------------------\n",
      "Epoch 13/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 1 2 5 29\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'COc1cccc(NC(=O)N2C[C@H](O)COC[C@@H]3O[C@@H](CC(=O)N4CCN(C)CC[C@@H]4C[C@@H]3C4)cc2)c1'\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'Cc1nc2c(C(=O)NCc3ccco3)cnn2c(=O)c1C1c3ccccc3Oc3ccccc231'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 5 6 8 11 13\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 15 29 30 31\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 9 11 12 13 20 23 24\n",
      "[19:05:48] SMILES Parse Error: syntax error while parsing: /C=C/c1ncc2c(n12)CCC(C)CC2\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES '/C=C/c1ncc2c(n12)CCC(C)CC2' for input: '/C=C/c1ncc2c(n12)CCC(C)CC2'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 21 22 23\n",
      "[19:05:48] SMILES Parse Error: extra open parentheses for input: 'CC(C(=O)N1CCc2c(cnc(S(=O)(=O)CC(C)(C)C)n2)(Cc2cccc(F)c2)C1'\n",
      "[19:05:48] SMILES Parse Error: ring closure 1 duplicates bond between atom 10 and atom 11 for input: 'N#Cc1cc2c3c(sc1nc1N1CCOCC1)CCCC3'\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CC1CCC(NS(=O)(=O)c2ccc3c(c2)nc(SCc2ccc(OC(C)C)cc2)C3CCCC2)CC1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 18 19 20 21\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 6 7 13 14 15 16 20\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'Cc1nc(-c2ccc([N+](=O)[O-])cc2)ncc1C(=O)N1[C@@H]2CCC[C@H]1CCN3C'\n",
      "[19:05:48] Conflicting single bond directions around double bond at index 2.\n",
      "[19:05:48]   BondStereo set to STEREONONE and single bond directions set to NONE.\n",
      "[19:05:48] SMILES Parse Error: extra close parentheses while parsing: CCCN1CC=C2C(C(C#N)=C(N)O)C/C(=C/c3cc(OC)c(OC)c(Br)c3)C2=O)c1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES 'CCCN1CC=C2C(C(C#N)=C(N)O)C/C(=C/c3cc(OC)c(OC)c(Br)c3)C2=O)c1' for input: 'CCCN1CC=C2C(C(C#N)=C(N)O)C/C(=C/c3cc(OC)c(OC)c(Br)c3)C2=O)c1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 5 6 8 10 11 12 13 14 16\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CSc1cccc(CN2C[C@@H](C)[C@@H](OC)CN(C)C(=O)C23SC[C@@H](O)CN2C)c1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 18\n",
      "[19:05:48] Explicit valence for atom # 23 O, 3, is greater than permitted\n",
      "[19:05:48] SMILES Parse Error: extra close parentheses while parsing: CC1CCCN1CCNC(=O)C1CCCN1C(=O)Nc1ccccc1)c1ccccc1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES 'CC1CCCN1CCNC(=O)C1CCCN1C(=O)Nc1ccccc1)c1ccccc1' for input: 'CC1CCCN1CCNC(=O)C1CCCN1C(=O)Nc1ccccc1)c1ccccc1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 14 15 17 18 19\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CC1(C)Oc2ccc3ccc(C)cc3c2-c2oc(=O)oc1[C@@H]2CCCCC2'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 23\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'Cc1cc2n(n1)c(CN(C)C(C)=O)c(=O)n(C)c12'\n",
      "[19:05:48] Explicit valence for atom # 9 N, 4, is greater than permitted\n",
      "[19:05:48] SMILES Parse Error: extra close parentheses while parsing: [N+](=O)[O-])ccc1N=Cc1cccs1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES '[N+](=O)[O-])ccc1N=Cc1cccs1' for input: '[N+](=O)[O-])ccc1N=Cc1cccs1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 18 25\n",
      "[19:05:48] SMILES Parse Error: syntax error while parsing: CC((O)Cc1nnc(-c2cccs2)o1)c1ccccc1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES 'CC((O)Cc1nnc(-c2cccs2)o1)c1ccccc1' for input: 'CC((O)Cc1nnc(-c2cccs2)o1)c1ccccc1'\n",
      "[19:05:48] SMILES Parse Error: ring closure 1 duplicates bond between atom 24 and atom 25 for input: 'C=CCSc1nnc2c(n1)OC(c1ccc3n(c1=O)OC(C)C3)N=C1c1ccccc1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 23 24 25 26 27\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 10 11 12 14\n",
      "[19:05:48] SMILES Parse Error: syntax error while parsing: CC(C)(O)/C(=C\\/C(=O)c1ccccc1)Nc1ccc(Br)cc1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES 'CC(C)(O)/C(=C\\/C(=O)c1ccccc1)Nc1ccc(Br)cc1' for input: 'CC(C)(O)/C(=C\\/C(=O)c1ccccc1)Nc1ccc(Br)cc1'\n",
      "[19:05:48] SMILES Parse Error: ring closure 1 duplicates bond between atom 10 and atom 11 for input: 'COc1ccc2c(c1)CNC1c1ccccc1OCCC(N)=O'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 27\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C#Cc2ccc([C@@H]3[C@H](CO)N4C(CCN(C)CC4)C[C@@H]34)cc2)c1'\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'C=C[C@@H]1C(=O)CC=C2CCC=C12C(=O)NCc1ccccc12'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 1 2 6 7 8 9 10 11 13\n",
      "[19:05:48] SMILES Parse Error: extra open parentheses for input: 'CCN(CC)S(=O)(=O)c1ccc(OC)c(N2CCN(S(=O)(=O)c3c(C)noc3/C=C/N(C)C)nc21'\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CC1CCCN(CCCNC(=O)c2ccc3c(c2)C(=O)CC2)C1'\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'Nc1ccc(-c2nnn3c2NC(c2ccccc2)c2ccccc2)cc1'\n",
      "[19:05:48] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CCn1c(CCNc2nc(Nc3ccc(N4CCOCC4)cc3)nc3c2c2ncnc2ccccn23)nc(C)c1O'\n",
      "[19:05:48] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 5 7\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'Cc1nnc2cc(-c3ccc(F)cc3)nn1-c1ccccc1Oc1ccccc1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 26\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CC1CC(=O)Nc2cccc3c2N(C(=O)Cc3ccc(F)cc3)CCCC12'\n",
      "[19:05:48] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)NC2CCN(C(=O)C3CC3c2ccc(F)cc2F)CC(C)C)cc1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 3 12 19\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 9 10 22 23 24 25 26 27 28\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1NC(=O)[C@@]2(C)[C@@H](c3ccc(-c4ccccc4)cc3)[C@@H]3COC(=O)[C@@H]3[C@@H]2CC[C@@H]12'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 14 15 16 19 21\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 8 9 10 35 37\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'Nc1c(C(=O)c2ccccc2Cl)sc2nc3c(cc2c12)CCCCC3'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 7 8 9 23 28\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2c(c1)OCO2)c1csc(NN5CCc4ccccc4O)cc2c1=O'\n",
      "[19:05:48] Explicit valence for atom # 13 N, 5, is greater than permitted\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 28 29\n",
      "[19:05:48] Explicit valence for atom # 3 C, 5, is greater than permitted\n",
      "[19:05:48] SMILES Parse Error: syntax error while parsing: CCOc1ccc(N=Nc2c(C)[nH]n(-c(=S)=)c(=S)[nH]2)cc1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES 'CCOc1ccc(N=Nc2c(C)[nH]n(-c(=S)=)c(=S)[nH]2)cc1' for input: 'CCOc1ccc(N=Nc2c(C)[nH]n(-c(=S)=)c(=S)[nH]2)cc1'\n",
      "[19:05:48] SMILES Parse Error: extra close parentheses while parsing: O=C(c1ccco1)N1N=C(c2cc3c(cc2Br)OCO3)C1=O)c1ccccc1\n",
      "[19:05:48] SMILES Parse Error: Failed parsing SMILES 'O=C(c1ccco1)N1N=C(c2cc3c(cc2Br)OCO3)C1=O)c1ccccc1' for input: 'O=C(c1ccco1)N1N=C(c2cc3c(cc2Br)OCO3)C1=O)c1ccccc1'\n",
      "[19:05:48] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 31\n",
      "[19:05:48] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CNC(=O)c1ccc(S(=O)(=O)N(C)Cc2nc(C)nc3sc4c(c2c2=O)CCC4)cc1'\n",
      "[19:05:48] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C(NC(=O)c1ccccc1)(Nc1sc2c(c1C(=O)OC1)CCCCC2)Cc1ccccc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 -- Batch 1/ 842, training loss 0.3668293356895447\n",
      "Epoch 13 -- Batch 2/ 842, training loss 0.3572969436645508\n",
      "Epoch 13 -- Batch 3/ 842, training loss 0.3705778121948242\n",
      "Epoch 13 -- Batch 4/ 842, training loss 0.3794900178909302\n",
      "Epoch 13 -- Batch 5/ 842, training loss 0.35983702540397644\n",
      "Epoch 13 -- Batch 6/ 842, training loss 0.36621636152267456\n",
      "Epoch 13 -- Batch 7/ 842, training loss 0.3768928647041321\n",
      "Epoch 13 -- Batch 8/ 842, training loss 0.3647754192352295\n",
      "Epoch 13 -- Batch 9/ 842, training loss 0.36129236221313477\n",
      "Epoch 13 -- Batch 10/ 842, training loss 0.36579766869544983\n",
      "Epoch 13 -- Batch 11/ 842, training loss 0.3711300790309906\n",
      "Epoch 13 -- Batch 12/ 842, training loss 0.3492865264415741\n",
      "Epoch 13 -- Batch 13/ 842, training loss 0.3785533308982849\n",
      "Epoch 13 -- Batch 14/ 842, training loss 0.3649480640888214\n",
      "Epoch 13 -- Batch 15/ 842, training loss 0.35713592171669006\n",
      "Epoch 13 -- Batch 16/ 842, training loss 0.3861989974975586\n",
      "Epoch 13 -- Batch 17/ 842, training loss 0.3831537663936615\n",
      "Epoch 13 -- Batch 18/ 842, training loss 0.36341986060142517\n",
      "Epoch 13 -- Batch 19/ 842, training loss 0.3580893874168396\n",
      "Epoch 13 -- Batch 20/ 842, training loss 0.3641737401485443\n",
      "Epoch 13 -- Batch 21/ 842, training loss 0.3784426748752594\n",
      "Epoch 13 -- Batch 22/ 842, training loss 0.3721691370010376\n",
      "Epoch 13 -- Batch 23/ 842, training loss 0.3695635199546814\n",
      "Epoch 13 -- Batch 24/ 842, training loss 0.37672296166419983\n",
      "Epoch 13 -- Batch 25/ 842, training loss 0.37165388464927673\n",
      "Epoch 13 -- Batch 26/ 842, training loss 0.36456984281539917\n",
      "Epoch 13 -- Batch 27/ 842, training loss 0.3559968173503876\n",
      "Epoch 13 -- Batch 28/ 842, training loss 0.36464738845825195\n",
      "Epoch 13 -- Batch 29/ 842, training loss 0.3711394667625427\n",
      "Epoch 13 -- Batch 30/ 842, training loss 0.3672328591346741\n",
      "Epoch 13 -- Batch 31/ 842, training loss 0.3752146363258362\n",
      "Epoch 13 -- Batch 32/ 842, training loss 0.3642237186431885\n",
      "Epoch 13 -- Batch 33/ 842, training loss 0.36106985807418823\n",
      "Epoch 13 -- Batch 34/ 842, training loss 0.36555519700050354\n",
      "Epoch 13 -- Batch 35/ 842, training loss 0.3692089915275574\n",
      "Epoch 13 -- Batch 36/ 842, training loss 0.36240074038505554\n",
      "Epoch 13 -- Batch 37/ 842, training loss 0.36581408977508545\n",
      "Epoch 13 -- Batch 38/ 842, training loss 0.35797086358070374\n",
      "Epoch 13 -- Batch 39/ 842, training loss 0.35890811681747437\n",
      "Epoch 13 -- Batch 40/ 842, training loss 0.3773663341999054\n",
      "Epoch 13 -- Batch 41/ 842, training loss 0.36665886640548706\n",
      "Epoch 13 -- Batch 42/ 842, training loss 0.36907026171684265\n",
      "Epoch 13 -- Batch 43/ 842, training loss 0.36312592029571533\n",
      "Epoch 13 -- Batch 44/ 842, training loss 0.3662049174308777\n",
      "Epoch 13 -- Batch 45/ 842, training loss 0.3781856298446655\n",
      "Epoch 13 -- Batch 46/ 842, training loss 0.37450358271598816\n",
      "Epoch 13 -- Batch 47/ 842, training loss 0.3799281418323517\n",
      "Epoch 13 -- Batch 48/ 842, training loss 0.3632468581199646\n",
      "Epoch 13 -- Batch 49/ 842, training loss 0.3801461160182953\n",
      "Epoch 13 -- Batch 50/ 842, training loss 0.3620529770851135\n",
      "Epoch 13 -- Batch 51/ 842, training loss 0.3709478974342346\n",
      "Epoch 13 -- Batch 52/ 842, training loss 0.3647454082965851\n",
      "Epoch 13 -- Batch 53/ 842, training loss 0.35504505038261414\n",
      "Epoch 13 -- Batch 54/ 842, training loss 0.36069026589393616\n",
      "Epoch 13 -- Batch 55/ 842, training loss 0.3620677590370178\n",
      "Epoch 13 -- Batch 56/ 842, training loss 0.3741147220134735\n",
      "Epoch 13 -- Batch 57/ 842, training loss 0.37376582622528076\n",
      "Epoch 13 -- Batch 58/ 842, training loss 0.35389527678489685\n",
      "Epoch 13 -- Batch 59/ 842, training loss 0.36544448137283325\n",
      "Epoch 13 -- Batch 60/ 842, training loss 0.36266204714775085\n",
      "Epoch 13 -- Batch 61/ 842, training loss 0.373963326215744\n",
      "Epoch 13 -- Batch 62/ 842, training loss 0.36944636702537537\n",
      "Epoch 13 -- Batch 63/ 842, training loss 0.3732195794582367\n",
      "Epoch 13 -- Batch 64/ 842, training loss 0.36666974425315857\n",
      "Epoch 13 -- Batch 65/ 842, training loss 0.3790059983730316\n",
      "Epoch 13 -- Batch 66/ 842, training loss 0.38049980998039246\n",
      "Epoch 13 -- Batch 67/ 842, training loss 0.34719374775886536\n",
      "Epoch 13 -- Batch 68/ 842, training loss 0.38424184918403625\n",
      "Epoch 13 -- Batch 69/ 842, training loss 0.3740757405757904\n",
      "Epoch 13 -- Batch 70/ 842, training loss 0.3813896179199219\n",
      "Epoch 13 -- Batch 71/ 842, training loss 0.3536069393157959\n",
      "Epoch 13 -- Batch 72/ 842, training loss 0.3670213222503662\n",
      "Epoch 13 -- Batch 73/ 842, training loss 0.37051233649253845\n",
      "Epoch 13 -- Batch 74/ 842, training loss 0.3663029074668884\n",
      "Epoch 13 -- Batch 75/ 842, training loss 0.3688788414001465\n",
      "Epoch 13 -- Batch 76/ 842, training loss 0.35105499625205994\n",
      "Epoch 13 -- Batch 77/ 842, training loss 0.37206849455833435\n",
      "Epoch 13 -- Batch 78/ 842, training loss 0.3676905333995819\n",
      "Epoch 13 -- Batch 79/ 842, training loss 0.35560306906700134\n",
      "Epoch 13 -- Batch 80/ 842, training loss 0.36663001775741577\n",
      "Epoch 13 -- Batch 81/ 842, training loss 0.37160709500312805\n",
      "Epoch 13 -- Batch 82/ 842, training loss 0.36846923828125\n",
      "Epoch 13 -- Batch 83/ 842, training loss 0.3619426190853119\n",
      "Epoch 13 -- Batch 84/ 842, training loss 0.3663446605205536\n",
      "Epoch 13 -- Batch 85/ 842, training loss 0.3663291037082672\n",
      "Epoch 13 -- Batch 86/ 842, training loss 0.3708575665950775\n",
      "Epoch 13 -- Batch 87/ 842, training loss 0.3691447079181671\n",
      "Epoch 13 -- Batch 88/ 842, training loss 0.3605482876300812\n",
      "Epoch 13 -- Batch 89/ 842, training loss 0.36510011553764343\n",
      "Epoch 13 -- Batch 90/ 842, training loss 0.36901581287384033\n",
      "Epoch 13 -- Batch 91/ 842, training loss 0.37644723057746887\n",
      "Epoch 13 -- Batch 92/ 842, training loss 0.3606410324573517\n",
      "Epoch 13 -- Batch 93/ 842, training loss 0.3764187693595886\n",
      "Epoch 13 -- Batch 94/ 842, training loss 0.37086617946624756\n",
      "Epoch 13 -- Batch 95/ 842, training loss 0.3747260570526123\n",
      "Epoch 13 -- Batch 96/ 842, training loss 0.3795471787452698\n",
      "Epoch 13 -- Batch 97/ 842, training loss 0.375995010137558\n",
      "Epoch 13 -- Batch 98/ 842, training loss 0.3630474805831909\n",
      "Epoch 13 -- Batch 99/ 842, training loss 0.3767417073249817\n",
      "Epoch 13 -- Batch 100/ 842, training loss 0.3784102499485016\n",
      "Epoch 13 -- Batch 101/ 842, training loss 0.3608066141605377\n",
      "Epoch 13 -- Batch 102/ 842, training loss 0.36796194314956665\n",
      "Epoch 13 -- Batch 103/ 842, training loss 0.37699466943740845\n",
      "Epoch 13 -- Batch 104/ 842, training loss 0.38103801012039185\n",
      "Epoch 13 -- Batch 105/ 842, training loss 0.37731286883354187\n",
      "Epoch 13 -- Batch 106/ 842, training loss 0.3654174208641052\n",
      "Epoch 13 -- Batch 107/ 842, training loss 0.35621801018714905\n",
      "Epoch 13 -- Batch 108/ 842, training loss 0.3474035859107971\n",
      "Epoch 13 -- Batch 109/ 842, training loss 0.3744448721408844\n",
      "Epoch 13 -- Batch 110/ 842, training loss 0.3628474473953247\n",
      "Epoch 13 -- Batch 111/ 842, training loss 0.36893707513809204\n",
      "Epoch 13 -- Batch 112/ 842, training loss 0.3733992874622345\n",
      "Epoch 13 -- Batch 113/ 842, training loss 0.3607587218284607\n",
      "Epoch 13 -- Batch 114/ 842, training loss 0.3677736222743988\n",
      "Epoch 13 -- Batch 115/ 842, training loss 0.3743534982204437\n",
      "Epoch 13 -- Batch 116/ 842, training loss 0.36290228366851807\n",
      "Epoch 13 -- Batch 117/ 842, training loss 0.36891451478004456\n",
      "Epoch 13 -- Batch 118/ 842, training loss 0.36309707164764404\n",
      "Epoch 13 -- Batch 119/ 842, training loss 0.37138357758522034\n",
      "Epoch 13 -- Batch 120/ 842, training loss 0.3539583086967468\n",
      "Epoch 13 -- Batch 121/ 842, training loss 0.359182745218277\n",
      "Epoch 13 -- Batch 122/ 842, training loss 0.3698500692844391\n",
      "Epoch 13 -- Batch 123/ 842, training loss 0.36839985847473145\n",
      "Epoch 13 -- Batch 124/ 842, training loss 0.3688856363296509\n",
      "Epoch 13 -- Batch 125/ 842, training loss 0.3572518527507782\n",
      "Epoch 13 -- Batch 126/ 842, training loss 0.36623093485832214\n",
      "Epoch 13 -- Batch 127/ 842, training loss 0.3660108745098114\n",
      "Epoch 13 -- Batch 128/ 842, training loss 0.3806784749031067\n",
      "Epoch 13 -- Batch 129/ 842, training loss 0.36044904589653015\n",
      "Epoch 13 -- Batch 130/ 842, training loss 0.3751474916934967\n",
      "Epoch 13 -- Batch 131/ 842, training loss 0.3740917444229126\n",
      "Epoch 13 -- Batch 132/ 842, training loss 0.3775123953819275\n",
      "Epoch 13 -- Batch 133/ 842, training loss 0.35113129019737244\n",
      "Epoch 13 -- Batch 134/ 842, training loss 0.37854212522506714\n",
      "Epoch 13 -- Batch 135/ 842, training loss 0.37573039531707764\n",
      "Epoch 13 -- Batch 136/ 842, training loss 0.3639288544654846\n",
      "Epoch 13 -- Batch 137/ 842, training loss 0.3716675937175751\n",
      "Epoch 13 -- Batch 138/ 842, training loss 0.36328160762786865\n",
      "Epoch 13 -- Batch 139/ 842, training loss 0.3698558509349823\n",
      "Epoch 13 -- Batch 140/ 842, training loss 0.3596467673778534\n",
      "Epoch 13 -- Batch 141/ 842, training loss 0.35825711488723755\n",
      "Epoch 13 -- Batch 142/ 842, training loss 0.37632274627685547\n",
      "Epoch 13 -- Batch 143/ 842, training loss 0.3656599819660187\n",
      "Epoch 13 -- Batch 144/ 842, training loss 0.3748166263103485\n",
      "Epoch 13 -- Batch 145/ 842, training loss 0.35839980840682983\n",
      "Epoch 13 -- Batch 146/ 842, training loss 0.3785179853439331\n",
      "Epoch 13 -- Batch 147/ 842, training loss 0.36362770199775696\n",
      "Epoch 13 -- Batch 148/ 842, training loss 0.3669862747192383\n",
      "Epoch 13 -- Batch 149/ 842, training loss 0.3667900562286377\n",
      "Epoch 13 -- Batch 150/ 842, training loss 0.3769912123680115\n",
      "Epoch 13 -- Batch 151/ 842, training loss 0.3596371114253998\n",
      "Epoch 13 -- Batch 152/ 842, training loss 0.3654412627220154\n",
      "Epoch 13 -- Batch 153/ 842, training loss 0.35797587037086487\n",
      "Epoch 13 -- Batch 154/ 842, training loss 0.37164899706840515\n",
      "Epoch 13 -- Batch 155/ 842, training loss 0.3697509467601776\n",
      "Epoch 13 -- Batch 156/ 842, training loss 0.3644721508026123\n",
      "Epoch 13 -- Batch 157/ 842, training loss 0.3680320084095001\n",
      "Epoch 13 -- Batch 158/ 842, training loss 0.36528462171554565\n",
      "Epoch 13 -- Batch 159/ 842, training loss 0.36526840925216675\n",
      "Epoch 13 -- Batch 160/ 842, training loss 0.37461328506469727\n",
      "Epoch 13 -- Batch 161/ 842, training loss 0.37313300371170044\n",
      "Epoch 13 -- Batch 162/ 842, training loss 0.3756999373435974\n",
      "Epoch 13 -- Batch 163/ 842, training loss 0.3704870343208313\n",
      "Epoch 13 -- Batch 164/ 842, training loss 0.3602583706378937\n",
      "Epoch 13 -- Batch 165/ 842, training loss 0.3648492991924286\n",
      "Epoch 13 -- Batch 166/ 842, training loss 0.37660452723503113\n",
      "Epoch 13 -- Batch 167/ 842, training loss 0.38020020723342896\n",
      "Epoch 13 -- Batch 168/ 842, training loss 0.3816697299480438\n",
      "Epoch 13 -- Batch 169/ 842, training loss 0.36529871821403503\n",
      "Epoch 13 -- Batch 170/ 842, training loss 0.36371392011642456\n",
      "Epoch 13 -- Batch 171/ 842, training loss 0.3726843297481537\n",
      "Epoch 13 -- Batch 172/ 842, training loss 0.36695995926856995\n",
      "Epoch 13 -- Batch 173/ 842, training loss 0.3514823615550995\n",
      "Epoch 13 -- Batch 174/ 842, training loss 0.3678761422634125\n",
      "Epoch 13 -- Batch 175/ 842, training loss 0.3841387629508972\n",
      "Epoch 13 -- Batch 176/ 842, training loss 0.38022956252098083\n",
      "Epoch 13 -- Batch 177/ 842, training loss 0.378048837184906\n",
      "Epoch 13 -- Batch 178/ 842, training loss 0.3591032326221466\n",
      "Epoch 13 -- Batch 179/ 842, training loss 0.3738914728164673\n",
      "Epoch 13 -- Batch 180/ 842, training loss 0.3649335503578186\n",
      "Epoch 13 -- Batch 181/ 842, training loss 0.3699685335159302\n",
      "Epoch 13 -- Batch 182/ 842, training loss 0.3698095977306366\n",
      "Epoch 13 -- Batch 183/ 842, training loss 0.3837841749191284\n",
      "Epoch 13 -- Batch 184/ 842, training loss 0.37488043308258057\n",
      "Epoch 13 -- Batch 185/ 842, training loss 0.37721920013427734\n",
      "Epoch 13 -- Batch 186/ 842, training loss 0.3617367744445801\n",
      "Epoch 13 -- Batch 187/ 842, training loss 0.37286651134490967\n",
      "Epoch 13 -- Batch 188/ 842, training loss 0.35793691873550415\n",
      "Epoch 13 -- Batch 189/ 842, training loss 0.3717394471168518\n",
      "Epoch 13 -- Batch 190/ 842, training loss 0.36655279994010925\n",
      "Epoch 13 -- Batch 191/ 842, training loss 0.3718489408493042\n",
      "Epoch 13 -- Batch 192/ 842, training loss 0.37654373049736023\n",
      "Epoch 13 -- Batch 193/ 842, training loss 0.3621954321861267\n",
      "Epoch 13 -- Batch 194/ 842, training loss 0.3734952509403229\n",
      "Epoch 13 -- Batch 195/ 842, training loss 0.3856770098209381\n",
      "Epoch 13 -- Batch 196/ 842, training loss 0.3743681311607361\n",
      "Epoch 13 -- Batch 197/ 842, training loss 0.37453940510749817\n",
      "Epoch 13 -- Batch 198/ 842, training loss 0.3692609369754791\n",
      "Epoch 13 -- Batch 199/ 842, training loss 0.36788037419319153\n",
      "Epoch 13 -- Batch 200/ 842, training loss 0.36219391226768494\n",
      "Epoch 13 -- Batch 201/ 842, training loss 0.36224761605262756\n",
      "Epoch 13 -- Batch 202/ 842, training loss 0.3787871301174164\n",
      "Epoch 13 -- Batch 203/ 842, training loss 0.37080612778663635\n",
      "Epoch 13 -- Batch 204/ 842, training loss 0.36293864250183105\n",
      "Epoch 13 -- Batch 205/ 842, training loss 0.3736654818058014\n",
      "Epoch 13 -- Batch 206/ 842, training loss 0.3647304177284241\n",
      "Epoch 13 -- Batch 207/ 842, training loss 0.3755713403224945\n",
      "Epoch 13 -- Batch 208/ 842, training loss 0.3763231635093689\n",
      "Epoch 13 -- Batch 209/ 842, training loss 0.3769892454147339\n",
      "Epoch 13 -- Batch 210/ 842, training loss 0.36648282408714294\n",
      "Epoch 13 -- Batch 211/ 842, training loss 0.36527714133262634\n",
      "Epoch 13 -- Batch 212/ 842, training loss 0.3619047999382019\n",
      "Epoch 13 -- Batch 213/ 842, training loss 0.36238721013069153\n",
      "Epoch 13 -- Batch 214/ 842, training loss 0.3695000112056732\n",
      "Epoch 13 -- Batch 215/ 842, training loss 0.3762126564979553\n",
      "Epoch 13 -- Batch 216/ 842, training loss 0.37014028429985046\n",
      "Epoch 13 -- Batch 217/ 842, training loss 0.35852575302124023\n",
      "Epoch 13 -- Batch 218/ 842, training loss 0.366742342710495\n",
      "Epoch 13 -- Batch 219/ 842, training loss 0.3763272762298584\n",
      "Epoch 13 -- Batch 220/ 842, training loss 0.3806753158569336\n",
      "Epoch 13 -- Batch 221/ 842, training loss 0.3671603798866272\n",
      "Epoch 13 -- Batch 222/ 842, training loss 0.35889145731925964\n",
      "Epoch 13 -- Batch 223/ 842, training loss 0.35955262184143066\n",
      "Epoch 13 -- Batch 224/ 842, training loss 0.36823439598083496\n",
      "Epoch 13 -- Batch 225/ 842, training loss 0.35933437943458557\n",
      "Epoch 13 -- Batch 226/ 842, training loss 0.3667936623096466\n",
      "Epoch 13 -- Batch 227/ 842, training loss 0.36528879404067993\n",
      "Epoch 13 -- Batch 228/ 842, training loss 0.37485742568969727\n",
      "Epoch 13 -- Batch 229/ 842, training loss 0.3823002278804779\n",
      "Epoch 13 -- Batch 230/ 842, training loss 0.38286709785461426\n",
      "Epoch 13 -- Batch 231/ 842, training loss 0.3750734329223633\n",
      "Epoch 13 -- Batch 232/ 842, training loss 0.3717114329338074\n",
      "Epoch 13 -- Batch 233/ 842, training loss 0.3749295175075531\n",
      "Epoch 13 -- Batch 234/ 842, training loss 0.3597758412361145\n",
      "Epoch 13 -- Batch 235/ 842, training loss 0.3772636353969574\n",
      "Epoch 13 -- Batch 236/ 842, training loss 0.3768845498561859\n",
      "Epoch 13 -- Batch 237/ 842, training loss 0.3853115737438202\n",
      "Epoch 13 -- Batch 238/ 842, training loss 0.38083675503730774\n",
      "Epoch 13 -- Batch 239/ 842, training loss 0.37472566962242126\n",
      "Epoch 13 -- Batch 240/ 842, training loss 0.37220466136932373\n",
      "Epoch 13 -- Batch 241/ 842, training loss 0.37839627265930176\n",
      "Epoch 13 -- Batch 242/ 842, training loss 0.36769184470176697\n",
      "Epoch 13 -- Batch 243/ 842, training loss 0.3716692626476288\n",
      "Epoch 13 -- Batch 244/ 842, training loss 0.3827657401561737\n",
      "Epoch 13 -- Batch 245/ 842, training loss 0.3741472065448761\n",
      "Epoch 13 -- Batch 246/ 842, training loss 0.373740017414093\n",
      "Epoch 13 -- Batch 247/ 842, training loss 0.3796447515487671\n",
      "Epoch 13 -- Batch 248/ 842, training loss 0.3620734214782715\n",
      "Epoch 13 -- Batch 249/ 842, training loss 0.3674221336841583\n",
      "Epoch 13 -- Batch 250/ 842, training loss 0.3713074028491974\n",
      "Epoch 13 -- Batch 251/ 842, training loss 0.3820940852165222\n",
      "Epoch 13 -- Batch 252/ 842, training loss 0.36218681931495667\n",
      "Epoch 13 -- Batch 253/ 842, training loss 0.36652109026908875\n",
      "Epoch 13 -- Batch 254/ 842, training loss 0.3474157452583313\n",
      "Epoch 13 -- Batch 255/ 842, training loss 0.36364540457725525\n",
      "Epoch 13 -- Batch 256/ 842, training loss 0.3770589232444763\n",
      "Epoch 13 -- Batch 257/ 842, training loss 0.36749812960624695\n",
      "Epoch 13 -- Batch 258/ 842, training loss 0.3723126947879791\n",
      "Epoch 13 -- Batch 259/ 842, training loss 0.3732934296131134\n",
      "Epoch 13 -- Batch 260/ 842, training loss 0.3728533387184143\n",
      "Epoch 13 -- Batch 261/ 842, training loss 0.3616822361946106\n",
      "Epoch 13 -- Batch 262/ 842, training loss 0.3720768988132477\n",
      "Epoch 13 -- Batch 263/ 842, training loss 0.37274739146232605\n",
      "Epoch 13 -- Batch 264/ 842, training loss 0.36985471844673157\n",
      "Epoch 13 -- Batch 265/ 842, training loss 0.38165637850761414\n",
      "Epoch 13 -- Batch 266/ 842, training loss 0.38252776861190796\n",
      "Epoch 13 -- Batch 267/ 842, training loss 0.3807018995285034\n",
      "Epoch 13 -- Batch 268/ 842, training loss 0.37580034136772156\n",
      "Epoch 13 -- Batch 269/ 842, training loss 0.36546963453292847\n",
      "Epoch 13 -- Batch 270/ 842, training loss 0.3706575632095337\n",
      "Epoch 13 -- Batch 271/ 842, training loss 0.37769412994384766\n",
      "Epoch 13 -- Batch 272/ 842, training loss 0.37636563181877136\n",
      "Epoch 13 -- Batch 273/ 842, training loss 0.37746942043304443\n",
      "Epoch 13 -- Batch 274/ 842, training loss 0.377058207988739\n",
      "Epoch 13 -- Batch 275/ 842, training loss 0.3694373667240143\n",
      "Epoch 13 -- Batch 276/ 842, training loss 0.3663179278373718\n",
      "Epoch 13 -- Batch 277/ 842, training loss 0.37418654561042786\n",
      "Epoch 13 -- Batch 278/ 842, training loss 0.3682664930820465\n",
      "Epoch 13 -- Batch 279/ 842, training loss 0.36418724060058594\n",
      "Epoch 13 -- Batch 280/ 842, training loss 0.372210294008255\n",
      "Epoch 13 -- Batch 281/ 842, training loss 0.3788222670555115\n",
      "Epoch 13 -- Batch 282/ 842, training loss 0.3600131869316101\n",
      "Epoch 13 -- Batch 283/ 842, training loss 0.3693559169769287\n",
      "Epoch 13 -- Batch 284/ 842, training loss 0.3820831775665283\n",
      "Epoch 13 -- Batch 285/ 842, training loss 0.36793825030326843\n",
      "Epoch 13 -- Batch 286/ 842, training loss 0.36450666189193726\n",
      "Epoch 13 -- Batch 287/ 842, training loss 0.3605528771877289\n",
      "Epoch 13 -- Batch 288/ 842, training loss 0.3712182641029358\n",
      "Epoch 13 -- Batch 289/ 842, training loss 0.385408490896225\n",
      "Epoch 13 -- Batch 290/ 842, training loss 0.36184215545654297\n",
      "Epoch 13 -- Batch 291/ 842, training loss 0.37688663601875305\n",
      "Epoch 13 -- Batch 292/ 842, training loss 0.36131852865219116\n",
      "Epoch 13 -- Batch 293/ 842, training loss 0.368568480014801\n",
      "Epoch 13 -- Batch 294/ 842, training loss 0.3691839277744293\n",
      "Epoch 13 -- Batch 295/ 842, training loss 0.3670309782028198\n",
      "Epoch 13 -- Batch 296/ 842, training loss 0.360027551651001\n",
      "Epoch 13 -- Batch 297/ 842, training loss 0.3884385824203491\n",
      "Epoch 13 -- Batch 298/ 842, training loss 0.3790605366230011\n",
      "Epoch 13 -- Batch 299/ 842, training loss 0.35747843980789185\n",
      "Epoch 13 -- Batch 300/ 842, training loss 0.38673314452171326\n",
      "Epoch 13 -- Batch 301/ 842, training loss 0.3723849654197693\n",
      "Epoch 13 -- Batch 302/ 842, training loss 0.37703219056129456\n",
      "Epoch 13 -- Batch 303/ 842, training loss 0.3659727871417999\n",
      "Epoch 13 -- Batch 304/ 842, training loss 0.3593021035194397\n",
      "Epoch 13 -- Batch 305/ 842, training loss 0.3825070261955261\n",
      "Epoch 13 -- Batch 306/ 842, training loss 0.3740033805370331\n",
      "Epoch 13 -- Batch 307/ 842, training loss 0.36282357573509216\n",
      "Epoch 13 -- Batch 308/ 842, training loss 0.37184062600135803\n",
      "Epoch 13 -- Batch 309/ 842, training loss 0.3616947531700134\n",
      "Epoch 13 -- Batch 310/ 842, training loss 0.37263548374176025\n",
      "Epoch 13 -- Batch 311/ 842, training loss 0.372663289308548\n",
      "Epoch 13 -- Batch 312/ 842, training loss 0.3801155984401703\n",
      "Epoch 13 -- Batch 313/ 842, training loss 0.37755143642425537\n",
      "Epoch 13 -- Batch 314/ 842, training loss 0.36327528953552246\n",
      "Epoch 13 -- Batch 315/ 842, training loss 0.36507752537727356\n",
      "Epoch 13 -- Batch 316/ 842, training loss 0.37611135840415955\n",
      "Epoch 13 -- Batch 317/ 842, training loss 0.35553207993507385\n",
      "Epoch 13 -- Batch 318/ 842, training loss 0.3574264645576477\n",
      "Epoch 13 -- Batch 319/ 842, training loss 0.36660870909690857\n",
      "Epoch 13 -- Batch 320/ 842, training loss 0.3800397217273712\n",
      "Epoch 13 -- Batch 321/ 842, training loss 0.3709363341331482\n",
      "Epoch 13 -- Batch 322/ 842, training loss 0.38969892263412476\n",
      "Epoch 13 -- Batch 323/ 842, training loss 0.36889755725860596\n",
      "Epoch 13 -- Batch 324/ 842, training loss 0.36866143345832825\n",
      "Epoch 13 -- Batch 325/ 842, training loss 0.37232959270477295\n",
      "Epoch 13 -- Batch 326/ 842, training loss 0.36715367436408997\n",
      "Epoch 13 -- Batch 327/ 842, training loss 0.35946720838546753\n",
      "Epoch 13 -- Batch 328/ 842, training loss 0.3851979672908783\n",
      "Epoch 13 -- Batch 329/ 842, training loss 0.3650059401988983\n",
      "Epoch 13 -- Batch 330/ 842, training loss 0.37005794048309326\n",
      "Epoch 13 -- Batch 331/ 842, training loss 0.3643333911895752\n",
      "Epoch 13 -- Batch 332/ 842, training loss 0.3830685019493103\n",
      "Epoch 13 -- Batch 333/ 842, training loss 0.36188578605651855\n",
      "Epoch 13 -- Batch 334/ 842, training loss 0.3619731366634369\n",
      "Epoch 13 -- Batch 335/ 842, training loss 0.3589881360530853\n",
      "Epoch 13 -- Batch 336/ 842, training loss 0.37698379158973694\n",
      "Epoch 13 -- Batch 337/ 842, training loss 0.36595821380615234\n",
      "Epoch 13 -- Batch 338/ 842, training loss 0.3617924153804779\n",
      "Epoch 13 -- Batch 339/ 842, training loss 0.37104588747024536\n",
      "Epoch 13 -- Batch 340/ 842, training loss 0.3600437641143799\n",
      "Epoch 13 -- Batch 341/ 842, training loss 0.370289146900177\n",
      "Epoch 13 -- Batch 342/ 842, training loss 0.3608052134513855\n",
      "Epoch 13 -- Batch 343/ 842, training loss 0.3621880114078522\n",
      "Epoch 13 -- Batch 344/ 842, training loss 0.3700197637081146\n",
      "Epoch 13 -- Batch 345/ 842, training loss 0.359001487493515\n",
      "Epoch 13 -- Batch 346/ 842, training loss 0.3782653212547302\n",
      "Epoch 13 -- Batch 347/ 842, training loss 0.3510507345199585\n",
      "Epoch 13 -- Batch 348/ 842, training loss 0.38075438141822815\n",
      "Epoch 13 -- Batch 349/ 842, training loss 0.3641057312488556\n",
      "Epoch 13 -- Batch 350/ 842, training loss 0.36928248405456543\n",
      "Epoch 13 -- Batch 351/ 842, training loss 0.37438344955444336\n",
      "Epoch 13 -- Batch 352/ 842, training loss 0.3730960786342621\n",
      "Epoch 13 -- Batch 353/ 842, training loss 0.37058964371681213\n",
      "Epoch 13 -- Batch 354/ 842, training loss 0.3685321807861328\n",
      "Epoch 13 -- Batch 355/ 842, training loss 0.3648454546928406\n",
      "Epoch 13 -- Batch 356/ 842, training loss 0.37372422218322754\n",
      "Epoch 13 -- Batch 357/ 842, training loss 0.3590240478515625\n",
      "Epoch 13 -- Batch 358/ 842, training loss 0.3692173957824707\n",
      "Epoch 13 -- Batch 359/ 842, training loss 0.3714594542980194\n",
      "Epoch 13 -- Batch 360/ 842, training loss 0.37923693656921387\n",
      "Epoch 13 -- Batch 361/ 842, training loss 0.3612428903579712\n",
      "Epoch 13 -- Batch 362/ 842, training loss 0.36009347438812256\n",
      "Epoch 13 -- Batch 363/ 842, training loss 0.3615175187587738\n",
      "Epoch 13 -- Batch 364/ 842, training loss 0.3655630350112915\n",
      "Epoch 13 -- Batch 365/ 842, training loss 0.37812188267707825\n",
      "Epoch 13 -- Batch 366/ 842, training loss 0.3558303713798523\n",
      "Epoch 13 -- Batch 367/ 842, training loss 0.3821260929107666\n",
      "Epoch 13 -- Batch 368/ 842, training loss 0.3731828033924103\n",
      "Epoch 13 -- Batch 369/ 842, training loss 0.3912285566329956\n",
      "Epoch 13 -- Batch 370/ 842, training loss 0.39090418815612793\n",
      "Epoch 13 -- Batch 371/ 842, training loss 0.37055128812789917\n",
      "Epoch 13 -- Batch 372/ 842, training loss 0.3818565905094147\n",
      "Epoch 13 -- Batch 373/ 842, training loss 0.3678884506225586\n",
      "Epoch 13 -- Batch 374/ 842, training loss 0.36916717886924744\n",
      "Epoch 13 -- Batch 375/ 842, training loss 0.3753085732460022\n",
      "Epoch 13 -- Batch 376/ 842, training loss 0.37779155373573303\n",
      "Epoch 13 -- Batch 377/ 842, training loss 0.37198567390441895\n",
      "Epoch 13 -- Batch 378/ 842, training loss 0.3674813508987427\n",
      "Epoch 13 -- Batch 379/ 842, training loss 0.38494929671287537\n",
      "Epoch 13 -- Batch 380/ 842, training loss 0.3701011538505554\n",
      "Epoch 13 -- Batch 381/ 842, training loss 0.3644607365131378\n",
      "Epoch 13 -- Batch 382/ 842, training loss 0.36821600794792175\n",
      "Epoch 13 -- Batch 383/ 842, training loss 0.38858523964881897\n",
      "Epoch 13 -- Batch 384/ 842, training loss 0.3815745711326599\n",
      "Epoch 13 -- Batch 385/ 842, training loss 0.3694998025894165\n",
      "Epoch 13 -- Batch 386/ 842, training loss 0.3461036682128906\n",
      "Epoch 13 -- Batch 387/ 842, training loss 0.3657723665237427\n",
      "Epoch 13 -- Batch 388/ 842, training loss 0.3745257556438446\n",
      "Epoch 13 -- Batch 389/ 842, training loss 0.38134801387786865\n",
      "Epoch 13 -- Batch 390/ 842, training loss 0.3641338050365448\n",
      "Epoch 13 -- Batch 391/ 842, training loss 0.36312758922576904\n",
      "Epoch 13 -- Batch 392/ 842, training loss 0.36963048577308655\n",
      "Epoch 13 -- Batch 393/ 842, training loss 0.38063302636146545\n",
      "Epoch 13 -- Batch 394/ 842, training loss 0.3643363416194916\n",
      "Epoch 13 -- Batch 395/ 842, training loss 0.36768612265586853\n",
      "Epoch 13 -- Batch 396/ 842, training loss 0.384409099817276\n",
      "Epoch 13 -- Batch 397/ 842, training loss 0.3764798045158386\n",
      "Epoch 13 -- Batch 398/ 842, training loss 0.3677974343299866\n",
      "Epoch 13 -- Batch 399/ 842, training loss 0.3648874759674072\n",
      "Epoch 13 -- Batch 400/ 842, training loss 0.3702315092086792\n",
      "Epoch 13 -- Batch 401/ 842, training loss 0.37247201800346375\n",
      "Epoch 13 -- Batch 402/ 842, training loss 0.363156795501709\n",
      "Epoch 13 -- Batch 403/ 842, training loss 0.36483636498451233\n",
      "Epoch 13 -- Batch 404/ 842, training loss 0.37559840083122253\n",
      "Epoch 13 -- Batch 405/ 842, training loss 0.3632728159427643\n",
      "Epoch 13 -- Batch 406/ 842, training loss 0.36866244673728943\n",
      "Epoch 13 -- Batch 407/ 842, training loss 0.3689565360546112\n",
      "Epoch 13 -- Batch 408/ 842, training loss 0.36954423785209656\n",
      "Epoch 13 -- Batch 409/ 842, training loss 0.37113311886787415\n",
      "Epoch 13 -- Batch 410/ 842, training loss 0.3856852650642395\n",
      "Epoch 13 -- Batch 411/ 842, training loss 0.3728190064430237\n",
      "Epoch 13 -- Batch 412/ 842, training loss 0.359589546918869\n",
      "Epoch 13 -- Batch 413/ 842, training loss 0.3734361529350281\n",
      "Epoch 13 -- Batch 414/ 842, training loss 0.37760597467422485\n",
      "Epoch 13 -- Batch 415/ 842, training loss 0.38299185037612915\n",
      "Epoch 13 -- Batch 416/ 842, training loss 0.36034122109413147\n",
      "Epoch 13 -- Batch 417/ 842, training loss 0.3800276219844818\n",
      "Epoch 13 -- Batch 418/ 842, training loss 0.3646537959575653\n",
      "Epoch 13 -- Batch 419/ 842, training loss 0.3680851459503174\n",
      "Epoch 13 -- Batch 420/ 842, training loss 0.36346572637557983\n",
      "Epoch 13 -- Batch 421/ 842, training loss 0.38242653012275696\n",
      "Epoch 13 -- Batch 422/ 842, training loss 0.3710590898990631\n",
      "Epoch 13 -- Batch 423/ 842, training loss 0.3712843060493469\n",
      "Epoch 13 -- Batch 424/ 842, training loss 0.3707253336906433\n",
      "Epoch 13 -- Batch 425/ 842, training loss 0.37055596709251404\n",
      "Epoch 13 -- Batch 426/ 842, training loss 0.3664412498474121\n",
      "Epoch 13 -- Batch 427/ 842, training loss 0.3946240544319153\n",
      "Epoch 13 -- Batch 428/ 842, training loss 0.3799561858177185\n",
      "Epoch 13 -- Batch 429/ 842, training loss 0.35625872015953064\n",
      "Epoch 13 -- Batch 430/ 842, training loss 0.37574660778045654\n",
      "Epoch 13 -- Batch 431/ 842, training loss 0.368229478597641\n",
      "Epoch 13 -- Batch 432/ 842, training loss 0.36599692702293396\n",
      "Epoch 13 -- Batch 433/ 842, training loss 0.37788671255111694\n",
      "Epoch 13 -- Batch 434/ 842, training loss 0.3750339150428772\n",
      "Epoch 13 -- Batch 435/ 842, training loss 0.36539241671562195\n",
      "Epoch 13 -- Batch 436/ 842, training loss 0.3621361553668976\n",
      "Epoch 13 -- Batch 437/ 842, training loss 0.3523818850517273\n",
      "Epoch 13 -- Batch 438/ 842, training loss 0.35923147201538086\n",
      "Epoch 13 -- Batch 439/ 842, training loss 0.377322793006897\n",
      "Epoch 13 -- Batch 440/ 842, training loss 0.3797142207622528\n",
      "Epoch 13 -- Batch 441/ 842, training loss 0.3809138536453247\n",
      "Epoch 13 -- Batch 442/ 842, training loss 0.38421887159347534\n",
      "Epoch 13 -- Batch 443/ 842, training loss 0.37116676568984985\n",
      "Epoch 13 -- Batch 444/ 842, training loss 0.36964523792266846\n",
      "Epoch 13 -- Batch 445/ 842, training loss 0.37393391132354736\n",
      "Epoch 13 -- Batch 446/ 842, training loss 0.3583858907222748\n",
      "Epoch 13 -- Batch 447/ 842, training loss 0.3759058117866516\n",
      "Epoch 13 -- Batch 448/ 842, training loss 0.3736797571182251\n",
      "Epoch 13 -- Batch 449/ 842, training loss 0.36020949482917786\n",
      "Epoch 13 -- Batch 450/ 842, training loss 0.36877039074897766\n",
      "Epoch 13 -- Batch 451/ 842, training loss 0.3609599173069\n",
      "Epoch 13 -- Batch 452/ 842, training loss 0.3653319776058197\n",
      "Epoch 13 -- Batch 453/ 842, training loss 0.37473440170288086\n",
      "Epoch 13 -- Batch 454/ 842, training loss 0.3983495533466339\n",
      "Epoch 13 -- Batch 455/ 842, training loss 0.3792611360549927\n",
      "Epoch 13 -- Batch 456/ 842, training loss 0.37552520632743835\n",
      "Epoch 13 -- Batch 457/ 842, training loss 0.3652670979499817\n",
      "Epoch 13 -- Batch 458/ 842, training loss 0.38793137669563293\n",
      "Epoch 13 -- Batch 459/ 842, training loss 0.3733782470226288\n",
      "Epoch 13 -- Batch 460/ 842, training loss 0.3775686025619507\n",
      "Epoch 13 -- Batch 461/ 842, training loss 0.3753361701965332\n",
      "Epoch 13 -- Batch 462/ 842, training loss 0.38472428917884827\n",
      "Epoch 13 -- Batch 463/ 842, training loss 0.3660491704940796\n",
      "Epoch 13 -- Batch 464/ 842, training loss 0.40039679408073425\n",
      "Epoch 13 -- Batch 465/ 842, training loss 0.37318912148475647\n",
      "Epoch 13 -- Batch 466/ 842, training loss 0.3718603253364563\n",
      "Epoch 13 -- Batch 467/ 842, training loss 0.36545613408088684\n",
      "Epoch 13 -- Batch 468/ 842, training loss 0.3674163222312927\n",
      "Epoch 13 -- Batch 469/ 842, training loss 0.3713656961917877\n",
      "Epoch 13 -- Batch 470/ 842, training loss 0.3593653440475464\n",
      "Epoch 13 -- Batch 471/ 842, training loss 0.3709888756275177\n",
      "Epoch 13 -- Batch 472/ 842, training loss 0.38095328211784363\n",
      "Epoch 13 -- Batch 473/ 842, training loss 0.37651577591896057\n",
      "Epoch 13 -- Batch 474/ 842, training loss 0.37811729311943054\n",
      "Epoch 13 -- Batch 475/ 842, training loss 0.3721406161785126\n",
      "Epoch 13 -- Batch 476/ 842, training loss 0.38067981600761414\n",
      "Epoch 13 -- Batch 477/ 842, training loss 0.3708246648311615\n",
      "Epoch 13 -- Batch 478/ 842, training loss 0.3721630275249481\n",
      "Epoch 13 -- Batch 479/ 842, training loss 0.36786341667175293\n",
      "Epoch 13 -- Batch 480/ 842, training loss 0.37484851479530334\n",
      "Epoch 13 -- Batch 481/ 842, training loss 0.3774610161781311\n",
      "Epoch 13 -- Batch 482/ 842, training loss 0.3633151352405548\n",
      "Epoch 13 -- Batch 483/ 842, training loss 0.37475940585136414\n",
      "Epoch 13 -- Batch 484/ 842, training loss 0.3672015368938446\n",
      "Epoch 13 -- Batch 485/ 842, training loss 0.383238822221756\n",
      "Epoch 13 -- Batch 486/ 842, training loss 0.3876710534095764\n",
      "Epoch 13 -- Batch 487/ 842, training loss 0.36741483211517334\n",
      "Epoch 13 -- Batch 488/ 842, training loss 0.3857784867286682\n",
      "Epoch 13 -- Batch 489/ 842, training loss 0.36687615513801575\n",
      "Epoch 13 -- Batch 490/ 842, training loss 0.37160971760749817\n",
      "Epoch 13 -- Batch 491/ 842, training loss 0.35651445388793945\n",
      "Epoch 13 -- Batch 492/ 842, training loss 0.37877315282821655\n",
      "Epoch 13 -- Batch 493/ 842, training loss 0.36068665981292725\n",
      "Epoch 13 -- Batch 494/ 842, training loss 0.37655577063560486\n",
      "Epoch 13 -- Batch 495/ 842, training loss 0.3720631003379822\n",
      "Epoch 13 -- Batch 496/ 842, training loss 0.37967100739479065\n",
      "Epoch 13 -- Batch 497/ 842, training loss 0.37294021248817444\n",
      "Epoch 13 -- Batch 498/ 842, training loss 0.370582640171051\n",
      "Epoch 13 -- Batch 499/ 842, training loss 0.3771035075187683\n",
      "Epoch 13 -- Batch 500/ 842, training loss 0.3594551980495453\n",
      "Epoch 13 -- Batch 501/ 842, training loss 0.3704373836517334\n",
      "Epoch 13 -- Batch 502/ 842, training loss 0.37083184719085693\n",
      "Epoch 13 -- Batch 503/ 842, training loss 0.3600230813026428\n",
      "Epoch 13 -- Batch 504/ 842, training loss 0.3746644854545593\n",
      "Epoch 13 -- Batch 505/ 842, training loss 0.36797666549682617\n",
      "Epoch 13 -- Batch 506/ 842, training loss 0.3787471652030945\n",
      "Epoch 13 -- Batch 507/ 842, training loss 0.35143154859542847\n",
      "Epoch 13 -- Batch 508/ 842, training loss 0.3699568510055542\n",
      "Epoch 13 -- Batch 509/ 842, training loss 0.3790285885334015\n",
      "Epoch 13 -- Batch 510/ 842, training loss 0.37683841586112976\n",
      "Epoch 13 -- Batch 511/ 842, training loss 0.374400794506073\n",
      "Epoch 13 -- Batch 512/ 842, training loss 0.36557862162590027\n",
      "Epoch 13 -- Batch 513/ 842, training loss 0.39041605591773987\n",
      "Epoch 13 -- Batch 514/ 842, training loss 0.39029601216316223\n",
      "Epoch 13 -- Batch 515/ 842, training loss 0.36575353145599365\n",
      "Epoch 13 -- Batch 516/ 842, training loss 0.3826376497745514\n",
      "Epoch 13 -- Batch 517/ 842, training loss 0.3776611089706421\n",
      "Epoch 13 -- Batch 518/ 842, training loss 0.38143831491470337\n",
      "Epoch 13 -- Batch 519/ 842, training loss 0.3943076729774475\n",
      "Epoch 13 -- Batch 520/ 842, training loss 0.3720708191394806\n",
      "Epoch 13 -- Batch 521/ 842, training loss 0.3629244565963745\n",
      "Epoch 13 -- Batch 522/ 842, training loss 0.3705857992172241\n",
      "Epoch 13 -- Batch 523/ 842, training loss 0.3756504952907562\n",
      "Epoch 13 -- Batch 524/ 842, training loss 0.37341541051864624\n",
      "Epoch 13 -- Batch 525/ 842, training loss 0.38397765159606934\n",
      "Epoch 13 -- Batch 526/ 842, training loss 0.36421507596969604\n",
      "Epoch 13 -- Batch 527/ 842, training loss 0.3656076490879059\n",
      "Epoch 13 -- Batch 528/ 842, training loss 0.37457430362701416\n",
      "Epoch 13 -- Batch 529/ 842, training loss 0.38086849451065063\n",
      "Epoch 13 -- Batch 530/ 842, training loss 0.36457863450050354\n",
      "Epoch 13 -- Batch 531/ 842, training loss 0.36955466866493225\n",
      "Epoch 13 -- Batch 532/ 842, training loss 0.3516201376914978\n",
      "Epoch 13 -- Batch 533/ 842, training loss 0.37710878252983093\n",
      "Epoch 13 -- Batch 534/ 842, training loss 0.37288814783096313\n",
      "Epoch 13 -- Batch 535/ 842, training loss 0.37782809138298035\n",
      "Epoch 13 -- Batch 536/ 842, training loss 0.37180376052856445\n",
      "Epoch 13 -- Batch 537/ 842, training loss 0.36383745074272156\n",
      "Epoch 13 -- Batch 538/ 842, training loss 0.37136605381965637\n",
      "Epoch 13 -- Batch 539/ 842, training loss 0.36114081740379333\n",
      "Epoch 13 -- Batch 540/ 842, training loss 0.3739229738712311\n",
      "Epoch 13 -- Batch 541/ 842, training loss 0.38342151045799255\n",
      "Epoch 13 -- Batch 542/ 842, training loss 0.37349432706832886\n",
      "Epoch 13 -- Batch 543/ 842, training loss 0.36490005254745483\n",
      "Epoch 13 -- Batch 544/ 842, training loss 0.35914909839630127\n",
      "Epoch 13 -- Batch 545/ 842, training loss 0.3813581168651581\n",
      "Epoch 13 -- Batch 546/ 842, training loss 0.3765266537666321\n",
      "Epoch 13 -- Batch 547/ 842, training loss 0.36117637157440186\n",
      "Epoch 13 -- Batch 548/ 842, training loss 0.3773079216480255\n",
      "Epoch 13 -- Batch 549/ 842, training loss 0.37659531831741333\n",
      "Epoch 13 -- Batch 550/ 842, training loss 0.36773350834846497\n",
      "Epoch 13 -- Batch 551/ 842, training loss 0.37033119797706604\n",
      "Epoch 13 -- Batch 552/ 842, training loss 0.3804851174354553\n",
      "Epoch 13 -- Batch 553/ 842, training loss 0.3639127016067505\n",
      "Epoch 13 -- Batch 554/ 842, training loss 0.37266460061073303\n",
      "Epoch 13 -- Batch 555/ 842, training loss 0.36583831906318665\n",
      "Epoch 13 -- Batch 556/ 842, training loss 0.3621881306171417\n",
      "Epoch 13 -- Batch 557/ 842, training loss 0.36346378922462463\n",
      "Epoch 13 -- Batch 558/ 842, training loss 0.3773512840270996\n",
      "Epoch 13 -- Batch 559/ 842, training loss 0.3756474554538727\n",
      "Epoch 13 -- Batch 560/ 842, training loss 0.3659968972206116\n",
      "Epoch 13 -- Batch 561/ 842, training loss 0.3744565546512604\n",
      "Epoch 13 -- Batch 562/ 842, training loss 0.3696542978286743\n",
      "Epoch 13 -- Batch 563/ 842, training loss 0.38318830728530884\n",
      "Epoch 13 -- Batch 564/ 842, training loss 0.37643536925315857\n",
      "Epoch 13 -- Batch 565/ 842, training loss 0.3701087236404419\n",
      "Epoch 13 -- Batch 566/ 842, training loss 0.36546167731285095\n",
      "Epoch 13 -- Batch 567/ 842, training loss 0.359364777803421\n",
      "Epoch 13 -- Batch 568/ 842, training loss 0.3603394627571106\n",
      "Epoch 13 -- Batch 569/ 842, training loss 0.3866839110851288\n",
      "Epoch 13 -- Batch 570/ 842, training loss 0.35967817902565\n",
      "Epoch 13 -- Batch 571/ 842, training loss 0.3618834316730499\n",
      "Epoch 13 -- Batch 572/ 842, training loss 0.37942972779273987\n",
      "Epoch 13 -- Batch 573/ 842, training loss 0.37291133403778076\n",
      "Epoch 13 -- Batch 574/ 842, training loss 0.36757439374923706\n",
      "Epoch 13 -- Batch 575/ 842, training loss 0.3697235882282257\n",
      "Epoch 13 -- Batch 576/ 842, training loss 0.36695367097854614\n",
      "Epoch 13 -- Batch 577/ 842, training loss 0.3771477937698364\n",
      "Epoch 13 -- Batch 578/ 842, training loss 0.3595094084739685\n",
      "Epoch 13 -- Batch 579/ 842, training loss 0.3734973967075348\n",
      "Epoch 13 -- Batch 580/ 842, training loss 0.3683561086654663\n",
      "Epoch 13 -- Batch 581/ 842, training loss 0.36840540170669556\n",
      "Epoch 13 -- Batch 582/ 842, training loss 0.3711180090904236\n",
      "Epoch 13 -- Batch 583/ 842, training loss 0.3709394931793213\n",
      "Epoch 13 -- Batch 584/ 842, training loss 0.37241366505622864\n",
      "Epoch 13 -- Batch 585/ 842, training loss 0.37184658646583557\n",
      "Epoch 13 -- Batch 586/ 842, training loss 0.3825002908706665\n",
      "Epoch 13 -- Batch 587/ 842, training loss 0.36609092354774475\n",
      "Epoch 13 -- Batch 588/ 842, training loss 0.3736095726490021\n",
      "Epoch 13 -- Batch 589/ 842, training loss 0.371623694896698\n",
      "Epoch 13 -- Batch 590/ 842, training loss 0.37515494227409363\n",
      "Epoch 13 -- Batch 591/ 842, training loss 0.36929282546043396\n",
      "Epoch 13 -- Batch 592/ 842, training loss 0.37802961468696594\n",
      "Epoch 13 -- Batch 593/ 842, training loss 0.3841170370578766\n",
      "Epoch 13 -- Batch 594/ 842, training loss 0.3671635091304779\n",
      "Epoch 13 -- Batch 595/ 842, training loss 0.38063374161720276\n",
      "Epoch 13 -- Batch 596/ 842, training loss 0.3768557906150818\n",
      "Epoch 13 -- Batch 597/ 842, training loss 0.36436182260513306\n",
      "Epoch 13 -- Batch 598/ 842, training loss 0.37927675247192383\n",
      "Epoch 13 -- Batch 599/ 842, training loss 0.3648817539215088\n",
      "Epoch 13 -- Batch 600/ 842, training loss 0.3570866584777832\n",
      "Epoch 13 -- Batch 601/ 842, training loss 0.36748194694519043\n",
      "Epoch 13 -- Batch 602/ 842, training loss 0.3779639005661011\n",
      "Epoch 13 -- Batch 603/ 842, training loss 0.3692798912525177\n",
      "Epoch 13 -- Batch 604/ 842, training loss 0.383449912071228\n",
      "Epoch 13 -- Batch 605/ 842, training loss 0.3800031542778015\n",
      "Epoch 13 -- Batch 606/ 842, training loss 0.37309396266937256\n",
      "Epoch 13 -- Batch 607/ 842, training loss 0.36648258566856384\n",
      "Epoch 13 -- Batch 608/ 842, training loss 0.38532423973083496\n",
      "Epoch 13 -- Batch 609/ 842, training loss 0.37011465430259705\n",
      "Epoch 13 -- Batch 610/ 842, training loss 0.35412463545799255\n",
      "Epoch 13 -- Batch 611/ 842, training loss 0.3643747866153717\n",
      "Epoch 13 -- Batch 612/ 842, training loss 0.3619925379753113\n",
      "Epoch 13 -- Batch 613/ 842, training loss 0.3675546944141388\n",
      "Epoch 13 -- Batch 614/ 842, training loss 0.36966198682785034\n",
      "Epoch 13 -- Batch 615/ 842, training loss 0.3673070967197418\n",
      "Epoch 13 -- Batch 616/ 842, training loss 0.3916245996952057\n",
      "Epoch 13 -- Batch 617/ 842, training loss 0.3689568042755127\n",
      "Epoch 13 -- Batch 618/ 842, training loss 0.3667079508304596\n",
      "Epoch 13 -- Batch 619/ 842, training loss 0.3636437952518463\n",
      "Epoch 13 -- Batch 620/ 842, training loss 0.3743980824947357\n",
      "Epoch 13 -- Batch 621/ 842, training loss 0.37630003690719604\n",
      "Epoch 13 -- Batch 622/ 842, training loss 0.3747078776359558\n",
      "Epoch 13 -- Batch 623/ 842, training loss 0.37386810779571533\n",
      "Epoch 13 -- Batch 624/ 842, training loss 0.3666921854019165\n",
      "Epoch 13 -- Batch 625/ 842, training loss 0.37896114587783813\n",
      "Epoch 13 -- Batch 626/ 842, training loss 0.3912103772163391\n",
      "Epoch 13 -- Batch 627/ 842, training loss 0.3679564595222473\n",
      "Epoch 13 -- Batch 628/ 842, training loss 0.35958370566368103\n",
      "Epoch 13 -- Batch 629/ 842, training loss 0.3669146001338959\n",
      "Epoch 13 -- Batch 630/ 842, training loss 0.37443435192108154\n",
      "Epoch 13 -- Batch 631/ 842, training loss 0.36649417877197266\n",
      "Epoch 13 -- Batch 632/ 842, training loss 0.36954009532928467\n",
      "Epoch 13 -- Batch 633/ 842, training loss 0.3585801124572754\n",
      "Epoch 13 -- Batch 634/ 842, training loss 0.36810022592544556\n",
      "Epoch 13 -- Batch 635/ 842, training loss 0.3660610020160675\n",
      "Epoch 13 -- Batch 636/ 842, training loss 0.3684435784816742\n",
      "Epoch 13 -- Batch 637/ 842, training loss 0.369905948638916\n",
      "Epoch 13 -- Batch 638/ 842, training loss 0.35902637243270874\n",
      "Epoch 13 -- Batch 639/ 842, training loss 0.36308377981185913\n",
      "Epoch 13 -- Batch 640/ 842, training loss 0.3764599561691284\n",
      "Epoch 13 -- Batch 641/ 842, training loss 0.3791710138320923\n",
      "Epoch 13 -- Batch 642/ 842, training loss 0.3623071610927582\n",
      "Epoch 13 -- Batch 643/ 842, training loss 0.37954598665237427\n",
      "Epoch 13 -- Batch 644/ 842, training loss 0.37687262892723083\n",
      "Epoch 13 -- Batch 645/ 842, training loss 0.36703330278396606\n",
      "Epoch 13 -- Batch 646/ 842, training loss 0.3732878863811493\n",
      "Epoch 13 -- Batch 647/ 842, training loss 0.3802005350589752\n",
      "Epoch 13 -- Batch 648/ 842, training loss 0.37110888957977295\n",
      "Epoch 13 -- Batch 649/ 842, training loss 0.37166061997413635\n",
      "Epoch 13 -- Batch 650/ 842, training loss 0.37188228964805603\n",
      "Epoch 13 -- Batch 651/ 842, training loss 0.3571156859397888\n",
      "Epoch 13 -- Batch 652/ 842, training loss 0.36839181184768677\n",
      "Epoch 13 -- Batch 653/ 842, training loss 0.37195220589637756\n",
      "Epoch 13 -- Batch 654/ 842, training loss 0.36750519275665283\n",
      "Epoch 13 -- Batch 655/ 842, training loss 0.3842293620109558\n",
      "Epoch 13 -- Batch 656/ 842, training loss 0.37298300862312317\n",
      "Epoch 13 -- Batch 657/ 842, training loss 0.3709949254989624\n",
      "Epoch 13 -- Batch 658/ 842, training loss 0.3764125406742096\n",
      "Epoch 13 -- Batch 659/ 842, training loss 0.37961846590042114\n",
      "Epoch 13 -- Batch 660/ 842, training loss 0.36813417077064514\n",
      "Epoch 13 -- Batch 661/ 842, training loss 0.37386640906333923\n",
      "Epoch 13 -- Batch 662/ 842, training loss 0.36698412895202637\n",
      "Epoch 13 -- Batch 663/ 842, training loss 0.37679407000541687\n",
      "Epoch 13 -- Batch 664/ 842, training loss 0.3755200505256653\n",
      "Epoch 13 -- Batch 665/ 842, training loss 0.3683566451072693\n",
      "Epoch 13 -- Batch 666/ 842, training loss 0.3725992441177368\n",
      "Epoch 13 -- Batch 667/ 842, training loss 0.3879337012767792\n",
      "Epoch 13 -- Batch 668/ 842, training loss 0.36793753504753113\n",
      "Epoch 13 -- Batch 669/ 842, training loss 0.35588183999061584\n",
      "Epoch 13 -- Batch 670/ 842, training loss 0.3649007976055145\n",
      "Epoch 13 -- Batch 671/ 842, training loss 0.35959744453430176\n",
      "Epoch 13 -- Batch 672/ 842, training loss 0.37334418296813965\n",
      "Epoch 13 -- Batch 673/ 842, training loss 0.3813800513744354\n",
      "Epoch 13 -- Batch 674/ 842, training loss 0.36811012029647827\n",
      "Epoch 13 -- Batch 675/ 842, training loss 0.3632197678089142\n",
      "Epoch 13 -- Batch 676/ 842, training loss 0.37793901562690735\n",
      "Epoch 13 -- Batch 677/ 842, training loss 0.36462628841400146\n",
      "Epoch 13 -- Batch 678/ 842, training loss 0.3670996129512787\n",
      "Epoch 13 -- Batch 679/ 842, training loss 0.3808690309524536\n",
      "Epoch 13 -- Batch 680/ 842, training loss 0.367257684469223\n",
      "Epoch 13 -- Batch 681/ 842, training loss 0.37408870458602905\n",
      "Epoch 13 -- Batch 682/ 842, training loss 0.3702738583087921\n",
      "Epoch 13 -- Batch 683/ 842, training loss 0.36910921335220337\n",
      "Epoch 13 -- Batch 684/ 842, training loss 0.3825328052043915\n",
      "Epoch 13 -- Batch 685/ 842, training loss 0.3731883466243744\n",
      "Epoch 13 -- Batch 686/ 842, training loss 0.36284807324409485\n",
      "Epoch 13 -- Batch 687/ 842, training loss 0.38247841596603394\n",
      "Epoch 13 -- Batch 688/ 842, training loss 0.36309099197387695\n",
      "Epoch 13 -- Batch 689/ 842, training loss 0.35917797684669495\n",
      "Epoch 13 -- Batch 690/ 842, training loss 0.39041879773139954\n",
      "Epoch 13 -- Batch 691/ 842, training loss 0.3699878454208374\n",
      "Epoch 13 -- Batch 692/ 842, training loss 0.37547749280929565\n",
      "Epoch 13 -- Batch 693/ 842, training loss 0.3544718027114868\n",
      "Epoch 13 -- Batch 694/ 842, training loss 0.3651009500026703\n",
      "Epoch 13 -- Batch 695/ 842, training loss 0.3545893728733063\n",
      "Epoch 13 -- Batch 696/ 842, training loss 0.3777965009212494\n",
      "Epoch 13 -- Batch 697/ 842, training loss 0.36535847187042236\n",
      "Epoch 13 -- Batch 698/ 842, training loss 0.3568548262119293\n",
      "Epoch 13 -- Batch 699/ 842, training loss 0.37146273255348206\n",
      "Epoch 13 -- Batch 700/ 842, training loss 0.3714044690132141\n",
      "Epoch 13 -- Batch 701/ 842, training loss 0.38302314281463623\n",
      "Epoch 13 -- Batch 702/ 842, training loss 0.3631848692893982\n",
      "Epoch 13 -- Batch 703/ 842, training loss 0.36585283279418945\n",
      "Epoch 13 -- Batch 704/ 842, training loss 0.35921233892440796\n",
      "Epoch 13 -- Batch 705/ 842, training loss 0.3853980302810669\n",
      "Epoch 13 -- Batch 706/ 842, training loss 0.36361396312713623\n",
      "Epoch 13 -- Batch 707/ 842, training loss 0.37006914615631104\n",
      "Epoch 13 -- Batch 708/ 842, training loss 0.36527082324028015\n",
      "Epoch 13 -- Batch 709/ 842, training loss 0.37111398577690125\n",
      "Epoch 13 -- Batch 710/ 842, training loss 0.3657400906085968\n",
      "Epoch 13 -- Batch 711/ 842, training loss 0.3770343065261841\n",
      "Epoch 13 -- Batch 712/ 842, training loss 0.36036673188209534\n",
      "Epoch 13 -- Batch 713/ 842, training loss 0.37889379262924194\n",
      "Epoch 13 -- Batch 714/ 842, training loss 0.3808976709842682\n",
      "Epoch 13 -- Batch 715/ 842, training loss 0.36816921830177307\n",
      "Epoch 13 -- Batch 716/ 842, training loss 0.36610686779022217\n",
      "Epoch 13 -- Batch 717/ 842, training loss 0.3640521466732025\n",
      "Epoch 13 -- Batch 718/ 842, training loss 0.3777064085006714\n",
      "Epoch 13 -- Batch 719/ 842, training loss 0.37181714177131653\n",
      "Epoch 13 -- Batch 720/ 842, training loss 0.3803548216819763\n",
      "Epoch 13 -- Batch 721/ 842, training loss 0.37304049730300903\n",
      "Epoch 13 -- Batch 722/ 842, training loss 0.36194390058517456\n",
      "Epoch 13 -- Batch 723/ 842, training loss 0.3724524974822998\n",
      "Epoch 13 -- Batch 724/ 842, training loss 0.3796292543411255\n",
      "Epoch 13 -- Batch 725/ 842, training loss 0.37403303384780884\n",
      "Epoch 13 -- Batch 726/ 842, training loss 0.3610819876194\n",
      "Epoch 13 -- Batch 727/ 842, training loss 0.3840882480144501\n",
      "Epoch 13 -- Batch 728/ 842, training loss 0.3740680515766144\n",
      "Epoch 13 -- Batch 729/ 842, training loss 0.3671691417694092\n",
      "Epoch 13 -- Batch 730/ 842, training loss 0.37922900915145874\n",
      "Epoch 13 -- Batch 731/ 842, training loss 0.35619470477104187\n",
      "Epoch 13 -- Batch 732/ 842, training loss 0.3708917796611786\n",
      "Epoch 13 -- Batch 733/ 842, training loss 0.36740049719810486\n",
      "Epoch 13 -- Batch 734/ 842, training loss 0.37837135791778564\n",
      "Epoch 13 -- Batch 735/ 842, training loss 0.37218692898750305\n",
      "Epoch 13 -- Batch 736/ 842, training loss 0.38509756326675415\n",
      "Epoch 13 -- Batch 737/ 842, training loss 0.3715992569923401\n",
      "Epoch 13 -- Batch 738/ 842, training loss 0.37637126445770264\n",
      "Epoch 13 -- Batch 739/ 842, training loss 0.3735192120075226\n",
      "Epoch 13 -- Batch 740/ 842, training loss 0.3794993758201599\n",
      "Epoch 13 -- Batch 741/ 842, training loss 0.36818253993988037\n",
      "Epoch 13 -- Batch 742/ 842, training loss 0.3804177939891815\n",
      "Epoch 13 -- Batch 743/ 842, training loss 0.3715013265609741\n",
      "Epoch 13 -- Batch 744/ 842, training loss 0.37088754773139954\n",
      "Epoch 13 -- Batch 745/ 842, training loss 0.3709133267402649\n",
      "Epoch 13 -- Batch 746/ 842, training loss 0.37035706639289856\n",
      "Epoch 13 -- Batch 747/ 842, training loss 0.3688405156135559\n",
      "Epoch 13 -- Batch 748/ 842, training loss 0.3543953597545624\n",
      "Epoch 13 -- Batch 749/ 842, training loss 0.3773810565471649\n",
      "Epoch 13 -- Batch 750/ 842, training loss 0.3659767210483551\n",
      "Epoch 13 -- Batch 751/ 842, training loss 0.3655812740325928\n",
      "Epoch 13 -- Batch 752/ 842, training loss 0.3576813042163849\n",
      "Epoch 13 -- Batch 753/ 842, training loss 0.37091872096061707\n",
      "Epoch 13 -- Batch 754/ 842, training loss 0.3814117908477783\n",
      "Epoch 13 -- Batch 755/ 842, training loss 0.3899926245212555\n",
      "Epoch 13 -- Batch 756/ 842, training loss 0.3762207329273224\n",
      "Epoch 13 -- Batch 757/ 842, training loss 0.3792480230331421\n",
      "Epoch 13 -- Batch 758/ 842, training loss 0.37169089913368225\n",
      "Epoch 13 -- Batch 759/ 842, training loss 0.3657793402671814\n",
      "Epoch 13 -- Batch 760/ 842, training loss 0.37608012557029724\n",
      "Epoch 13 -- Batch 761/ 842, training loss 0.36679983139038086\n",
      "Epoch 13 -- Batch 762/ 842, training loss 0.37411192059516907\n",
      "Epoch 13 -- Batch 763/ 842, training loss 0.3657516837120056\n",
      "Epoch 13 -- Batch 764/ 842, training loss 0.37182512879371643\n",
      "Epoch 13 -- Batch 765/ 842, training loss 0.3613815903663635\n",
      "Epoch 13 -- Batch 766/ 842, training loss 0.3787592649459839\n",
      "Epoch 13 -- Batch 767/ 842, training loss 0.3736196458339691\n",
      "Epoch 13 -- Batch 768/ 842, training loss 0.3787252604961395\n",
      "Epoch 13 -- Batch 769/ 842, training loss 0.3782225549221039\n",
      "Epoch 13 -- Batch 770/ 842, training loss 0.3680216073989868\n",
      "Epoch 13 -- Batch 771/ 842, training loss 0.37474724650382996\n",
      "Epoch 13 -- Batch 772/ 842, training loss 0.35802406072616577\n",
      "Epoch 13 -- Batch 773/ 842, training loss 0.3627950847148895\n",
      "Epoch 13 -- Batch 774/ 842, training loss 0.3652713894844055\n",
      "Epoch 13 -- Batch 775/ 842, training loss 0.3791615664958954\n",
      "Epoch 13 -- Batch 776/ 842, training loss 0.36659061908721924\n",
      "Epoch 13 -- Batch 777/ 842, training loss 0.36934059858322144\n",
      "Epoch 13 -- Batch 778/ 842, training loss 0.3557446300983429\n",
      "Epoch 13 -- Batch 779/ 842, training loss 0.3599313497543335\n",
      "Epoch 13 -- Batch 780/ 842, training loss 0.37877926230430603\n",
      "Epoch 13 -- Batch 781/ 842, training loss 0.36910438537597656\n",
      "Epoch 13 -- Batch 782/ 842, training loss 0.380974143743515\n",
      "Epoch 13 -- Batch 783/ 842, training loss 0.3673813045024872\n",
      "Epoch 13 -- Batch 784/ 842, training loss 0.3624691367149353\n",
      "Epoch 13 -- Batch 785/ 842, training loss 0.37653425335884094\n",
      "Epoch 13 -- Batch 786/ 842, training loss 0.3628937602043152\n",
      "Epoch 13 -- Batch 787/ 842, training loss 0.3819500803947449\n",
      "Epoch 13 -- Batch 788/ 842, training loss 0.3730921447277069\n",
      "Epoch 13 -- Batch 789/ 842, training loss 0.3593575954437256\n",
      "Epoch 13 -- Batch 790/ 842, training loss 0.37138763070106506\n",
      "Epoch 13 -- Batch 791/ 842, training loss 0.3710707724094391\n",
      "Epoch 13 -- Batch 792/ 842, training loss 0.3759254515171051\n",
      "Epoch 13 -- Batch 793/ 842, training loss 0.371198832988739\n",
      "Epoch 13 -- Batch 794/ 842, training loss 0.3678237795829773\n",
      "Epoch 13 -- Batch 795/ 842, training loss 0.3594139516353607\n",
      "Epoch 13 -- Batch 796/ 842, training loss 0.3623631000518799\n",
      "Epoch 13 -- Batch 797/ 842, training loss 0.36793678998947144\n",
      "Epoch 13 -- Batch 798/ 842, training loss 0.3658851385116577\n",
      "Epoch 13 -- Batch 799/ 842, training loss 0.365690141916275\n",
      "Epoch 13 -- Batch 800/ 842, training loss 0.3718048334121704\n",
      "Epoch 13 -- Batch 801/ 842, training loss 0.36175158619880676\n",
      "Epoch 13 -- Batch 802/ 842, training loss 0.37445566058158875\n",
      "Epoch 13 -- Batch 803/ 842, training loss 0.3612535893917084\n",
      "Epoch 13 -- Batch 804/ 842, training loss 0.3823469877243042\n",
      "Epoch 13 -- Batch 805/ 842, training loss 0.3655107319355011\n",
      "Epoch 13 -- Batch 806/ 842, training loss 0.36338144540786743\n",
      "Epoch 13 -- Batch 807/ 842, training loss 0.35844647884368896\n",
      "Epoch 13 -- Batch 808/ 842, training loss 0.36684122681617737\n",
      "Epoch 13 -- Batch 809/ 842, training loss 0.3624033033847809\n",
      "Epoch 13 -- Batch 810/ 842, training loss 0.3759799599647522\n",
      "Epoch 13 -- Batch 811/ 842, training loss 0.3708603084087372\n",
      "Epoch 13 -- Batch 812/ 842, training loss 0.35275331139564514\n",
      "Epoch 13 -- Batch 813/ 842, training loss 0.37494760751724243\n",
      "Epoch 13 -- Batch 814/ 842, training loss 0.37653014063835144\n",
      "Epoch 13 -- Batch 815/ 842, training loss 0.3640425205230713\n",
      "Epoch 13 -- Batch 816/ 842, training loss 0.37116512656211853\n",
      "Epoch 13 -- Batch 817/ 842, training loss 0.3617619276046753\n",
      "Epoch 13 -- Batch 818/ 842, training loss 0.37623411417007446\n",
      "Epoch 13 -- Batch 819/ 842, training loss 0.3799247741699219\n",
      "Epoch 13 -- Batch 820/ 842, training loss 0.3685958981513977\n",
      "Epoch 13 -- Batch 821/ 842, training loss 0.3574434816837311\n",
      "Epoch 13 -- Batch 822/ 842, training loss 0.3639671802520752\n",
      "Epoch 13 -- Batch 823/ 842, training loss 0.3606848418712616\n",
      "Epoch 13 -- Batch 824/ 842, training loss 0.3738630414009094\n",
      "Epoch 13 -- Batch 825/ 842, training loss 0.3749465048313141\n",
      "Epoch 13 -- Batch 826/ 842, training loss 0.3668234646320343\n",
      "Epoch 13 -- Batch 827/ 842, training loss 0.35815078020095825\n",
      "Epoch 13 -- Batch 828/ 842, training loss 0.3673282265663147\n",
      "Epoch 13 -- Batch 829/ 842, training loss 0.38921651244163513\n",
      "Epoch 13 -- Batch 830/ 842, training loss 0.3620087504386902\n",
      "Epoch 13 -- Batch 831/ 842, training loss 0.3651604652404785\n",
      "Epoch 13 -- Batch 832/ 842, training loss 0.36290302872657776\n",
      "Epoch 13 -- Batch 833/ 842, training loss 0.37253788113594055\n",
      "Epoch 13 -- Batch 834/ 842, training loss 0.36562252044677734\n",
      "Epoch 13 -- Batch 835/ 842, training loss 0.3645049035549164\n",
      "Epoch 13 -- Batch 836/ 842, training loss 0.3650837540626526\n",
      "Epoch 13 -- Batch 837/ 842, training loss 0.3685312867164612\n",
      "Epoch 13 -- Batch 838/ 842, training loss 0.371095210313797\n",
      "Epoch 13 -- Batch 839/ 842, training loss 0.35794398188591003\n",
      "Epoch 13 -- Batch 840/ 842, training loss 0.3756679594516754\n",
      "Epoch 13 -- Batch 841/ 842, training loss 0.37484943866729736\n",
      "Epoch 13 -- Batch 842/ 842, training loss 0.3449031412601471\n",
      "----------------------------------------------------------------------\n",
      "Epoch 13 -- Batch 1/ 94, validation loss 0.3608498275279999\n",
      "Epoch 13 -- Batch 2/ 94, validation loss 0.34595006704330444\n",
      "Epoch 13 -- Batch 3/ 94, validation loss 0.35791099071502686\n",
      "Epoch 13 -- Batch 4/ 94, validation loss 0.3558577597141266\n",
      "Epoch 13 -- Batch 5/ 94, validation loss 0.3549536466598511\n",
      "Epoch 13 -- Batch 6/ 94, validation loss 0.3627859055995941\n",
      "Epoch 13 -- Batch 7/ 94, validation loss 0.3530852496623993\n",
      "Epoch 13 -- Batch 8/ 94, validation loss 0.38049519062042236\n",
      "Epoch 13 -- Batch 9/ 94, validation loss 0.38580548763275146\n",
      "Epoch 13 -- Batch 10/ 94, validation loss 0.3599557876586914\n",
      "Epoch 13 -- Batch 11/ 94, validation loss 0.3703708350658417\n",
      "Epoch 13 -- Batch 12/ 94, validation loss 0.39443138241767883\n",
      "Epoch 13 -- Batch 13/ 94, validation loss 0.35671064257621765\n",
      "Epoch 13 -- Batch 14/ 94, validation loss 0.35791414976119995\n",
      "Epoch 13 -- Batch 15/ 94, validation loss 0.37538307905197144\n",
      "Epoch 13 -- Batch 16/ 94, validation loss 0.3675376772880554\n",
      "Epoch 13 -- Batch 17/ 94, validation loss 0.37461531162261963\n",
      "Epoch 13 -- Batch 18/ 94, validation loss 0.3557283580303192\n",
      "Epoch 13 -- Batch 19/ 94, validation loss 0.37034842371940613\n",
      "Epoch 13 -- Batch 20/ 94, validation loss 0.3689882755279541\n",
      "Epoch 13 -- Batch 21/ 94, validation loss 0.3676706552505493\n",
      "Epoch 13 -- Batch 22/ 94, validation loss 0.3690286874771118\n",
      "Epoch 13 -- Batch 23/ 94, validation loss 0.3528713583946228\n",
      "Epoch 13 -- Batch 24/ 94, validation loss 0.3651926815509796\n",
      "Epoch 13 -- Batch 25/ 94, validation loss 0.38028138875961304\n",
      "Epoch 13 -- Batch 26/ 94, validation loss 0.3556625247001648\n",
      "Epoch 13 -- Batch 27/ 94, validation loss 0.3544468879699707\n",
      "Epoch 13 -- Batch 28/ 94, validation loss 0.36280909180641174\n",
      "Epoch 13 -- Batch 29/ 94, validation loss 0.35784202814102173\n",
      "Epoch 13 -- Batch 30/ 94, validation loss 0.36730697751045227\n",
      "Epoch 13 -- Batch 31/ 94, validation loss 0.36907488107681274\n",
      "Epoch 13 -- Batch 32/ 94, validation loss 0.3453594744205475\n",
      "Epoch 13 -- Batch 33/ 94, validation loss 0.3675752580165863\n",
      "Epoch 13 -- Batch 34/ 94, validation loss 0.3809572756290436\n",
      "Epoch 13 -- Batch 35/ 94, validation loss 0.3665182590484619\n",
      "Epoch 13 -- Batch 36/ 94, validation loss 0.3572910726070404\n",
      "Epoch 13 -- Batch 37/ 94, validation loss 0.3742649555206299\n",
      "Epoch 13 -- Batch 38/ 94, validation loss 0.3465498983860016\n",
      "Epoch 13 -- Batch 39/ 94, validation loss 0.3594292104244232\n",
      "Epoch 13 -- Batch 40/ 94, validation loss 0.3725971579551697\n",
      "Epoch 13 -- Batch 41/ 94, validation loss 0.3712587058544159\n",
      "Epoch 13 -- Batch 42/ 94, validation loss 0.3642231822013855\n",
      "Epoch 13 -- Batch 43/ 94, validation loss 0.3634364604949951\n",
      "Epoch 13 -- Batch 44/ 94, validation loss 0.3622500002384186\n",
      "Epoch 13 -- Batch 45/ 94, validation loss 0.3634830117225647\n",
      "Epoch 13 -- Batch 46/ 94, validation loss 0.3706836700439453\n",
      "Epoch 13 -- Batch 47/ 94, validation loss 0.364541232585907\n",
      "Epoch 13 -- Batch 48/ 94, validation loss 0.3677692413330078\n",
      "Epoch 13 -- Batch 49/ 94, validation loss 0.37105458974838257\n",
      "Epoch 13 -- Batch 50/ 94, validation loss 0.3606790006160736\n",
      "Epoch 13 -- Batch 51/ 94, validation loss 0.37241053581237793\n",
      "Epoch 13 -- Batch 52/ 94, validation loss 0.3613746166229248\n",
      "Epoch 13 -- Batch 53/ 94, validation loss 0.36836183071136475\n",
      "Epoch 13 -- Batch 54/ 94, validation loss 0.3668055832386017\n",
      "Epoch 13 -- Batch 55/ 94, validation loss 0.36464688181877136\n",
      "Epoch 13 -- Batch 56/ 94, validation loss 0.3594423234462738\n",
      "Epoch 13 -- Batch 57/ 94, validation loss 0.37623435258865356\n",
      "Epoch 13 -- Batch 58/ 94, validation loss 0.3986850380897522\n",
      "Epoch 13 -- Batch 59/ 94, validation loss 0.36234456300735474\n",
      "Epoch 13 -- Batch 60/ 94, validation loss 0.36955899000167847\n",
      "Epoch 13 -- Batch 61/ 94, validation loss 0.37091219425201416\n",
      "Epoch 13 -- Batch 62/ 94, validation loss 0.36807015538215637\n",
      "Epoch 13 -- Batch 63/ 94, validation loss 0.36204853653907776\n",
      "Epoch 13 -- Batch 64/ 94, validation loss 0.3554263710975647\n",
      "Epoch 13 -- Batch 65/ 94, validation loss 0.3661029636859894\n",
      "Epoch 13 -- Batch 66/ 94, validation loss 0.39574089646339417\n",
      "Epoch 13 -- Batch 67/ 94, validation loss 0.36277860403060913\n",
      "Epoch 13 -- Batch 68/ 94, validation loss 0.37181854248046875\n",
      "Epoch 13 -- Batch 69/ 94, validation loss 0.37103819847106934\n",
      "Epoch 13 -- Batch 70/ 94, validation loss 0.3632427453994751\n",
      "Epoch 13 -- Batch 71/ 94, validation loss 0.3619428873062134\n",
      "Epoch 13 -- Batch 72/ 94, validation loss 0.37338727712631226\n",
      "Epoch 13 -- Batch 73/ 94, validation loss 0.35169634222984314\n",
      "Epoch 13 -- Batch 74/ 94, validation loss 0.36641615629196167\n",
      "Epoch 13 -- Batch 75/ 94, validation loss 0.35373061895370483\n",
      "Epoch 13 -- Batch 76/ 94, validation loss 0.3573314845561981\n",
      "Epoch 13 -- Batch 77/ 94, validation loss 0.36714473366737366\n",
      "Epoch 13 -- Batch 78/ 94, validation loss 0.3647470772266388\n",
      "Epoch 13 -- Batch 79/ 94, validation loss 0.3694021701812744\n",
      "Epoch 13 -- Batch 80/ 94, validation loss 0.3583475947380066\n",
      "Epoch 13 -- Batch 81/ 94, validation loss 0.3842277526855469\n",
      "Epoch 13 -- Batch 82/ 94, validation loss 0.3700348436832428\n",
      "Epoch 13 -- Batch 83/ 94, validation loss 0.36827459931373596\n",
      "Epoch 13 -- Batch 84/ 94, validation loss 0.3867242634296417\n",
      "Epoch 13 -- Batch 85/ 94, validation loss 0.3624752163887024\n",
      "Epoch 13 -- Batch 86/ 94, validation loss 0.3784339427947998\n",
      "Epoch 13 -- Batch 87/ 94, validation loss 0.36686623096466064\n",
      "Epoch 13 -- Batch 88/ 94, validation loss 0.35607677698135376\n",
      "Epoch 13 -- Batch 89/ 94, validation loss 0.36793720722198486\n",
      "Epoch 13 -- Batch 90/ 94, validation loss 0.37897637486457825\n",
      "Epoch 13 -- Batch 91/ 94, validation loss 0.35013940930366516\n",
      "Epoch 13 -- Batch 92/ 94, validation loss 0.38641613721847534\n",
      "Epoch 13 -- Batch 93/ 94, validation loss 0.36946970224380493\n",
      "Epoch 13 -- Batch 94/ 94, validation loss 0.351047545671463\n",
      "----------------------------------------------------------------------\n",
      "Epoch 13 loss: Training 0.3703554570674896, Validation 0.351047545671463\n",
      "----------------------------------------------------------------------\n",
      "Epoch 14/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cc1cccn2c(=O)c3c4c(sc3n1)CCCC3'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 11 12 13\n",
      "[19:05:58] SMILES Parse Error: extra close parentheses while parsing: Cc1sc2c3c(ccc2[n+]1CCS(=O)(=O)NCC1CCCCC1)OCO3)c1ccccc1\n",
      "[19:05:58] SMILES Parse Error: Failed parsing SMILES 'Cc1sc2c3c(ccc2[n+]1CCS(=O)(=O)NCC1CCCCC1)OCO3)c1ccccc1' for input: 'Cc1sc2c3c(ccc2[n+]1CCS(=O)(=O)NCC1CCCCC1)OCO3)c1ccccc1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cn1ccnc1Sc1ccc(/C(O)=C2[N+](=O)[O-])cc2ccnc12'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COc1cccc(NC(=O)C23CCN(C(=O)[C@H](CCC(N)=O)CC(C)C)CCC22)c1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14 15 16 17 22\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'O=C1C2C3C=CC(C2)C2C(=O)N1NCc1ccc(F)cc1'\n",
      "[19:05:58] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(-n2c(SCC(=O)Nc3ccccc3-c3c(ccc3ccccc34)nc3cccnc32)ccc1=O'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CO[C@@H](CN(C)C(=O)CC1CCCCC1)[C@@H](CO)N2C(=O)c1cccnc1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'O=C1c2ccccc2-c2oc3c(c3ccccc32)C(c2ccc(Br)cc2)N1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 2 3 4 28 29\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 5 6 7 15 16 17 18\n",
      "[19:05:58] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'OCCCc1nc2n(n1)C(c1cccs1)CC1C(c1cccs1)N2'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN2CCN(C(=O)c3cccn4c(=O)c5ccccc5n3)CC2)cc1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CC(C)[C@H]1NCc23cn[nH]c2S(=O)(=O)N(C)C[C@@H]1O2'\n",
      "[19:05:58] SMILES Parse Error: ring closure 2 duplicates bond between atom 14 and atom 15 for input: 'CCCNC(=O)N[C@H]1c2ccccc2C2(c2ncn[nH]2)CCCC1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 1 2 17 18 19 20 21 22 23 24 25\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 16 17\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COC[C@H]1CCCC[C@@H]1O[C@H](CN(C)C)Oc1cc(C#Cc3ccccc3)ccc2S1(=O)=O'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 7 8 10\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)Nc1ccc2c(c1)OC[C@@H](C)NC(=O)c1cccc(F)c1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)CCN(C(=S)O)CCN1C(=O)c2ccccc2C1=O'\n",
      "[19:05:58] SMILES Parse Error: extra open parentheses for input: 'CCCCN(C(=O)CSc1nnc(Cn2c(=O)[nH]c3ccccc3c2=O)s1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C)c2-nc(N3CC(C)CC(C)C3)c3nnnn13'\n",
      "[19:05:58] Explicit valence for atom # 6 O, 3, is greater than permitted\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CN1CCN(c2cccc3c2S(=O)(=O)N[C@@H](C2CC2)CC1)c1ccccc1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COc1ccc(S(=O)(=O)N[C@H]2C=C[C@@H](CC(=O)NCC(c3cccnc3)O[C@@H]3CO)cc2)cc1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C(C)=N/N=C2\\N=C(c3cc3c(cc3Br)OCO4)OC2=O)cc1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 15 18 19 20 21 22 23\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Fc1ccccc1C1CC(c2ccncc2)N1c1cccc(-c2cccc2c2)C1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)c1c3n2CCCN(C(=O)c3ccccc3F)C1P(=O)[O-]O'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 9\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2nc3c(ccn3CCNC(=O)C(C)(C)C)cn23)cc1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C(C)=N/NC(=O)Cc2ccc3c(C)ccc(=O)oc2c2)cc1OC'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2nnc(N3CCC(n4cnc(C/C)Cc4ccccc4)CC3)nc2c1'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(C(=O)Nc2ccc3c(c2)C(=O)N(C)[C@H]2CC[C@H](CC(=O)Nc3Cn3cccc4)O[C@@H]2CO3)cc1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24 25 26 27 28 29 30\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 6 7 8 10 11\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10 11 22\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(NC(=O)c2ccnn2C)sc2c1-c1ccc(OC)cc1OC1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 21\n",
      "[19:05:58] Explicit valence for atom # 19 O, 3, is greater than permitted\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'N=C1C(c2nc3ccccc3c(=O)[nH]2)=C(O)CN1S1(=O)=O'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 13 14 15 26 27\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 13 14 15 25 26\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 6 7 9 10 11 19 20\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 2 3 11 13 14 18 22\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'O=C(NC1OCCC1)C1=C(c2ccccc2)OC1(Coc1ccccc2)CC1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 20 21 26 27 28\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 13\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 1 2 19 20 21 23 24 25 26 27 28\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'COc1cccc2c1OC[C@@H]1CN(Cc2ccn(C)nc2)CCN1C(=O)[C@H](C)O'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1Cn1cc(CN2[C@@H](C)COc3ccc(N5CCCC(O)C4)cc3C2)nn1'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\n",
      "[19:05:58] SMILES Parse Error: ring closure 4 duplicates bond between atom 18 and atom 19 for input: 'COc1ccc(N2CCN(CC(O)COc3ccc4c4c(=CCCC5)CC3)cc2)CC1'\n",
      "[19:05:58] Explicit valence for atom # 26 Br, 2, is greater than permitted\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 16 17 18 20 21 22 23 25 26\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 17 20 21\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'O=c1c(-c2ccnn2-c2ccc(F)cc2)sc2nc3c(-c4cccc5c(c4)OCO5)nnc12'\n",
      "[19:05:58] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(NC(/C=N/S(=O)(=O)c2ccccc2)sc2C)csc1C'\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 2 3 4 18 19 20 21 23 24\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 4 5 18 19 21 29 30\n",
      "[19:05:58] Can't kekulize mol.  Unkekulized atoms: 10 11 13 24 25 26 27 28 29\n",
      "[19:05:58] Explicit valence for atom # 16 N, 5, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 -- Batch 1/ 842, training loss 0.3551945090293884\n",
      "Epoch 14 -- Batch 2/ 842, training loss 0.3650028109550476\n",
      "Epoch 14 -- Batch 3/ 842, training loss 0.36570820212364197\n",
      "Epoch 14 -- Batch 4/ 842, training loss 0.3622930943965912\n",
      "Epoch 14 -- Batch 5/ 842, training loss 0.3525550067424774\n",
      "Epoch 14 -- Batch 6/ 842, training loss 0.36388352513313293\n",
      "Epoch 14 -- Batch 7/ 842, training loss 0.3654446005821228\n",
      "Epoch 14 -- Batch 8/ 842, training loss 0.34828805923461914\n",
      "Epoch 14 -- Batch 9/ 842, training loss 0.3518986105918884\n",
      "Epoch 14 -- Batch 10/ 842, training loss 0.35674723982810974\n",
      "Epoch 14 -- Batch 11/ 842, training loss 0.36640968918800354\n",
      "Epoch 14 -- Batch 12/ 842, training loss 0.37129321694374084\n",
      "Epoch 14 -- Batch 13/ 842, training loss 0.3595750629901886\n",
      "Epoch 14 -- Batch 14/ 842, training loss 0.356548547744751\n",
      "Epoch 14 -- Batch 15/ 842, training loss 0.3583643436431885\n",
      "Epoch 14 -- Batch 16/ 842, training loss 0.36356380581855774\n",
      "Epoch 14 -- Batch 17/ 842, training loss 0.35289883613586426\n",
      "Epoch 14 -- Batch 18/ 842, training loss 0.36608991026878357\n",
      "Epoch 14 -- Batch 19/ 842, training loss 0.3609126806259155\n",
      "Epoch 14 -- Batch 20/ 842, training loss 0.3615880012512207\n",
      "Epoch 14 -- Batch 21/ 842, training loss 0.3642213046550751\n",
      "Epoch 14 -- Batch 22/ 842, training loss 0.3599281907081604\n",
      "Epoch 14 -- Batch 23/ 842, training loss 0.36348316073417664\n",
      "Epoch 14 -- Batch 24/ 842, training loss 0.34489816427230835\n",
      "Epoch 14 -- Batch 25/ 842, training loss 0.3584086000919342\n",
      "Epoch 14 -- Batch 26/ 842, training loss 0.37319961190223694\n",
      "Epoch 14 -- Batch 27/ 842, training loss 0.35898497700691223\n",
      "Epoch 14 -- Batch 28/ 842, training loss 0.3487088978290558\n",
      "Epoch 14 -- Batch 29/ 842, training loss 0.3426264226436615\n",
      "Epoch 14 -- Batch 30/ 842, training loss 0.35047996044158936\n",
      "Epoch 14 -- Batch 31/ 842, training loss 0.3513466715812683\n",
      "Epoch 14 -- Batch 32/ 842, training loss 0.36850011348724365\n",
      "Epoch 14 -- Batch 33/ 842, training loss 0.35591718554496765\n",
      "Epoch 14 -- Batch 34/ 842, training loss 0.3443294167518616\n",
      "Epoch 14 -- Batch 35/ 842, training loss 0.3600468635559082\n",
      "Epoch 14 -- Batch 36/ 842, training loss 0.3499700129032135\n",
      "Epoch 14 -- Batch 37/ 842, training loss 0.35579225420951843\n",
      "Epoch 14 -- Batch 38/ 842, training loss 0.3546733558177948\n",
      "Epoch 14 -- Batch 39/ 842, training loss 0.3497686982154846\n",
      "Epoch 14 -- Batch 40/ 842, training loss 0.35607439279556274\n",
      "Epoch 14 -- Batch 41/ 842, training loss 0.34657740592956543\n",
      "Epoch 14 -- Batch 42/ 842, training loss 0.3506939709186554\n",
      "Epoch 14 -- Batch 43/ 842, training loss 0.36072561144828796\n",
      "Epoch 14 -- Batch 44/ 842, training loss 0.36440223455429077\n",
      "Epoch 14 -- Batch 45/ 842, training loss 0.36222347617149353\n",
      "Epoch 14 -- Batch 46/ 842, training loss 0.36332419514656067\n",
      "Epoch 14 -- Batch 47/ 842, training loss 0.3627523183822632\n",
      "Epoch 14 -- Batch 48/ 842, training loss 0.35532283782958984\n",
      "Epoch 14 -- Batch 49/ 842, training loss 0.343904972076416\n",
      "Epoch 14 -- Batch 50/ 842, training loss 0.3636532425880432\n",
      "Epoch 14 -- Batch 51/ 842, training loss 0.35340920090675354\n",
      "Epoch 14 -- Batch 52/ 842, training loss 0.36096981167793274\n",
      "Epoch 14 -- Batch 53/ 842, training loss 0.3567073345184326\n",
      "Epoch 14 -- Batch 54/ 842, training loss 0.3437674343585968\n",
      "Epoch 14 -- Batch 55/ 842, training loss 0.3555225133895874\n",
      "Epoch 14 -- Batch 56/ 842, training loss 0.353497713804245\n",
      "Epoch 14 -- Batch 57/ 842, training loss 0.3556289076805115\n",
      "Epoch 14 -- Batch 58/ 842, training loss 0.3625912368297577\n",
      "Epoch 14 -- Batch 59/ 842, training loss 0.3603852093219757\n",
      "Epoch 14 -- Batch 60/ 842, training loss 0.3588862419128418\n",
      "Epoch 14 -- Batch 61/ 842, training loss 0.3609573245048523\n",
      "Epoch 14 -- Batch 62/ 842, training loss 0.3570278584957123\n",
      "Epoch 14 -- Batch 63/ 842, training loss 0.3618326783180237\n",
      "Epoch 14 -- Batch 64/ 842, training loss 0.3585721254348755\n",
      "Epoch 14 -- Batch 65/ 842, training loss 0.3669494688510895\n",
      "Epoch 14 -- Batch 66/ 842, training loss 0.3559141159057617\n",
      "Epoch 14 -- Batch 67/ 842, training loss 0.36941948533058167\n",
      "Epoch 14 -- Batch 68/ 842, training loss 0.3509668707847595\n",
      "Epoch 14 -- Batch 69/ 842, training loss 0.3503269553184509\n",
      "Epoch 14 -- Batch 70/ 842, training loss 0.3576657772064209\n",
      "Epoch 14 -- Batch 71/ 842, training loss 0.35544145107269287\n",
      "Epoch 14 -- Batch 72/ 842, training loss 0.3579297363758087\n",
      "Epoch 14 -- Batch 73/ 842, training loss 0.3564751446247101\n",
      "Epoch 14 -- Batch 74/ 842, training loss 0.35967135429382324\n",
      "Epoch 14 -- Batch 75/ 842, training loss 0.3523685038089752\n",
      "Epoch 14 -- Batch 76/ 842, training loss 0.3577379882335663\n",
      "Epoch 14 -- Batch 77/ 842, training loss 0.3590397238731384\n",
      "Epoch 14 -- Batch 78/ 842, training loss 0.3542380928993225\n",
      "Epoch 14 -- Batch 79/ 842, training loss 0.36587440967559814\n",
      "Epoch 14 -- Batch 80/ 842, training loss 0.3561323583126068\n",
      "Epoch 14 -- Batch 81/ 842, training loss 0.3638461232185364\n",
      "Epoch 14 -- Batch 82/ 842, training loss 0.3608328402042389\n",
      "Epoch 14 -- Batch 83/ 842, training loss 0.3681009113788605\n",
      "Epoch 14 -- Batch 84/ 842, training loss 0.35281533002853394\n",
      "Epoch 14 -- Batch 85/ 842, training loss 0.3574073314666748\n",
      "Epoch 14 -- Batch 86/ 842, training loss 0.3391622304916382\n",
      "Epoch 14 -- Batch 87/ 842, training loss 0.34772858023643494\n",
      "Epoch 14 -- Batch 88/ 842, training loss 0.3564033508300781\n",
      "Epoch 14 -- Batch 89/ 842, training loss 0.3461093604564667\n",
      "Epoch 14 -- Batch 90/ 842, training loss 0.3537079989910126\n",
      "Epoch 14 -- Batch 91/ 842, training loss 0.3597544729709625\n",
      "Epoch 14 -- Batch 92/ 842, training loss 0.36915189027786255\n",
      "Epoch 14 -- Batch 93/ 842, training loss 0.35742464661598206\n",
      "Epoch 14 -- Batch 94/ 842, training loss 0.35891515016555786\n",
      "Epoch 14 -- Batch 95/ 842, training loss 0.3645109534263611\n",
      "Epoch 14 -- Batch 96/ 842, training loss 0.3596965968608856\n",
      "Epoch 14 -- Batch 97/ 842, training loss 0.35707059502601624\n",
      "Epoch 14 -- Batch 98/ 842, training loss 0.3611927330493927\n",
      "Epoch 14 -- Batch 99/ 842, training loss 0.3582248091697693\n",
      "Epoch 14 -- Batch 100/ 842, training loss 0.35857072472572327\n",
      "Epoch 14 -- Batch 101/ 842, training loss 0.356518030166626\n",
      "Epoch 14 -- Batch 102/ 842, training loss 0.35080793499946594\n",
      "Epoch 14 -- Batch 103/ 842, training loss 0.35322877764701843\n",
      "Epoch 14 -- Batch 104/ 842, training loss 0.35182368755340576\n",
      "Epoch 14 -- Batch 105/ 842, training loss 0.3688633441925049\n",
      "Epoch 14 -- Batch 106/ 842, training loss 0.36986640095710754\n",
      "Epoch 14 -- Batch 107/ 842, training loss 0.3647448420524597\n",
      "Epoch 14 -- Batch 108/ 842, training loss 0.3587004244327545\n",
      "Epoch 14 -- Batch 109/ 842, training loss 0.3619977533817291\n",
      "Epoch 14 -- Batch 110/ 842, training loss 0.3537558317184448\n",
      "Epoch 14 -- Batch 111/ 842, training loss 0.3484257757663727\n",
      "Epoch 14 -- Batch 112/ 842, training loss 0.36717113852500916\n",
      "Epoch 14 -- Batch 113/ 842, training loss 0.36372485756874084\n",
      "Epoch 14 -- Batch 114/ 842, training loss 0.36247870326042175\n",
      "Epoch 14 -- Batch 115/ 842, training loss 0.3649190664291382\n",
      "Epoch 14 -- Batch 116/ 842, training loss 0.35418716073036194\n",
      "Epoch 14 -- Batch 117/ 842, training loss 0.360536128282547\n",
      "Epoch 14 -- Batch 118/ 842, training loss 0.36540791392326355\n",
      "Epoch 14 -- Batch 119/ 842, training loss 0.35243648290634155\n",
      "Epoch 14 -- Batch 120/ 842, training loss 0.35519474744796753\n",
      "Epoch 14 -- Batch 121/ 842, training loss 0.35393527150154114\n",
      "Epoch 14 -- Batch 122/ 842, training loss 0.3649974465370178\n",
      "Epoch 14 -- Batch 123/ 842, training loss 0.3545740246772766\n",
      "Epoch 14 -- Batch 124/ 842, training loss 0.37017104029655457\n",
      "Epoch 14 -- Batch 125/ 842, training loss 0.36064577102661133\n",
      "Epoch 14 -- Batch 126/ 842, training loss 0.3792570233345032\n",
      "Epoch 14 -- Batch 127/ 842, training loss 0.355620801448822\n",
      "Epoch 14 -- Batch 128/ 842, training loss 0.3630107641220093\n",
      "Epoch 14 -- Batch 129/ 842, training loss 0.38254064321517944\n",
      "Epoch 14 -- Batch 130/ 842, training loss 0.35646292567253113\n",
      "Epoch 14 -- Batch 131/ 842, training loss 0.3587934374809265\n",
      "Epoch 14 -- Batch 132/ 842, training loss 0.35126519203186035\n",
      "Epoch 14 -- Batch 133/ 842, training loss 0.36738336086273193\n",
      "Epoch 14 -- Batch 134/ 842, training loss 0.35543039441108704\n",
      "Epoch 14 -- Batch 135/ 842, training loss 0.34796011447906494\n",
      "Epoch 14 -- Batch 136/ 842, training loss 0.3657000660896301\n",
      "Epoch 14 -- Batch 137/ 842, training loss 0.36691153049468994\n",
      "Epoch 14 -- Batch 138/ 842, training loss 0.3478044867515564\n",
      "Epoch 14 -- Batch 139/ 842, training loss 0.3519890606403351\n",
      "Epoch 14 -- Batch 140/ 842, training loss 0.35793814063072205\n",
      "Epoch 14 -- Batch 141/ 842, training loss 0.35064417123794556\n",
      "Epoch 14 -- Batch 142/ 842, training loss 0.3553970456123352\n",
      "Epoch 14 -- Batch 143/ 842, training loss 0.3503756523132324\n",
      "Epoch 14 -- Batch 144/ 842, training loss 0.3705292344093323\n",
      "Epoch 14 -- Batch 145/ 842, training loss 0.3541938364505768\n",
      "Epoch 14 -- Batch 146/ 842, training loss 0.3682768642902374\n",
      "Epoch 14 -- Batch 147/ 842, training loss 0.3491482138633728\n",
      "Epoch 14 -- Batch 148/ 842, training loss 0.3604835271835327\n",
      "Epoch 14 -- Batch 149/ 842, training loss 0.3649897873401642\n",
      "Epoch 14 -- Batch 150/ 842, training loss 0.34282130002975464\n",
      "Epoch 14 -- Batch 151/ 842, training loss 0.3553655743598938\n",
      "Epoch 14 -- Batch 152/ 842, training loss 0.36548125743865967\n",
      "Epoch 14 -- Batch 153/ 842, training loss 0.36359092593193054\n",
      "Epoch 14 -- Batch 154/ 842, training loss 0.37046727538108826\n",
      "Epoch 14 -- Batch 155/ 842, training loss 0.33741870522499084\n",
      "Epoch 14 -- Batch 156/ 842, training loss 0.3508142828941345\n",
      "Epoch 14 -- Batch 157/ 842, training loss 0.36650213599205017\n",
      "Epoch 14 -- Batch 158/ 842, training loss 0.3787749707698822\n",
      "Epoch 14 -- Batch 159/ 842, training loss 0.35378995537757874\n",
      "Epoch 14 -- Batch 160/ 842, training loss 0.36155930161476135\n",
      "Epoch 14 -- Batch 161/ 842, training loss 0.36002445220947266\n",
      "Epoch 14 -- Batch 162/ 842, training loss 0.36463531851768494\n",
      "Epoch 14 -- Batch 163/ 842, training loss 0.3652283251285553\n",
      "Epoch 14 -- Batch 164/ 842, training loss 0.3666839897632599\n",
      "Epoch 14 -- Batch 165/ 842, training loss 0.34998568892478943\n",
      "Epoch 14 -- Batch 166/ 842, training loss 0.36133742332458496\n",
      "Epoch 14 -- Batch 167/ 842, training loss 0.36062926054000854\n",
      "Epoch 14 -- Batch 168/ 842, training loss 0.3466644883155823\n",
      "Epoch 14 -- Batch 169/ 842, training loss 0.36225777864456177\n",
      "Epoch 14 -- Batch 170/ 842, training loss 0.36675339937210083\n",
      "Epoch 14 -- Batch 171/ 842, training loss 0.3437061607837677\n",
      "Epoch 14 -- Batch 172/ 842, training loss 0.36355888843536377\n",
      "Epoch 14 -- Batch 173/ 842, training loss 0.3631223738193512\n",
      "Epoch 14 -- Batch 174/ 842, training loss 0.36912208795547485\n",
      "Epoch 14 -- Batch 175/ 842, training loss 0.35932233929634094\n",
      "Epoch 14 -- Batch 176/ 842, training loss 0.3552255928516388\n",
      "Epoch 14 -- Batch 177/ 842, training loss 0.3648909330368042\n",
      "Epoch 14 -- Batch 178/ 842, training loss 0.36321642994880676\n",
      "Epoch 14 -- Batch 179/ 842, training loss 0.3685266077518463\n",
      "Epoch 14 -- Batch 180/ 842, training loss 0.3566078245639801\n",
      "Epoch 14 -- Batch 181/ 842, training loss 0.3744507133960724\n",
      "Epoch 14 -- Batch 182/ 842, training loss 0.37728744745254517\n",
      "Epoch 14 -- Batch 183/ 842, training loss 0.363951176404953\n",
      "Epoch 14 -- Batch 184/ 842, training loss 0.3423411250114441\n",
      "Epoch 14 -- Batch 185/ 842, training loss 0.3655362129211426\n",
      "Epoch 14 -- Batch 186/ 842, training loss 0.359031081199646\n",
      "Epoch 14 -- Batch 187/ 842, training loss 0.36053338646888733\n",
      "Epoch 14 -- Batch 188/ 842, training loss 0.3464270234107971\n",
      "Epoch 14 -- Batch 189/ 842, training loss 0.35373827815055847\n",
      "Epoch 14 -- Batch 190/ 842, training loss 0.36387479305267334\n",
      "Epoch 14 -- Batch 191/ 842, training loss 0.3648541271686554\n",
      "Epoch 14 -- Batch 192/ 842, training loss 0.3585265278816223\n",
      "Epoch 14 -- Batch 193/ 842, training loss 0.3671993315219879\n",
      "Epoch 14 -- Batch 194/ 842, training loss 0.36118441820144653\n",
      "Epoch 14 -- Batch 195/ 842, training loss 0.36383506655693054\n",
      "Epoch 14 -- Batch 196/ 842, training loss 0.35906925797462463\n",
      "Epoch 14 -- Batch 197/ 842, training loss 0.36959272623062134\n",
      "Epoch 14 -- Batch 198/ 842, training loss 0.3572983741760254\n",
      "Epoch 14 -- Batch 199/ 842, training loss 0.3735440671443939\n",
      "Epoch 14 -- Batch 200/ 842, training loss 0.35722383856773376\n",
      "Epoch 14 -- Batch 201/ 842, training loss 0.3582426607608795\n",
      "Epoch 14 -- Batch 202/ 842, training loss 0.35465654730796814\n",
      "Epoch 14 -- Batch 203/ 842, training loss 0.37655580043792725\n",
      "Epoch 14 -- Batch 204/ 842, training loss 0.37345418334007263\n",
      "Epoch 14 -- Batch 205/ 842, training loss 0.3735906481742859\n",
      "Epoch 14 -- Batch 206/ 842, training loss 0.36321938037872314\n",
      "Epoch 14 -- Batch 207/ 842, training loss 0.35053977370262146\n",
      "Epoch 14 -- Batch 208/ 842, training loss 0.3699128329753876\n",
      "Epoch 14 -- Batch 209/ 842, training loss 0.3778809607028961\n",
      "Epoch 14 -- Batch 210/ 842, training loss 0.36681079864501953\n",
      "Epoch 14 -- Batch 211/ 842, training loss 0.3481960594654083\n",
      "Epoch 14 -- Batch 212/ 842, training loss 0.36850565671920776\n",
      "Epoch 14 -- Batch 213/ 842, training loss 0.3499279320240021\n",
      "Epoch 14 -- Batch 214/ 842, training loss 0.3531561493873596\n",
      "Epoch 14 -- Batch 215/ 842, training loss 0.36330997943878174\n",
      "Epoch 14 -- Batch 216/ 842, training loss 0.34806954860687256\n",
      "Epoch 14 -- Batch 217/ 842, training loss 0.3612244129180908\n",
      "Epoch 14 -- Batch 218/ 842, training loss 0.3685690760612488\n",
      "Epoch 14 -- Batch 219/ 842, training loss 0.3589777648448944\n",
      "Epoch 14 -- Batch 220/ 842, training loss 0.3642496168613434\n",
      "Epoch 14 -- Batch 221/ 842, training loss 0.3734194338321686\n",
      "Epoch 14 -- Batch 222/ 842, training loss 0.36054733395576477\n",
      "Epoch 14 -- Batch 223/ 842, training loss 0.36348509788513184\n",
      "Epoch 14 -- Batch 224/ 842, training loss 0.37854310870170593\n",
      "Epoch 14 -- Batch 225/ 842, training loss 0.3567335903644562\n",
      "Epoch 14 -- Batch 226/ 842, training loss 0.3523111045360565\n",
      "Epoch 14 -- Batch 227/ 842, training loss 0.36351278424263\n",
      "Epoch 14 -- Batch 228/ 842, training loss 0.37542784214019775\n",
      "Epoch 14 -- Batch 229/ 842, training loss 0.3654274344444275\n",
      "Epoch 14 -- Batch 230/ 842, training loss 0.35199806094169617\n",
      "Epoch 14 -- Batch 231/ 842, training loss 0.35595622658729553\n",
      "Epoch 14 -- Batch 232/ 842, training loss 0.3661169111728668\n",
      "Epoch 14 -- Batch 233/ 842, training loss 0.3622983694076538\n",
      "Epoch 14 -- Batch 234/ 842, training loss 0.3643152713775635\n",
      "Epoch 14 -- Batch 235/ 842, training loss 0.3573189079761505\n",
      "Epoch 14 -- Batch 236/ 842, training loss 0.36239731311798096\n",
      "Epoch 14 -- Batch 237/ 842, training loss 0.35880041122436523\n",
      "Epoch 14 -- Batch 238/ 842, training loss 0.367533415555954\n",
      "Epoch 14 -- Batch 239/ 842, training loss 0.36269113421440125\n",
      "Epoch 14 -- Batch 240/ 842, training loss 0.3577800989151001\n",
      "Epoch 14 -- Batch 241/ 842, training loss 0.356088250875473\n",
      "Epoch 14 -- Batch 242/ 842, training loss 0.3825124502182007\n",
      "Epoch 14 -- Batch 243/ 842, training loss 0.3654758930206299\n",
      "Epoch 14 -- Batch 244/ 842, training loss 0.38763564825057983\n",
      "Epoch 14 -- Batch 245/ 842, training loss 0.36670759320259094\n",
      "Epoch 14 -- Batch 246/ 842, training loss 0.3554125726222992\n",
      "Epoch 14 -- Batch 247/ 842, training loss 0.36523109674453735\n",
      "Epoch 14 -- Batch 248/ 842, training loss 0.36462101340293884\n",
      "Epoch 14 -- Batch 249/ 842, training loss 0.36178621649742126\n",
      "Epoch 14 -- Batch 250/ 842, training loss 0.3623480200767517\n",
      "Epoch 14 -- Batch 251/ 842, training loss 0.368599534034729\n",
      "Epoch 14 -- Batch 252/ 842, training loss 0.3566351532936096\n",
      "Epoch 14 -- Batch 253/ 842, training loss 0.3679760992527008\n",
      "Epoch 14 -- Batch 254/ 842, training loss 0.37829872965812683\n",
      "Epoch 14 -- Batch 255/ 842, training loss 0.35361212491989136\n",
      "Epoch 14 -- Batch 256/ 842, training loss 0.3666291832923889\n",
      "Epoch 14 -- Batch 257/ 842, training loss 0.36390221118927\n",
      "Epoch 14 -- Batch 258/ 842, training loss 0.3605128228664398\n",
      "Epoch 14 -- Batch 259/ 842, training loss 0.3649476170539856\n",
      "Epoch 14 -- Batch 260/ 842, training loss 0.36593589186668396\n",
      "Epoch 14 -- Batch 261/ 842, training loss 0.36003345251083374\n",
      "Epoch 14 -- Batch 262/ 842, training loss 0.3683083951473236\n",
      "Epoch 14 -- Batch 263/ 842, training loss 0.36025938391685486\n",
      "Epoch 14 -- Batch 264/ 842, training loss 0.3629132807254791\n",
      "Epoch 14 -- Batch 265/ 842, training loss 0.35689035058021545\n",
      "Epoch 14 -- Batch 266/ 842, training loss 0.3687851130962372\n",
      "Epoch 14 -- Batch 267/ 842, training loss 0.36066341400146484\n",
      "Epoch 14 -- Batch 268/ 842, training loss 0.3705980181694031\n",
      "Epoch 14 -- Batch 269/ 842, training loss 0.3808771073818207\n",
      "Epoch 14 -- Batch 270/ 842, training loss 0.360657662153244\n",
      "Epoch 14 -- Batch 271/ 842, training loss 0.3857562839984894\n",
      "Epoch 14 -- Batch 272/ 842, training loss 0.3627610504627228\n",
      "Epoch 14 -- Batch 273/ 842, training loss 0.3676467835903168\n",
      "Epoch 14 -- Batch 274/ 842, training loss 0.363159716129303\n",
      "Epoch 14 -- Batch 275/ 842, training loss 0.35078123211860657\n",
      "Epoch 14 -- Batch 276/ 842, training loss 0.3601003587245941\n",
      "Epoch 14 -- Batch 277/ 842, training loss 0.3660999834537506\n",
      "Epoch 14 -- Batch 278/ 842, training loss 0.378917932510376\n",
      "Epoch 14 -- Batch 279/ 842, training loss 0.3744886815547943\n",
      "Epoch 14 -- Batch 280/ 842, training loss 0.35365670919418335\n",
      "Epoch 14 -- Batch 281/ 842, training loss 0.3710483908653259\n",
      "Epoch 14 -- Batch 282/ 842, training loss 0.36936017870903015\n",
      "Epoch 14 -- Batch 283/ 842, training loss 0.359151691198349\n",
      "Epoch 14 -- Batch 284/ 842, training loss 0.3637802302837372\n",
      "Epoch 14 -- Batch 285/ 842, training loss 0.36201968789100647\n",
      "Epoch 14 -- Batch 286/ 842, training loss 0.3483610153198242\n",
      "Epoch 14 -- Batch 287/ 842, training loss 0.36491164565086365\n",
      "Epoch 14 -- Batch 288/ 842, training loss 0.37079742550849915\n",
      "Epoch 14 -- Batch 289/ 842, training loss 0.36526787281036377\n",
      "Epoch 14 -- Batch 290/ 842, training loss 0.36103197932243347\n",
      "Epoch 14 -- Batch 291/ 842, training loss 0.3719945549964905\n",
      "Epoch 14 -- Batch 292/ 842, training loss 0.35969287157058716\n",
      "Epoch 14 -- Batch 293/ 842, training loss 0.3542538583278656\n",
      "Epoch 14 -- Batch 294/ 842, training loss 0.34951990842819214\n",
      "Epoch 14 -- Batch 295/ 842, training loss 0.35932523012161255\n",
      "Epoch 14 -- Batch 296/ 842, training loss 0.3479103147983551\n",
      "Epoch 14 -- Batch 297/ 842, training loss 0.36369672417640686\n",
      "Epoch 14 -- Batch 298/ 842, training loss 0.36794501543045044\n",
      "Epoch 14 -- Batch 299/ 842, training loss 0.3584018647670746\n",
      "Epoch 14 -- Batch 300/ 842, training loss 0.3665756285190582\n",
      "Epoch 14 -- Batch 301/ 842, training loss 0.35518699884414673\n",
      "Epoch 14 -- Batch 302/ 842, training loss 0.3477361500263214\n",
      "Epoch 14 -- Batch 303/ 842, training loss 0.35508283972740173\n",
      "Epoch 14 -- Batch 304/ 842, training loss 0.3554615378379822\n",
      "Epoch 14 -- Batch 305/ 842, training loss 0.3591376543045044\n",
      "Epoch 14 -- Batch 306/ 842, training loss 0.3549543023109436\n",
      "Epoch 14 -- Batch 307/ 842, training loss 0.3507261872291565\n",
      "Epoch 14 -- Batch 308/ 842, training loss 0.3571631610393524\n",
      "Epoch 14 -- Batch 309/ 842, training loss 0.358582079410553\n",
      "Epoch 14 -- Batch 310/ 842, training loss 0.35454854369163513\n",
      "Epoch 14 -- Batch 311/ 842, training loss 0.38106656074523926\n",
      "Epoch 14 -- Batch 312/ 842, training loss 0.3696572184562683\n",
      "Epoch 14 -- Batch 313/ 842, training loss 0.37471097707748413\n",
      "Epoch 14 -- Batch 314/ 842, training loss 0.35839924216270447\n",
      "Epoch 14 -- Batch 315/ 842, training loss 0.352233350276947\n",
      "Epoch 14 -- Batch 316/ 842, training loss 0.3560526967048645\n",
      "Epoch 14 -- Batch 317/ 842, training loss 0.36026981472969055\n",
      "Epoch 14 -- Batch 318/ 842, training loss 0.34477922320365906\n",
      "Epoch 14 -- Batch 319/ 842, training loss 0.3476879894733429\n",
      "Epoch 14 -- Batch 320/ 842, training loss 0.3563009798526764\n",
      "Epoch 14 -- Batch 321/ 842, training loss 0.36137619614601135\n",
      "Epoch 14 -- Batch 322/ 842, training loss 0.34671759605407715\n",
      "Epoch 14 -- Batch 323/ 842, training loss 0.3713948428630829\n",
      "Epoch 14 -- Batch 324/ 842, training loss 0.35311219096183777\n",
      "Epoch 14 -- Batch 325/ 842, training loss 0.3674931228160858\n",
      "Epoch 14 -- Batch 326/ 842, training loss 0.3712369501590729\n",
      "Epoch 14 -- Batch 327/ 842, training loss 0.3740130662918091\n",
      "Epoch 14 -- Batch 328/ 842, training loss 0.3626239001750946\n",
      "Epoch 14 -- Batch 329/ 842, training loss 0.36362484097480774\n",
      "Epoch 14 -- Batch 330/ 842, training loss 0.3671524226665497\n",
      "Epoch 14 -- Batch 331/ 842, training loss 0.35540106892585754\n",
      "Epoch 14 -- Batch 332/ 842, training loss 0.3854905366897583\n",
      "Epoch 14 -- Batch 333/ 842, training loss 0.36192283034324646\n",
      "Epoch 14 -- Batch 334/ 842, training loss 0.3650408089160919\n",
      "Epoch 14 -- Batch 335/ 842, training loss 0.36994436383247375\n",
      "Epoch 14 -- Batch 336/ 842, training loss 0.3629100024700165\n",
      "Epoch 14 -- Batch 337/ 842, training loss 0.3603001832962036\n",
      "Epoch 14 -- Batch 338/ 842, training loss 0.3582334518432617\n",
      "Epoch 14 -- Batch 339/ 842, training loss 0.361331045627594\n",
      "Epoch 14 -- Batch 340/ 842, training loss 0.35352784395217896\n",
      "Epoch 14 -- Batch 341/ 842, training loss 0.36292389035224915\n",
      "Epoch 14 -- Batch 342/ 842, training loss 0.35695910453796387\n",
      "Epoch 14 -- Batch 343/ 842, training loss 0.37103769183158875\n",
      "Epoch 14 -- Batch 344/ 842, training loss 0.35606974363327026\n",
      "Epoch 14 -- Batch 345/ 842, training loss 0.3685113489627838\n",
      "Epoch 14 -- Batch 346/ 842, training loss 0.3656538724899292\n",
      "Epoch 14 -- Batch 347/ 842, training loss 0.3701663315296173\n",
      "Epoch 14 -- Batch 348/ 842, training loss 0.3620421290397644\n",
      "Epoch 14 -- Batch 349/ 842, training loss 0.35576876997947693\n",
      "Epoch 14 -- Batch 350/ 842, training loss 0.379902720451355\n",
      "Epoch 14 -- Batch 351/ 842, training loss 0.37575942277908325\n",
      "Epoch 14 -- Batch 352/ 842, training loss 0.37657999992370605\n",
      "Epoch 14 -- Batch 353/ 842, training loss 0.3590256869792938\n",
      "Epoch 14 -- Batch 354/ 842, training loss 0.3742373585700989\n",
      "Epoch 14 -- Batch 355/ 842, training loss 0.35926276445388794\n",
      "Epoch 14 -- Batch 356/ 842, training loss 0.37168651819229126\n",
      "Epoch 14 -- Batch 357/ 842, training loss 0.3807274401187897\n",
      "Epoch 14 -- Batch 358/ 842, training loss 0.3564816415309906\n",
      "Epoch 14 -- Batch 359/ 842, training loss 0.3573170602321625\n",
      "Epoch 14 -- Batch 360/ 842, training loss 0.3553239405155182\n",
      "Epoch 14 -- Batch 361/ 842, training loss 0.3699348568916321\n",
      "Epoch 14 -- Batch 362/ 842, training loss 0.3575538098812103\n",
      "Epoch 14 -- Batch 363/ 842, training loss 0.37790292501449585\n",
      "Epoch 14 -- Batch 364/ 842, training loss 0.35833990573883057\n",
      "Epoch 14 -- Batch 365/ 842, training loss 0.3561091721057892\n",
      "Epoch 14 -- Batch 366/ 842, training loss 0.3628729283809662\n",
      "Epoch 14 -- Batch 367/ 842, training loss 0.377498984336853\n",
      "Epoch 14 -- Batch 368/ 842, training loss 0.34903186559677124\n",
      "Epoch 14 -- Batch 369/ 842, training loss 0.37289974093437195\n",
      "Epoch 14 -- Batch 370/ 842, training loss 0.36700043082237244\n",
      "Epoch 14 -- Batch 371/ 842, training loss 0.35832011699676514\n",
      "Epoch 14 -- Batch 372/ 842, training loss 0.37075328826904297\n",
      "Epoch 14 -- Batch 373/ 842, training loss 0.36063939332962036\n",
      "Epoch 14 -- Batch 374/ 842, training loss 0.3722192645072937\n",
      "Epoch 14 -- Batch 375/ 842, training loss 0.369684636592865\n",
      "Epoch 14 -- Batch 376/ 842, training loss 0.35844525694847107\n",
      "Epoch 14 -- Batch 377/ 842, training loss 0.36652588844299316\n",
      "Epoch 14 -- Batch 378/ 842, training loss 0.3790096640586853\n",
      "Epoch 14 -- Batch 379/ 842, training loss 0.3565346598625183\n",
      "Epoch 14 -- Batch 380/ 842, training loss 0.37942272424697876\n",
      "Epoch 14 -- Batch 381/ 842, training loss 0.3679758310317993\n",
      "Epoch 14 -- Batch 382/ 842, training loss 0.3501642346382141\n",
      "Epoch 14 -- Batch 383/ 842, training loss 0.36374631524086\n",
      "Epoch 14 -- Batch 384/ 842, training loss 0.3795713186264038\n",
      "Epoch 14 -- Batch 385/ 842, training loss 0.36112353205680847\n",
      "Epoch 14 -- Batch 386/ 842, training loss 0.36118224263191223\n",
      "Epoch 14 -- Batch 387/ 842, training loss 0.36484286189079285\n",
      "Epoch 14 -- Batch 388/ 842, training loss 0.3521912395954132\n",
      "Epoch 14 -- Batch 389/ 842, training loss 0.3720637261867523\n",
      "Epoch 14 -- Batch 390/ 842, training loss 0.36228513717651367\n",
      "Epoch 14 -- Batch 391/ 842, training loss 0.3518233001232147\n",
      "Epoch 14 -- Batch 392/ 842, training loss 0.3454601764678955\n",
      "Epoch 14 -- Batch 393/ 842, training loss 0.3571529984474182\n",
      "Epoch 14 -- Batch 394/ 842, training loss 0.36389636993408203\n",
      "Epoch 14 -- Batch 395/ 842, training loss 0.3604937195777893\n",
      "Epoch 14 -- Batch 396/ 842, training loss 0.355457067489624\n",
      "Epoch 14 -- Batch 397/ 842, training loss 0.36842045187950134\n",
      "Epoch 14 -- Batch 398/ 842, training loss 0.3669736981391907\n",
      "Epoch 14 -- Batch 399/ 842, training loss 0.3782063126564026\n",
      "Epoch 14 -- Batch 400/ 842, training loss 0.36407744884490967\n",
      "Epoch 14 -- Batch 401/ 842, training loss 0.34978625178337097\n",
      "Epoch 14 -- Batch 402/ 842, training loss 0.36475345492362976\n",
      "Epoch 14 -- Batch 403/ 842, training loss 0.35393354296684265\n",
      "Epoch 14 -- Batch 404/ 842, training loss 0.3590085208415985\n",
      "Epoch 14 -- Batch 405/ 842, training loss 0.36077451705932617\n",
      "Epoch 14 -- Batch 406/ 842, training loss 0.3471064865589142\n",
      "Epoch 14 -- Batch 407/ 842, training loss 0.36638233065605164\n",
      "Epoch 14 -- Batch 408/ 842, training loss 0.35787031054496765\n",
      "Epoch 14 -- Batch 409/ 842, training loss 0.3676919937133789\n",
      "Epoch 14 -- Batch 410/ 842, training loss 0.3569689095020294\n",
      "Epoch 14 -- Batch 411/ 842, training loss 0.3570675849914551\n",
      "Epoch 14 -- Batch 412/ 842, training loss 0.3622106611728668\n",
      "Epoch 14 -- Batch 413/ 842, training loss 0.3618312478065491\n",
      "Epoch 14 -- Batch 414/ 842, training loss 0.36667218804359436\n",
      "Epoch 14 -- Batch 415/ 842, training loss 0.36756107211112976\n",
      "Epoch 14 -- Batch 416/ 842, training loss 0.3674527108669281\n",
      "Epoch 14 -- Batch 417/ 842, training loss 0.35953590273857117\n",
      "Epoch 14 -- Batch 418/ 842, training loss 0.37784719467163086\n",
      "Epoch 14 -- Batch 419/ 842, training loss 0.3547346591949463\n",
      "Epoch 14 -- Batch 420/ 842, training loss 0.349213182926178\n",
      "Epoch 14 -- Batch 421/ 842, training loss 0.3621450662612915\n",
      "Epoch 14 -- Batch 422/ 842, training loss 0.35750478506088257\n",
      "Epoch 14 -- Batch 423/ 842, training loss 0.35670918226242065\n",
      "Epoch 14 -- Batch 424/ 842, training loss 0.34942156076431274\n",
      "Epoch 14 -- Batch 425/ 842, training loss 0.3659445643424988\n",
      "Epoch 14 -- Batch 426/ 842, training loss 0.37090152502059937\n",
      "Epoch 14 -- Batch 427/ 842, training loss 0.35313868522644043\n",
      "Epoch 14 -- Batch 428/ 842, training loss 0.3608756363391876\n",
      "Epoch 14 -- Batch 429/ 842, training loss 0.37571898102760315\n",
      "Epoch 14 -- Batch 430/ 842, training loss 0.37554579973220825\n",
      "Epoch 14 -- Batch 431/ 842, training loss 0.3557547330856323\n",
      "Epoch 14 -- Batch 432/ 842, training loss 0.3816119134426117\n",
      "Epoch 14 -- Batch 433/ 842, training loss 0.35577723383903503\n",
      "Epoch 14 -- Batch 434/ 842, training loss 0.37546271085739136\n",
      "Epoch 14 -- Batch 435/ 842, training loss 0.3688029646873474\n",
      "Epoch 14 -- Batch 436/ 842, training loss 0.34209972620010376\n",
      "Epoch 14 -- Batch 437/ 842, training loss 0.36613374948501587\n",
      "Epoch 14 -- Batch 438/ 842, training loss 0.37682315707206726\n",
      "Epoch 14 -- Batch 439/ 842, training loss 0.3643103241920471\n",
      "Epoch 14 -- Batch 440/ 842, training loss 0.36549797654151917\n",
      "Epoch 14 -- Batch 441/ 842, training loss 0.3600350618362427\n",
      "Epoch 14 -- Batch 442/ 842, training loss 0.3679402768611908\n",
      "Epoch 14 -- Batch 443/ 842, training loss 0.36027011275291443\n",
      "Epoch 14 -- Batch 444/ 842, training loss 0.36457931995391846\n",
      "Epoch 14 -- Batch 445/ 842, training loss 0.37799617648124695\n",
      "Epoch 14 -- Batch 446/ 842, training loss 0.36224234104156494\n",
      "Epoch 14 -- Batch 447/ 842, training loss 0.373606413602829\n",
      "Epoch 14 -- Batch 448/ 842, training loss 0.36974769830703735\n",
      "Epoch 14 -- Batch 449/ 842, training loss 0.3817141354084015\n",
      "Epoch 14 -- Batch 450/ 842, training loss 0.3660629987716675\n",
      "Epoch 14 -- Batch 451/ 842, training loss 0.35738396644592285\n",
      "Epoch 14 -- Batch 452/ 842, training loss 0.3518940508365631\n",
      "Epoch 14 -- Batch 453/ 842, training loss 0.3545343279838562\n",
      "Epoch 14 -- Batch 454/ 842, training loss 0.3696172833442688\n",
      "Epoch 14 -- Batch 455/ 842, training loss 0.37596213817596436\n",
      "Epoch 14 -- Batch 456/ 842, training loss 0.3528079390525818\n",
      "Epoch 14 -- Batch 457/ 842, training loss 0.3740713894367218\n",
      "Epoch 14 -- Batch 458/ 842, training loss 0.3695189356803894\n",
      "Epoch 14 -- Batch 459/ 842, training loss 0.3693524897098541\n",
      "Epoch 14 -- Batch 460/ 842, training loss 0.37453868985176086\n",
      "Epoch 14 -- Batch 461/ 842, training loss 0.364974707365036\n",
      "Epoch 14 -- Batch 462/ 842, training loss 0.3693799674510956\n",
      "Epoch 14 -- Batch 463/ 842, training loss 0.37015289068222046\n",
      "Epoch 14 -- Batch 464/ 842, training loss 0.3619585931301117\n",
      "Epoch 14 -- Batch 465/ 842, training loss 0.35599833726882935\n",
      "Epoch 14 -- Batch 466/ 842, training loss 0.35648757219314575\n",
      "Epoch 14 -- Batch 467/ 842, training loss 0.3751218914985657\n",
      "Epoch 14 -- Batch 468/ 842, training loss 0.36002692580223083\n",
      "Epoch 14 -- Batch 469/ 842, training loss 0.3464335799217224\n",
      "Epoch 14 -- Batch 470/ 842, training loss 0.3584842383861542\n",
      "Epoch 14 -- Batch 471/ 842, training loss 0.3518277108669281\n",
      "Epoch 14 -- Batch 472/ 842, training loss 0.3591303527355194\n",
      "Epoch 14 -- Batch 473/ 842, training loss 0.35494038462638855\n",
      "Epoch 14 -- Batch 474/ 842, training loss 0.36109426617622375\n",
      "Epoch 14 -- Batch 475/ 842, training loss 0.3559807240962982\n",
      "Epoch 14 -- Batch 476/ 842, training loss 0.3487433195114136\n",
      "Epoch 14 -- Batch 477/ 842, training loss 0.3505188226699829\n",
      "Epoch 14 -- Batch 478/ 842, training loss 0.371143102645874\n",
      "Epoch 14 -- Batch 479/ 842, training loss 0.362952321767807\n",
      "Epoch 14 -- Batch 480/ 842, training loss 0.360259085893631\n",
      "Epoch 14 -- Batch 481/ 842, training loss 0.35208505392074585\n",
      "Epoch 14 -- Batch 482/ 842, training loss 0.34991639852523804\n",
      "Epoch 14 -- Batch 483/ 842, training loss 0.3647855818271637\n",
      "Epoch 14 -- Batch 484/ 842, training loss 0.3656383156776428\n",
      "Epoch 14 -- Batch 485/ 842, training loss 0.3490869700908661\n",
      "Epoch 14 -- Batch 486/ 842, training loss 0.3590167760848999\n",
      "Epoch 14 -- Batch 487/ 842, training loss 0.37290334701538086\n",
      "Epoch 14 -- Batch 488/ 842, training loss 0.3605063259601593\n",
      "Epoch 14 -- Batch 489/ 842, training loss 0.3558156490325928\n",
      "Epoch 14 -- Batch 490/ 842, training loss 0.3743698000907898\n",
      "Epoch 14 -- Batch 491/ 842, training loss 0.3704604208469391\n",
      "Epoch 14 -- Batch 492/ 842, training loss 0.3654486835002899\n",
      "Epoch 14 -- Batch 493/ 842, training loss 0.3554120659828186\n",
      "Epoch 14 -- Batch 494/ 842, training loss 0.35254529118537903\n",
      "Epoch 14 -- Batch 495/ 842, training loss 0.34645718336105347\n",
      "Epoch 14 -- Batch 496/ 842, training loss 0.3738415241241455\n",
      "Epoch 14 -- Batch 497/ 842, training loss 0.369770884513855\n",
      "Epoch 14 -- Batch 498/ 842, training loss 0.3606311082839966\n",
      "Epoch 14 -- Batch 499/ 842, training loss 0.3808627128601074\n",
      "Epoch 14 -- Batch 500/ 842, training loss 0.3610343635082245\n",
      "Epoch 14 -- Batch 501/ 842, training loss 0.36910557746887207\n",
      "Epoch 14 -- Batch 502/ 842, training loss 0.3644445836544037\n",
      "Epoch 14 -- Batch 503/ 842, training loss 0.3724960386753082\n",
      "Epoch 14 -- Batch 504/ 842, training loss 0.35880061984062195\n",
      "Epoch 14 -- Batch 505/ 842, training loss 0.36205461621284485\n",
      "Epoch 14 -- Batch 506/ 842, training loss 0.3661477267742157\n",
      "Epoch 14 -- Batch 507/ 842, training loss 0.3620269298553467\n",
      "Epoch 14 -- Batch 508/ 842, training loss 0.3643379211425781\n",
      "Epoch 14 -- Batch 509/ 842, training loss 0.36354076862335205\n",
      "Epoch 14 -- Batch 510/ 842, training loss 0.36166703701019287\n",
      "Epoch 14 -- Batch 511/ 842, training loss 0.3694145083427429\n",
      "Epoch 14 -- Batch 512/ 842, training loss 0.3580915927886963\n",
      "Epoch 14 -- Batch 513/ 842, training loss 0.36795639991760254\n",
      "Epoch 14 -- Batch 514/ 842, training loss 0.360432505607605\n",
      "Epoch 14 -- Batch 515/ 842, training loss 0.365117609500885\n",
      "Epoch 14 -- Batch 516/ 842, training loss 0.35729333758354187\n",
      "Epoch 14 -- Batch 517/ 842, training loss 0.370737224817276\n",
      "Epoch 14 -- Batch 518/ 842, training loss 0.36751219630241394\n",
      "Epoch 14 -- Batch 519/ 842, training loss 0.3625582754611969\n",
      "Epoch 14 -- Batch 520/ 842, training loss 0.36451971530914307\n",
      "Epoch 14 -- Batch 521/ 842, training loss 0.36404499411582947\n",
      "Epoch 14 -- Batch 522/ 842, training loss 0.3586910367012024\n",
      "Epoch 14 -- Batch 523/ 842, training loss 0.3756360411643982\n",
      "Epoch 14 -- Batch 524/ 842, training loss 0.35511481761932373\n",
      "Epoch 14 -- Batch 525/ 842, training loss 0.35172712802886963\n",
      "Epoch 14 -- Batch 526/ 842, training loss 0.3604559600353241\n",
      "Epoch 14 -- Batch 527/ 842, training loss 0.3726169764995575\n",
      "Epoch 14 -- Batch 528/ 842, training loss 0.3665565252304077\n",
      "Epoch 14 -- Batch 529/ 842, training loss 0.3658304512500763\n",
      "Epoch 14 -- Batch 530/ 842, training loss 0.36340636014938354\n",
      "Epoch 14 -- Batch 531/ 842, training loss 0.3548365533351898\n",
      "Epoch 14 -- Batch 532/ 842, training loss 0.35559165477752686\n",
      "Epoch 14 -- Batch 533/ 842, training loss 0.35314956307411194\n",
      "Epoch 14 -- Batch 534/ 842, training loss 0.35021263360977173\n",
      "Epoch 14 -- Batch 535/ 842, training loss 0.36516714096069336\n",
      "Epoch 14 -- Batch 536/ 842, training loss 0.36365842819213867\n",
      "Epoch 14 -- Batch 537/ 842, training loss 0.3581048250198364\n",
      "Epoch 14 -- Batch 538/ 842, training loss 0.35308313369750977\n",
      "Epoch 14 -- Batch 539/ 842, training loss 0.36409538984298706\n",
      "Epoch 14 -- Batch 540/ 842, training loss 0.35803309082984924\n",
      "Epoch 14 -- Batch 541/ 842, training loss 0.36293745040893555\n",
      "Epoch 14 -- Batch 542/ 842, training loss 0.3584707975387573\n",
      "Epoch 14 -- Batch 543/ 842, training loss 0.3619500994682312\n",
      "Epoch 14 -- Batch 544/ 842, training loss 0.35842716693878174\n",
      "Epoch 14 -- Batch 545/ 842, training loss 0.3560757637023926\n",
      "Epoch 14 -- Batch 546/ 842, training loss 0.3616432547569275\n",
      "Epoch 14 -- Batch 547/ 842, training loss 0.3617132902145386\n",
      "Epoch 14 -- Batch 548/ 842, training loss 0.3596310615539551\n",
      "Epoch 14 -- Batch 549/ 842, training loss 0.36232519149780273\n",
      "Epoch 14 -- Batch 550/ 842, training loss 0.358777791261673\n",
      "Epoch 14 -- Batch 551/ 842, training loss 0.3746741712093353\n",
      "Epoch 14 -- Batch 552/ 842, training loss 0.37630966305732727\n",
      "Epoch 14 -- Batch 553/ 842, training loss 0.34653565287590027\n",
      "Epoch 14 -- Batch 554/ 842, training loss 0.349590539932251\n",
      "Epoch 14 -- Batch 555/ 842, training loss 0.36543405055999756\n",
      "Epoch 14 -- Batch 556/ 842, training loss 0.37629976868629456\n",
      "Epoch 14 -- Batch 557/ 842, training loss 0.36434605717658997\n",
      "Epoch 14 -- Batch 558/ 842, training loss 0.3556840717792511\n",
      "Epoch 14 -- Batch 559/ 842, training loss 0.38201019167900085\n",
      "Epoch 14 -- Batch 560/ 842, training loss 0.35818639397621155\n",
      "Epoch 14 -- Batch 561/ 842, training loss 0.3610183000564575\n",
      "Epoch 14 -- Batch 562/ 842, training loss 0.3660832345485687\n",
      "Epoch 14 -- Batch 563/ 842, training loss 0.35472965240478516\n",
      "Epoch 14 -- Batch 564/ 842, training loss 0.3455811142921448\n",
      "Epoch 14 -- Batch 565/ 842, training loss 0.3566575050354004\n",
      "Epoch 14 -- Batch 566/ 842, training loss 0.35741978883743286\n",
      "Epoch 14 -- Batch 567/ 842, training loss 0.3685249388217926\n",
      "Epoch 14 -- Batch 568/ 842, training loss 0.3566757142543793\n",
      "Epoch 14 -- Batch 569/ 842, training loss 0.3579859733581543\n",
      "Epoch 14 -- Batch 570/ 842, training loss 0.35589346289634705\n",
      "Epoch 14 -- Batch 571/ 842, training loss 0.3651502728462219\n",
      "Epoch 14 -- Batch 572/ 842, training loss 0.35795077681541443\n",
      "Epoch 14 -- Batch 573/ 842, training loss 0.3571779429912567\n",
      "Epoch 14 -- Batch 574/ 842, training loss 0.35189107060432434\n",
      "Epoch 14 -- Batch 575/ 842, training loss 0.36634883284568787\n",
      "Epoch 14 -- Batch 576/ 842, training loss 0.3611249625682831\n",
      "Epoch 14 -- Batch 577/ 842, training loss 0.35403579473495483\n",
      "Epoch 14 -- Batch 578/ 842, training loss 0.3630841076374054\n",
      "Epoch 14 -- Batch 579/ 842, training loss 0.3696292042732239\n",
      "Epoch 14 -- Batch 580/ 842, training loss 0.3803168833255768\n",
      "Epoch 14 -- Batch 581/ 842, training loss 0.36747169494628906\n",
      "Epoch 14 -- Batch 582/ 842, training loss 0.35643237829208374\n",
      "Epoch 14 -- Batch 583/ 842, training loss 0.3684956431388855\n",
      "Epoch 14 -- Batch 584/ 842, training loss 0.34645310044288635\n",
      "Epoch 14 -- Batch 585/ 842, training loss 0.3716900944709778\n",
      "Epoch 14 -- Batch 586/ 842, training loss 0.3562433421611786\n",
      "Epoch 14 -- Batch 587/ 842, training loss 0.364070326089859\n",
      "Epoch 14 -- Batch 588/ 842, training loss 0.364288330078125\n",
      "Epoch 14 -- Batch 589/ 842, training loss 0.352713406085968\n",
      "Epoch 14 -- Batch 590/ 842, training loss 0.37070995569229126\n",
      "Epoch 14 -- Batch 591/ 842, training loss 0.35278820991516113\n",
      "Epoch 14 -- Batch 592/ 842, training loss 0.36389079689979553\n",
      "Epoch 14 -- Batch 593/ 842, training loss 0.3601548969745636\n",
      "Epoch 14 -- Batch 594/ 842, training loss 0.37374812364578247\n",
      "Epoch 14 -- Batch 595/ 842, training loss 0.36065545678138733\n",
      "Epoch 14 -- Batch 596/ 842, training loss 0.3657224476337433\n",
      "Epoch 14 -- Batch 597/ 842, training loss 0.3739529848098755\n",
      "Epoch 14 -- Batch 598/ 842, training loss 0.37302592396736145\n",
      "Epoch 14 -- Batch 599/ 842, training loss 0.3723328709602356\n",
      "Epoch 14 -- Batch 600/ 842, training loss 0.35760006308555603\n",
      "Epoch 14 -- Batch 601/ 842, training loss 0.36601722240448\n",
      "Epoch 14 -- Batch 602/ 842, training loss 0.35713550448417664\n",
      "Epoch 14 -- Batch 603/ 842, training loss 0.3656127452850342\n",
      "Epoch 14 -- Batch 604/ 842, training loss 0.369417279958725\n",
      "Epoch 14 -- Batch 605/ 842, training loss 0.3658096492290497\n",
      "Epoch 14 -- Batch 606/ 842, training loss 0.36586183309555054\n",
      "Epoch 14 -- Batch 607/ 842, training loss 0.3601698577404022\n",
      "Epoch 14 -- Batch 608/ 842, training loss 0.3600788712501526\n",
      "Epoch 14 -- Batch 609/ 842, training loss 0.3697168529033661\n",
      "Epoch 14 -- Batch 610/ 842, training loss 0.35861191153526306\n",
      "Epoch 14 -- Batch 611/ 842, training loss 0.37270787358283997\n",
      "Epoch 14 -- Batch 612/ 842, training loss 0.35532188415527344\n",
      "Epoch 14 -- Batch 613/ 842, training loss 0.3615524172782898\n",
      "Epoch 14 -- Batch 614/ 842, training loss 0.36858877539634705\n",
      "Epoch 14 -- Batch 615/ 842, training loss 0.36198800802230835\n",
      "Epoch 14 -- Batch 616/ 842, training loss 0.3603857755661011\n",
      "Epoch 14 -- Batch 617/ 842, training loss 0.35254448652267456\n",
      "Epoch 14 -- Batch 618/ 842, training loss 0.3496847450733185\n",
      "Epoch 14 -- Batch 619/ 842, training loss 0.3600751757621765\n",
      "Epoch 14 -- Batch 620/ 842, training loss 0.358692467212677\n",
      "Epoch 14 -- Batch 621/ 842, training loss 0.36177295446395874\n",
      "Epoch 14 -- Batch 622/ 842, training loss 0.35719650983810425\n",
      "Epoch 14 -- Batch 623/ 842, training loss 0.3606918454170227\n",
      "Epoch 14 -- Batch 624/ 842, training loss 0.3542453944683075\n",
      "Epoch 14 -- Batch 625/ 842, training loss 0.35764726996421814\n",
      "Epoch 14 -- Batch 626/ 842, training loss 0.3660449683666229\n",
      "Epoch 14 -- Batch 627/ 842, training loss 0.3812792897224426\n",
      "Epoch 14 -- Batch 628/ 842, training loss 0.36567145586013794\n",
      "Epoch 14 -- Batch 629/ 842, training loss 0.3632200062274933\n",
      "Epoch 14 -- Batch 630/ 842, training loss 0.3466474413871765\n",
      "Epoch 14 -- Batch 631/ 842, training loss 0.3651822805404663\n",
      "Epoch 14 -- Batch 632/ 842, training loss 0.3388141989707947\n",
      "Epoch 14 -- Batch 633/ 842, training loss 0.3482249081134796\n",
      "Epoch 14 -- Batch 634/ 842, training loss 0.35834819078445435\n",
      "Epoch 14 -- Batch 635/ 842, training loss 0.35954904556274414\n",
      "Epoch 14 -- Batch 636/ 842, training loss 0.36884650588035583\n",
      "Epoch 14 -- Batch 637/ 842, training loss 0.36309751868247986\n",
      "Epoch 14 -- Batch 638/ 842, training loss 0.35006505250930786\n",
      "Epoch 14 -- Batch 639/ 842, training loss 0.3551616966724396\n",
      "Epoch 14 -- Batch 640/ 842, training loss 0.3645334541797638\n",
      "Epoch 14 -- Batch 641/ 842, training loss 0.3578062355518341\n",
      "Epoch 14 -- Batch 642/ 842, training loss 0.3533264398574829\n",
      "Epoch 14 -- Batch 643/ 842, training loss 0.3674670159816742\n",
      "Epoch 14 -- Batch 644/ 842, training loss 0.3721466064453125\n",
      "Epoch 14 -- Batch 645/ 842, training loss 0.3612503409385681\n",
      "Epoch 14 -- Batch 646/ 842, training loss 0.37086692452430725\n",
      "Epoch 14 -- Batch 647/ 842, training loss 0.36883777379989624\n",
      "Epoch 14 -- Batch 648/ 842, training loss 0.3660159409046173\n",
      "Epoch 14 -- Batch 649/ 842, training loss 0.37303268909454346\n",
      "Epoch 14 -- Batch 650/ 842, training loss 0.36462563276290894\n",
      "Epoch 14 -- Batch 651/ 842, training loss 0.3676716387271881\n",
      "Epoch 14 -- Batch 652/ 842, training loss 0.3502342104911804\n",
      "Epoch 14 -- Batch 653/ 842, training loss 0.37267714738845825\n",
      "Epoch 14 -- Batch 654/ 842, training loss 0.35222259163856506\n",
      "Epoch 14 -- Batch 655/ 842, training loss 0.37039443850517273\n",
      "Epoch 14 -- Batch 656/ 842, training loss 0.3531480133533478\n",
      "Epoch 14 -- Batch 657/ 842, training loss 0.3660466969013214\n",
      "Epoch 14 -- Batch 658/ 842, training loss 0.34920915961265564\n",
      "Epoch 14 -- Batch 659/ 842, training loss 0.3731972277164459\n",
      "Epoch 14 -- Batch 660/ 842, training loss 0.3520471751689911\n",
      "Epoch 14 -- Batch 661/ 842, training loss 0.3553386330604553\n",
      "Epoch 14 -- Batch 662/ 842, training loss 0.36676856875419617\n",
      "Epoch 14 -- Batch 663/ 842, training loss 0.36148834228515625\n",
      "Epoch 14 -- Batch 664/ 842, training loss 0.34110894799232483\n",
      "Epoch 14 -- Batch 665/ 842, training loss 0.3861134648323059\n",
      "Epoch 14 -- Batch 666/ 842, training loss 0.35506176948547363\n",
      "Epoch 14 -- Batch 667/ 842, training loss 0.3572862148284912\n",
      "Epoch 14 -- Batch 668/ 842, training loss 0.3646305203437805\n",
      "Epoch 14 -- Batch 669/ 842, training loss 0.36267024278640747\n",
      "Epoch 14 -- Batch 670/ 842, training loss 0.3650793731212616\n",
      "Epoch 14 -- Batch 671/ 842, training loss 0.3743228614330292\n",
      "Epoch 14 -- Batch 672/ 842, training loss 0.3592188358306885\n",
      "Epoch 14 -- Batch 673/ 842, training loss 0.3691417872905731\n",
      "Epoch 14 -- Batch 674/ 842, training loss 0.36287280917167664\n",
      "Epoch 14 -- Batch 675/ 842, training loss 0.3554384708404541\n",
      "Epoch 14 -- Batch 676/ 842, training loss 0.3591053783893585\n",
      "Epoch 14 -- Batch 677/ 842, training loss 0.35019585490226746\n",
      "Epoch 14 -- Batch 678/ 842, training loss 0.3656408190727234\n",
      "Epoch 14 -- Batch 679/ 842, training loss 0.3575115501880646\n",
      "Epoch 14 -- Batch 680/ 842, training loss 0.3786506652832031\n",
      "Epoch 14 -- Batch 681/ 842, training loss 0.3573622703552246\n",
      "Epoch 14 -- Batch 682/ 842, training loss 0.34612488746643066\n",
      "Epoch 14 -- Batch 683/ 842, training loss 0.36899974942207336\n",
      "Epoch 14 -- Batch 684/ 842, training loss 0.3546714186668396\n",
      "Epoch 14 -- Batch 685/ 842, training loss 0.37297937273979187\n",
      "Epoch 14 -- Batch 686/ 842, training loss 0.354931503534317\n",
      "Epoch 14 -- Batch 687/ 842, training loss 0.3723357617855072\n",
      "Epoch 14 -- Batch 688/ 842, training loss 0.3679010570049286\n",
      "Epoch 14 -- Batch 689/ 842, training loss 0.3714464604854584\n",
      "Epoch 14 -- Batch 690/ 842, training loss 0.3669789135456085\n",
      "Epoch 14 -- Batch 691/ 842, training loss 0.35997024178504944\n",
      "Epoch 14 -- Batch 692/ 842, training loss 0.3773525059223175\n",
      "Epoch 14 -- Batch 693/ 842, training loss 0.37580323219299316\n",
      "Epoch 14 -- Batch 694/ 842, training loss 0.3517002463340759\n",
      "Epoch 14 -- Batch 695/ 842, training loss 0.3566901981830597\n",
      "Epoch 14 -- Batch 696/ 842, training loss 0.3573647737503052\n",
      "Epoch 14 -- Batch 697/ 842, training loss 0.3741406798362732\n",
      "Epoch 14 -- Batch 698/ 842, training loss 0.3604517877101898\n",
      "Epoch 14 -- Batch 699/ 842, training loss 0.3738974928855896\n",
      "Epoch 14 -- Batch 700/ 842, training loss 0.35808998346328735\n",
      "Epoch 14 -- Batch 701/ 842, training loss 0.36934205889701843\n",
      "Epoch 14 -- Batch 702/ 842, training loss 0.3780735731124878\n",
      "Epoch 14 -- Batch 703/ 842, training loss 0.3684004247188568\n",
      "Epoch 14 -- Batch 704/ 842, training loss 0.36488887667655945\n",
      "Epoch 14 -- Batch 705/ 842, training loss 0.35182836651802063\n",
      "Epoch 14 -- Batch 706/ 842, training loss 0.35725298523902893\n",
      "Epoch 14 -- Batch 707/ 842, training loss 0.3564334511756897\n",
      "Epoch 14 -- Batch 708/ 842, training loss 0.34916073083877563\n",
      "Epoch 14 -- Batch 709/ 842, training loss 0.369473397731781\n",
      "Epoch 14 -- Batch 710/ 842, training loss 0.37437859177589417\n",
      "Epoch 14 -- Batch 711/ 842, training loss 0.34852728247642517\n",
      "Epoch 14 -- Batch 712/ 842, training loss 0.37723270058631897\n",
      "Epoch 14 -- Batch 713/ 842, training loss 0.36059296131134033\n",
      "Epoch 14 -- Batch 714/ 842, training loss 0.35977306962013245\n",
      "Epoch 14 -- Batch 715/ 842, training loss 0.3699198067188263\n",
      "Epoch 14 -- Batch 716/ 842, training loss 0.36624500155448914\n",
      "Epoch 14 -- Batch 717/ 842, training loss 0.36791229248046875\n",
      "Epoch 14 -- Batch 718/ 842, training loss 0.34746044874191284\n",
      "Epoch 14 -- Batch 719/ 842, training loss 0.374015748500824\n",
      "Epoch 14 -- Batch 720/ 842, training loss 0.3554653227329254\n",
      "Epoch 14 -- Batch 721/ 842, training loss 0.3620257079601288\n",
      "Epoch 14 -- Batch 722/ 842, training loss 0.3670523464679718\n",
      "Epoch 14 -- Batch 723/ 842, training loss 0.38584819436073303\n",
      "Epoch 14 -- Batch 724/ 842, training loss 0.35993555188179016\n",
      "Epoch 14 -- Batch 725/ 842, training loss 0.37955525517463684\n",
      "Epoch 14 -- Batch 726/ 842, training loss 0.35648995637893677\n",
      "Epoch 14 -- Batch 727/ 842, training loss 0.36465224623680115\n",
      "Epoch 14 -- Batch 728/ 842, training loss 0.37646958231925964\n",
      "Epoch 14 -- Batch 729/ 842, training loss 0.3528573215007782\n",
      "Epoch 14 -- Batch 730/ 842, training loss 0.3522396683692932\n",
      "Epoch 14 -- Batch 731/ 842, training loss 0.35781314969062805\n",
      "Epoch 14 -- Batch 732/ 842, training loss 0.3500039875507355\n",
      "Epoch 14 -- Batch 733/ 842, training loss 0.36733099818229675\n",
      "Epoch 14 -- Batch 734/ 842, training loss 0.35423901677131653\n",
      "Epoch 14 -- Batch 735/ 842, training loss 0.3629089295864105\n",
      "Epoch 14 -- Batch 736/ 842, training loss 0.36275678873062134\n",
      "Epoch 14 -- Batch 737/ 842, training loss 0.37016811966896057\n",
      "Epoch 14 -- Batch 738/ 842, training loss 0.35436367988586426\n",
      "Epoch 14 -- Batch 739/ 842, training loss 0.36737895011901855\n",
      "Epoch 14 -- Batch 740/ 842, training loss 0.370957612991333\n",
      "Epoch 14 -- Batch 741/ 842, training loss 0.35571059584617615\n",
      "Epoch 14 -- Batch 742/ 842, training loss 0.37555131316185\n",
      "Epoch 14 -- Batch 743/ 842, training loss 0.3485397696495056\n",
      "Epoch 14 -- Batch 744/ 842, training loss 0.360146701335907\n",
      "Epoch 14 -- Batch 745/ 842, training loss 0.36081770062446594\n",
      "Epoch 14 -- Batch 746/ 842, training loss 0.375665545463562\n",
      "Epoch 14 -- Batch 747/ 842, training loss 0.3564840853214264\n",
      "Epoch 14 -- Batch 748/ 842, training loss 0.3647835850715637\n",
      "Epoch 14 -- Batch 749/ 842, training loss 0.359003484249115\n",
      "Epoch 14 -- Batch 750/ 842, training loss 0.35919520258903503\n",
      "Epoch 14 -- Batch 751/ 842, training loss 0.3615243434906006\n",
      "Epoch 14 -- Batch 752/ 842, training loss 0.35411182045936584\n",
      "Epoch 14 -- Batch 753/ 842, training loss 0.34864935278892517\n",
      "Epoch 14 -- Batch 754/ 842, training loss 0.3458815813064575\n",
      "Epoch 14 -- Batch 755/ 842, training loss 0.3514416217803955\n",
      "Epoch 14 -- Batch 756/ 842, training loss 0.363334596157074\n",
      "Epoch 14 -- Batch 757/ 842, training loss 0.35860466957092285\n",
      "Epoch 14 -- Batch 758/ 842, training loss 0.37104129791259766\n",
      "Epoch 14 -- Batch 759/ 842, training loss 0.3587011396884918\n",
      "Epoch 14 -- Batch 760/ 842, training loss 0.3577556014060974\n",
      "Epoch 14 -- Batch 761/ 842, training loss 0.3591558635234833\n",
      "Epoch 14 -- Batch 762/ 842, training loss 0.35928449034690857\n",
      "Epoch 14 -- Batch 763/ 842, training loss 0.36576157808303833\n",
      "Epoch 14 -- Batch 764/ 842, training loss 0.35532256960868835\n",
      "Epoch 14 -- Batch 765/ 842, training loss 0.36096611618995667\n",
      "Epoch 14 -- Batch 766/ 842, training loss 0.3696236312389374\n",
      "Epoch 14 -- Batch 767/ 842, training loss 0.3551363945007324\n",
      "Epoch 14 -- Batch 768/ 842, training loss 0.3626575469970703\n",
      "Epoch 14 -- Batch 769/ 842, training loss 0.3668414056301117\n",
      "Epoch 14 -- Batch 770/ 842, training loss 0.35838863253593445\n",
      "Epoch 14 -- Batch 771/ 842, training loss 0.35021206736564636\n",
      "Epoch 14 -- Batch 772/ 842, training loss 0.3663962185382843\n",
      "Epoch 14 -- Batch 773/ 842, training loss 0.3584763705730438\n",
      "Epoch 14 -- Batch 774/ 842, training loss 0.3465714454650879\n",
      "Epoch 14 -- Batch 775/ 842, training loss 0.3525368869304657\n",
      "Epoch 14 -- Batch 776/ 842, training loss 0.3513548970222473\n",
      "Epoch 14 -- Batch 777/ 842, training loss 0.3612089455127716\n",
      "Epoch 14 -- Batch 778/ 842, training loss 0.36311647295951843\n",
      "Epoch 14 -- Batch 779/ 842, training loss 0.36557069420814514\n",
      "Epoch 14 -- Batch 780/ 842, training loss 0.3513919413089752\n",
      "Epoch 14 -- Batch 781/ 842, training loss 0.3725445568561554\n",
      "Epoch 14 -- Batch 782/ 842, training loss 0.35762935876846313\n",
      "Epoch 14 -- Batch 783/ 842, training loss 0.35152220726013184\n",
      "Epoch 14 -- Batch 784/ 842, training loss 0.36543992161750793\n",
      "Epoch 14 -- Batch 785/ 842, training loss 0.3545272648334503\n",
      "Epoch 14 -- Batch 786/ 842, training loss 0.36685821413993835\n",
      "Epoch 14 -- Batch 787/ 842, training loss 0.34378913044929504\n",
      "Epoch 14 -- Batch 788/ 842, training loss 0.37398454546928406\n",
      "Epoch 14 -- Batch 789/ 842, training loss 0.36910170316696167\n",
      "Epoch 14 -- Batch 790/ 842, training loss 0.355538934469223\n",
      "Epoch 14 -- Batch 791/ 842, training loss 0.3751010000705719\n",
      "Epoch 14 -- Batch 792/ 842, training loss 0.3722759187221527\n",
      "Epoch 14 -- Batch 793/ 842, training loss 0.3707660436630249\n",
      "Epoch 14 -- Batch 794/ 842, training loss 0.3607785999774933\n",
      "Epoch 14 -- Batch 795/ 842, training loss 0.36557790637016296\n",
      "Epoch 14 -- Batch 796/ 842, training loss 0.3582182228565216\n",
      "Epoch 14 -- Batch 797/ 842, training loss 0.3685881793498993\n",
      "Epoch 14 -- Batch 798/ 842, training loss 0.3667185604572296\n",
      "Epoch 14 -- Batch 799/ 842, training loss 0.3625379204750061\n",
      "Epoch 14 -- Batch 800/ 842, training loss 0.36757123470306396\n",
      "Epoch 14 -- Batch 801/ 842, training loss 0.35280558466911316\n",
      "Epoch 14 -- Batch 802/ 842, training loss 0.3787364065647125\n",
      "Epoch 14 -- Batch 803/ 842, training loss 0.3565181791782379\n",
      "Epoch 14 -- Batch 804/ 842, training loss 0.3656654357910156\n",
      "Epoch 14 -- Batch 805/ 842, training loss 0.3630395233631134\n",
      "Epoch 14 -- Batch 806/ 842, training loss 0.35608237981796265\n",
      "Epoch 14 -- Batch 807/ 842, training loss 0.3501647412776947\n",
      "Epoch 14 -- Batch 808/ 842, training loss 0.354979008436203\n",
      "Epoch 14 -- Batch 809/ 842, training loss 0.35107937455177307\n",
      "Epoch 14 -- Batch 810/ 842, training loss 0.3699285387992859\n",
      "Epoch 14 -- Batch 811/ 842, training loss 0.36788210272789\n",
      "Epoch 14 -- Batch 812/ 842, training loss 0.3570635914802551\n",
      "Epoch 14 -- Batch 813/ 842, training loss 0.34415534138679504\n",
      "Epoch 14 -- Batch 814/ 842, training loss 0.3731704652309418\n",
      "Epoch 14 -- Batch 815/ 842, training loss 0.37432581186294556\n",
      "Epoch 14 -- Batch 816/ 842, training loss 0.3579482138156891\n",
      "Epoch 14 -- Batch 817/ 842, training loss 0.3680723309516907\n",
      "Epoch 14 -- Batch 818/ 842, training loss 0.365896075963974\n",
      "Epoch 14 -- Batch 819/ 842, training loss 0.3700128197669983\n",
      "Epoch 14 -- Batch 820/ 842, training loss 0.3528103828430176\n",
      "Epoch 14 -- Batch 821/ 842, training loss 0.3620665967464447\n",
      "Epoch 14 -- Batch 822/ 842, training loss 0.37889477610588074\n",
      "Epoch 14 -- Batch 823/ 842, training loss 0.3448277413845062\n",
      "Epoch 14 -- Batch 824/ 842, training loss 0.3622393012046814\n",
      "Epoch 14 -- Batch 825/ 842, training loss 0.39468470215797424\n",
      "Epoch 14 -- Batch 826/ 842, training loss 0.3590087890625\n",
      "Epoch 14 -- Batch 827/ 842, training loss 0.3624143600463867\n",
      "Epoch 14 -- Batch 828/ 842, training loss 0.35088467597961426\n",
      "Epoch 14 -- Batch 829/ 842, training loss 0.36414214968681335\n",
      "Epoch 14 -- Batch 830/ 842, training loss 0.3587700426578522\n",
      "Epoch 14 -- Batch 831/ 842, training loss 0.36743220686912537\n",
      "Epoch 14 -- Batch 832/ 842, training loss 0.3483763039112091\n",
      "Epoch 14 -- Batch 833/ 842, training loss 0.3732560873031616\n",
      "Epoch 14 -- Batch 834/ 842, training loss 0.3833550214767456\n",
      "Epoch 14 -- Batch 835/ 842, training loss 0.3706744611263275\n",
      "Epoch 14 -- Batch 836/ 842, training loss 0.36313849687576294\n",
      "Epoch 14 -- Batch 837/ 842, training loss 0.37214139103889465\n",
      "Epoch 14 -- Batch 838/ 842, training loss 0.3475906252861023\n",
      "Epoch 14 -- Batch 839/ 842, training loss 0.35678189992904663\n",
      "Epoch 14 -- Batch 840/ 842, training loss 0.36701637506484985\n",
      "Epoch 14 -- Batch 841/ 842, training loss 0.35795941948890686\n",
      "Epoch 14 -- Batch 842/ 842, training loss 0.3539963662624359\n",
      "----------------------------------------------------------------------\n",
      "Epoch 14 -- Batch 1/ 94, validation loss 0.3686467409133911\n",
      "Epoch 14 -- Batch 2/ 94, validation loss 0.3530575931072235\n",
      "Epoch 14 -- Batch 3/ 94, validation loss 0.3491576015949249\n",
      "Epoch 14 -- Batch 4/ 94, validation loss 0.36641639471054077\n",
      "Epoch 14 -- Batch 5/ 94, validation loss 0.34610143303871155\n",
      "Epoch 14 -- Batch 6/ 94, validation loss 0.35327333211898804\n",
      "Epoch 14 -- Batch 7/ 94, validation loss 0.34571611881256104\n",
      "Epoch 14 -- Batch 8/ 94, validation loss 0.3423801064491272\n",
      "Epoch 14 -- Batch 9/ 94, validation loss 0.36313965916633606\n",
      "Epoch 14 -- Batch 10/ 94, validation loss 0.3583877980709076\n",
      "Epoch 14 -- Batch 11/ 94, validation loss 0.35411930084228516\n",
      "Epoch 14 -- Batch 12/ 94, validation loss 0.33803296089172363\n",
      "Epoch 14 -- Batch 13/ 94, validation loss 0.3550138771533966\n",
      "Epoch 14 -- Batch 14/ 94, validation loss 0.34797513484954834\n",
      "Epoch 14 -- Batch 15/ 94, validation loss 0.3404988646507263\n",
      "Epoch 14 -- Batch 16/ 94, validation loss 0.3656464219093323\n",
      "Epoch 14 -- Batch 17/ 94, validation loss 0.3513112962245941\n",
      "Epoch 14 -- Batch 18/ 94, validation loss 0.3530966639518738\n",
      "Epoch 14 -- Batch 19/ 94, validation loss 0.37656170129776\n",
      "Epoch 14 -- Batch 20/ 94, validation loss 0.3526548743247986\n",
      "Epoch 14 -- Batch 21/ 94, validation loss 0.3580009639263153\n",
      "Epoch 14 -- Batch 22/ 94, validation loss 0.35356804728507996\n",
      "Epoch 14 -- Batch 23/ 94, validation loss 0.3455844521522522\n",
      "Epoch 14 -- Batch 24/ 94, validation loss 0.3444647490978241\n",
      "Epoch 14 -- Batch 25/ 94, validation loss 0.3512340486049652\n",
      "Epoch 14 -- Batch 26/ 94, validation loss 0.35083749890327454\n",
      "Epoch 14 -- Batch 27/ 94, validation loss 0.35321950912475586\n",
      "Epoch 14 -- Batch 28/ 94, validation loss 0.3554895520210266\n",
      "Epoch 14 -- Batch 29/ 94, validation loss 0.36789125204086304\n",
      "Epoch 14 -- Batch 30/ 94, validation loss 0.35465937852859497\n",
      "Epoch 14 -- Batch 31/ 94, validation loss 0.3610401153564453\n",
      "Epoch 14 -- Batch 32/ 94, validation loss 0.3643651008605957\n",
      "Epoch 14 -- Batch 33/ 94, validation loss 0.3516223132610321\n",
      "Epoch 14 -- Batch 34/ 94, validation loss 0.34808462858200073\n",
      "Epoch 14 -- Batch 35/ 94, validation loss 0.3650050163269043\n",
      "Epoch 14 -- Batch 36/ 94, validation loss 0.34857022762298584\n",
      "Epoch 14 -- Batch 37/ 94, validation loss 0.35109174251556396\n",
      "Epoch 14 -- Batch 38/ 94, validation loss 0.3432753384113312\n",
      "Epoch 14 -- Batch 39/ 94, validation loss 0.36031052470207214\n",
      "Epoch 14 -- Batch 40/ 94, validation loss 0.3453318178653717\n",
      "Epoch 14 -- Batch 41/ 94, validation loss 0.361227422952652\n",
      "Epoch 14 -- Batch 42/ 94, validation loss 0.3500329852104187\n",
      "Epoch 14 -- Batch 43/ 94, validation loss 0.35761556029319763\n",
      "Epoch 14 -- Batch 44/ 94, validation loss 0.3494608998298645\n",
      "Epoch 14 -- Batch 45/ 94, validation loss 0.3690766394138336\n",
      "Epoch 14 -- Batch 46/ 94, validation loss 0.35852354764938354\n",
      "Epoch 14 -- Batch 47/ 94, validation loss 0.3512009382247925\n",
      "Epoch 14 -- Batch 48/ 94, validation loss 0.34177640080451965\n",
      "Epoch 14 -- Batch 49/ 94, validation loss 0.3555237352848053\n",
      "Epoch 14 -- Batch 50/ 94, validation loss 0.3603568375110626\n",
      "Epoch 14 -- Batch 51/ 94, validation loss 0.355243980884552\n",
      "Epoch 14 -- Batch 52/ 94, validation loss 0.35484954714775085\n",
      "Epoch 14 -- Batch 53/ 94, validation loss 0.3554724156856537\n",
      "Epoch 14 -- Batch 54/ 94, validation loss 0.34809884428977966\n",
      "Epoch 14 -- Batch 55/ 94, validation loss 0.3596802055835724\n",
      "Epoch 14 -- Batch 56/ 94, validation loss 0.3666222095489502\n",
      "Epoch 14 -- Batch 57/ 94, validation loss 0.36434003710746765\n",
      "Epoch 14 -- Batch 58/ 94, validation loss 0.35509443283081055\n",
      "Epoch 14 -- Batch 59/ 94, validation loss 0.38391682505607605\n",
      "Epoch 14 -- Batch 60/ 94, validation loss 0.3674844205379486\n",
      "Epoch 14 -- Batch 61/ 94, validation loss 0.37145549058914185\n",
      "Epoch 14 -- Batch 62/ 94, validation loss 0.3381946086883545\n",
      "Epoch 14 -- Batch 63/ 94, validation loss 0.35204318165779114\n",
      "Epoch 14 -- Batch 64/ 94, validation loss 0.36276718974113464\n",
      "Epoch 14 -- Batch 65/ 94, validation loss 0.3550233244895935\n",
      "Epoch 14 -- Batch 66/ 94, validation loss 0.37416261434555054\n",
      "Epoch 14 -- Batch 67/ 94, validation loss 0.3394765257835388\n",
      "Epoch 14 -- Batch 68/ 94, validation loss 0.3470826745033264\n",
      "Epoch 14 -- Batch 69/ 94, validation loss 0.35901790857315063\n",
      "Epoch 14 -- Batch 70/ 94, validation loss 0.3901999890804291\n",
      "Epoch 14 -- Batch 71/ 94, validation loss 0.3538868725299835\n",
      "Epoch 14 -- Batch 72/ 94, validation loss 0.357516884803772\n",
      "Epoch 14 -- Batch 73/ 94, validation loss 0.3636173605918884\n",
      "Epoch 14 -- Batch 74/ 94, validation loss 0.35054343938827515\n",
      "Epoch 14 -- Batch 75/ 94, validation loss 0.36083555221557617\n",
      "Epoch 14 -- Batch 76/ 94, validation loss 0.35153329372406006\n",
      "Epoch 14 -- Batch 77/ 94, validation loss 0.35318514704704285\n",
      "Epoch 14 -- Batch 78/ 94, validation loss 0.35251545906066895\n",
      "Epoch 14 -- Batch 79/ 94, validation loss 0.3509500026702881\n",
      "Epoch 14 -- Batch 80/ 94, validation loss 0.35553407669067383\n",
      "Epoch 14 -- Batch 81/ 94, validation loss 0.35897043347358704\n",
      "Epoch 14 -- Batch 82/ 94, validation loss 0.36566224694252014\n",
      "Epoch 14 -- Batch 83/ 94, validation loss 0.3620183765888214\n",
      "Epoch 14 -- Batch 84/ 94, validation loss 0.36352765560150146\n",
      "Epoch 14 -- Batch 85/ 94, validation loss 0.3541973829269409\n",
      "Epoch 14 -- Batch 86/ 94, validation loss 0.37635838985443115\n",
      "Epoch 14 -- Batch 87/ 94, validation loss 0.370146781206131\n",
      "Epoch 14 -- Batch 88/ 94, validation loss 0.35123151540756226\n",
      "Epoch 14 -- Batch 89/ 94, validation loss 0.3577972948551178\n",
      "Epoch 14 -- Batch 90/ 94, validation loss 0.35036689043045044\n",
      "Epoch 14 -- Batch 91/ 94, validation loss 0.35631388425827026\n",
      "Epoch 14 -- Batch 92/ 94, validation loss 0.3329094648361206\n",
      "Epoch 14 -- Batch 93/ 94, validation loss 0.37400829792022705\n",
      "Epoch 14 -- Batch 94/ 94, validation loss 0.3736545443534851\n",
      "----------------------------------------------------------------------\n",
      "Epoch 14 loss: Training 0.3617418706417084, Validation 0.3736545443534851\n",
      "----------------------------------------------------------------------\n",
      "Epoch 15/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 10 11\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14\n",
      "[19:06:09] Explicit valence for atom # 1 O, 3, is greater than permitted\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 5 22 23 24 25 26 27\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 2 3 4 8 9\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 27\n",
      "[19:06:09] Explicit valence for atom # 6 F, 2, is greater than permitted\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Cc1nnc2ccc3cc(-c4cccnc4)ccn12'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 10 12 13 14 16 17 18\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 6 7 20\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 1 3 4 5 9 24 26 29 30\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CN(C)c1ccc(C2C3=C(CCCC3=O)Nc3c2c(=O)n2Cc2ccccc2)(CCF)cc1'\n",
      "[19:06:09] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'O=c1c2cc(-c3ccc4c(c3)OCCO4)oc2ccc(-c3ccccc3)nc12'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 2 3 4 8 21 22 23\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 1 3 25\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CC1CCN(C(=O)C23CCC(C)(C(=O)O2)CC(c3ccco3)C2)CC1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC2OC(=O)c3c2ccc(OC)cc2O)cc1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 12 13 29\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17 18 19 21 22\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CC(C)Cn1c(=O)[nH]c2nc(-c3ccc(C(=O)NC4CC5)cc3)cnc21'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'O=C1c2oc3ccc(Cl)cc3c(=O)c2C(c2ccc3c(c2)OCO3)N1[C@H](CO)C(=O)N1Cc1ccc(C(F)(F)F)cc1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Cc1sc2nc(C)n3c4c(=O)n(CC(=O)N4CCOCC4)nc(C)c3cc2c1=O'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Cc1[nH]nc2c1C1(C(=O)N(C1CCCc3ccccc31)OCCO2)CN2CCC1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc2nc(NS(=O)(=O)c3ccc4cc[nH]c(c35)/C=N/c3scnc3C)cc(C)c2c1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'c1ccc(C2CN(c3nc(-c4ccc(O)cc4)nc4c3ncn4C2CCCCC3)CC2)cc1'\n",
      "[19:06:09] SMILES Parse Error: extra open parentheses for input: 'CCCN1C(=O)/C(=C/C=C2\\N(C(=O)OC(C)(C)OC2OC)C(=O)OC1c1ccccc1'\n",
      "[19:06:09] SMILES Parse Error: extra open parentheses for input: 'Cc1cc(S(=O)(=O)N[C@H](C(=O)N(C)Cc2cccc(F)c2)c(N2CCC(C)CC2)no1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Nc1ncnc(-c2cccc(NC(=O)c3cc4c(sc3c3CCCC5)c2=O)C1CC#N)c1ccccc1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 6 12 13\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 23 25 26\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 16 17 24 27\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CC1(C)N2C(=O)C(=NC(Cc3ccccc3)C(=O)OC3CC3)C2=C1C(=O)CC(C)(C)C2'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 9 10 11 18 19\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CCCCOc1ccc(C23C(=O)N(C)C(=O)N(C(C)C)C2=O)cc1OC'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 5 13\n",
      "[19:06:09] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(Nc2nn(C33CC(CC(C)(C)C4)C(=O)c3ccccc32)cc1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CC(=O)N[C@H]1C[C@H](O)[C@@H]2O[C@@H]1CCc1ccccc1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CN(Cc1nc2ccccc2n1C)C(=O)c1scc2c1OCCO'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2CC2=NCC(c3ccccc3)N2)c(OC)c1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 9 10\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Clc1ccc(-c2ccc(C3OCCOC4)nc2)cc1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 8 9 17\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 22\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 18 23 24 26 27\n",
      "[19:06:09] SMILES Parse Error: extra open parentheses for input: 'Cn(c(=O)c2ccccc2n1CC(=O)Nc1ccc2c(c1)OCCO2'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 19 20 21 23 24\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 26 27 33\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'COc1cc(-c2cccc3c2OC[C@@H]2C[C@H](O)CN2Cc2cccc(Cl)c2)cc(OC)c1OC'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 5\n",
      "[19:06:09] SMILES Parse Error: duplicated ring closure 3 bonds atom 27 to itself for input: 'COc1ccc(CCNC(=O)C2CCN(S(=O)(=O)N3CCOc3ccccc33)CC2)cc1OC'\n",
      "[19:06:09] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CCCn1c(CC(=O)NNC(=O)ccccc2C)nc2ccccc21'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 1 2 3 7 24 25\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 8 9 11 12 13 14 15 16 17\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Cc1c(C)c2ccc(OCC(=O)N3CCN(c4ccc([N+](=O)[O-])c5)CC3)cc2oc1=O'\n",
      "[19:06:09] non-ring atom 15 marked aromatic\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[19:06:09] Explicit valence for atom # 22 N, 6, is greater than permitted\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 10 11 13 14 15\n",
      "[19:06:09] Explicit valence for atom # 37 O, 3, is greater than permitted\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1COC(=O)c1ccc(NC(=O)C2C3C4c2ccccc3C2(C)C)cc1'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(=O)N2CCc3c(sc4[nH]c(=O)n(C5CCCCC5)c(=O)c33)C2)c(C)c1'\n",
      "[19:06:09] SMILES Parse Error: syntax error while parsing: Cc1ccc(C)c(C)N1-=C1c2ccccc2C2(CCN(Cc3nn(C(C)(C)C)c(=O)c33)C(=O)C23)[C@H]1CO\n",
      "[19:06:09] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(C)c(C)N1-=C1c2ccccc2C2(CCN(Cc3nn(C(C)(C)C)c(=O)c33)C(=O)C23)[C@H]1CO' for input: 'Cc1ccc(C)c(C)N1-=C1c2ccccc2C2(CCN(Cc3nn(C(C)(C)C)c(=O)c33)C(=O)C23)[C@H]1CO'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1ccco1)c1nn(Cc2ccc(C(=O)N3CCN(Cc4ccccc4)CC3)cc2)c(=O)n2c1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 23\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1cccc(F)c1)N1[C@H]2CN(C(=O)c3ccncc3)C[C@@H]2C[C@@H]1n2c1ccccc1'\n",
      "[19:06:09] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'CCOc1cc(C2N3C(=O)C(C)OS(=O)(=O)C3C2(C#N)Cc3ccccc32)cc(OC)c1OC'\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'N=C1CC2C3C/C(=N/O)C4C3CC2C(C)C1C(C)(C)C=C3C(=O)OCC(=O)N12'\n",
      "[19:06:09] SMILES Parse Error: syntax error while parsing: O=[N+]([O-])c1ccc(/S==C\\c2ccc(F)cc2)c(Cl)c1\n",
      "[19:06:09] SMILES Parse Error: Failed parsing SMILES 'O=[N+]([O-])c1ccc(/S==C\\c2ccc(F)cc2)c(Cl)c1' for input: 'O=[N+]([O-])c1ccc(/S==C\\c2ccc(F)cc2)c(Cl)c1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 2 4 18 19 23\n",
      "[19:06:09] SMILES Parse Error: unclosed ring for input: 'O=c1c2ccccc2nc(/C=C/c2ccc(Br)cc1)n1cccc1=O'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 19 28 30 31\n",
      "[19:06:09] SMILES Parse Error: syntax error while parsing: CC1=C(C(=O)N(c2ccc(C(=O)O)cc2)C(c2ccc(O)c()c2)C2=C(CCCC2=O)N1\n",
      "[19:06:09] SMILES Parse Error: Failed parsing SMILES 'CC1=C(C(=O)N(c2ccc(C(=O)O)cc2)C(c2ccc(O)c()c2)C2=C(CCCC2=O)N1' for input: 'CC1=C(C(=O)N(c2ccc(C(=O)O)cc2)C(c2ccc(O)c()c2)C2=C(CCCC2=O)N1'\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 8 9 10 12 13 15 24 26 27\n",
      "[19:06:09] Explicit valence for atom # 5 O, 3, is greater than permitted\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 1 3 4 5 6 16 17\n",
      "[19:06:09] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 12 26 27 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 -- Batch 1/ 842, training loss 0.35131216049194336\n",
      "Epoch 15 -- Batch 2/ 842, training loss 0.3488949239253998\n",
      "Epoch 15 -- Batch 3/ 842, training loss 0.3464715778827667\n",
      "Epoch 15 -- Batch 4/ 842, training loss 0.34852030873298645\n",
      "Epoch 15 -- Batch 5/ 842, training loss 0.35005152225494385\n",
      "Epoch 15 -- Batch 6/ 842, training loss 0.3545200526714325\n",
      "Epoch 15 -- Batch 7/ 842, training loss 0.34256240725517273\n",
      "Epoch 15 -- Batch 8/ 842, training loss 0.34702298045158386\n",
      "Epoch 15 -- Batch 9/ 842, training loss 0.36478620767593384\n",
      "Epoch 15 -- Batch 10/ 842, training loss 0.3552722632884979\n",
      "Epoch 15 -- Batch 11/ 842, training loss 0.34794601798057556\n",
      "Epoch 15 -- Batch 12/ 842, training loss 0.34574273228645325\n",
      "Epoch 15 -- Batch 13/ 842, training loss 0.35690850019454956\n",
      "Epoch 15 -- Batch 14/ 842, training loss 0.3492535352706909\n",
      "Epoch 15 -- Batch 15/ 842, training loss 0.35627296566963196\n",
      "Epoch 15 -- Batch 16/ 842, training loss 0.3438059091567993\n",
      "Epoch 15 -- Batch 17/ 842, training loss 0.35416746139526367\n",
      "Epoch 15 -- Batch 18/ 842, training loss 0.3585502803325653\n",
      "Epoch 15 -- Batch 19/ 842, training loss 0.3474912643432617\n",
      "Epoch 15 -- Batch 20/ 842, training loss 0.35367876291275024\n",
      "Epoch 15 -- Batch 21/ 842, training loss 0.35448211431503296\n",
      "Epoch 15 -- Batch 22/ 842, training loss 0.3345623016357422\n",
      "Epoch 15 -- Batch 23/ 842, training loss 0.3711409270763397\n",
      "Epoch 15 -- Batch 24/ 842, training loss 0.3428994119167328\n",
      "Epoch 15 -- Batch 25/ 842, training loss 0.35779377818107605\n",
      "Epoch 15 -- Batch 26/ 842, training loss 0.3476580083370209\n",
      "Epoch 15 -- Batch 27/ 842, training loss 0.354770302772522\n",
      "Epoch 15 -- Batch 28/ 842, training loss 0.34617310762405396\n",
      "Epoch 15 -- Batch 29/ 842, training loss 0.36438000202178955\n",
      "Epoch 15 -- Batch 30/ 842, training loss 0.3357982933521271\n",
      "Epoch 15 -- Batch 31/ 842, training loss 0.3598324954509735\n",
      "Epoch 15 -- Batch 32/ 842, training loss 0.3533897399902344\n",
      "Epoch 15 -- Batch 33/ 842, training loss 0.3438100814819336\n",
      "Epoch 15 -- Batch 34/ 842, training loss 0.35147809982299805\n",
      "Epoch 15 -- Batch 35/ 842, training loss 0.3453131914138794\n",
      "Epoch 15 -- Batch 36/ 842, training loss 0.34798315167427063\n",
      "Epoch 15 -- Batch 37/ 842, training loss 0.361011803150177\n",
      "Epoch 15 -- Batch 38/ 842, training loss 0.3480290472507477\n",
      "Epoch 15 -- Batch 39/ 842, training loss 0.35870566964149475\n",
      "Epoch 15 -- Batch 40/ 842, training loss 0.35525110363960266\n",
      "Epoch 15 -- Batch 41/ 842, training loss 0.35987570881843567\n",
      "Epoch 15 -- Batch 42/ 842, training loss 0.3419649004936218\n",
      "Epoch 15 -- Batch 43/ 842, training loss 0.36708739399909973\n",
      "Epoch 15 -- Batch 44/ 842, training loss 0.355069637298584\n",
      "Epoch 15 -- Batch 45/ 842, training loss 0.34251168370246887\n",
      "Epoch 15 -- Batch 46/ 842, training loss 0.35552528500556946\n",
      "Epoch 15 -- Batch 47/ 842, training loss 0.3604254722595215\n",
      "Epoch 15 -- Batch 48/ 842, training loss 0.3649212419986725\n",
      "Epoch 15 -- Batch 49/ 842, training loss 0.35943177342414856\n",
      "Epoch 15 -- Batch 50/ 842, training loss 0.3492797017097473\n",
      "Epoch 15 -- Batch 51/ 842, training loss 0.34509310126304626\n",
      "Epoch 15 -- Batch 52/ 842, training loss 0.3408339023590088\n",
      "Epoch 15 -- Batch 53/ 842, training loss 0.3497564196586609\n",
      "Epoch 15 -- Batch 54/ 842, training loss 0.34288138151168823\n",
      "Epoch 15 -- Batch 55/ 842, training loss 0.3454545736312866\n",
      "Epoch 15 -- Batch 56/ 842, training loss 0.34669578075408936\n",
      "Epoch 15 -- Batch 57/ 842, training loss 0.33856531977653503\n",
      "Epoch 15 -- Batch 58/ 842, training loss 0.3436294198036194\n",
      "Epoch 15 -- Batch 59/ 842, training loss 0.3417544364929199\n",
      "Epoch 15 -- Batch 60/ 842, training loss 0.3550952672958374\n",
      "Epoch 15 -- Batch 61/ 842, training loss 0.34872275590896606\n",
      "Epoch 15 -- Batch 62/ 842, training loss 0.3610166311264038\n",
      "Epoch 15 -- Batch 63/ 842, training loss 0.352888822555542\n",
      "Epoch 15 -- Batch 64/ 842, training loss 0.3483150601387024\n",
      "Epoch 15 -- Batch 65/ 842, training loss 0.35427311062812805\n",
      "Epoch 15 -- Batch 66/ 842, training loss 0.35118192434310913\n",
      "Epoch 15 -- Batch 67/ 842, training loss 0.33796483278274536\n",
      "Epoch 15 -- Batch 68/ 842, training loss 0.33932963013648987\n",
      "Epoch 15 -- Batch 69/ 842, training loss 0.35455840826034546\n",
      "Epoch 15 -- Batch 70/ 842, training loss 0.34990793466567993\n",
      "Epoch 15 -- Batch 71/ 842, training loss 0.3493422269821167\n",
      "Epoch 15 -- Batch 72/ 842, training loss 0.3451652526855469\n",
      "Epoch 15 -- Batch 73/ 842, training loss 0.3580300211906433\n",
      "Epoch 15 -- Batch 74/ 842, training loss 0.35575219988822937\n",
      "Epoch 15 -- Batch 75/ 842, training loss 0.34771469235420227\n",
      "Epoch 15 -- Batch 76/ 842, training loss 0.35550302267074585\n",
      "Epoch 15 -- Batch 77/ 842, training loss 0.3513343632221222\n",
      "Epoch 15 -- Batch 78/ 842, training loss 0.34246912598609924\n",
      "Epoch 15 -- Batch 79/ 842, training loss 0.35050705075263977\n",
      "Epoch 15 -- Batch 80/ 842, training loss 0.35500213503837585\n",
      "Epoch 15 -- Batch 81/ 842, training loss 0.3563966751098633\n",
      "Epoch 15 -- Batch 82/ 842, training loss 0.36340853571891785\n",
      "Epoch 15 -- Batch 83/ 842, training loss 0.3501741886138916\n",
      "Epoch 15 -- Batch 84/ 842, training loss 0.3503148853778839\n",
      "Epoch 15 -- Batch 85/ 842, training loss 0.3664090037345886\n",
      "Epoch 15 -- Batch 86/ 842, training loss 0.3542315363883972\n",
      "Epoch 15 -- Batch 87/ 842, training loss 0.34863710403442383\n",
      "Epoch 15 -- Batch 88/ 842, training loss 0.3506881594657898\n",
      "Epoch 15 -- Batch 89/ 842, training loss 0.3582804203033447\n",
      "Epoch 15 -- Batch 90/ 842, training loss 0.35306981205940247\n",
      "Epoch 15 -- Batch 91/ 842, training loss 0.35373178124427795\n",
      "Epoch 15 -- Batch 92/ 842, training loss 0.34692636132240295\n",
      "Epoch 15 -- Batch 93/ 842, training loss 0.33198583126068115\n",
      "Epoch 15 -- Batch 94/ 842, training loss 0.362373948097229\n",
      "Epoch 15 -- Batch 95/ 842, training loss 0.3491031229496002\n",
      "Epoch 15 -- Batch 96/ 842, training loss 0.3593362271785736\n",
      "Epoch 15 -- Batch 97/ 842, training loss 0.3557834327220917\n",
      "Epoch 15 -- Batch 98/ 842, training loss 0.34682685136795044\n",
      "Epoch 15 -- Batch 99/ 842, training loss 0.35064390301704407\n",
      "Epoch 15 -- Batch 100/ 842, training loss 0.3527609705924988\n",
      "Epoch 15 -- Batch 101/ 842, training loss 0.3492319881916046\n",
      "Epoch 15 -- Batch 102/ 842, training loss 0.34279146790504456\n",
      "Epoch 15 -- Batch 103/ 842, training loss 0.34962528944015503\n",
      "Epoch 15 -- Batch 104/ 842, training loss 0.36147475242614746\n",
      "Epoch 15 -- Batch 105/ 842, training loss 0.34465304017066956\n",
      "Epoch 15 -- Batch 106/ 842, training loss 0.360517680644989\n",
      "Epoch 15 -- Batch 107/ 842, training loss 0.36544203758239746\n",
      "Epoch 15 -- Batch 108/ 842, training loss 0.3515685200691223\n",
      "Epoch 15 -- Batch 109/ 842, training loss 0.36079639196395874\n",
      "Epoch 15 -- Batch 110/ 842, training loss 0.34673234820365906\n",
      "Epoch 15 -- Batch 111/ 842, training loss 0.34752416610717773\n",
      "Epoch 15 -- Batch 112/ 842, training loss 0.3626493215560913\n",
      "Epoch 15 -- Batch 113/ 842, training loss 0.3596404790878296\n",
      "Epoch 15 -- Batch 114/ 842, training loss 0.3644774556159973\n",
      "Epoch 15 -- Batch 115/ 842, training loss 0.3387760818004608\n",
      "Epoch 15 -- Batch 116/ 842, training loss 0.355500191450119\n",
      "Epoch 15 -- Batch 117/ 842, training loss 0.3456462323665619\n",
      "Epoch 15 -- Batch 118/ 842, training loss 0.3485512137413025\n",
      "Epoch 15 -- Batch 119/ 842, training loss 0.3509722054004669\n",
      "Epoch 15 -- Batch 120/ 842, training loss 0.34760379791259766\n",
      "Epoch 15 -- Batch 121/ 842, training loss 0.34799179434776306\n",
      "Epoch 15 -- Batch 122/ 842, training loss 0.3610195219516754\n",
      "Epoch 15 -- Batch 123/ 842, training loss 0.3538280725479126\n",
      "Epoch 15 -- Batch 124/ 842, training loss 0.3459034264087677\n",
      "Epoch 15 -- Batch 125/ 842, training loss 0.3501970171928406\n",
      "Epoch 15 -- Batch 126/ 842, training loss 0.3424716293811798\n",
      "Epoch 15 -- Batch 127/ 842, training loss 0.3522298336029053\n",
      "Epoch 15 -- Batch 128/ 842, training loss 0.35505563020706177\n",
      "Epoch 15 -- Batch 129/ 842, training loss 0.34617361426353455\n",
      "Epoch 15 -- Batch 130/ 842, training loss 0.3507593274116516\n",
      "Epoch 15 -- Batch 131/ 842, training loss 0.34662970900535583\n",
      "Epoch 15 -- Batch 132/ 842, training loss 0.3526657223701477\n",
      "Epoch 15 -- Batch 133/ 842, training loss 0.3448057770729065\n",
      "Epoch 15 -- Batch 134/ 842, training loss 0.3562103509902954\n",
      "Epoch 15 -- Batch 135/ 842, training loss 0.3482086658477783\n",
      "Epoch 15 -- Batch 136/ 842, training loss 0.3601868748664856\n",
      "Epoch 15 -- Batch 137/ 842, training loss 0.36181512475013733\n",
      "Epoch 15 -- Batch 138/ 842, training loss 0.34099602699279785\n",
      "Epoch 15 -- Batch 139/ 842, training loss 0.3471066951751709\n",
      "Epoch 15 -- Batch 140/ 842, training loss 0.36467444896698\n",
      "Epoch 15 -- Batch 141/ 842, training loss 0.36238527297973633\n",
      "Epoch 15 -- Batch 142/ 842, training loss 0.3500395715236664\n",
      "Epoch 15 -- Batch 143/ 842, training loss 0.3545500338077545\n",
      "Epoch 15 -- Batch 144/ 842, training loss 0.35061123967170715\n",
      "Epoch 15 -- Batch 145/ 842, training loss 0.3614957332611084\n",
      "Epoch 15 -- Batch 146/ 842, training loss 0.3502252697944641\n",
      "Epoch 15 -- Batch 147/ 842, training loss 0.3639970123767853\n",
      "Epoch 15 -- Batch 148/ 842, training loss 0.3463625907897949\n",
      "Epoch 15 -- Batch 149/ 842, training loss 0.3535175323486328\n",
      "Epoch 15 -- Batch 150/ 842, training loss 0.35077112913131714\n",
      "Epoch 15 -- Batch 151/ 842, training loss 0.36645689606666565\n",
      "Epoch 15 -- Batch 152/ 842, training loss 0.34070077538490295\n",
      "Epoch 15 -- Batch 153/ 842, training loss 0.35279494524002075\n",
      "Epoch 15 -- Batch 154/ 842, training loss 0.3533330261707306\n",
      "Epoch 15 -- Batch 155/ 842, training loss 0.3614329695701599\n",
      "Epoch 15 -- Batch 156/ 842, training loss 0.3594890236854553\n",
      "Epoch 15 -- Batch 157/ 842, training loss 0.35390275716781616\n",
      "Epoch 15 -- Batch 158/ 842, training loss 0.3477517366409302\n",
      "Epoch 15 -- Batch 159/ 842, training loss 0.36412620544433594\n",
      "Epoch 15 -- Batch 160/ 842, training loss 0.35447371006011963\n",
      "Epoch 15 -- Batch 161/ 842, training loss 0.349760502576828\n",
      "Epoch 15 -- Batch 162/ 842, training loss 0.3643328845500946\n",
      "Epoch 15 -- Batch 163/ 842, training loss 0.35283029079437256\n",
      "Epoch 15 -- Batch 164/ 842, training loss 0.3421335816383362\n",
      "Epoch 15 -- Batch 165/ 842, training loss 0.3512020707130432\n",
      "Epoch 15 -- Batch 166/ 842, training loss 0.33601218461990356\n",
      "Epoch 15 -- Batch 167/ 842, training loss 0.3476054072380066\n",
      "Epoch 15 -- Batch 168/ 842, training loss 0.34319040179252625\n",
      "Epoch 15 -- Batch 169/ 842, training loss 0.34675607085227966\n",
      "Epoch 15 -- Batch 170/ 842, training loss 0.3485010862350464\n",
      "Epoch 15 -- Batch 171/ 842, training loss 0.3487766981124878\n",
      "Epoch 15 -- Batch 172/ 842, training loss 0.34320101141929626\n",
      "Epoch 15 -- Batch 173/ 842, training loss 0.3639639914035797\n",
      "Epoch 15 -- Batch 174/ 842, training loss 0.3529876470565796\n",
      "Epoch 15 -- Batch 175/ 842, training loss 0.3591272830963135\n",
      "Epoch 15 -- Batch 176/ 842, training loss 0.3681822419166565\n",
      "Epoch 15 -- Batch 177/ 842, training loss 0.3528313934803009\n",
      "Epoch 15 -- Batch 178/ 842, training loss 0.3528558015823364\n",
      "Epoch 15 -- Batch 179/ 842, training loss 0.33905503153800964\n",
      "Epoch 15 -- Batch 180/ 842, training loss 0.34934860467910767\n",
      "Epoch 15 -- Batch 181/ 842, training loss 0.3489518165588379\n",
      "Epoch 15 -- Batch 182/ 842, training loss 0.34974876046180725\n",
      "Epoch 15 -- Batch 183/ 842, training loss 0.3399094045162201\n",
      "Epoch 15 -- Batch 184/ 842, training loss 0.3410053253173828\n",
      "Epoch 15 -- Batch 185/ 842, training loss 0.3568859100341797\n",
      "Epoch 15 -- Batch 186/ 842, training loss 0.35401833057403564\n",
      "Epoch 15 -- Batch 187/ 842, training loss 0.35236966609954834\n",
      "Epoch 15 -- Batch 188/ 842, training loss 0.3608658015727997\n",
      "Epoch 15 -- Batch 189/ 842, training loss 0.3434912860393524\n",
      "Epoch 15 -- Batch 190/ 842, training loss 0.3493933379650116\n",
      "Epoch 15 -- Batch 191/ 842, training loss 0.35018980503082275\n",
      "Epoch 15 -- Batch 192/ 842, training loss 0.33536162972450256\n",
      "Epoch 15 -- Batch 193/ 842, training loss 0.35803577303886414\n",
      "Epoch 15 -- Batch 194/ 842, training loss 0.3581475615501404\n",
      "Epoch 15 -- Batch 195/ 842, training loss 0.3623787462711334\n",
      "Epoch 15 -- Batch 196/ 842, training loss 0.3585282862186432\n",
      "Epoch 15 -- Batch 197/ 842, training loss 0.37304478883743286\n",
      "Epoch 15 -- Batch 198/ 842, training loss 0.35632818937301636\n",
      "Epoch 15 -- Batch 199/ 842, training loss 0.35869649052619934\n",
      "Epoch 15 -- Batch 200/ 842, training loss 0.3564918041229248\n",
      "Epoch 15 -- Batch 201/ 842, training loss 0.3548244833946228\n",
      "Epoch 15 -- Batch 202/ 842, training loss 0.3591075539588928\n",
      "Epoch 15 -- Batch 203/ 842, training loss 0.3419911861419678\n",
      "Epoch 15 -- Batch 204/ 842, training loss 0.3412991762161255\n",
      "Epoch 15 -- Batch 205/ 842, training loss 0.35921144485473633\n",
      "Epoch 15 -- Batch 206/ 842, training loss 0.3541443645954132\n",
      "Epoch 15 -- Batch 207/ 842, training loss 0.3549184799194336\n",
      "Epoch 15 -- Batch 208/ 842, training loss 0.36085933446884155\n",
      "Epoch 15 -- Batch 209/ 842, training loss 0.3442372977733612\n",
      "Epoch 15 -- Batch 210/ 842, training loss 0.35964518785476685\n",
      "Epoch 15 -- Batch 211/ 842, training loss 0.3498951494693756\n",
      "Epoch 15 -- Batch 212/ 842, training loss 0.3451569080352783\n",
      "Epoch 15 -- Batch 213/ 842, training loss 0.34956908226013184\n",
      "Epoch 15 -- Batch 214/ 842, training loss 0.3540784418582916\n",
      "Epoch 15 -- Batch 215/ 842, training loss 0.3609551191329956\n",
      "Epoch 15 -- Batch 216/ 842, training loss 0.34323281049728394\n",
      "Epoch 15 -- Batch 217/ 842, training loss 0.3563523292541504\n",
      "Epoch 15 -- Batch 218/ 842, training loss 0.3451656699180603\n",
      "Epoch 15 -- Batch 219/ 842, training loss 0.36054280400276184\n",
      "Epoch 15 -- Batch 220/ 842, training loss 0.36001670360565186\n",
      "Epoch 15 -- Batch 221/ 842, training loss 0.35473358631134033\n",
      "Epoch 15 -- Batch 222/ 842, training loss 0.3389773666858673\n",
      "Epoch 15 -- Batch 223/ 842, training loss 0.36021775007247925\n",
      "Epoch 15 -- Batch 224/ 842, training loss 0.3567507266998291\n",
      "Epoch 15 -- Batch 225/ 842, training loss 0.351900190114975\n",
      "Epoch 15 -- Batch 226/ 842, training loss 0.35120072960853577\n",
      "Epoch 15 -- Batch 227/ 842, training loss 0.34350091218948364\n",
      "Epoch 15 -- Batch 228/ 842, training loss 0.3490959405899048\n",
      "Epoch 15 -- Batch 229/ 842, training loss 0.3582935035228729\n",
      "Epoch 15 -- Batch 230/ 842, training loss 0.3434065878391266\n",
      "Epoch 15 -- Batch 231/ 842, training loss 0.3490656912326813\n",
      "Epoch 15 -- Batch 232/ 842, training loss 0.34814679622650146\n",
      "Epoch 15 -- Batch 233/ 842, training loss 0.3608173429965973\n",
      "Epoch 15 -- Batch 234/ 842, training loss 0.3649563193321228\n",
      "Epoch 15 -- Batch 235/ 842, training loss 0.3492445647716522\n",
      "Epoch 15 -- Batch 236/ 842, training loss 0.35244056582450867\n",
      "Epoch 15 -- Batch 237/ 842, training loss 0.35301458835601807\n",
      "Epoch 15 -- Batch 238/ 842, training loss 0.3530578315258026\n",
      "Epoch 15 -- Batch 239/ 842, training loss 0.3447577953338623\n",
      "Epoch 15 -- Batch 240/ 842, training loss 0.3503272235393524\n",
      "Epoch 15 -- Batch 241/ 842, training loss 0.37066784501075745\n",
      "Epoch 15 -- Batch 242/ 842, training loss 0.3544001579284668\n",
      "Epoch 15 -- Batch 243/ 842, training loss 0.34610456228256226\n",
      "Epoch 15 -- Batch 244/ 842, training loss 0.3563742935657501\n",
      "Epoch 15 -- Batch 245/ 842, training loss 0.3469959795475006\n",
      "Epoch 15 -- Batch 246/ 842, training loss 0.36626875400543213\n",
      "Epoch 15 -- Batch 247/ 842, training loss 0.34608080983161926\n",
      "Epoch 15 -- Batch 248/ 842, training loss 0.3446491062641144\n",
      "Epoch 15 -- Batch 249/ 842, training loss 0.35661888122558594\n",
      "Epoch 15 -- Batch 250/ 842, training loss 0.34402891993522644\n",
      "Epoch 15 -- Batch 251/ 842, training loss 0.35692235827445984\n",
      "Epoch 15 -- Batch 252/ 842, training loss 0.34349122643470764\n",
      "Epoch 15 -- Batch 253/ 842, training loss 0.35881319642066956\n",
      "Epoch 15 -- Batch 254/ 842, training loss 0.34330812096595764\n",
      "Epoch 15 -- Batch 255/ 842, training loss 0.36965620517730713\n",
      "Epoch 15 -- Batch 256/ 842, training loss 0.35430002212524414\n",
      "Epoch 15 -- Batch 257/ 842, training loss 0.35610368847846985\n",
      "Epoch 15 -- Batch 258/ 842, training loss 0.3475402891635895\n",
      "Epoch 15 -- Batch 259/ 842, training loss 0.3642578423023224\n",
      "Epoch 15 -- Batch 260/ 842, training loss 0.3551439344882965\n",
      "Epoch 15 -- Batch 261/ 842, training loss 0.35648417472839355\n",
      "Epoch 15 -- Batch 262/ 842, training loss 0.3592703640460968\n",
      "Epoch 15 -- Batch 263/ 842, training loss 0.34837251901626587\n",
      "Epoch 15 -- Batch 264/ 842, training loss 0.3544440269470215\n",
      "Epoch 15 -- Batch 265/ 842, training loss 0.34254756569862366\n",
      "Epoch 15 -- Batch 266/ 842, training loss 0.34973421692848206\n",
      "Epoch 15 -- Batch 267/ 842, training loss 0.34619811177253723\n",
      "Epoch 15 -- Batch 268/ 842, training loss 0.35744038224220276\n",
      "Epoch 15 -- Batch 269/ 842, training loss 0.35039058327674866\n",
      "Epoch 15 -- Batch 270/ 842, training loss 0.3440256714820862\n",
      "Epoch 15 -- Batch 271/ 842, training loss 0.3572700321674347\n",
      "Epoch 15 -- Batch 272/ 842, training loss 0.35852479934692383\n",
      "Epoch 15 -- Batch 273/ 842, training loss 0.34734222292900085\n",
      "Epoch 15 -- Batch 274/ 842, training loss 0.34542447328567505\n",
      "Epoch 15 -- Batch 275/ 842, training loss 0.3546861410140991\n",
      "Epoch 15 -- Batch 276/ 842, training loss 0.3506985008716583\n",
      "Epoch 15 -- Batch 277/ 842, training loss 0.35702794790267944\n",
      "Epoch 15 -- Batch 278/ 842, training loss 0.35940346121788025\n",
      "Epoch 15 -- Batch 279/ 842, training loss 0.35570046305656433\n",
      "Epoch 15 -- Batch 280/ 842, training loss 0.3418086767196655\n",
      "Epoch 15 -- Batch 281/ 842, training loss 0.3418494462966919\n",
      "Epoch 15 -- Batch 282/ 842, training loss 0.34631213545799255\n",
      "Epoch 15 -- Batch 283/ 842, training loss 0.3501381576061249\n",
      "Epoch 15 -- Batch 284/ 842, training loss 0.36106058955192566\n",
      "Epoch 15 -- Batch 285/ 842, training loss 0.35383835434913635\n",
      "Epoch 15 -- Batch 286/ 842, training loss 0.3525053858757019\n",
      "Epoch 15 -- Batch 287/ 842, training loss 0.3594653606414795\n",
      "Epoch 15 -- Batch 288/ 842, training loss 0.3613452911376953\n",
      "Epoch 15 -- Batch 289/ 842, training loss 0.355649471282959\n",
      "Epoch 15 -- Batch 290/ 842, training loss 0.34170588850975037\n",
      "Epoch 15 -- Batch 291/ 842, training loss 0.3534439206123352\n",
      "Epoch 15 -- Batch 292/ 842, training loss 0.3631066679954529\n",
      "Epoch 15 -- Batch 293/ 842, training loss 0.3486846089363098\n",
      "Epoch 15 -- Batch 294/ 842, training loss 0.3485408425331116\n",
      "Epoch 15 -- Batch 295/ 842, training loss 0.34635189175605774\n",
      "Epoch 15 -- Batch 296/ 842, training loss 0.3507354259490967\n",
      "Epoch 15 -- Batch 297/ 842, training loss 0.35780784487724304\n",
      "Epoch 15 -- Batch 298/ 842, training loss 0.3456370532512665\n",
      "Epoch 15 -- Batch 299/ 842, training loss 0.3583512306213379\n",
      "Epoch 15 -- Batch 300/ 842, training loss 0.3558831512928009\n",
      "Epoch 15 -- Batch 301/ 842, training loss 0.3591645359992981\n",
      "Epoch 15 -- Batch 302/ 842, training loss 0.3644352853298187\n",
      "Epoch 15 -- Batch 303/ 842, training loss 0.3449040353298187\n",
      "Epoch 15 -- Batch 304/ 842, training loss 0.360493004322052\n",
      "Epoch 15 -- Batch 305/ 842, training loss 0.36291250586509705\n",
      "Epoch 15 -- Batch 306/ 842, training loss 0.3590214252471924\n",
      "Epoch 15 -- Batch 307/ 842, training loss 0.36325833201408386\n",
      "Epoch 15 -- Batch 308/ 842, training loss 0.3418789803981781\n",
      "Epoch 15 -- Batch 309/ 842, training loss 0.3404410481452942\n",
      "Epoch 15 -- Batch 310/ 842, training loss 0.3562926948070526\n",
      "Epoch 15 -- Batch 311/ 842, training loss 0.366166889667511\n",
      "Epoch 15 -- Batch 312/ 842, training loss 0.3511553406715393\n",
      "Epoch 15 -- Batch 313/ 842, training loss 0.34677350521087646\n",
      "Epoch 15 -- Batch 314/ 842, training loss 0.3476051688194275\n",
      "Epoch 15 -- Batch 315/ 842, training loss 0.3517460823059082\n",
      "Epoch 15 -- Batch 316/ 842, training loss 0.3476899266242981\n",
      "Epoch 15 -- Batch 317/ 842, training loss 0.35679891705513\n",
      "Epoch 15 -- Batch 318/ 842, training loss 0.3666425347328186\n",
      "Epoch 15 -- Batch 319/ 842, training loss 0.3512471914291382\n",
      "Epoch 15 -- Batch 320/ 842, training loss 0.3512686491012573\n",
      "Epoch 15 -- Batch 321/ 842, training loss 0.34747034311294556\n",
      "Epoch 15 -- Batch 322/ 842, training loss 0.34861940145492554\n",
      "Epoch 15 -- Batch 323/ 842, training loss 0.35034745931625366\n",
      "Epoch 15 -- Batch 324/ 842, training loss 0.3634366989135742\n",
      "Epoch 15 -- Batch 325/ 842, training loss 0.36411339044570923\n",
      "Epoch 15 -- Batch 326/ 842, training loss 0.3475732207298279\n",
      "Epoch 15 -- Batch 327/ 842, training loss 0.34641021490097046\n",
      "Epoch 15 -- Batch 328/ 842, training loss 0.35148903727531433\n",
      "Epoch 15 -- Batch 329/ 842, training loss 0.3625229597091675\n",
      "Epoch 15 -- Batch 330/ 842, training loss 0.3545118272304535\n",
      "Epoch 15 -- Batch 331/ 842, training loss 0.359507292509079\n",
      "Epoch 15 -- Batch 332/ 842, training loss 0.3595466613769531\n",
      "Epoch 15 -- Batch 333/ 842, training loss 0.35007765889167786\n",
      "Epoch 15 -- Batch 334/ 842, training loss 0.3403621315956116\n",
      "Epoch 15 -- Batch 335/ 842, training loss 0.3691558241844177\n",
      "Epoch 15 -- Batch 336/ 842, training loss 0.3605496883392334\n",
      "Epoch 15 -- Batch 337/ 842, training loss 0.3606669008731842\n",
      "Epoch 15 -- Batch 338/ 842, training loss 0.3549802601337433\n",
      "Epoch 15 -- Batch 339/ 842, training loss 0.36759042739868164\n",
      "Epoch 15 -- Batch 340/ 842, training loss 0.36463648080825806\n",
      "Epoch 15 -- Batch 341/ 842, training loss 0.34032556414604187\n",
      "Epoch 15 -- Batch 342/ 842, training loss 0.3507055938243866\n",
      "Epoch 15 -- Batch 343/ 842, training loss 0.34967952966690063\n",
      "Epoch 15 -- Batch 344/ 842, training loss 0.3461951017379761\n",
      "Epoch 15 -- Batch 345/ 842, training loss 0.35923802852630615\n",
      "Epoch 15 -- Batch 346/ 842, training loss 0.3508661687374115\n",
      "Epoch 15 -- Batch 347/ 842, training loss 0.35640257596969604\n",
      "Epoch 15 -- Batch 348/ 842, training loss 0.34038496017456055\n",
      "Epoch 15 -- Batch 349/ 842, training loss 0.34862419962882996\n",
      "Epoch 15 -- Batch 350/ 842, training loss 0.35398852825164795\n",
      "Epoch 15 -- Batch 351/ 842, training loss 0.358951210975647\n",
      "Epoch 15 -- Batch 352/ 842, training loss 0.35837942361831665\n",
      "Epoch 15 -- Batch 353/ 842, training loss 0.3558330535888672\n",
      "Epoch 15 -- Batch 354/ 842, training loss 0.3435848653316498\n",
      "Epoch 15 -- Batch 355/ 842, training loss 0.3561936914920807\n",
      "Epoch 15 -- Batch 356/ 842, training loss 0.3556680679321289\n",
      "Epoch 15 -- Batch 357/ 842, training loss 0.3521241247653961\n",
      "Epoch 15 -- Batch 358/ 842, training loss 0.3491092026233673\n",
      "Epoch 15 -- Batch 359/ 842, training loss 0.34852030873298645\n",
      "Epoch 15 -- Batch 360/ 842, training loss 0.34843406081199646\n",
      "Epoch 15 -- Batch 361/ 842, training loss 0.3594237267971039\n",
      "Epoch 15 -- Batch 362/ 842, training loss 0.36666586995124817\n",
      "Epoch 15 -- Batch 363/ 842, training loss 0.3584083616733551\n",
      "Epoch 15 -- Batch 364/ 842, training loss 0.34553515911102295\n",
      "Epoch 15 -- Batch 365/ 842, training loss 0.3631332516670227\n",
      "Epoch 15 -- Batch 366/ 842, training loss 0.3539395034313202\n",
      "Epoch 15 -- Batch 367/ 842, training loss 0.34066179394721985\n",
      "Epoch 15 -- Batch 368/ 842, training loss 0.3470875322818756\n",
      "Epoch 15 -- Batch 369/ 842, training loss 0.35367506742477417\n",
      "Epoch 15 -- Batch 370/ 842, training loss 0.3470899760723114\n",
      "Epoch 15 -- Batch 371/ 842, training loss 0.3446405529975891\n",
      "Epoch 15 -- Batch 372/ 842, training loss 0.35211578011512756\n",
      "Epoch 15 -- Batch 373/ 842, training loss 0.34975308179855347\n",
      "Epoch 15 -- Batch 374/ 842, training loss 0.3442349433898926\n",
      "Epoch 15 -- Batch 375/ 842, training loss 0.3473561406135559\n",
      "Epoch 15 -- Batch 376/ 842, training loss 0.34221258759498596\n",
      "Epoch 15 -- Batch 377/ 842, training loss 0.3581624925136566\n",
      "Epoch 15 -- Batch 378/ 842, training loss 0.3508327901363373\n",
      "Epoch 15 -- Batch 379/ 842, training loss 0.34549176692962646\n",
      "Epoch 15 -- Batch 380/ 842, training loss 0.353563517332077\n",
      "Epoch 15 -- Batch 381/ 842, training loss 0.3541625440120697\n",
      "Epoch 15 -- Batch 382/ 842, training loss 0.3533019423484802\n",
      "Epoch 15 -- Batch 383/ 842, training loss 0.3510581851005554\n",
      "Epoch 15 -- Batch 384/ 842, training loss 0.3538188934326172\n",
      "Epoch 15 -- Batch 385/ 842, training loss 0.3448968231678009\n",
      "Epoch 15 -- Batch 386/ 842, training loss 0.35154134035110474\n",
      "Epoch 15 -- Batch 387/ 842, training loss 0.3600742518901825\n",
      "Epoch 15 -- Batch 388/ 842, training loss 0.3613232672214508\n",
      "Epoch 15 -- Batch 389/ 842, training loss 0.3384319543838501\n",
      "Epoch 15 -- Batch 390/ 842, training loss 0.35915449261665344\n",
      "Epoch 15 -- Batch 391/ 842, training loss 0.35191455483436584\n",
      "Epoch 15 -- Batch 392/ 842, training loss 0.38747429847717285\n",
      "Epoch 15 -- Batch 393/ 842, training loss 0.3432178497314453\n",
      "Epoch 15 -- Batch 394/ 842, training loss 0.36369431018829346\n",
      "Epoch 15 -- Batch 395/ 842, training loss 0.362365186214447\n",
      "Epoch 15 -- Batch 396/ 842, training loss 0.34832146763801575\n",
      "Epoch 15 -- Batch 397/ 842, training loss 0.3565531373023987\n",
      "Epoch 15 -- Batch 398/ 842, training loss 0.35880574584007263\n",
      "Epoch 15 -- Batch 399/ 842, training loss 0.3474838435649872\n",
      "Epoch 15 -- Batch 400/ 842, training loss 0.3515785038471222\n",
      "Epoch 15 -- Batch 401/ 842, training loss 0.3455747067928314\n",
      "Epoch 15 -- Batch 402/ 842, training loss 0.35446426272392273\n",
      "Epoch 15 -- Batch 403/ 842, training loss 0.3586026132106781\n",
      "Epoch 15 -- Batch 404/ 842, training loss 0.35699644684791565\n",
      "Epoch 15 -- Batch 405/ 842, training loss 0.369595468044281\n",
      "Epoch 15 -- Batch 406/ 842, training loss 0.3609388768672943\n",
      "Epoch 15 -- Batch 407/ 842, training loss 0.3398725688457489\n",
      "Epoch 15 -- Batch 408/ 842, training loss 0.35204777121543884\n",
      "Epoch 15 -- Batch 409/ 842, training loss 0.3639439046382904\n",
      "Epoch 15 -- Batch 410/ 842, training loss 0.3427674174308777\n",
      "Epoch 15 -- Batch 411/ 842, training loss 0.3553398549556732\n",
      "Epoch 15 -- Batch 412/ 842, training loss 0.3519027531147003\n",
      "Epoch 15 -- Batch 413/ 842, training loss 0.33959075808525085\n",
      "Epoch 15 -- Batch 414/ 842, training loss 0.3588939905166626\n",
      "Epoch 15 -- Batch 415/ 842, training loss 0.36557409167289734\n",
      "Epoch 15 -- Batch 416/ 842, training loss 0.3504678010940552\n",
      "Epoch 15 -- Batch 417/ 842, training loss 0.35406380891799927\n",
      "Epoch 15 -- Batch 418/ 842, training loss 0.33985161781311035\n",
      "Epoch 15 -- Batch 419/ 842, training loss 0.34712204337120056\n",
      "Epoch 15 -- Batch 420/ 842, training loss 0.3626394271850586\n",
      "Epoch 15 -- Batch 421/ 842, training loss 0.3401288092136383\n",
      "Epoch 15 -- Batch 422/ 842, training loss 0.347573846578598\n",
      "Epoch 15 -- Batch 423/ 842, training loss 0.3689579367637634\n",
      "Epoch 15 -- Batch 424/ 842, training loss 0.351857453584671\n",
      "Epoch 15 -- Batch 425/ 842, training loss 0.3421213924884796\n",
      "Epoch 15 -- Batch 426/ 842, training loss 0.36856886744499207\n",
      "Epoch 15 -- Batch 427/ 842, training loss 0.3708811402320862\n",
      "Epoch 15 -- Batch 428/ 842, training loss 0.360247939825058\n",
      "Epoch 15 -- Batch 429/ 842, training loss 0.3617565929889679\n",
      "Epoch 15 -- Batch 430/ 842, training loss 0.3704952001571655\n",
      "Epoch 15 -- Batch 431/ 842, training loss 0.35341677069664\n",
      "Epoch 15 -- Batch 432/ 842, training loss 0.3399091064929962\n",
      "Epoch 15 -- Batch 433/ 842, training loss 0.3577388823032379\n",
      "Epoch 15 -- Batch 434/ 842, training loss 0.3532843291759491\n",
      "Epoch 15 -- Batch 435/ 842, training loss 0.35933151841163635\n",
      "Epoch 15 -- Batch 436/ 842, training loss 0.341108113527298\n",
      "Epoch 15 -- Batch 437/ 842, training loss 0.3735344111919403\n",
      "Epoch 15 -- Batch 438/ 842, training loss 0.3549703061580658\n",
      "Epoch 15 -- Batch 439/ 842, training loss 0.35840094089508057\n",
      "Epoch 15 -- Batch 440/ 842, training loss 0.3660637438297272\n",
      "Epoch 15 -- Batch 441/ 842, training loss 0.36431798338890076\n",
      "Epoch 15 -- Batch 442/ 842, training loss 0.3588831424713135\n",
      "Epoch 15 -- Batch 443/ 842, training loss 0.36681947112083435\n",
      "Epoch 15 -- Batch 444/ 842, training loss 0.37007054686546326\n",
      "Epoch 15 -- Batch 445/ 842, training loss 0.36748144030570984\n",
      "Epoch 15 -- Batch 446/ 842, training loss 0.3543207347393036\n",
      "Epoch 15 -- Batch 447/ 842, training loss 0.36034315824508667\n",
      "Epoch 15 -- Batch 448/ 842, training loss 0.3647998869419098\n",
      "Epoch 15 -- Batch 449/ 842, training loss 0.3510078489780426\n",
      "Epoch 15 -- Batch 450/ 842, training loss 0.36709752678871155\n",
      "Epoch 15 -- Batch 451/ 842, training loss 0.3630741238594055\n",
      "Epoch 15 -- Batch 452/ 842, training loss 0.3537474572658539\n",
      "Epoch 15 -- Batch 453/ 842, training loss 0.3570984899997711\n",
      "Epoch 15 -- Batch 454/ 842, training loss 0.3525194227695465\n",
      "Epoch 15 -- Batch 455/ 842, training loss 0.3534444272518158\n",
      "Epoch 15 -- Batch 456/ 842, training loss 0.35520294308662415\n",
      "Epoch 15 -- Batch 457/ 842, training loss 0.35640400648117065\n",
      "Epoch 15 -- Batch 458/ 842, training loss 0.34862545132637024\n",
      "Epoch 15 -- Batch 459/ 842, training loss 0.36570489406585693\n",
      "Epoch 15 -- Batch 460/ 842, training loss 0.344717413187027\n",
      "Epoch 15 -- Batch 461/ 842, training loss 0.32944372296333313\n",
      "Epoch 15 -- Batch 462/ 842, training loss 0.3564261794090271\n",
      "Epoch 15 -- Batch 463/ 842, training loss 0.35892874002456665\n",
      "Epoch 15 -- Batch 464/ 842, training loss 0.35178080201148987\n",
      "Epoch 15 -- Batch 465/ 842, training loss 0.35819458961486816\n",
      "Epoch 15 -- Batch 466/ 842, training loss 0.35277554392814636\n",
      "Epoch 15 -- Batch 467/ 842, training loss 0.35650986433029175\n",
      "Epoch 15 -- Batch 468/ 842, training loss 0.34922167658805847\n",
      "Epoch 15 -- Batch 469/ 842, training loss 0.36497822403907776\n",
      "Epoch 15 -- Batch 470/ 842, training loss 0.3608790934085846\n",
      "Epoch 15 -- Batch 471/ 842, training loss 0.34415990114212036\n",
      "Epoch 15 -- Batch 472/ 842, training loss 0.3567686676979065\n",
      "Epoch 15 -- Batch 473/ 842, training loss 0.34974443912506104\n",
      "Epoch 15 -- Batch 474/ 842, training loss 0.362315833568573\n",
      "Epoch 15 -- Batch 475/ 842, training loss 0.3460746705532074\n",
      "Epoch 15 -- Batch 476/ 842, training loss 0.3575729727745056\n",
      "Epoch 15 -- Batch 477/ 842, training loss 0.3627646267414093\n",
      "Epoch 15 -- Batch 478/ 842, training loss 0.3634515702724457\n",
      "Epoch 15 -- Batch 479/ 842, training loss 0.3647277355194092\n",
      "Epoch 15 -- Batch 480/ 842, training loss 0.35724902153015137\n",
      "Epoch 15 -- Batch 481/ 842, training loss 0.35242173075675964\n",
      "Epoch 15 -- Batch 482/ 842, training loss 0.36794960498809814\n",
      "Epoch 15 -- Batch 483/ 842, training loss 0.36082619428634644\n",
      "Epoch 15 -- Batch 484/ 842, training loss 0.3656229078769684\n",
      "Epoch 15 -- Batch 485/ 842, training loss 0.36340799927711487\n",
      "Epoch 15 -- Batch 486/ 842, training loss 0.3539586067199707\n",
      "Epoch 15 -- Batch 487/ 842, training loss 0.35659047961235046\n",
      "Epoch 15 -- Batch 488/ 842, training loss 0.35888633131980896\n",
      "Epoch 15 -- Batch 489/ 842, training loss 0.36368829011917114\n",
      "Epoch 15 -- Batch 490/ 842, training loss 0.34967902302742004\n",
      "Epoch 15 -- Batch 491/ 842, training loss 0.351960688829422\n",
      "Epoch 15 -- Batch 492/ 842, training loss 0.34542953968048096\n",
      "Epoch 15 -- Batch 493/ 842, training loss 0.3514332175254822\n",
      "Epoch 15 -- Batch 494/ 842, training loss 0.354838490486145\n",
      "Epoch 15 -- Batch 495/ 842, training loss 0.3407987952232361\n",
      "Epoch 15 -- Batch 496/ 842, training loss 0.36408278346061707\n",
      "Epoch 15 -- Batch 497/ 842, training loss 0.3492274582386017\n",
      "Epoch 15 -- Batch 498/ 842, training loss 0.35861384868621826\n",
      "Epoch 15 -- Batch 499/ 842, training loss 0.35421448945999146\n",
      "Epoch 15 -- Batch 500/ 842, training loss 0.3480048179626465\n",
      "Epoch 15 -- Batch 501/ 842, training loss 0.35380110144615173\n",
      "Epoch 15 -- Batch 502/ 842, training loss 0.34693190455436707\n",
      "Epoch 15 -- Batch 503/ 842, training loss 0.36873286962509155\n",
      "Epoch 15 -- Batch 504/ 842, training loss 0.3574292063713074\n",
      "Epoch 15 -- Batch 505/ 842, training loss 0.3657706677913666\n",
      "Epoch 15 -- Batch 506/ 842, training loss 0.35057875514030457\n",
      "Epoch 15 -- Batch 507/ 842, training loss 0.3591756522655487\n",
      "Epoch 15 -- Batch 508/ 842, training loss 0.3581867516040802\n",
      "Epoch 15 -- Batch 509/ 842, training loss 0.36839404702186584\n",
      "Epoch 15 -- Batch 510/ 842, training loss 0.34900233149528503\n",
      "Epoch 15 -- Batch 511/ 842, training loss 0.3491455614566803\n",
      "Epoch 15 -- Batch 512/ 842, training loss 0.350100040435791\n",
      "Epoch 15 -- Batch 513/ 842, training loss 0.3534759283065796\n",
      "Epoch 15 -- Batch 514/ 842, training loss 0.368308424949646\n",
      "Epoch 15 -- Batch 515/ 842, training loss 0.3617509603500366\n",
      "Epoch 15 -- Batch 516/ 842, training loss 0.3585813343524933\n",
      "Epoch 15 -- Batch 517/ 842, training loss 0.3568245470523834\n",
      "Epoch 15 -- Batch 518/ 842, training loss 0.34462064504623413\n",
      "Epoch 15 -- Batch 519/ 842, training loss 0.357250839471817\n",
      "Epoch 15 -- Batch 520/ 842, training loss 0.3683757185935974\n",
      "Epoch 15 -- Batch 521/ 842, training loss 0.36844494938850403\n",
      "Epoch 15 -- Batch 522/ 842, training loss 0.3597696125507355\n",
      "Epoch 15 -- Batch 523/ 842, training loss 0.3436935245990753\n",
      "Epoch 15 -- Batch 524/ 842, training loss 0.3314768373966217\n",
      "Epoch 15 -- Batch 525/ 842, training loss 0.35208821296691895\n",
      "Epoch 15 -- Batch 526/ 842, training loss 0.3526952266693115\n",
      "Epoch 15 -- Batch 527/ 842, training loss 0.3529399037361145\n",
      "Epoch 15 -- Batch 528/ 842, training loss 0.36275514960289\n",
      "Epoch 15 -- Batch 529/ 842, training loss 0.3559444546699524\n",
      "Epoch 15 -- Batch 530/ 842, training loss 0.3444700837135315\n",
      "Epoch 15 -- Batch 531/ 842, training loss 0.3484078645706177\n",
      "Epoch 15 -- Batch 532/ 842, training loss 0.3486942648887634\n",
      "Epoch 15 -- Batch 533/ 842, training loss 0.3593498766422272\n",
      "Epoch 15 -- Batch 534/ 842, training loss 0.3480319380760193\n",
      "Epoch 15 -- Batch 535/ 842, training loss 0.3612605035305023\n",
      "Epoch 15 -- Batch 536/ 842, training loss 0.349021315574646\n",
      "Epoch 15 -- Batch 537/ 842, training loss 0.35895758867263794\n",
      "Epoch 15 -- Batch 538/ 842, training loss 0.3439587354660034\n",
      "Epoch 15 -- Batch 539/ 842, training loss 0.3480179011821747\n",
      "Epoch 15 -- Batch 540/ 842, training loss 0.36772602796554565\n",
      "Epoch 15 -- Batch 541/ 842, training loss 0.362716406583786\n",
      "Epoch 15 -- Batch 542/ 842, training loss 0.37041622400283813\n",
      "Epoch 15 -- Batch 543/ 842, training loss 0.364518404006958\n",
      "Epoch 15 -- Batch 544/ 842, training loss 0.36130353808403015\n",
      "Epoch 15 -- Batch 545/ 842, training loss 0.35125303268432617\n",
      "Epoch 15 -- Batch 546/ 842, training loss 0.354271799325943\n",
      "Epoch 15 -- Batch 547/ 842, training loss 0.35693418979644775\n",
      "Epoch 15 -- Batch 548/ 842, training loss 0.3556663691997528\n",
      "Epoch 15 -- Batch 549/ 842, training loss 0.3591904938220978\n",
      "Epoch 15 -- Batch 550/ 842, training loss 0.3582492768764496\n",
      "Epoch 15 -- Batch 551/ 842, training loss 0.3533695936203003\n",
      "Epoch 15 -- Batch 552/ 842, training loss 0.35080769658088684\n",
      "Epoch 15 -- Batch 553/ 842, training loss 0.34938231110572815\n",
      "Epoch 15 -- Batch 554/ 842, training loss 0.3520093858242035\n",
      "Epoch 15 -- Batch 555/ 842, training loss 0.37182319164276123\n",
      "Epoch 15 -- Batch 556/ 842, training loss 0.35142067074775696\n",
      "Epoch 15 -- Batch 557/ 842, training loss 0.3457440137863159\n",
      "Epoch 15 -- Batch 558/ 842, training loss 0.3558410406112671\n",
      "Epoch 15 -- Batch 559/ 842, training loss 0.3540458381175995\n",
      "Epoch 15 -- Batch 560/ 842, training loss 0.3604236841201782\n",
      "Epoch 15 -- Batch 561/ 842, training loss 0.3446207344532013\n",
      "Epoch 15 -- Batch 562/ 842, training loss 0.35655611753463745\n",
      "Epoch 15 -- Batch 563/ 842, training loss 0.3548712432384491\n",
      "Epoch 15 -- Batch 564/ 842, training loss 0.365765780210495\n",
      "Epoch 15 -- Batch 565/ 842, training loss 0.3434396982192993\n",
      "Epoch 15 -- Batch 566/ 842, training loss 0.3511742651462555\n",
      "Epoch 15 -- Batch 567/ 842, training loss 0.35990262031555176\n",
      "Epoch 15 -- Batch 568/ 842, training loss 0.353165864944458\n",
      "Epoch 15 -- Batch 569/ 842, training loss 0.35146564245224\n",
      "Epoch 15 -- Batch 570/ 842, training loss 0.36832284927368164\n",
      "Epoch 15 -- Batch 571/ 842, training loss 0.3573899567127228\n",
      "Epoch 15 -- Batch 572/ 842, training loss 0.3499099016189575\n",
      "Epoch 15 -- Batch 573/ 842, training loss 0.3494552969932556\n",
      "Epoch 15 -- Batch 574/ 842, training loss 0.35272228717803955\n",
      "Epoch 15 -- Batch 575/ 842, training loss 0.34586283564567566\n",
      "Epoch 15 -- Batch 576/ 842, training loss 0.37063390016555786\n",
      "Epoch 15 -- Batch 577/ 842, training loss 0.36785927414894104\n",
      "Epoch 15 -- Batch 578/ 842, training loss 0.3599700927734375\n",
      "Epoch 15 -- Batch 579/ 842, training loss 0.3627348840236664\n",
      "Epoch 15 -- Batch 580/ 842, training loss 0.3520481288433075\n",
      "Epoch 15 -- Batch 581/ 842, training loss 0.3626479506492615\n",
      "Epoch 15 -- Batch 582/ 842, training loss 0.3610993027687073\n",
      "Epoch 15 -- Batch 583/ 842, training loss 0.3566792905330658\n",
      "Epoch 15 -- Batch 584/ 842, training loss 0.3444882333278656\n",
      "Epoch 15 -- Batch 585/ 842, training loss 0.35450658202171326\n",
      "Epoch 15 -- Batch 586/ 842, training loss 0.35463958978652954\n",
      "Epoch 15 -- Batch 587/ 842, training loss 0.3586716651916504\n",
      "Epoch 15 -- Batch 588/ 842, training loss 0.3533422350883484\n",
      "Epoch 15 -- Batch 589/ 842, training loss 0.35492855310440063\n",
      "Epoch 15 -- Batch 590/ 842, training loss 0.36038532853126526\n",
      "Epoch 15 -- Batch 591/ 842, training loss 0.35875365138053894\n",
      "Epoch 15 -- Batch 592/ 842, training loss 0.37756410241127014\n",
      "Epoch 15 -- Batch 593/ 842, training loss 0.3605986535549164\n",
      "Epoch 15 -- Batch 594/ 842, training loss 0.35347533226013184\n",
      "Epoch 15 -- Batch 595/ 842, training loss 0.3573926091194153\n",
      "Epoch 15 -- Batch 596/ 842, training loss 0.36364132165908813\n",
      "Epoch 15 -- Batch 597/ 842, training loss 0.3445640802383423\n",
      "Epoch 15 -- Batch 598/ 842, training loss 0.34729641675949097\n",
      "Epoch 15 -- Batch 599/ 842, training loss 0.3408902585506439\n",
      "Epoch 15 -- Batch 600/ 842, training loss 0.3445034623146057\n",
      "Epoch 15 -- Batch 601/ 842, training loss 0.36453887820243835\n",
      "Epoch 15 -- Batch 602/ 842, training loss 0.3730546236038208\n",
      "Epoch 15 -- Batch 603/ 842, training loss 0.35526928305625916\n",
      "Epoch 15 -- Batch 604/ 842, training loss 0.3505423963069916\n",
      "Epoch 15 -- Batch 605/ 842, training loss 0.36492714285850525\n",
      "Epoch 15 -- Batch 606/ 842, training loss 0.36170098185539246\n",
      "Epoch 15 -- Batch 607/ 842, training loss 0.35374000668525696\n",
      "Epoch 15 -- Batch 608/ 842, training loss 0.3599216341972351\n",
      "Epoch 15 -- Batch 609/ 842, training loss 0.3435836732387543\n",
      "Epoch 15 -- Batch 610/ 842, training loss 0.3659646511077881\n",
      "Epoch 15 -- Batch 611/ 842, training loss 0.3709704577922821\n",
      "Epoch 15 -- Batch 612/ 842, training loss 0.3530779778957367\n",
      "Epoch 15 -- Batch 613/ 842, training loss 0.3659159541130066\n",
      "Epoch 15 -- Batch 614/ 842, training loss 0.3585858941078186\n",
      "Epoch 15 -- Batch 615/ 842, training loss 0.37345555424690247\n",
      "Epoch 15 -- Batch 616/ 842, training loss 0.3511214256286621\n",
      "Epoch 15 -- Batch 617/ 842, training loss 0.34098389744758606\n",
      "Epoch 15 -- Batch 618/ 842, training loss 0.35432249307632446\n",
      "Epoch 15 -- Batch 619/ 842, training loss 0.343992680311203\n",
      "Epoch 15 -- Batch 620/ 842, training loss 0.3738895058631897\n",
      "Epoch 15 -- Batch 621/ 842, training loss 0.34766948223114014\n",
      "Epoch 15 -- Batch 622/ 842, training loss 0.3466969430446625\n",
      "Epoch 15 -- Batch 623/ 842, training loss 0.3604784309864044\n",
      "Epoch 15 -- Batch 624/ 842, training loss 0.37180715799331665\n",
      "Epoch 15 -- Batch 625/ 842, training loss 0.3404946029186249\n",
      "Epoch 15 -- Batch 626/ 842, training loss 0.3380545973777771\n",
      "Epoch 15 -- Batch 627/ 842, training loss 0.3670944571495056\n",
      "Epoch 15 -- Batch 628/ 842, training loss 0.35327866673469543\n",
      "Epoch 15 -- Batch 629/ 842, training loss 0.3555615544319153\n",
      "Epoch 15 -- Batch 630/ 842, training loss 0.35612836480140686\n",
      "Epoch 15 -- Batch 631/ 842, training loss 0.3478088974952698\n",
      "Epoch 15 -- Batch 632/ 842, training loss 0.3505106568336487\n",
      "Epoch 15 -- Batch 633/ 842, training loss 0.3484955430030823\n",
      "Epoch 15 -- Batch 634/ 842, training loss 0.36023977398872375\n",
      "Epoch 15 -- Batch 635/ 842, training loss 0.3601381480693817\n",
      "Epoch 15 -- Batch 636/ 842, training loss 0.351245641708374\n",
      "Epoch 15 -- Batch 637/ 842, training loss 0.35036563873291016\n",
      "Epoch 15 -- Batch 638/ 842, training loss 0.34902724623680115\n",
      "Epoch 15 -- Batch 639/ 842, training loss 0.34597906470298767\n",
      "Epoch 15 -- Batch 640/ 842, training loss 0.35134080052375793\n",
      "Epoch 15 -- Batch 641/ 842, training loss 0.3596496880054474\n",
      "Epoch 15 -- Batch 642/ 842, training loss 0.34718266129493713\n",
      "Epoch 15 -- Batch 643/ 842, training loss 0.3627724349498749\n",
      "Epoch 15 -- Batch 644/ 842, training loss 0.3592049777507782\n",
      "Epoch 15 -- Batch 645/ 842, training loss 0.35629570484161377\n",
      "Epoch 15 -- Batch 646/ 842, training loss 0.3598911464214325\n",
      "Epoch 15 -- Batch 647/ 842, training loss 0.363818496465683\n",
      "Epoch 15 -- Batch 648/ 842, training loss 0.3392040431499481\n",
      "Epoch 15 -- Batch 649/ 842, training loss 0.348449170589447\n",
      "Epoch 15 -- Batch 650/ 842, training loss 0.3570934236049652\n",
      "Epoch 15 -- Batch 651/ 842, training loss 0.35968396067619324\n",
      "Epoch 15 -- Batch 652/ 842, training loss 0.34669744968414307\n",
      "Epoch 15 -- Batch 653/ 842, training loss 0.3584527373313904\n",
      "Epoch 15 -- Batch 654/ 842, training loss 0.3462950885295868\n",
      "Epoch 15 -- Batch 655/ 842, training loss 0.3631267547607422\n",
      "Epoch 15 -- Batch 656/ 842, training loss 0.3643922209739685\n",
      "Epoch 15 -- Batch 657/ 842, training loss 0.3531431257724762\n",
      "Epoch 15 -- Batch 658/ 842, training loss 0.35232406854629517\n",
      "Epoch 15 -- Batch 659/ 842, training loss 0.3539310097694397\n",
      "Epoch 15 -- Batch 660/ 842, training loss 0.35734814405441284\n",
      "Epoch 15 -- Batch 661/ 842, training loss 0.3671717047691345\n",
      "Epoch 15 -- Batch 662/ 842, training loss 0.3503726124763489\n",
      "Epoch 15 -- Batch 663/ 842, training loss 0.35845422744750977\n",
      "Epoch 15 -- Batch 664/ 842, training loss 0.3613117039203644\n",
      "Epoch 15 -- Batch 665/ 842, training loss 0.3537423312664032\n",
      "Epoch 15 -- Batch 666/ 842, training loss 0.34701254963874817\n",
      "Epoch 15 -- Batch 667/ 842, training loss 0.3514546751976013\n",
      "Epoch 15 -- Batch 668/ 842, training loss 0.35320383310317993\n",
      "Epoch 15 -- Batch 669/ 842, training loss 0.35226744413375854\n",
      "Epoch 15 -- Batch 670/ 842, training loss 0.3472922444343567\n",
      "Epoch 15 -- Batch 671/ 842, training loss 0.38568347692489624\n",
      "Epoch 15 -- Batch 672/ 842, training loss 0.3522011637687683\n",
      "Epoch 15 -- Batch 673/ 842, training loss 0.34986743330955505\n",
      "Epoch 15 -- Batch 674/ 842, training loss 0.35851964354515076\n",
      "Epoch 15 -- Batch 675/ 842, training loss 0.35023781657218933\n",
      "Epoch 15 -- Batch 676/ 842, training loss 0.36206525564193726\n",
      "Epoch 15 -- Batch 677/ 842, training loss 0.34636858105659485\n",
      "Epoch 15 -- Batch 678/ 842, training loss 0.34283915162086487\n",
      "Epoch 15 -- Batch 679/ 842, training loss 0.34680286049842834\n",
      "Epoch 15 -- Batch 680/ 842, training loss 0.3703746497631073\n",
      "Epoch 15 -- Batch 681/ 842, training loss 0.36259329319000244\n",
      "Epoch 15 -- Batch 682/ 842, training loss 0.3626740276813507\n",
      "Epoch 15 -- Batch 683/ 842, training loss 0.3731142282485962\n",
      "Epoch 15 -- Batch 684/ 842, training loss 0.36194905638694763\n",
      "Epoch 15 -- Batch 685/ 842, training loss 0.3485759496688843\n",
      "Epoch 15 -- Batch 686/ 842, training loss 0.34617847204208374\n",
      "Epoch 15 -- Batch 687/ 842, training loss 0.3489607274532318\n",
      "Epoch 15 -- Batch 688/ 842, training loss 0.3498467803001404\n",
      "Epoch 15 -- Batch 689/ 842, training loss 0.37082770466804504\n",
      "Epoch 15 -- Batch 690/ 842, training loss 0.36499759554862976\n",
      "Epoch 15 -- Batch 691/ 842, training loss 0.3572847247123718\n",
      "Epoch 15 -- Batch 692/ 842, training loss 0.35690414905548096\n",
      "Epoch 15 -- Batch 693/ 842, training loss 0.35308897495269775\n",
      "Epoch 15 -- Batch 694/ 842, training loss 0.36067917943000793\n",
      "Epoch 15 -- Batch 695/ 842, training loss 0.36586469411849976\n",
      "Epoch 15 -- Batch 696/ 842, training loss 0.3496188223361969\n",
      "Epoch 15 -- Batch 697/ 842, training loss 0.3400415778160095\n",
      "Epoch 15 -- Batch 698/ 842, training loss 0.3448392152786255\n",
      "Epoch 15 -- Batch 699/ 842, training loss 0.3421318531036377\n",
      "Epoch 15 -- Batch 700/ 842, training loss 0.34500375390052795\n",
      "Epoch 15 -- Batch 701/ 842, training loss 0.3636094927787781\n",
      "Epoch 15 -- Batch 702/ 842, training loss 0.3610457479953766\n",
      "Epoch 15 -- Batch 703/ 842, training loss 0.37003374099731445\n",
      "Epoch 15 -- Batch 704/ 842, training loss 0.3578983545303345\n",
      "Epoch 15 -- Batch 705/ 842, training loss 0.37132883071899414\n",
      "Epoch 15 -- Batch 706/ 842, training loss 0.354919970035553\n",
      "Epoch 15 -- Batch 707/ 842, training loss 0.34695327281951904\n",
      "Epoch 15 -- Batch 708/ 842, training loss 0.3454252779483795\n",
      "Epoch 15 -- Batch 709/ 842, training loss 0.35392439365386963\n",
      "Epoch 15 -- Batch 710/ 842, training loss 0.3684230446815491\n",
      "Epoch 15 -- Batch 711/ 842, training loss 0.3574232757091522\n",
      "Epoch 15 -- Batch 712/ 842, training loss 0.34613779187202454\n",
      "Epoch 15 -- Batch 713/ 842, training loss 0.3592531681060791\n",
      "Epoch 15 -- Batch 714/ 842, training loss 0.3619415760040283\n",
      "Epoch 15 -- Batch 715/ 842, training loss 0.3617832064628601\n",
      "Epoch 15 -- Batch 716/ 842, training loss 0.3517681956291199\n",
      "Epoch 15 -- Batch 717/ 842, training loss 0.3649081289768219\n",
      "Epoch 15 -- Batch 718/ 842, training loss 0.36577051877975464\n",
      "Epoch 15 -- Batch 719/ 842, training loss 0.361512690782547\n",
      "Epoch 15 -- Batch 720/ 842, training loss 0.34753331542015076\n",
      "Epoch 15 -- Batch 721/ 842, training loss 0.35536935925483704\n",
      "Epoch 15 -- Batch 722/ 842, training loss 0.3762715756893158\n",
      "Epoch 15 -- Batch 723/ 842, training loss 0.35422083735466003\n",
      "Epoch 15 -- Batch 724/ 842, training loss 0.36669909954071045\n",
      "Epoch 15 -- Batch 725/ 842, training loss 0.35012832283973694\n",
      "Epoch 15 -- Batch 726/ 842, training loss 0.3498995006084442\n",
      "Epoch 15 -- Batch 727/ 842, training loss 0.3402445316314697\n",
      "Epoch 15 -- Batch 728/ 842, training loss 0.3619197905063629\n",
      "Epoch 15 -- Batch 729/ 842, training loss 0.36049142479896545\n",
      "Epoch 15 -- Batch 730/ 842, training loss 0.3504285514354706\n",
      "Epoch 15 -- Batch 731/ 842, training loss 0.350014865398407\n",
      "Epoch 15 -- Batch 732/ 842, training loss 0.35181254148483276\n",
      "Epoch 15 -- Batch 733/ 842, training loss 0.3565974533557892\n",
      "Epoch 15 -- Batch 734/ 842, training loss 0.3590521812438965\n",
      "Epoch 15 -- Batch 735/ 842, training loss 0.35388991236686707\n",
      "Epoch 15 -- Batch 736/ 842, training loss 0.3460579812526703\n",
      "Epoch 15 -- Batch 737/ 842, training loss 0.34476780891418457\n",
      "Epoch 15 -- Batch 738/ 842, training loss 0.3559933006763458\n",
      "Epoch 15 -- Batch 739/ 842, training loss 0.3596583604812622\n",
      "Epoch 15 -- Batch 740/ 842, training loss 0.35015079379081726\n",
      "Epoch 15 -- Batch 741/ 842, training loss 0.34667038917541504\n",
      "Epoch 15 -- Batch 742/ 842, training loss 0.3428278863430023\n",
      "Epoch 15 -- Batch 743/ 842, training loss 0.3581584393978119\n",
      "Epoch 15 -- Batch 744/ 842, training loss 0.3771747946739197\n",
      "Epoch 15 -- Batch 745/ 842, training loss 0.35395219922065735\n",
      "Epoch 15 -- Batch 746/ 842, training loss 0.3652758300304413\n",
      "Epoch 15 -- Batch 747/ 842, training loss 0.3533291816711426\n",
      "Epoch 15 -- Batch 748/ 842, training loss 0.3693793714046478\n",
      "Epoch 15 -- Batch 749/ 842, training loss 0.3645133674144745\n",
      "Epoch 15 -- Batch 750/ 842, training loss 0.35703256726264954\n",
      "Epoch 15 -- Batch 751/ 842, training loss 0.3546817898750305\n",
      "Epoch 15 -- Batch 752/ 842, training loss 0.3626323640346527\n",
      "Epoch 15 -- Batch 753/ 842, training loss 0.35845333337783813\n",
      "Epoch 15 -- Batch 754/ 842, training loss 0.3521377146244049\n",
      "Epoch 15 -- Batch 755/ 842, training loss 0.3626822233200073\n",
      "Epoch 15 -- Batch 756/ 842, training loss 0.369253933429718\n",
      "Epoch 15 -- Batch 757/ 842, training loss 0.3431141972541809\n",
      "Epoch 15 -- Batch 758/ 842, training loss 0.37618985772132874\n",
      "Epoch 15 -- Batch 759/ 842, training loss 0.3581852912902832\n",
      "Epoch 15 -- Batch 760/ 842, training loss 0.35671913623809814\n",
      "Epoch 15 -- Batch 761/ 842, training loss 0.3664356768131256\n",
      "Epoch 15 -- Batch 762/ 842, training loss 0.3635232150554657\n",
      "Epoch 15 -- Batch 763/ 842, training loss 0.3570432662963867\n",
      "Epoch 15 -- Batch 764/ 842, training loss 0.36054569482803345\n",
      "Epoch 15 -- Batch 765/ 842, training loss 0.36234086751937866\n",
      "Epoch 15 -- Batch 766/ 842, training loss 0.34540459513664246\n",
      "Epoch 15 -- Batch 767/ 842, training loss 0.36123043298721313\n",
      "Epoch 15 -- Batch 768/ 842, training loss 0.35600733757019043\n",
      "Epoch 15 -- Batch 769/ 842, training loss 0.3434745669364929\n",
      "Epoch 15 -- Batch 770/ 842, training loss 0.3552224040031433\n",
      "Epoch 15 -- Batch 771/ 842, training loss 0.34625616669654846\n",
      "Epoch 15 -- Batch 772/ 842, training loss 0.36593103408813477\n",
      "Epoch 15 -- Batch 773/ 842, training loss 0.35610949993133545\n",
      "Epoch 15 -- Batch 774/ 842, training loss 0.3583327531814575\n",
      "Epoch 15 -- Batch 775/ 842, training loss 0.3526330292224884\n",
      "Epoch 15 -- Batch 776/ 842, training loss 0.35270288586616516\n",
      "Epoch 15 -- Batch 777/ 842, training loss 0.35632699728012085\n",
      "Epoch 15 -- Batch 778/ 842, training loss 0.3586258292198181\n",
      "Epoch 15 -- Batch 779/ 842, training loss 0.3529992401599884\n",
      "Epoch 15 -- Batch 780/ 842, training loss 0.3694216310977936\n",
      "Epoch 15 -- Batch 781/ 842, training loss 0.35157227516174316\n",
      "Epoch 15 -- Batch 782/ 842, training loss 0.35009393095970154\n",
      "Epoch 15 -- Batch 783/ 842, training loss 0.3385917544364929\n",
      "Epoch 15 -- Batch 784/ 842, training loss 0.3596355617046356\n",
      "Epoch 15 -- Batch 785/ 842, training loss 0.363904744386673\n",
      "Epoch 15 -- Batch 786/ 842, training loss 0.35518211126327515\n",
      "Epoch 15 -- Batch 787/ 842, training loss 0.34605199098587036\n",
      "Epoch 15 -- Batch 788/ 842, training loss 0.3552083671092987\n",
      "Epoch 15 -- Batch 789/ 842, training loss 0.3485757112503052\n",
      "Epoch 15 -- Batch 790/ 842, training loss 0.3386203646659851\n",
      "Epoch 15 -- Batch 791/ 842, training loss 0.3638114333152771\n",
      "Epoch 15 -- Batch 792/ 842, training loss 0.3554575443267822\n",
      "Epoch 15 -- Batch 793/ 842, training loss 0.34104955196380615\n",
      "Epoch 15 -- Batch 794/ 842, training loss 0.35562655329704285\n",
      "Epoch 15 -- Batch 795/ 842, training loss 0.3588457405567169\n",
      "Epoch 15 -- Batch 796/ 842, training loss 0.35291150212287903\n",
      "Epoch 15 -- Batch 797/ 842, training loss 0.3491089940071106\n",
      "Epoch 15 -- Batch 798/ 842, training loss 0.3516686260700226\n",
      "Epoch 15 -- Batch 799/ 842, training loss 0.366994708776474\n",
      "Epoch 15 -- Batch 800/ 842, training loss 0.35097506642341614\n",
      "Epoch 15 -- Batch 801/ 842, training loss 0.3602333664894104\n",
      "Epoch 15 -- Batch 802/ 842, training loss 0.37382644414901733\n",
      "Epoch 15 -- Batch 803/ 842, training loss 0.35954153537750244\n",
      "Epoch 15 -- Batch 804/ 842, training loss 0.35816171765327454\n",
      "Epoch 15 -- Batch 805/ 842, training loss 0.35153448581695557\n",
      "Epoch 15 -- Batch 806/ 842, training loss 0.3638165295124054\n",
      "Epoch 15 -- Batch 807/ 842, training loss 0.3708169162273407\n",
      "Epoch 15 -- Batch 808/ 842, training loss 0.35906174778938293\n",
      "Epoch 15 -- Batch 809/ 842, training loss 0.3690217137336731\n",
      "Epoch 15 -- Batch 810/ 842, training loss 0.3619594871997833\n",
      "Epoch 15 -- Batch 811/ 842, training loss 0.35678529739379883\n",
      "Epoch 15 -- Batch 812/ 842, training loss 0.35868188738822937\n",
      "Epoch 15 -- Batch 813/ 842, training loss 0.35247451066970825\n",
      "Epoch 15 -- Batch 814/ 842, training loss 0.36044493317604065\n",
      "Epoch 15 -- Batch 815/ 842, training loss 0.34183254837989807\n",
      "Epoch 15 -- Batch 816/ 842, training loss 0.34032508730888367\n",
      "Epoch 15 -- Batch 817/ 842, training loss 0.3621193468570709\n",
      "Epoch 15 -- Batch 818/ 842, training loss 0.36277592182159424\n",
      "Epoch 15 -- Batch 819/ 842, training loss 0.3654022216796875\n",
      "Epoch 15 -- Batch 820/ 842, training loss 0.3693262040615082\n",
      "Epoch 15 -- Batch 821/ 842, training loss 0.3471096456050873\n",
      "Epoch 15 -- Batch 822/ 842, training loss 0.3515265882015228\n",
      "Epoch 15 -- Batch 823/ 842, training loss 0.36640822887420654\n",
      "Epoch 15 -- Batch 824/ 842, training loss 0.35069143772125244\n",
      "Epoch 15 -- Batch 825/ 842, training loss 0.3435741364955902\n",
      "Epoch 15 -- Batch 826/ 842, training loss 0.35610222816467285\n",
      "Epoch 15 -- Batch 827/ 842, training loss 0.3416837751865387\n",
      "Epoch 15 -- Batch 828/ 842, training loss 0.348710834980011\n",
      "Epoch 15 -- Batch 829/ 842, training loss 0.360053151845932\n",
      "Epoch 15 -- Batch 830/ 842, training loss 0.34061333537101746\n",
      "Epoch 15 -- Batch 831/ 842, training loss 0.3438839912414551\n",
      "Epoch 15 -- Batch 832/ 842, training loss 0.3492908477783203\n",
      "Epoch 15 -- Batch 833/ 842, training loss 0.3683624267578125\n",
      "Epoch 15 -- Batch 834/ 842, training loss 0.3474423587322235\n",
      "Epoch 15 -- Batch 835/ 842, training loss 0.35639074444770813\n",
      "Epoch 15 -- Batch 836/ 842, training loss 0.3641526401042938\n",
      "Epoch 15 -- Batch 837/ 842, training loss 0.3566908538341522\n",
      "Epoch 15 -- Batch 838/ 842, training loss 0.35450279712677\n",
      "Epoch 15 -- Batch 839/ 842, training loss 0.3481956422328949\n",
      "Epoch 15 -- Batch 840/ 842, training loss 0.3568374812602997\n",
      "Epoch 15 -- Batch 841/ 842, training loss 0.3648364841938019\n",
      "Epoch 15 -- Batch 842/ 842, training loss 0.348927766084671\n",
      "----------------------------------------------------------------------\n",
      "Epoch 15 -- Batch 1/ 94, validation loss 0.34712156653404236\n",
      "Epoch 15 -- Batch 2/ 94, validation loss 0.3485117554664612\n",
      "Epoch 15 -- Batch 3/ 94, validation loss 0.3369167447090149\n",
      "Epoch 15 -- Batch 4/ 94, validation loss 0.35106372833251953\n",
      "Epoch 15 -- Batch 5/ 94, validation loss 0.35245567560195923\n",
      "Epoch 15 -- Batch 6/ 94, validation loss 0.37099912762641907\n",
      "Epoch 15 -- Batch 7/ 94, validation loss 0.3366941213607788\n",
      "Epoch 15 -- Batch 8/ 94, validation loss 0.3477292060852051\n",
      "Epoch 15 -- Batch 9/ 94, validation loss 0.357084721326828\n",
      "Epoch 15 -- Batch 10/ 94, validation loss 0.3485196828842163\n",
      "Epoch 15 -- Batch 11/ 94, validation loss 0.33434924483299255\n",
      "Epoch 15 -- Batch 12/ 94, validation loss 0.3546145558357239\n",
      "Epoch 15 -- Batch 13/ 94, validation loss 0.3483567237854004\n",
      "Epoch 15 -- Batch 14/ 94, validation loss 0.35510122776031494\n",
      "Epoch 15 -- Batch 15/ 94, validation loss 0.338885098695755\n",
      "Epoch 15 -- Batch 16/ 94, validation loss 0.3489904999732971\n",
      "Epoch 15 -- Batch 17/ 94, validation loss 0.34458082914352417\n",
      "Epoch 15 -- Batch 18/ 94, validation loss 0.3450094759464264\n",
      "Epoch 15 -- Batch 19/ 94, validation loss 0.3568134605884552\n",
      "Epoch 15 -- Batch 20/ 94, validation loss 0.34999334812164307\n",
      "Epoch 15 -- Batch 21/ 94, validation loss 0.3331790864467621\n",
      "Epoch 15 -- Batch 22/ 94, validation loss 0.3512841761112213\n",
      "Epoch 15 -- Batch 23/ 94, validation loss 0.3365885019302368\n",
      "Epoch 15 -- Batch 24/ 94, validation loss 0.35259392857551575\n",
      "Epoch 15 -- Batch 25/ 94, validation loss 0.33681806921958923\n",
      "Epoch 15 -- Batch 26/ 94, validation loss 0.343391478061676\n",
      "Epoch 15 -- Batch 27/ 94, validation loss 0.3458402752876282\n",
      "Epoch 15 -- Batch 28/ 94, validation loss 0.3433677852153778\n",
      "Epoch 15 -- Batch 29/ 94, validation loss 0.34618911147117615\n",
      "Epoch 15 -- Batch 30/ 94, validation loss 0.33928266167640686\n",
      "Epoch 15 -- Batch 31/ 94, validation loss 0.33437415957450867\n",
      "Epoch 15 -- Batch 32/ 94, validation loss 0.340485543012619\n",
      "Epoch 15 -- Batch 33/ 94, validation loss 0.3459642827510834\n",
      "Epoch 15 -- Batch 34/ 94, validation loss 0.3370276093482971\n",
      "Epoch 15 -- Batch 35/ 94, validation loss 0.34890279173851013\n",
      "Epoch 15 -- Batch 36/ 94, validation loss 0.3417041301727295\n",
      "Epoch 15 -- Batch 37/ 94, validation loss 0.35828879475593567\n",
      "Epoch 15 -- Batch 38/ 94, validation loss 0.34568753838539124\n",
      "Epoch 15 -- Batch 39/ 94, validation loss 0.33917611837387085\n",
      "Epoch 15 -- Batch 40/ 94, validation loss 0.3578839600086212\n",
      "Epoch 15 -- Batch 41/ 94, validation loss 0.35690543055534363\n",
      "Epoch 15 -- Batch 42/ 94, validation loss 0.36267906427383423\n",
      "Epoch 15 -- Batch 43/ 94, validation loss 0.3337620496749878\n",
      "Epoch 15 -- Batch 44/ 94, validation loss 0.3593003749847412\n",
      "Epoch 15 -- Batch 45/ 94, validation loss 0.33301183581352234\n",
      "Epoch 15 -- Batch 46/ 94, validation loss 0.3465368151664734\n",
      "Epoch 15 -- Batch 47/ 94, validation loss 0.35096174478530884\n",
      "Epoch 15 -- Batch 48/ 94, validation loss 0.3334159553050995\n",
      "Epoch 15 -- Batch 49/ 94, validation loss 0.34574204683303833\n",
      "Epoch 15 -- Batch 50/ 94, validation loss 0.35898151993751526\n",
      "Epoch 15 -- Batch 51/ 94, validation loss 0.3421087861061096\n",
      "Epoch 15 -- Batch 52/ 94, validation loss 0.34098899364471436\n",
      "Epoch 15 -- Batch 53/ 94, validation loss 0.3465910255908966\n",
      "Epoch 15 -- Batch 54/ 94, validation loss 0.35262537002563477\n",
      "Epoch 15 -- Batch 55/ 94, validation loss 0.35877594351768494\n",
      "Epoch 15 -- Batch 56/ 94, validation loss 0.3353337049484253\n",
      "Epoch 15 -- Batch 57/ 94, validation loss 0.34030845761299133\n",
      "Epoch 15 -- Batch 58/ 94, validation loss 0.3580296039581299\n",
      "Epoch 15 -- Batch 59/ 94, validation loss 0.3430444896221161\n",
      "Epoch 15 -- Batch 60/ 94, validation loss 0.3537742793560028\n",
      "Epoch 15 -- Batch 61/ 94, validation loss 0.3396201431751251\n",
      "Epoch 15 -- Batch 62/ 94, validation loss 0.3585798740386963\n",
      "Epoch 15 -- Batch 63/ 94, validation loss 0.3517757058143616\n",
      "Epoch 15 -- Batch 64/ 94, validation loss 0.33345505595207214\n",
      "Epoch 15 -- Batch 65/ 94, validation loss 0.351285845041275\n",
      "Epoch 15 -- Batch 66/ 94, validation loss 0.3531949520111084\n",
      "Epoch 15 -- Batch 67/ 94, validation loss 0.33032768964767456\n",
      "Epoch 15 -- Batch 68/ 94, validation loss 0.34768542647361755\n",
      "Epoch 15 -- Batch 69/ 94, validation loss 0.3532683551311493\n",
      "Epoch 15 -- Batch 70/ 94, validation loss 0.3577463626861572\n",
      "Epoch 15 -- Batch 71/ 94, validation loss 0.3798512816429138\n",
      "Epoch 15 -- Batch 72/ 94, validation loss 0.3419772684574127\n",
      "Epoch 15 -- Batch 73/ 94, validation loss 0.3524690568447113\n",
      "Epoch 15 -- Batch 74/ 94, validation loss 0.34491556882858276\n",
      "Epoch 15 -- Batch 75/ 94, validation loss 0.34723296761512756\n",
      "Epoch 15 -- Batch 76/ 94, validation loss 0.3510065972805023\n",
      "Epoch 15 -- Batch 77/ 94, validation loss 0.34320366382598877\n",
      "Epoch 15 -- Batch 78/ 94, validation loss 0.3355288803577423\n",
      "Epoch 15 -- Batch 79/ 94, validation loss 0.33568739891052246\n",
      "Epoch 15 -- Batch 80/ 94, validation loss 0.35938242077827454\n",
      "Epoch 15 -- Batch 81/ 94, validation loss 0.3396480083465576\n",
      "Epoch 15 -- Batch 82/ 94, validation loss 0.3479325771331787\n",
      "Epoch 15 -- Batch 83/ 94, validation loss 0.33997678756713867\n",
      "Epoch 15 -- Batch 84/ 94, validation loss 0.3724505603313446\n",
      "Epoch 15 -- Batch 85/ 94, validation loss 0.34896957874298096\n",
      "Epoch 15 -- Batch 86/ 94, validation loss 0.3444484770298004\n",
      "Epoch 15 -- Batch 87/ 94, validation loss 0.3401484489440918\n",
      "Epoch 15 -- Batch 88/ 94, validation loss 0.3459414541721344\n",
      "Epoch 15 -- Batch 89/ 94, validation loss 0.3450428247451782\n",
      "Epoch 15 -- Batch 90/ 94, validation loss 0.3409760892391205\n",
      "Epoch 15 -- Batch 91/ 94, validation loss 0.35071951150894165\n",
      "Epoch 15 -- Batch 92/ 94, validation loss 0.35494115948677063\n",
      "Epoch 15 -- Batch 93/ 94, validation loss 0.34450045228004456\n",
      "Epoch 15 -- Batch 94/ 94, validation loss 0.3406834900379181\n",
      "----------------------------------------------------------------------\n",
      "Epoch 15 loss: Training 0.35433411598205566, Validation 0.3406834900379181\n",
      "----------------------------------------------------------------------\n",
      "Epoch 16/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 15 16 17 19 21\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CCc1nnc(CNC(=O)CCc2nnc3n3CCCCC3)n(C)n1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[19:06:19] SMILES Parse Error: extra open parentheses for input: 'CN1CCC(N(CC(=O)N(Cc2ccc3c(c2)OCO3)Cc2cccs2)CC1'\n",
      "[19:06:19] SMILES Parse Error: ring closure 2 duplicates bond between atom 9 and atom 10 for input: 'CCCc1ccc(OCC2c2cc/c(=O)4c4ccc(C)cc5nc3n2C)cc1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 7 8 15 16 17\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)NC1c2cc(OC)c(OC)cc2C1=O'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'O=C(C1CCCC1)N1C[C@H]2N[C@H](C1)C3c1ccc(-c2ccccc2)cc1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CN1[C@H]2CC[C@@H]1CN(Cc1nc(Cc3cccc(F)c3)ns2)CCC12'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9\n",
      "[19:06:19] SMILES Parse Error: syntax error while parsing: CCCCNc1nc2c(-1)c(=O)[nH]c(=O)n2Cc1cccc([N+](=O)[O-])c1\n",
      "[19:06:19] SMILES Parse Error: Failed parsing SMILES 'CCCCNc1nc2c(-1)c(=O)[nH]c(=O)n2Cc1cccc([N+](=O)[O-])c1' for input: 'CCCCNc1nc2c(-1)c(=O)[nH]c(=O)n2Cc1cccc([N+](=O)[O-])c1'\n",
      "[19:06:19] SMILES Parse Error: syntax error while parsing: O=C1OC(c2cc([N+](=O)[O-])ccc2[N+]Cl=)=NC1=Cc1ccccc1\n",
      "[19:06:19] SMILES Parse Error: Failed parsing SMILES 'O=C1OC(c2cc([N+](=O)[O-])ccc2[N+]Cl=)=NC1=Cc1ccccc1' for input: 'O=C1OC(c2cc([N+](=O)[O-])ccc2[N+]Cl=)=NC1=Cc1ccccc1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'O=C1C(c2cc2c(cc2Br)OCO3)C2=C(CCCC2=O)N1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 17 18 19 21 29\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 14 15 16 19 20\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN2C[C@H](O)COC[C@@H]3O[C@@H](CC(=O)N[C@H](Cc4ccccc4)C(=O)N4C)CC2)cc1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 8 10\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 10 11 34\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 8 9 10 20 21\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 16 17 18\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 10 19 20 21 22 23 24\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 5 24 25\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CC1CCN(C(=O)N2C[C@@H]3[C@@H](c4ccccc42)[C@@H]2CC[C@@H](N2)CC1)C1'\n",
      "[19:06:19] Explicit valence for atom # 19 Br, 2, is greater than permitted\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CN1/C(=C/C=c2\\sc3nc4ccccc4n33)C2=C(C(=O)CCC2)C1c1ccc(/C=C/c2ccccc2)cc1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(Cn2c(=S)c3c(nc4c(C(F)(F)F)cnn33)n(C)c2=O)c1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'N#Cc1ccc(NC(=O)COC(=O)c2ncn(CC(=O)N3CCOCC3)c2ccccc2)cc1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CCCc1ncc(C(=O)N2C[C@H]3[C@@H](c4ccccc42)[C@H](CO)N2Cc2ccccn2)c1'\n",
      "[19:06:19] SMILES Parse Error: extra open parentheses for input: 'O=C(CNC(=O)C1CC12CCCN(C(=O)C1CC1)N2CCO'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CC(C)c1ccc(-c2nc3n(c(=O)Cn2c2=O)CCN(C(=O)c2cccs2)C3)cc1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 9 21 23 24\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 4 6 29\n",
      "[19:06:19] SMILES Parse Error: extra close parentheses while parsing: O=C(Cn1ncc2c1-c1ccccc1OC2)NCC1)NC1(C(=O)N2CCCCC2)CCCCC1\n",
      "[19:06:19] SMILES Parse Error: Failed parsing SMILES 'O=C(Cn1ncc2c1-c1ccccc1OC2)NCC1)NC1(C(=O)N2CCCCC2)CCCCC1' for input: 'O=C(Cn1ncc2c1-c1ccccc1OC2)NCC1)NC1(C(=O)N2CCCCC2)CCCCC1'\n",
      "[19:06:19] SMILES Parse Error: extra close parentheses while parsing: CCCCn1nnnc1C(c1cc2cc3c(cc2Br)OCO3)N1CCN1CCOCC1)c1ccccc1\n",
      "[19:06:19] SMILES Parse Error: Failed parsing SMILES 'CCCCn1nnnc1C(c1cc2cc3c(cc2Br)OCO3)N1CCN1CCOCC1)c1ccccc1' for input: 'CCCCn1nnnc1C(c1cc2cc3c(cc2Br)OCO3)N1CCN1CCOCC1)c1ccccc1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(=O)N[C@H]2C[C@@H]3C(=O)N[C@H](Cc4c[nH]c5ccccc44)C(=O)N3C2)cc1'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CC1CCN(S(=O)(=O)c2ccc3c(c2)n(C)n(S(=O)(=O)c3ccccc3)c2=O)CC1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 20 21\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 9 18 19\n",
      "[19:06:19] Explicit valence for atom # 4 Cl, 2, is greater than permitted\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 2 3 17\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'COCCCN1COc2cc3c(cc2C2)OCO3'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 21\n",
      "[19:06:19] SMILES Parse Error: extra open parentheses for input: 'CN(C1C(C(=O)NS(=O)(=O)c2cccs2)CCN1Cc1ccccc1F'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'O=C(C1CCOCC1)N1CCC2(CC1)Nc1nc(N1CCN(Cc3ccccc3)CC1)CS2'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15\n",
      "[19:06:19] SMILES Parse Error: ring closure 2 duplicates bond between atom 7 and atom 8 for input: 'CCCC(=O)N1N=C2c2c([nH]c3ccccc23)C(c2ccc(N(C)C)cc2)N1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 5 7 8 9 10 19\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 7 9 23\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CCN1CCN(C(=O)c2ccc(Nc3c(C)cc(C)c(C(=O)N(C)C)c3s2)cc2)CC1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 7 8 12\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'COCCN=C1CCc2c1cc(OC)ccc2O1'\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24 25 26 27 29\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 7 8 15 16 29 30\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 2 3 5 6 7 8 11 12 13\n",
      "[19:06:19] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[19:06:19] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 16 17\n",
      "[19:06:19] SMILES Parse Error: ring closure 1 duplicates bond between atom 21 and atom 22 for input: 'O=C(CN1CCN(c2nc(-c3ccc(F)cc3)cs2)CC1)N1c1ccc2ccccc2c1'\n",
      "[19:06:19] SMILES Parse Error: extra open parentheses for input: 'CN1[C@H](C(=O)N[C@@H](C2CCCCC2)C(=O)C(CC(=O)NCCO)[C@H]2OC(C)(C)O[C@@H]12'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'CCCCOC(=O)c1c(C)c(-c2ccc(Cl)cc2)oc2cc1OC'\n",
      "[19:06:19] SMILES Parse Error: unclosed ring for input: 'COc1cc(CNC(=O)c2ccc(NC3CCCCc3cccnc3)c(OC)c2)cc(OC)c1OC'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 -- Batch 1/ 842, training loss 0.34887075424194336\n",
      "Epoch 16 -- Batch 2/ 842, training loss 0.3276153802871704\n",
      "Epoch 16 -- Batch 3/ 842, training loss 0.3511233329772949\n",
      "Epoch 16 -- Batch 4/ 842, training loss 0.33494797348976135\n",
      "Epoch 16 -- Batch 5/ 842, training loss 0.34347158670425415\n",
      "Epoch 16 -- Batch 6/ 842, training loss 0.34494441747665405\n",
      "Epoch 16 -- Batch 7/ 842, training loss 0.34301888942718506\n",
      "Epoch 16 -- Batch 8/ 842, training loss 0.3276536762714386\n",
      "Epoch 16 -- Batch 9/ 842, training loss 0.35543060302734375\n",
      "Epoch 16 -- Batch 10/ 842, training loss 0.3411652743816376\n",
      "Epoch 16 -- Batch 11/ 842, training loss 0.35644248127937317\n",
      "Epoch 16 -- Batch 12/ 842, training loss 0.34729889035224915\n",
      "Epoch 16 -- Batch 13/ 842, training loss 0.34343427419662476\n",
      "Epoch 16 -- Batch 14/ 842, training loss 0.34637388586997986\n",
      "Epoch 16 -- Batch 15/ 842, training loss 0.3553224802017212\n",
      "Epoch 16 -- Batch 16/ 842, training loss 0.3447275459766388\n",
      "Epoch 16 -- Batch 17/ 842, training loss 0.3495240807533264\n",
      "Epoch 16 -- Batch 18/ 842, training loss 0.3485884964466095\n",
      "Epoch 16 -- Batch 19/ 842, training loss 0.35503774881362915\n",
      "Epoch 16 -- Batch 20/ 842, training loss 0.3391343951225281\n",
      "Epoch 16 -- Batch 21/ 842, training loss 0.33238744735717773\n",
      "Epoch 16 -- Batch 22/ 842, training loss 0.34403684735298157\n",
      "Epoch 16 -- Batch 23/ 842, training loss 0.34615635871887207\n",
      "Epoch 16 -- Batch 24/ 842, training loss 0.3468521237373352\n",
      "Epoch 16 -- Batch 25/ 842, training loss 0.35076847672462463\n",
      "Epoch 16 -- Batch 26/ 842, training loss 0.3514355421066284\n",
      "Epoch 16 -- Batch 27/ 842, training loss 0.3543398082256317\n",
      "Epoch 16 -- Batch 28/ 842, training loss 0.33564046025276184\n",
      "Epoch 16 -- Batch 29/ 842, training loss 0.33376502990722656\n",
      "Epoch 16 -- Batch 30/ 842, training loss 0.33862438797950745\n",
      "Epoch 16 -- Batch 31/ 842, training loss 0.3473057150840759\n",
      "Epoch 16 -- Batch 32/ 842, training loss 0.3489686846733093\n",
      "Epoch 16 -- Batch 33/ 842, training loss 0.3391866981983185\n",
      "Epoch 16 -- Batch 34/ 842, training loss 0.3405371904373169\n",
      "Epoch 16 -- Batch 35/ 842, training loss 0.35444316267967224\n",
      "Epoch 16 -- Batch 36/ 842, training loss 0.347481906414032\n",
      "Epoch 16 -- Batch 37/ 842, training loss 0.3410906195640564\n",
      "Epoch 16 -- Batch 38/ 842, training loss 0.3463270664215088\n",
      "Epoch 16 -- Batch 39/ 842, training loss 0.3471134305000305\n",
      "Epoch 16 -- Batch 40/ 842, training loss 0.3518599569797516\n",
      "Epoch 16 -- Batch 41/ 842, training loss 0.33424267172813416\n",
      "Epoch 16 -- Batch 42/ 842, training loss 0.3434543013572693\n",
      "Epoch 16 -- Batch 43/ 842, training loss 0.334317684173584\n",
      "Epoch 16 -- Batch 44/ 842, training loss 0.34165671467781067\n",
      "Epoch 16 -- Batch 45/ 842, training loss 0.3380405306816101\n",
      "Epoch 16 -- Batch 46/ 842, training loss 0.3472471237182617\n",
      "Epoch 16 -- Batch 47/ 842, training loss 0.3557579219341278\n",
      "Epoch 16 -- Batch 48/ 842, training loss 0.343677818775177\n",
      "Epoch 16 -- Batch 49/ 842, training loss 0.35473310947418213\n",
      "Epoch 16 -- Batch 50/ 842, training loss 0.34738582372665405\n",
      "Epoch 16 -- Batch 51/ 842, training loss 0.34937384724617004\n",
      "Epoch 16 -- Batch 52/ 842, training loss 0.350240558385849\n",
      "Epoch 16 -- Batch 53/ 842, training loss 0.34313854575157166\n",
      "Epoch 16 -- Batch 54/ 842, training loss 0.3364396393299103\n",
      "Epoch 16 -- Batch 55/ 842, training loss 0.34409159421920776\n",
      "Epoch 16 -- Batch 56/ 842, training loss 0.3336862623691559\n",
      "Epoch 16 -- Batch 57/ 842, training loss 0.35118821263313293\n",
      "Epoch 16 -- Batch 58/ 842, training loss 0.3340636193752289\n",
      "Epoch 16 -- Batch 59/ 842, training loss 0.3539097011089325\n",
      "Epoch 16 -- Batch 60/ 842, training loss 0.34067636728286743\n",
      "Epoch 16 -- Batch 61/ 842, training loss 0.36859896779060364\n",
      "Epoch 16 -- Batch 62/ 842, training loss 0.3385375738143921\n",
      "Epoch 16 -- Batch 63/ 842, training loss 0.3346078097820282\n",
      "Epoch 16 -- Batch 64/ 842, training loss 0.3388977348804474\n",
      "Epoch 16 -- Batch 65/ 842, training loss 0.35000014305114746\n",
      "Epoch 16 -- Batch 66/ 842, training loss 0.35334575176239014\n",
      "Epoch 16 -- Batch 67/ 842, training loss 0.33538731932640076\n",
      "Epoch 16 -- Batch 68/ 842, training loss 0.3381798267364502\n",
      "Epoch 16 -- Batch 69/ 842, training loss 0.3423498272895813\n",
      "Epoch 16 -- Batch 70/ 842, training loss 0.3497534692287445\n",
      "Epoch 16 -- Batch 71/ 842, training loss 0.34015804529190063\n",
      "Epoch 16 -- Batch 72/ 842, training loss 0.33466818928718567\n",
      "Epoch 16 -- Batch 73/ 842, training loss 0.34970521926879883\n",
      "Epoch 16 -- Batch 74/ 842, training loss 0.36096662282943726\n",
      "Epoch 16 -- Batch 75/ 842, training loss 0.3327964246273041\n",
      "Epoch 16 -- Batch 76/ 842, training loss 0.35242727398872375\n",
      "Epoch 16 -- Batch 77/ 842, training loss 0.3395756483078003\n",
      "Epoch 16 -- Batch 78/ 842, training loss 0.35268574953079224\n",
      "Epoch 16 -- Batch 79/ 842, training loss 0.3419167101383209\n",
      "Epoch 16 -- Batch 80/ 842, training loss 0.34051162004470825\n",
      "Epoch 16 -- Batch 81/ 842, training loss 0.3429493010044098\n",
      "Epoch 16 -- Batch 82/ 842, training loss 0.3538875877857208\n",
      "Epoch 16 -- Batch 83/ 842, training loss 0.3429955840110779\n",
      "Epoch 16 -- Batch 84/ 842, training loss 0.35156455636024475\n",
      "Epoch 16 -- Batch 85/ 842, training loss 0.35509568452835083\n",
      "Epoch 16 -- Batch 86/ 842, training loss 0.35239630937576294\n",
      "Epoch 16 -- Batch 87/ 842, training loss 0.35157138109207153\n",
      "Epoch 16 -- Batch 88/ 842, training loss 0.33483171463012695\n",
      "Epoch 16 -- Batch 89/ 842, training loss 0.35722315311431885\n",
      "Epoch 16 -- Batch 90/ 842, training loss 0.3348056972026825\n",
      "Epoch 16 -- Batch 91/ 842, training loss 0.34963753819465637\n",
      "Epoch 16 -- Batch 92/ 842, training loss 0.3480486571788788\n",
      "Epoch 16 -- Batch 93/ 842, training loss 0.3294035792350769\n",
      "Epoch 16 -- Batch 94/ 842, training loss 0.3511328101158142\n",
      "Epoch 16 -- Batch 95/ 842, training loss 0.3377974033355713\n",
      "Epoch 16 -- Batch 96/ 842, training loss 0.35830360651016235\n",
      "Epoch 16 -- Batch 97/ 842, training loss 0.35214880108833313\n",
      "Epoch 16 -- Batch 98/ 842, training loss 0.3390323221683502\n",
      "Epoch 16 -- Batch 99/ 842, training loss 0.3317120671272278\n",
      "Epoch 16 -- Batch 100/ 842, training loss 0.3571814000606537\n",
      "Epoch 16 -- Batch 101/ 842, training loss 0.3433045446872711\n",
      "Epoch 16 -- Batch 102/ 842, training loss 0.34626829624176025\n",
      "Epoch 16 -- Batch 103/ 842, training loss 0.3491818606853485\n",
      "Epoch 16 -- Batch 104/ 842, training loss 0.34393996000289917\n",
      "Epoch 16 -- Batch 105/ 842, training loss 0.34768977761268616\n",
      "Epoch 16 -- Batch 106/ 842, training loss 0.33726733922958374\n",
      "Epoch 16 -- Batch 107/ 842, training loss 0.3395240306854248\n",
      "Epoch 16 -- Batch 108/ 842, training loss 0.34645330905914307\n",
      "Epoch 16 -- Batch 109/ 842, training loss 0.3452843725681305\n",
      "Epoch 16 -- Batch 110/ 842, training loss 0.3555251955986023\n",
      "Epoch 16 -- Batch 111/ 842, training loss 0.3383557200431824\n",
      "Epoch 16 -- Batch 112/ 842, training loss 0.3383692800998688\n",
      "Epoch 16 -- Batch 113/ 842, training loss 0.3479018211364746\n",
      "Epoch 16 -- Batch 114/ 842, training loss 0.3409017026424408\n",
      "Epoch 16 -- Batch 115/ 842, training loss 0.3576805591583252\n",
      "Epoch 16 -- Batch 116/ 842, training loss 0.3420042395591736\n",
      "Epoch 16 -- Batch 117/ 842, training loss 0.34172970056533813\n",
      "Epoch 16 -- Batch 118/ 842, training loss 0.3370002210140228\n",
      "Epoch 16 -- Batch 119/ 842, training loss 0.340414434671402\n",
      "Epoch 16 -- Batch 120/ 842, training loss 0.3499875068664551\n",
      "Epoch 16 -- Batch 121/ 842, training loss 0.3409380316734314\n",
      "Epoch 16 -- Batch 122/ 842, training loss 0.35450881719589233\n",
      "Epoch 16 -- Batch 123/ 842, training loss 0.3404461443424225\n",
      "Epoch 16 -- Batch 124/ 842, training loss 0.3418862521648407\n",
      "Epoch 16 -- Batch 125/ 842, training loss 0.34712547063827515\n",
      "Epoch 16 -- Batch 126/ 842, training loss 0.35441094636917114\n",
      "Epoch 16 -- Batch 127/ 842, training loss 0.3487498462200165\n",
      "Epoch 16 -- Batch 128/ 842, training loss 0.3635658621788025\n",
      "Epoch 16 -- Batch 129/ 842, training loss 0.34503474831581116\n",
      "Epoch 16 -- Batch 130/ 842, training loss 0.332681804895401\n",
      "Epoch 16 -- Batch 131/ 842, training loss 0.3483272194862366\n",
      "Epoch 16 -- Batch 132/ 842, training loss 0.3379628658294678\n",
      "Epoch 16 -- Batch 133/ 842, training loss 0.3389970660209656\n",
      "Epoch 16 -- Batch 134/ 842, training loss 0.36248844861984253\n",
      "Epoch 16 -- Batch 135/ 842, training loss 0.3455544114112854\n",
      "Epoch 16 -- Batch 136/ 842, training loss 0.3479960262775421\n",
      "Epoch 16 -- Batch 137/ 842, training loss 0.35294997692108154\n",
      "Epoch 16 -- Batch 138/ 842, training loss 0.34224364161491394\n",
      "Epoch 16 -- Batch 139/ 842, training loss 0.3523920476436615\n",
      "Epoch 16 -- Batch 140/ 842, training loss 0.3436896800994873\n",
      "Epoch 16 -- Batch 141/ 842, training loss 0.32074129581451416\n",
      "Epoch 16 -- Batch 142/ 842, training loss 0.3552071750164032\n",
      "Epoch 16 -- Batch 143/ 842, training loss 0.3426007330417633\n",
      "Epoch 16 -- Batch 144/ 842, training loss 0.3508865535259247\n",
      "Epoch 16 -- Batch 145/ 842, training loss 0.34294694662094116\n",
      "Epoch 16 -- Batch 146/ 842, training loss 0.3563583493232727\n",
      "Epoch 16 -- Batch 147/ 842, training loss 0.3299046754837036\n",
      "Epoch 16 -- Batch 148/ 842, training loss 0.3242397904396057\n",
      "Epoch 16 -- Batch 149/ 842, training loss 0.34197381138801575\n",
      "Epoch 16 -- Batch 150/ 842, training loss 0.3366129398345947\n",
      "Epoch 16 -- Batch 151/ 842, training loss 0.3446548581123352\n",
      "Epoch 16 -- Batch 152/ 842, training loss 0.34342068433761597\n",
      "Epoch 16 -- Batch 153/ 842, training loss 0.3255462944507599\n",
      "Epoch 16 -- Batch 154/ 842, training loss 0.340354323387146\n",
      "Epoch 16 -- Batch 155/ 842, training loss 0.3516247868537903\n",
      "Epoch 16 -- Batch 156/ 842, training loss 0.3470223844051361\n",
      "Epoch 16 -- Batch 157/ 842, training loss 0.33645227551460266\n",
      "Epoch 16 -- Batch 158/ 842, training loss 0.3484668433666229\n",
      "Epoch 16 -- Batch 159/ 842, training loss 0.3399807810783386\n",
      "Epoch 16 -- Batch 160/ 842, training loss 0.33879974484443665\n",
      "Epoch 16 -- Batch 161/ 842, training loss 0.34516361355781555\n",
      "Epoch 16 -- Batch 162/ 842, training loss 0.3536476194858551\n",
      "Epoch 16 -- Batch 163/ 842, training loss 0.3374006748199463\n",
      "Epoch 16 -- Batch 164/ 842, training loss 0.33897721767425537\n",
      "Epoch 16 -- Batch 165/ 842, training loss 0.3471100926399231\n",
      "Epoch 16 -- Batch 166/ 842, training loss 0.3415159285068512\n",
      "Epoch 16 -- Batch 167/ 842, training loss 0.3444363474845886\n",
      "Epoch 16 -- Batch 168/ 842, training loss 0.3457348644733429\n",
      "Epoch 16 -- Batch 169/ 842, training loss 0.35907092690467834\n",
      "Epoch 16 -- Batch 170/ 842, training loss 0.3353227376937866\n",
      "Epoch 16 -- Batch 171/ 842, training loss 0.3436676859855652\n",
      "Epoch 16 -- Batch 172/ 842, training loss 0.34468409419059753\n",
      "Epoch 16 -- Batch 173/ 842, training loss 0.3387371599674225\n",
      "Epoch 16 -- Batch 174/ 842, training loss 0.34364965558052063\n",
      "Epoch 16 -- Batch 175/ 842, training loss 0.3432730734348297\n",
      "Epoch 16 -- Batch 176/ 842, training loss 0.351600706577301\n",
      "Epoch 16 -- Batch 177/ 842, training loss 0.3521634638309479\n",
      "Epoch 16 -- Batch 178/ 842, training loss 0.3419972062110901\n",
      "Epoch 16 -- Batch 179/ 842, training loss 0.3617277145385742\n",
      "Epoch 16 -- Batch 180/ 842, training loss 0.33205491304397583\n",
      "Epoch 16 -- Batch 181/ 842, training loss 0.3636925220489502\n",
      "Epoch 16 -- Batch 182/ 842, training loss 0.32877686619758606\n",
      "Epoch 16 -- Batch 183/ 842, training loss 0.34706658124923706\n",
      "Epoch 16 -- Batch 184/ 842, training loss 0.34689995646476746\n",
      "Epoch 16 -- Batch 185/ 842, training loss 0.339034765958786\n",
      "Epoch 16 -- Batch 186/ 842, training loss 0.3371597230434418\n",
      "Epoch 16 -- Batch 187/ 842, training loss 0.33869460225105286\n",
      "Epoch 16 -- Batch 188/ 842, training loss 0.35772114992141724\n",
      "Epoch 16 -- Batch 189/ 842, training loss 0.3511515259742737\n",
      "Epoch 16 -- Batch 190/ 842, training loss 0.34963011741638184\n",
      "Epoch 16 -- Batch 191/ 842, training loss 0.34271785616874695\n",
      "Epoch 16 -- Batch 192/ 842, training loss 0.34909185767173767\n",
      "Epoch 16 -- Batch 193/ 842, training loss 0.3439841568470001\n",
      "Epoch 16 -- Batch 194/ 842, training loss 0.3589462637901306\n",
      "Epoch 16 -- Batch 195/ 842, training loss 0.3298112154006958\n",
      "Epoch 16 -- Batch 196/ 842, training loss 0.33785495162010193\n",
      "Epoch 16 -- Batch 197/ 842, training loss 0.34042778611183167\n",
      "Epoch 16 -- Batch 198/ 842, training loss 0.3534897267818451\n",
      "Epoch 16 -- Batch 199/ 842, training loss 0.3479996919631958\n",
      "Epoch 16 -- Batch 200/ 842, training loss 0.36123374104499817\n",
      "Epoch 16 -- Batch 201/ 842, training loss 0.352613627910614\n",
      "Epoch 16 -- Batch 202/ 842, training loss 0.35149890184402466\n",
      "Epoch 16 -- Batch 203/ 842, training loss 0.3409534990787506\n",
      "Epoch 16 -- Batch 204/ 842, training loss 0.35366290807724\n",
      "Epoch 16 -- Batch 205/ 842, training loss 0.3444300889968872\n",
      "Epoch 16 -- Batch 206/ 842, training loss 0.34856081008911133\n",
      "Epoch 16 -- Batch 207/ 842, training loss 0.33819735050201416\n",
      "Epoch 16 -- Batch 208/ 842, training loss 0.3455459475517273\n",
      "Epoch 16 -- Batch 209/ 842, training loss 0.3554166853427887\n",
      "Epoch 16 -- Batch 210/ 842, training loss 0.35058948397636414\n",
      "Epoch 16 -- Batch 211/ 842, training loss 0.33863887190818787\n",
      "Epoch 16 -- Batch 212/ 842, training loss 0.3528788387775421\n",
      "Epoch 16 -- Batch 213/ 842, training loss 0.3389130234718323\n",
      "Epoch 16 -- Batch 214/ 842, training loss 0.3536899983882904\n",
      "Epoch 16 -- Batch 215/ 842, training loss 0.3584137260913849\n",
      "Epoch 16 -- Batch 216/ 842, training loss 0.3416072428226471\n",
      "Epoch 16 -- Batch 217/ 842, training loss 0.34785500168800354\n",
      "Epoch 16 -- Batch 218/ 842, training loss 0.34142041206359863\n",
      "Epoch 16 -- Batch 219/ 842, training loss 0.34957897663116455\n",
      "Epoch 16 -- Batch 220/ 842, training loss 0.348688006401062\n",
      "Epoch 16 -- Batch 221/ 842, training loss 0.35820892453193665\n",
      "Epoch 16 -- Batch 222/ 842, training loss 0.3471046984195709\n",
      "Epoch 16 -- Batch 223/ 842, training loss 0.3463592827320099\n",
      "Epoch 16 -- Batch 224/ 842, training loss 0.3488014340400696\n",
      "Epoch 16 -- Batch 225/ 842, training loss 0.3465422987937927\n",
      "Epoch 16 -- Batch 226/ 842, training loss 0.34302613139152527\n",
      "Epoch 16 -- Batch 227/ 842, training loss 0.341648131608963\n",
      "Epoch 16 -- Batch 228/ 842, training loss 0.32820627093315125\n",
      "Epoch 16 -- Batch 229/ 842, training loss 0.36067917943000793\n",
      "Epoch 16 -- Batch 230/ 842, training loss 0.36175376176834106\n",
      "Epoch 16 -- Batch 231/ 842, training loss 0.3409240245819092\n",
      "Epoch 16 -- Batch 232/ 842, training loss 0.33883020281791687\n",
      "Epoch 16 -- Batch 233/ 842, training loss 0.341846764087677\n",
      "Epoch 16 -- Batch 234/ 842, training loss 0.35181155800819397\n",
      "Epoch 16 -- Batch 235/ 842, training loss 0.35527172684669495\n",
      "Epoch 16 -- Batch 236/ 842, training loss 0.3432169556617737\n",
      "Epoch 16 -- Batch 237/ 842, training loss 0.3585965931415558\n",
      "Epoch 16 -- Batch 238/ 842, training loss 0.3441387116909027\n",
      "Epoch 16 -- Batch 239/ 842, training loss 0.34950101375579834\n",
      "Epoch 16 -- Batch 240/ 842, training loss 0.34680959582328796\n",
      "Epoch 16 -- Batch 241/ 842, training loss 0.34111911058425903\n",
      "Epoch 16 -- Batch 242/ 842, training loss 0.3454960882663727\n",
      "Epoch 16 -- Batch 243/ 842, training loss 0.3429276943206787\n",
      "Epoch 16 -- Batch 244/ 842, training loss 0.351363867521286\n",
      "Epoch 16 -- Batch 245/ 842, training loss 0.34726569056510925\n",
      "Epoch 16 -- Batch 246/ 842, training loss 0.3520636260509491\n",
      "Epoch 16 -- Batch 247/ 842, training loss 0.33428043127059937\n",
      "Epoch 16 -- Batch 248/ 842, training loss 0.35310420393943787\n",
      "Epoch 16 -- Batch 249/ 842, training loss 0.34412944316864014\n",
      "Epoch 16 -- Batch 250/ 842, training loss 0.3562277555465698\n",
      "Epoch 16 -- Batch 251/ 842, training loss 0.3346622586250305\n",
      "Epoch 16 -- Batch 252/ 842, training loss 0.3336196839809418\n",
      "Epoch 16 -- Batch 253/ 842, training loss 0.3477535843849182\n",
      "Epoch 16 -- Batch 254/ 842, training loss 0.35247382521629333\n",
      "Epoch 16 -- Batch 255/ 842, training loss 0.35594621300697327\n",
      "Epoch 16 -- Batch 256/ 842, training loss 0.3507218062877655\n",
      "Epoch 16 -- Batch 257/ 842, training loss 0.34656262397766113\n",
      "Epoch 16 -- Batch 258/ 842, training loss 0.3512519299983978\n",
      "Epoch 16 -- Batch 259/ 842, training loss 0.35275599360466003\n",
      "Epoch 16 -- Batch 260/ 842, training loss 0.3449886739253998\n",
      "Epoch 16 -- Batch 261/ 842, training loss 0.3576097786426544\n",
      "Epoch 16 -- Batch 262/ 842, training loss 0.34788820147514343\n",
      "Epoch 16 -- Batch 263/ 842, training loss 0.34555500745773315\n",
      "Epoch 16 -- Batch 264/ 842, training loss 0.34690120816230774\n",
      "Epoch 16 -- Batch 265/ 842, training loss 0.3614375591278076\n",
      "Epoch 16 -- Batch 266/ 842, training loss 0.3476219177246094\n",
      "Epoch 16 -- Batch 267/ 842, training loss 0.3615598976612091\n",
      "Epoch 16 -- Batch 268/ 842, training loss 0.35767272114753723\n",
      "Epoch 16 -- Batch 269/ 842, training loss 0.344144731760025\n",
      "Epoch 16 -- Batch 270/ 842, training loss 0.3442304730415344\n",
      "Epoch 16 -- Batch 271/ 842, training loss 0.35977527499198914\n",
      "Epoch 16 -- Batch 272/ 842, training loss 0.34912869334220886\n",
      "Epoch 16 -- Batch 273/ 842, training loss 0.3466951251029968\n",
      "Epoch 16 -- Batch 274/ 842, training loss 0.34280434250831604\n",
      "Epoch 16 -- Batch 275/ 842, training loss 0.35370197892189026\n",
      "Epoch 16 -- Batch 276/ 842, training loss 0.34391358494758606\n",
      "Epoch 16 -- Batch 277/ 842, training loss 0.3617851734161377\n",
      "Epoch 16 -- Batch 278/ 842, training loss 0.35595595836639404\n",
      "Epoch 16 -- Batch 279/ 842, training loss 0.33449018001556396\n",
      "Epoch 16 -- Batch 280/ 842, training loss 0.3376922011375427\n",
      "Epoch 16 -- Batch 281/ 842, training loss 0.34773173928260803\n",
      "Epoch 16 -- Batch 282/ 842, training loss 0.34333378076553345\n",
      "Epoch 16 -- Batch 283/ 842, training loss 0.3467828333377838\n",
      "Epoch 16 -- Batch 284/ 842, training loss 0.3443935811519623\n",
      "Epoch 16 -- Batch 285/ 842, training loss 0.3395743668079376\n",
      "Epoch 16 -- Batch 286/ 842, training loss 0.34797582030296326\n",
      "Epoch 16 -- Batch 287/ 842, training loss 0.34883907437324524\n",
      "Epoch 16 -- Batch 288/ 842, training loss 0.3457530438899994\n",
      "Epoch 16 -- Batch 289/ 842, training loss 0.34537819027900696\n",
      "Epoch 16 -- Batch 290/ 842, training loss 0.34384164214134216\n",
      "Epoch 16 -- Batch 291/ 842, training loss 0.33678895235061646\n",
      "Epoch 16 -- Batch 292/ 842, training loss 0.35566475987434387\n",
      "Epoch 16 -- Batch 293/ 842, training loss 0.3422355055809021\n",
      "Epoch 16 -- Batch 294/ 842, training loss 0.35349133610725403\n",
      "Epoch 16 -- Batch 295/ 842, training loss 0.3397396504878998\n",
      "Epoch 16 -- Batch 296/ 842, training loss 0.3386702537536621\n",
      "Epoch 16 -- Batch 297/ 842, training loss 0.36457693576812744\n",
      "Epoch 16 -- Batch 298/ 842, training loss 0.3435980975627899\n",
      "Epoch 16 -- Batch 299/ 842, training loss 0.34083518385887146\n",
      "Epoch 16 -- Batch 300/ 842, training loss 0.343791663646698\n",
      "Epoch 16 -- Batch 301/ 842, training loss 0.3371374011039734\n",
      "Epoch 16 -- Batch 302/ 842, training loss 0.3355202376842499\n",
      "Epoch 16 -- Batch 303/ 842, training loss 0.3568572402000427\n",
      "Epoch 16 -- Batch 304/ 842, training loss 0.3417665660381317\n",
      "Epoch 16 -- Batch 305/ 842, training loss 0.34700682759284973\n",
      "Epoch 16 -- Batch 306/ 842, training loss 0.3476071059703827\n",
      "Epoch 16 -- Batch 307/ 842, training loss 0.35776209831237793\n",
      "Epoch 16 -- Batch 308/ 842, training loss 0.3377811312675476\n",
      "Epoch 16 -- Batch 309/ 842, training loss 0.3422161638736725\n",
      "Epoch 16 -- Batch 310/ 842, training loss 0.35421741008758545\n",
      "Epoch 16 -- Batch 311/ 842, training loss 0.3494746685028076\n",
      "Epoch 16 -- Batch 312/ 842, training loss 0.345580130815506\n",
      "Epoch 16 -- Batch 313/ 842, training loss 0.33720818161964417\n",
      "Epoch 16 -- Batch 314/ 842, training loss 0.34516066312789917\n",
      "Epoch 16 -- Batch 315/ 842, training loss 0.3445248305797577\n",
      "Epoch 16 -- Batch 316/ 842, training loss 0.35399100184440613\n",
      "Epoch 16 -- Batch 317/ 842, training loss 0.3517254889011383\n",
      "Epoch 16 -- Batch 318/ 842, training loss 0.34940609335899353\n",
      "Epoch 16 -- Batch 319/ 842, training loss 0.3488766849040985\n",
      "Epoch 16 -- Batch 320/ 842, training loss 0.3537868559360504\n",
      "Epoch 16 -- Batch 321/ 842, training loss 0.35436663031578064\n",
      "Epoch 16 -- Batch 322/ 842, training loss 0.34033969044685364\n",
      "Epoch 16 -- Batch 323/ 842, training loss 0.34737837314605713\n",
      "Epoch 16 -- Batch 324/ 842, training loss 0.34242144227027893\n",
      "Epoch 16 -- Batch 325/ 842, training loss 0.3388749361038208\n",
      "Epoch 16 -- Batch 326/ 842, training loss 0.3567635715007782\n",
      "Epoch 16 -- Batch 327/ 842, training loss 0.3594296872615814\n",
      "Epoch 16 -- Batch 328/ 842, training loss 0.3546348810195923\n",
      "Epoch 16 -- Batch 329/ 842, training loss 0.34538036584854126\n",
      "Epoch 16 -- Batch 330/ 842, training loss 0.33622920513153076\n",
      "Epoch 16 -- Batch 331/ 842, training loss 0.34929442405700684\n",
      "Epoch 16 -- Batch 332/ 842, training loss 0.35567060112953186\n",
      "Epoch 16 -- Batch 333/ 842, training loss 0.34476959705352783\n",
      "Epoch 16 -- Batch 334/ 842, training loss 0.35010120272636414\n",
      "Epoch 16 -- Batch 335/ 842, training loss 0.3545876443386078\n",
      "Epoch 16 -- Batch 336/ 842, training loss 0.35895365476608276\n",
      "Epoch 16 -- Batch 337/ 842, training loss 0.34499868750572205\n",
      "Epoch 16 -- Batch 338/ 842, training loss 0.35232803225517273\n",
      "Epoch 16 -- Batch 339/ 842, training loss 0.3461175858974457\n",
      "Epoch 16 -- Batch 340/ 842, training loss 0.3588133752346039\n",
      "Epoch 16 -- Batch 341/ 842, training loss 0.3376798629760742\n",
      "Epoch 16 -- Batch 342/ 842, training loss 0.34539344906806946\n",
      "Epoch 16 -- Batch 343/ 842, training loss 0.34870317578315735\n",
      "Epoch 16 -- Batch 344/ 842, training loss 0.3465805947780609\n",
      "Epoch 16 -- Batch 345/ 842, training loss 0.3509175479412079\n",
      "Epoch 16 -- Batch 346/ 842, training loss 0.3515233099460602\n",
      "Epoch 16 -- Batch 347/ 842, training loss 0.33641597628593445\n",
      "Epoch 16 -- Batch 348/ 842, training loss 0.34783345460891724\n",
      "Epoch 16 -- Batch 349/ 842, training loss 0.36780089139938354\n",
      "Epoch 16 -- Batch 350/ 842, training loss 0.3782138228416443\n",
      "Epoch 16 -- Batch 351/ 842, training loss 0.3493068218231201\n",
      "Epoch 16 -- Batch 352/ 842, training loss 0.35244643688201904\n",
      "Epoch 16 -- Batch 353/ 842, training loss 0.3552953898906708\n",
      "Epoch 16 -- Batch 354/ 842, training loss 0.34849122166633606\n",
      "Epoch 16 -- Batch 355/ 842, training loss 0.34725263714790344\n",
      "Epoch 16 -- Batch 356/ 842, training loss 0.33754411339759827\n",
      "Epoch 16 -- Batch 357/ 842, training loss 0.3508700430393219\n",
      "Epoch 16 -- Batch 358/ 842, training loss 0.3481924831867218\n",
      "Epoch 16 -- Batch 359/ 842, training loss 0.35000938177108765\n",
      "Epoch 16 -- Batch 360/ 842, training loss 0.33930179476737976\n",
      "Epoch 16 -- Batch 361/ 842, training loss 0.33716297149658203\n",
      "Epoch 16 -- Batch 362/ 842, training loss 0.34917792677879333\n",
      "Epoch 16 -- Batch 363/ 842, training loss 0.3527641296386719\n",
      "Epoch 16 -- Batch 364/ 842, training loss 0.33241498470306396\n",
      "Epoch 16 -- Batch 365/ 842, training loss 0.3453172445297241\n",
      "Epoch 16 -- Batch 366/ 842, training loss 0.3501003384590149\n",
      "Epoch 16 -- Batch 367/ 842, training loss 0.3579583764076233\n",
      "Epoch 16 -- Batch 368/ 842, training loss 0.35755208134651184\n",
      "Epoch 16 -- Batch 369/ 842, training loss 0.32741469144821167\n",
      "Epoch 16 -- Batch 370/ 842, training loss 0.35442355275154114\n",
      "Epoch 16 -- Batch 371/ 842, training loss 0.35195013880729675\n",
      "Epoch 16 -- Batch 372/ 842, training loss 0.34298714995384216\n",
      "Epoch 16 -- Batch 373/ 842, training loss 0.3463880121707916\n",
      "Epoch 16 -- Batch 374/ 842, training loss 0.3365258574485779\n",
      "Epoch 16 -- Batch 375/ 842, training loss 0.3483630120754242\n",
      "Epoch 16 -- Batch 376/ 842, training loss 0.3486984372138977\n",
      "Epoch 16 -- Batch 377/ 842, training loss 0.34033074975013733\n",
      "Epoch 16 -- Batch 378/ 842, training loss 0.34440258145332336\n",
      "Epoch 16 -- Batch 379/ 842, training loss 0.35482579469680786\n",
      "Epoch 16 -- Batch 380/ 842, training loss 0.3345109522342682\n",
      "Epoch 16 -- Batch 381/ 842, training loss 0.34853479266166687\n",
      "Epoch 16 -- Batch 382/ 842, training loss 0.3470897674560547\n",
      "Epoch 16 -- Batch 383/ 842, training loss 0.3650849461555481\n",
      "Epoch 16 -- Batch 384/ 842, training loss 0.35310035943984985\n",
      "Epoch 16 -- Batch 385/ 842, training loss 0.34939348697662354\n",
      "Epoch 16 -- Batch 386/ 842, training loss 0.3589048981666565\n",
      "Epoch 16 -- Batch 387/ 842, training loss 0.3460788428783417\n",
      "Epoch 16 -- Batch 388/ 842, training loss 0.35006073117256165\n",
      "Epoch 16 -- Batch 389/ 842, training loss 0.34328845143318176\n",
      "Epoch 16 -- Batch 390/ 842, training loss 0.3385642468929291\n",
      "Epoch 16 -- Batch 391/ 842, training loss 0.3589969277381897\n",
      "Epoch 16 -- Batch 392/ 842, training loss 0.3371216654777527\n",
      "Epoch 16 -- Batch 393/ 842, training loss 0.3513893485069275\n",
      "Epoch 16 -- Batch 394/ 842, training loss 0.33985111117362976\n",
      "Epoch 16 -- Batch 395/ 842, training loss 0.3483143448829651\n",
      "Epoch 16 -- Batch 396/ 842, training loss 0.36027637124061584\n",
      "Epoch 16 -- Batch 397/ 842, training loss 0.3506264090538025\n",
      "Epoch 16 -- Batch 398/ 842, training loss 0.3444102108478546\n",
      "Epoch 16 -- Batch 399/ 842, training loss 0.3567226827144623\n",
      "Epoch 16 -- Batch 400/ 842, training loss 0.34237560629844666\n",
      "Epoch 16 -- Batch 401/ 842, training loss 0.34262076020240784\n",
      "Epoch 16 -- Batch 402/ 842, training loss 0.33819419145584106\n",
      "Epoch 16 -- Batch 403/ 842, training loss 0.3352172374725342\n",
      "Epoch 16 -- Batch 404/ 842, training loss 0.35519248247146606\n",
      "Epoch 16 -- Batch 405/ 842, training loss 0.37012290954589844\n",
      "Epoch 16 -- Batch 406/ 842, training loss 0.34899115562438965\n",
      "Epoch 16 -- Batch 407/ 842, training loss 0.3396099805831909\n",
      "Epoch 16 -- Batch 408/ 842, training loss 0.3554675281047821\n",
      "Epoch 16 -- Batch 409/ 842, training loss 0.3323727548122406\n",
      "Epoch 16 -- Batch 410/ 842, training loss 0.3370368182659149\n",
      "Epoch 16 -- Batch 411/ 842, training loss 0.36036139726638794\n",
      "Epoch 16 -- Batch 412/ 842, training loss 0.3583441376686096\n",
      "Epoch 16 -- Batch 413/ 842, training loss 0.3592027723789215\n",
      "Epoch 16 -- Batch 414/ 842, training loss 0.3490016460418701\n",
      "Epoch 16 -- Batch 415/ 842, training loss 0.3381340503692627\n",
      "Epoch 16 -- Batch 416/ 842, training loss 0.3425711393356323\n",
      "Epoch 16 -- Batch 417/ 842, training loss 0.36140063405036926\n",
      "Epoch 16 -- Batch 418/ 842, training loss 0.3561989665031433\n",
      "Epoch 16 -- Batch 419/ 842, training loss 0.343627393245697\n",
      "Epoch 16 -- Batch 420/ 842, training loss 0.3581036925315857\n",
      "Epoch 16 -- Batch 421/ 842, training loss 0.3613590896129608\n",
      "Epoch 16 -- Batch 422/ 842, training loss 0.34384098649024963\n",
      "Epoch 16 -- Batch 423/ 842, training loss 0.3396608531475067\n",
      "Epoch 16 -- Batch 424/ 842, training loss 0.3525971472263336\n",
      "Epoch 16 -- Batch 425/ 842, training loss 0.3449193835258484\n",
      "Epoch 16 -- Batch 426/ 842, training loss 0.34258443117141724\n",
      "Epoch 16 -- Batch 427/ 842, training loss 0.36265119910240173\n",
      "Epoch 16 -- Batch 428/ 842, training loss 0.36209365725517273\n",
      "Epoch 16 -- Batch 429/ 842, training loss 0.33548763394355774\n",
      "Epoch 16 -- Batch 430/ 842, training loss 0.356665700674057\n",
      "Epoch 16 -- Batch 431/ 842, training loss 0.35800132155418396\n",
      "Epoch 16 -- Batch 432/ 842, training loss 0.3531882166862488\n",
      "Epoch 16 -- Batch 433/ 842, training loss 0.3583417534828186\n",
      "Epoch 16 -- Batch 434/ 842, training loss 0.3360190987586975\n",
      "Epoch 16 -- Batch 435/ 842, training loss 0.35178712010383606\n",
      "Epoch 16 -- Batch 436/ 842, training loss 0.36064496636390686\n",
      "Epoch 16 -- Batch 437/ 842, training loss 0.3455359935760498\n",
      "Epoch 16 -- Batch 438/ 842, training loss 0.35258010029792786\n",
      "Epoch 16 -- Batch 439/ 842, training loss 0.34279823303222656\n",
      "Epoch 16 -- Batch 440/ 842, training loss 0.3436707854270935\n",
      "Epoch 16 -- Batch 441/ 842, training loss 0.35975760221481323\n",
      "Epoch 16 -- Batch 442/ 842, training loss 0.3563476502895355\n",
      "Epoch 16 -- Batch 443/ 842, training loss 0.35171517729759216\n",
      "Epoch 16 -- Batch 444/ 842, training loss 0.35451740026474\n",
      "Epoch 16 -- Batch 445/ 842, training loss 0.3424312472343445\n",
      "Epoch 16 -- Batch 446/ 842, training loss 0.343645304441452\n",
      "Epoch 16 -- Batch 447/ 842, training loss 0.3386474847793579\n",
      "Epoch 16 -- Batch 448/ 842, training loss 0.3509206473827362\n",
      "Epoch 16 -- Batch 449/ 842, training loss 0.3520622253417969\n",
      "Epoch 16 -- Batch 450/ 842, training loss 0.3354790508747101\n",
      "Epoch 16 -- Batch 451/ 842, training loss 0.35564368963241577\n",
      "Epoch 16 -- Batch 452/ 842, training loss 0.3451656401157379\n",
      "Epoch 16 -- Batch 453/ 842, training loss 0.35613372921943665\n",
      "Epoch 16 -- Batch 454/ 842, training loss 0.33061203360557556\n",
      "Epoch 16 -- Batch 455/ 842, training loss 0.35037779808044434\n",
      "Epoch 16 -- Batch 456/ 842, training loss 0.3635435104370117\n",
      "Epoch 16 -- Batch 457/ 842, training loss 0.340488076210022\n",
      "Epoch 16 -- Batch 458/ 842, training loss 0.3476392924785614\n",
      "Epoch 16 -- Batch 459/ 842, training loss 0.35201579332351685\n",
      "Epoch 16 -- Batch 460/ 842, training loss 0.3329799175262451\n",
      "Epoch 16 -- Batch 461/ 842, training loss 0.33497869968414307\n",
      "Epoch 16 -- Batch 462/ 842, training loss 0.35497647523880005\n",
      "Epoch 16 -- Batch 463/ 842, training loss 0.3395746052265167\n",
      "Epoch 16 -- Batch 464/ 842, training loss 0.33687347173690796\n",
      "Epoch 16 -- Batch 465/ 842, training loss 0.3587521016597748\n",
      "Epoch 16 -- Batch 466/ 842, training loss 0.35290277004241943\n",
      "Epoch 16 -- Batch 467/ 842, training loss 0.35416358709335327\n",
      "Epoch 16 -- Batch 468/ 842, training loss 0.3419030010700226\n",
      "Epoch 16 -- Batch 469/ 842, training loss 0.3595214784145355\n",
      "Epoch 16 -- Batch 470/ 842, training loss 0.3549412190914154\n",
      "Epoch 16 -- Batch 471/ 842, training loss 0.3428385853767395\n",
      "Epoch 16 -- Batch 472/ 842, training loss 0.34308919310569763\n",
      "Epoch 16 -- Batch 473/ 842, training loss 0.34489327669143677\n",
      "Epoch 16 -- Batch 474/ 842, training loss 0.3494640290737152\n",
      "Epoch 16 -- Batch 475/ 842, training loss 0.34558820724487305\n",
      "Epoch 16 -- Batch 476/ 842, training loss 0.3376349210739136\n",
      "Epoch 16 -- Batch 477/ 842, training loss 0.3664151728153229\n",
      "Epoch 16 -- Batch 478/ 842, training loss 0.3540787398815155\n",
      "Epoch 16 -- Batch 479/ 842, training loss 0.34727364778518677\n",
      "Epoch 16 -- Batch 480/ 842, training loss 0.36015215516090393\n",
      "Epoch 16 -- Batch 481/ 842, training loss 0.35169321298599243\n",
      "Epoch 16 -- Batch 482/ 842, training loss 0.35791730880737305\n",
      "Epoch 16 -- Batch 483/ 842, training loss 0.3461529314517975\n",
      "Epoch 16 -- Batch 484/ 842, training loss 0.353063702583313\n",
      "Epoch 16 -- Batch 485/ 842, training loss 0.349529892206192\n",
      "Epoch 16 -- Batch 486/ 842, training loss 0.3627060651779175\n",
      "Epoch 16 -- Batch 487/ 842, training loss 0.346136212348938\n",
      "Epoch 16 -- Batch 488/ 842, training loss 0.34613582491874695\n",
      "Epoch 16 -- Batch 489/ 842, training loss 0.340564101934433\n",
      "Epoch 16 -- Batch 490/ 842, training loss 0.3421517312526703\n",
      "Epoch 16 -- Batch 491/ 842, training loss 0.35150858759880066\n",
      "Epoch 16 -- Batch 492/ 842, training loss 0.34953856468200684\n",
      "Epoch 16 -- Batch 493/ 842, training loss 0.3461652398109436\n",
      "Epoch 16 -- Batch 494/ 842, training loss 0.3560449481010437\n",
      "Epoch 16 -- Batch 495/ 842, training loss 0.352232426404953\n",
      "Epoch 16 -- Batch 496/ 842, training loss 0.3489566445350647\n",
      "Epoch 16 -- Batch 497/ 842, training loss 0.34596681594848633\n",
      "Epoch 16 -- Batch 498/ 842, training loss 0.3418290317058563\n",
      "Epoch 16 -- Batch 499/ 842, training loss 0.34676289558410645\n",
      "Epoch 16 -- Batch 500/ 842, training loss 0.3731042444705963\n",
      "Epoch 16 -- Batch 501/ 842, training loss 0.3517213463783264\n",
      "Epoch 16 -- Batch 502/ 842, training loss 0.350592702627182\n",
      "Epoch 16 -- Batch 503/ 842, training loss 0.35256823897361755\n",
      "Epoch 16 -- Batch 504/ 842, training loss 0.3437027633190155\n",
      "Epoch 16 -- Batch 505/ 842, training loss 0.3529127538204193\n",
      "Epoch 16 -- Batch 506/ 842, training loss 0.34577736258506775\n",
      "Epoch 16 -- Batch 507/ 842, training loss 0.36073923110961914\n",
      "Epoch 16 -- Batch 508/ 842, training loss 0.3655143678188324\n",
      "Epoch 16 -- Batch 509/ 842, training loss 0.34770575165748596\n",
      "Epoch 16 -- Batch 510/ 842, training loss 0.3410409688949585\n",
      "Epoch 16 -- Batch 511/ 842, training loss 0.35486218333244324\n",
      "Epoch 16 -- Batch 512/ 842, training loss 0.3484783172607422\n",
      "Epoch 16 -- Batch 513/ 842, training loss 0.35845962166786194\n",
      "Epoch 16 -- Batch 514/ 842, training loss 0.34189915657043457\n",
      "Epoch 16 -- Batch 515/ 842, training loss 0.35369494557380676\n",
      "Epoch 16 -- Batch 516/ 842, training loss 0.3431473970413208\n",
      "Epoch 16 -- Batch 517/ 842, training loss 0.3538338840007782\n",
      "Epoch 16 -- Batch 518/ 842, training loss 0.3596963882446289\n",
      "Epoch 16 -- Batch 519/ 842, training loss 0.3531937897205353\n",
      "Epoch 16 -- Batch 520/ 842, training loss 0.35024595260620117\n",
      "Epoch 16 -- Batch 521/ 842, training loss 0.35239458084106445\n",
      "Epoch 16 -- Batch 522/ 842, training loss 0.36411458253860474\n",
      "Epoch 16 -- Batch 523/ 842, training loss 0.35073545575141907\n",
      "Epoch 16 -- Batch 524/ 842, training loss 0.3467303216457367\n",
      "Epoch 16 -- Batch 525/ 842, training loss 0.35577860474586487\n",
      "Epoch 16 -- Batch 526/ 842, training loss 0.3644750714302063\n",
      "Epoch 16 -- Batch 527/ 842, training loss 0.3405214846134186\n",
      "Epoch 16 -- Batch 528/ 842, training loss 0.361929714679718\n",
      "Epoch 16 -- Batch 529/ 842, training loss 0.35482943058013916\n",
      "Epoch 16 -- Batch 530/ 842, training loss 0.3466898202896118\n",
      "Epoch 16 -- Batch 531/ 842, training loss 0.34227269887924194\n",
      "Epoch 16 -- Batch 532/ 842, training loss 0.35323047637939453\n",
      "Epoch 16 -- Batch 533/ 842, training loss 0.354949414730072\n",
      "Epoch 16 -- Batch 534/ 842, training loss 0.34388256072998047\n",
      "Epoch 16 -- Batch 535/ 842, training loss 0.34973207116127014\n",
      "Epoch 16 -- Batch 536/ 842, training loss 0.3395145833492279\n",
      "Epoch 16 -- Batch 537/ 842, training loss 0.3626856207847595\n",
      "Epoch 16 -- Batch 538/ 842, training loss 0.35160884261131287\n",
      "Epoch 16 -- Batch 539/ 842, training loss 0.3556804656982422\n",
      "Epoch 16 -- Batch 540/ 842, training loss 0.3406488299369812\n",
      "Epoch 16 -- Batch 541/ 842, training loss 0.34932154417037964\n",
      "Epoch 16 -- Batch 542/ 842, training loss 0.3446020781993866\n",
      "Epoch 16 -- Batch 543/ 842, training loss 0.33857762813568115\n",
      "Epoch 16 -- Batch 544/ 842, training loss 0.34888267517089844\n",
      "Epoch 16 -- Batch 545/ 842, training loss 0.35998377203941345\n",
      "Epoch 16 -- Batch 546/ 842, training loss 0.3478279411792755\n",
      "Epoch 16 -- Batch 547/ 842, training loss 0.34348952770233154\n",
      "Epoch 16 -- Batch 548/ 842, training loss 0.3562548756599426\n",
      "Epoch 16 -- Batch 549/ 842, training loss 0.3451785147190094\n",
      "Epoch 16 -- Batch 550/ 842, training loss 0.3479618430137634\n",
      "Epoch 16 -- Batch 551/ 842, training loss 0.3519957661628723\n",
      "Epoch 16 -- Batch 552/ 842, training loss 0.3503391146659851\n",
      "Epoch 16 -- Batch 553/ 842, training loss 0.3465840518474579\n",
      "Epoch 16 -- Batch 554/ 842, training loss 0.3505168557167053\n",
      "Epoch 16 -- Batch 555/ 842, training loss 0.34799623489379883\n",
      "Epoch 16 -- Batch 556/ 842, training loss 0.3501409888267517\n",
      "Epoch 16 -- Batch 557/ 842, training loss 0.34667930006980896\n",
      "Epoch 16 -- Batch 558/ 842, training loss 0.357186496257782\n",
      "Epoch 16 -- Batch 559/ 842, training loss 0.344781756401062\n",
      "Epoch 16 -- Batch 560/ 842, training loss 0.3452986180782318\n",
      "Epoch 16 -- Batch 561/ 842, training loss 0.3484336733818054\n",
      "Epoch 16 -- Batch 562/ 842, training loss 0.35038837790489197\n",
      "Epoch 16 -- Batch 563/ 842, training loss 0.3521316945552826\n",
      "Epoch 16 -- Batch 564/ 842, training loss 0.35007691383361816\n",
      "Epoch 16 -- Batch 565/ 842, training loss 0.34985432028770447\n",
      "Epoch 16 -- Batch 566/ 842, training loss 0.3465418517589569\n",
      "Epoch 16 -- Batch 567/ 842, training loss 0.3511008620262146\n",
      "Epoch 16 -- Batch 568/ 842, training loss 0.3413046598434448\n",
      "Epoch 16 -- Batch 569/ 842, training loss 0.3415178060531616\n",
      "Epoch 16 -- Batch 570/ 842, training loss 0.3479423522949219\n",
      "Epoch 16 -- Batch 571/ 842, training loss 0.3473542332649231\n",
      "Epoch 16 -- Batch 572/ 842, training loss 0.34238553047180176\n",
      "Epoch 16 -- Batch 573/ 842, training loss 0.3481524586677551\n",
      "Epoch 16 -- Batch 574/ 842, training loss 0.3416518568992615\n",
      "Epoch 16 -- Batch 575/ 842, training loss 0.35679343342781067\n",
      "Epoch 16 -- Batch 576/ 842, training loss 0.34351131319999695\n",
      "Epoch 16 -- Batch 577/ 842, training loss 0.3481374680995941\n",
      "Epoch 16 -- Batch 578/ 842, training loss 0.3524937629699707\n",
      "Epoch 16 -- Batch 579/ 842, training loss 0.37518590688705444\n",
      "Epoch 16 -- Batch 580/ 842, training loss 0.34843218326568604\n",
      "Epoch 16 -- Batch 581/ 842, training loss 0.34716182947158813\n",
      "Epoch 16 -- Batch 582/ 842, training loss 0.3547256588935852\n",
      "Epoch 16 -- Batch 583/ 842, training loss 0.3512556254863739\n",
      "Epoch 16 -- Batch 584/ 842, training loss 0.3462390899658203\n",
      "Epoch 16 -- Batch 585/ 842, training loss 0.35670900344848633\n",
      "Epoch 16 -- Batch 586/ 842, training loss 0.3446151614189148\n",
      "Epoch 16 -- Batch 587/ 842, training loss 0.3473050594329834\n",
      "Epoch 16 -- Batch 588/ 842, training loss 0.34388282895088196\n",
      "Epoch 16 -- Batch 589/ 842, training loss 0.35091304779052734\n",
      "Epoch 16 -- Batch 590/ 842, training loss 0.3338335454463959\n",
      "Epoch 16 -- Batch 591/ 842, training loss 0.3371239900588989\n",
      "Epoch 16 -- Batch 592/ 842, training loss 0.3601040542125702\n",
      "Epoch 16 -- Batch 593/ 842, training loss 0.34438735246658325\n",
      "Epoch 16 -- Batch 594/ 842, training loss 0.3550316393375397\n",
      "Epoch 16 -- Batch 595/ 842, training loss 0.3508368134498596\n",
      "Epoch 16 -- Batch 596/ 842, training loss 0.3331189751625061\n",
      "Epoch 16 -- Batch 597/ 842, training loss 0.34056898951530457\n",
      "Epoch 16 -- Batch 598/ 842, training loss 0.34487617015838623\n",
      "Epoch 16 -- Batch 599/ 842, training loss 0.34715867042541504\n",
      "Epoch 16 -- Batch 600/ 842, training loss 0.3473190665245056\n",
      "Epoch 16 -- Batch 601/ 842, training loss 0.3450222909450531\n",
      "Epoch 16 -- Batch 602/ 842, training loss 0.3623003661632538\n",
      "Epoch 16 -- Batch 603/ 842, training loss 0.3484877943992615\n",
      "Epoch 16 -- Batch 604/ 842, training loss 0.33363574743270874\n",
      "Epoch 16 -- Batch 605/ 842, training loss 0.33960992097854614\n",
      "Epoch 16 -- Batch 606/ 842, training loss 0.34738728404045105\n",
      "Epoch 16 -- Batch 607/ 842, training loss 0.3400691747665405\n",
      "Epoch 16 -- Batch 608/ 842, training loss 0.33814123272895813\n",
      "Epoch 16 -- Batch 609/ 842, training loss 0.34401634335517883\n",
      "Epoch 16 -- Batch 610/ 842, training loss 0.3328933119773865\n",
      "Epoch 16 -- Batch 611/ 842, training loss 0.3606767952442169\n",
      "Epoch 16 -- Batch 612/ 842, training loss 0.33730804920196533\n",
      "Epoch 16 -- Batch 613/ 842, training loss 0.3370317220687866\n",
      "Epoch 16 -- Batch 614/ 842, training loss 0.337189644575119\n",
      "Epoch 16 -- Batch 615/ 842, training loss 0.34278708696365356\n",
      "Epoch 16 -- Batch 616/ 842, training loss 0.3484916687011719\n",
      "Epoch 16 -- Batch 617/ 842, training loss 0.3587237000465393\n",
      "Epoch 16 -- Batch 618/ 842, training loss 0.34626010060310364\n",
      "Epoch 16 -- Batch 619/ 842, training loss 0.36134201288223267\n",
      "Epoch 16 -- Batch 620/ 842, training loss 0.34843653440475464\n",
      "Epoch 16 -- Batch 621/ 842, training loss 0.3397018015384674\n",
      "Epoch 16 -- Batch 622/ 842, training loss 0.33497676253318787\n",
      "Epoch 16 -- Batch 623/ 842, training loss 0.36095669865608215\n",
      "Epoch 16 -- Batch 624/ 842, training loss 0.34297749400138855\n",
      "Epoch 16 -- Batch 625/ 842, training loss 0.34559524059295654\n",
      "Epoch 16 -- Batch 626/ 842, training loss 0.35842281579971313\n",
      "Epoch 16 -- Batch 627/ 842, training loss 0.345176100730896\n",
      "Epoch 16 -- Batch 628/ 842, training loss 0.3491804003715515\n",
      "Epoch 16 -- Batch 629/ 842, training loss 0.35547223687171936\n",
      "Epoch 16 -- Batch 630/ 842, training loss 0.3456919491291046\n",
      "Epoch 16 -- Batch 631/ 842, training loss 0.3542354702949524\n",
      "Epoch 16 -- Batch 632/ 842, training loss 0.35747477412223816\n",
      "Epoch 16 -- Batch 633/ 842, training loss 0.350793719291687\n",
      "Epoch 16 -- Batch 634/ 842, training loss 0.3429463803768158\n",
      "Epoch 16 -- Batch 635/ 842, training loss 0.3416782021522522\n",
      "Epoch 16 -- Batch 636/ 842, training loss 0.33317112922668457\n",
      "Epoch 16 -- Batch 637/ 842, training loss 0.3457488715648651\n",
      "Epoch 16 -- Batch 638/ 842, training loss 0.3498653173446655\n",
      "Epoch 16 -- Batch 639/ 842, training loss 0.340965211391449\n",
      "Epoch 16 -- Batch 640/ 842, training loss 0.3494213819503784\n",
      "Epoch 16 -- Batch 641/ 842, training loss 0.3493023216724396\n",
      "Epoch 16 -- Batch 642/ 842, training loss 0.3531073331832886\n",
      "Epoch 16 -- Batch 643/ 842, training loss 0.3343769907951355\n",
      "Epoch 16 -- Batch 644/ 842, training loss 0.33937522768974304\n",
      "Epoch 16 -- Batch 645/ 842, training loss 0.3621639907360077\n",
      "Epoch 16 -- Batch 646/ 842, training loss 0.3388647437095642\n",
      "Epoch 16 -- Batch 647/ 842, training loss 0.35071510076522827\n",
      "Epoch 16 -- Batch 648/ 842, training loss 0.3410540223121643\n",
      "Epoch 16 -- Batch 649/ 842, training loss 0.3582140803337097\n",
      "Epoch 16 -- Batch 650/ 842, training loss 0.3491457998752594\n",
      "Epoch 16 -- Batch 651/ 842, training loss 0.3576883375644684\n",
      "Epoch 16 -- Batch 652/ 842, training loss 0.34305739402770996\n",
      "Epoch 16 -- Batch 653/ 842, training loss 0.3513597846031189\n",
      "Epoch 16 -- Batch 654/ 842, training loss 0.3384704291820526\n",
      "Epoch 16 -- Batch 655/ 842, training loss 0.34703198075294495\n",
      "Epoch 16 -- Batch 656/ 842, training loss 0.3448431193828583\n",
      "Epoch 16 -- Batch 657/ 842, training loss 0.3543887734413147\n",
      "Epoch 16 -- Batch 658/ 842, training loss 0.3326871693134308\n",
      "Epoch 16 -- Batch 659/ 842, training loss 0.35386595129966736\n",
      "Epoch 16 -- Batch 660/ 842, training loss 0.3551110327243805\n",
      "Epoch 16 -- Batch 661/ 842, training loss 0.34455156326293945\n",
      "Epoch 16 -- Batch 662/ 842, training loss 0.35886096954345703\n",
      "Epoch 16 -- Batch 663/ 842, training loss 0.34813007712364197\n",
      "Epoch 16 -- Batch 664/ 842, training loss 0.3563728928565979\n",
      "Epoch 16 -- Batch 665/ 842, training loss 0.3349894881248474\n",
      "Epoch 16 -- Batch 666/ 842, training loss 0.35487276315689087\n",
      "Epoch 16 -- Batch 667/ 842, training loss 0.34390637278556824\n",
      "Epoch 16 -- Batch 668/ 842, training loss 0.3401965796947479\n",
      "Epoch 16 -- Batch 669/ 842, training loss 0.3457202911376953\n",
      "Epoch 16 -- Batch 670/ 842, training loss 0.3482186794281006\n",
      "Epoch 16 -- Batch 671/ 842, training loss 0.3614027798175812\n",
      "Epoch 16 -- Batch 672/ 842, training loss 0.3556267321109772\n",
      "Epoch 16 -- Batch 673/ 842, training loss 0.34126806259155273\n",
      "Epoch 16 -- Batch 674/ 842, training loss 0.3441332280635834\n",
      "Epoch 16 -- Batch 675/ 842, training loss 0.32858288288116455\n",
      "Epoch 16 -- Batch 676/ 842, training loss 0.3386794924736023\n",
      "Epoch 16 -- Batch 677/ 842, training loss 0.34270110726356506\n",
      "Epoch 16 -- Batch 678/ 842, training loss 0.34845390915870667\n",
      "Epoch 16 -- Batch 679/ 842, training loss 0.35633331537246704\n",
      "Epoch 16 -- Batch 680/ 842, training loss 0.3488611578941345\n",
      "Epoch 16 -- Batch 681/ 842, training loss 0.35667315125465393\n",
      "Epoch 16 -- Batch 682/ 842, training loss 0.3350811004638672\n",
      "Epoch 16 -- Batch 683/ 842, training loss 0.3423117697238922\n",
      "Epoch 16 -- Batch 684/ 842, training loss 0.34284958243370056\n",
      "Epoch 16 -- Batch 685/ 842, training loss 0.3435555696487427\n",
      "Epoch 16 -- Batch 686/ 842, training loss 0.3625071048736572\n",
      "Epoch 16 -- Batch 687/ 842, training loss 0.34547847509384155\n",
      "Epoch 16 -- Batch 688/ 842, training loss 0.3558724820613861\n",
      "Epoch 16 -- Batch 689/ 842, training loss 0.3536057472229004\n",
      "Epoch 16 -- Batch 690/ 842, training loss 0.33816829323768616\n",
      "Epoch 16 -- Batch 691/ 842, training loss 0.34605708718299866\n",
      "Epoch 16 -- Batch 692/ 842, training loss 0.34863340854644775\n",
      "Epoch 16 -- Batch 693/ 842, training loss 0.3373047411441803\n",
      "Epoch 16 -- Batch 694/ 842, training loss 0.34244924783706665\n",
      "Epoch 16 -- Batch 695/ 842, training loss 0.3426884114742279\n",
      "Epoch 16 -- Batch 696/ 842, training loss 0.3516000807285309\n",
      "Epoch 16 -- Batch 697/ 842, training loss 0.3523581624031067\n",
      "Epoch 16 -- Batch 698/ 842, training loss 0.3405841290950775\n",
      "Epoch 16 -- Batch 699/ 842, training loss 0.33613741397857666\n",
      "Epoch 16 -- Batch 700/ 842, training loss 0.36603081226348877\n",
      "Epoch 16 -- Batch 701/ 842, training loss 0.3538263142108917\n",
      "Epoch 16 -- Batch 702/ 842, training loss 0.3565176725387573\n",
      "Epoch 16 -- Batch 703/ 842, training loss 0.3471939265727997\n",
      "Epoch 16 -- Batch 704/ 842, training loss 0.343350887298584\n",
      "Epoch 16 -- Batch 705/ 842, training loss 0.34382250905036926\n",
      "Epoch 16 -- Batch 706/ 842, training loss 0.34064778685569763\n",
      "Epoch 16 -- Batch 707/ 842, training loss 0.35408592224121094\n",
      "Epoch 16 -- Batch 708/ 842, training loss 0.33857908844947815\n",
      "Epoch 16 -- Batch 709/ 842, training loss 0.3416230380535126\n",
      "Epoch 16 -- Batch 710/ 842, training loss 0.34885692596435547\n",
      "Epoch 16 -- Batch 711/ 842, training loss 0.34881898760795593\n",
      "Epoch 16 -- Batch 712/ 842, training loss 0.35388514399528503\n",
      "Epoch 16 -- Batch 713/ 842, training loss 0.32756510376930237\n",
      "Epoch 16 -- Batch 714/ 842, training loss 0.35279086232185364\n",
      "Epoch 16 -- Batch 715/ 842, training loss 0.3407629728317261\n",
      "Epoch 16 -- Batch 716/ 842, training loss 0.34884223341941833\n",
      "Epoch 16 -- Batch 717/ 842, training loss 0.3392927944660187\n",
      "Epoch 16 -- Batch 718/ 842, training loss 0.34788277745246887\n",
      "Epoch 16 -- Batch 719/ 842, training loss 0.34770700335502625\n",
      "Epoch 16 -- Batch 720/ 842, training loss 0.3460346460342407\n",
      "Epoch 16 -- Batch 721/ 842, training loss 0.3588843047618866\n",
      "Epoch 16 -- Batch 722/ 842, training loss 0.3387090265750885\n",
      "Epoch 16 -- Batch 723/ 842, training loss 0.34855449199676514\n",
      "Epoch 16 -- Batch 724/ 842, training loss 0.3616342842578888\n",
      "Epoch 16 -- Batch 725/ 842, training loss 0.34074103832244873\n",
      "Epoch 16 -- Batch 726/ 842, training loss 0.34312573075294495\n",
      "Epoch 16 -- Batch 727/ 842, training loss 0.3348446190357208\n",
      "Epoch 16 -- Batch 728/ 842, training loss 0.3510691225528717\n",
      "Epoch 16 -- Batch 729/ 842, training loss 0.3373135030269623\n",
      "Epoch 16 -- Batch 730/ 842, training loss 0.34604910016059875\n",
      "Epoch 16 -- Batch 731/ 842, training loss 0.33682015538215637\n",
      "Epoch 16 -- Batch 732/ 842, training loss 0.3632611036300659\n",
      "Epoch 16 -- Batch 733/ 842, training loss 0.3543124496936798\n",
      "Epoch 16 -- Batch 734/ 842, training loss 0.36203351616859436\n",
      "Epoch 16 -- Batch 735/ 842, training loss 0.3409797251224518\n",
      "Epoch 16 -- Batch 736/ 842, training loss 0.33746397495269775\n",
      "Epoch 16 -- Batch 737/ 842, training loss 0.3502296507358551\n",
      "Epoch 16 -- Batch 738/ 842, training loss 0.35969454050064087\n",
      "Epoch 16 -- Batch 739/ 842, training loss 0.3700377941131592\n",
      "Epoch 16 -- Batch 740/ 842, training loss 0.34814339876174927\n",
      "Epoch 16 -- Batch 741/ 842, training loss 0.3512967526912689\n",
      "Epoch 16 -- Batch 742/ 842, training loss 0.32920217514038086\n",
      "Epoch 16 -- Batch 743/ 842, training loss 0.362991601228714\n",
      "Epoch 16 -- Batch 744/ 842, training loss 0.3535503149032593\n",
      "Epoch 16 -- Batch 745/ 842, training loss 0.3502499461174011\n",
      "Epoch 16 -- Batch 746/ 842, training loss 0.3505449891090393\n",
      "Epoch 16 -- Batch 747/ 842, training loss 0.35730257630348206\n",
      "Epoch 16 -- Batch 748/ 842, training loss 0.33909058570861816\n",
      "Epoch 16 -- Batch 749/ 842, training loss 0.3476457893848419\n",
      "Epoch 16 -- Batch 750/ 842, training loss 0.34156227111816406\n",
      "Epoch 16 -- Batch 751/ 842, training loss 0.34609079360961914\n",
      "Epoch 16 -- Batch 752/ 842, training loss 0.3374650180339813\n",
      "Epoch 16 -- Batch 753/ 842, training loss 0.34738361835479736\n",
      "Epoch 16 -- Batch 754/ 842, training loss 0.3538392186164856\n",
      "Epoch 16 -- Batch 755/ 842, training loss 0.34334424138069153\n",
      "Epoch 16 -- Batch 756/ 842, training loss 0.3535009026527405\n",
      "Epoch 16 -- Batch 757/ 842, training loss 0.34479600191116333\n",
      "Epoch 16 -- Batch 758/ 842, training loss 0.330206960439682\n",
      "Epoch 16 -- Batch 759/ 842, training loss 0.3378508985042572\n",
      "Epoch 16 -- Batch 760/ 842, training loss 0.35850977897644043\n",
      "Epoch 16 -- Batch 761/ 842, training loss 0.3408079743385315\n",
      "Epoch 16 -- Batch 762/ 842, training loss 0.366639107465744\n",
      "Epoch 16 -- Batch 763/ 842, training loss 0.3597637712955475\n",
      "Epoch 16 -- Batch 764/ 842, training loss 0.34582963585853577\n",
      "Epoch 16 -- Batch 765/ 842, training loss 0.35137122869491577\n",
      "Epoch 16 -- Batch 766/ 842, training loss 0.3582502007484436\n",
      "Epoch 16 -- Batch 767/ 842, training loss 0.3457925617694855\n",
      "Epoch 16 -- Batch 768/ 842, training loss 0.3618195056915283\n",
      "Epoch 16 -- Batch 769/ 842, training loss 0.3403085470199585\n",
      "Epoch 16 -- Batch 770/ 842, training loss 0.35201695561408997\n",
      "Epoch 16 -- Batch 771/ 842, training loss 0.34657543897628784\n",
      "Epoch 16 -- Batch 772/ 842, training loss 0.35940322279930115\n",
      "Epoch 16 -- Batch 773/ 842, training loss 0.35353997349739075\n",
      "Epoch 16 -- Batch 774/ 842, training loss 0.3489009141921997\n",
      "Epoch 16 -- Batch 775/ 842, training loss 0.35728681087493896\n",
      "Epoch 16 -- Batch 776/ 842, training loss 0.3572368919849396\n",
      "Epoch 16 -- Batch 777/ 842, training loss 0.3514426648616791\n",
      "Epoch 16 -- Batch 778/ 842, training loss 0.3522092401981354\n",
      "Epoch 16 -- Batch 779/ 842, training loss 0.34743204712867737\n",
      "Epoch 16 -- Batch 780/ 842, training loss 0.33880615234375\n",
      "Epoch 16 -- Batch 781/ 842, training loss 0.35581982135772705\n",
      "Epoch 16 -- Batch 782/ 842, training loss 0.35265156626701355\n",
      "Epoch 16 -- Batch 783/ 842, training loss 0.35371828079223633\n",
      "Epoch 16 -- Batch 784/ 842, training loss 0.3543066680431366\n",
      "Epoch 16 -- Batch 785/ 842, training loss 0.3435860872268677\n",
      "Epoch 16 -- Batch 786/ 842, training loss 0.3553481698036194\n",
      "Epoch 16 -- Batch 787/ 842, training loss 0.34771406650543213\n",
      "Epoch 16 -- Batch 788/ 842, training loss 0.3491817116737366\n",
      "Epoch 16 -- Batch 789/ 842, training loss 0.3656301498413086\n",
      "Epoch 16 -- Batch 790/ 842, training loss 0.35516026616096497\n",
      "Epoch 16 -- Batch 791/ 842, training loss 0.34985825419425964\n",
      "Epoch 16 -- Batch 792/ 842, training loss 0.3520633280277252\n",
      "Epoch 16 -- Batch 793/ 842, training loss 0.3487280607223511\n",
      "Epoch 16 -- Batch 794/ 842, training loss 0.3479461967945099\n",
      "Epoch 16 -- Batch 795/ 842, training loss 0.34817004203796387\n",
      "Epoch 16 -- Batch 796/ 842, training loss 0.3414008319377899\n",
      "Epoch 16 -- Batch 797/ 842, training loss 0.3459903597831726\n",
      "Epoch 16 -- Batch 798/ 842, training loss 0.35465753078460693\n",
      "Epoch 16 -- Batch 799/ 842, training loss 0.34522104263305664\n",
      "Epoch 16 -- Batch 800/ 842, training loss 0.35104823112487793\n",
      "Epoch 16 -- Batch 801/ 842, training loss 0.34533578157424927\n",
      "Epoch 16 -- Batch 802/ 842, training loss 0.3491193652153015\n",
      "Epoch 16 -- Batch 803/ 842, training loss 0.3420775830745697\n",
      "Epoch 16 -- Batch 804/ 842, training loss 0.3486148715019226\n",
      "Epoch 16 -- Batch 805/ 842, training loss 0.3584604263305664\n",
      "Epoch 16 -- Batch 806/ 842, training loss 0.34619688987731934\n",
      "Epoch 16 -- Batch 807/ 842, training loss 0.3396076560020447\n",
      "Epoch 16 -- Batch 808/ 842, training loss 0.34486281871795654\n",
      "Epoch 16 -- Batch 809/ 842, training loss 0.34948667883872986\n",
      "Epoch 16 -- Batch 810/ 842, training loss 0.3557663559913635\n",
      "Epoch 16 -- Batch 811/ 842, training loss 0.34897366166114807\n",
      "Epoch 16 -- Batch 812/ 842, training loss 0.3536943793296814\n",
      "Epoch 16 -- Batch 813/ 842, training loss 0.3453854024410248\n",
      "Epoch 16 -- Batch 814/ 842, training loss 0.3414846956729889\n",
      "Epoch 16 -- Batch 815/ 842, training loss 0.33738139271736145\n",
      "Epoch 16 -- Batch 816/ 842, training loss 0.34845051169395447\n",
      "Epoch 16 -- Batch 817/ 842, training loss 0.3428383767604828\n",
      "Epoch 16 -- Batch 818/ 842, training loss 0.34004637598991394\n",
      "Epoch 16 -- Batch 819/ 842, training loss 0.3410884439945221\n",
      "Epoch 16 -- Batch 820/ 842, training loss 0.35326653718948364\n",
      "Epoch 16 -- Batch 821/ 842, training loss 0.34356561303138733\n",
      "Epoch 16 -- Batch 822/ 842, training loss 0.35443568229675293\n",
      "Epoch 16 -- Batch 823/ 842, training loss 0.34521952271461487\n",
      "Epoch 16 -- Batch 824/ 842, training loss 0.34453949332237244\n",
      "Epoch 16 -- Batch 825/ 842, training loss 0.3326788544654846\n",
      "Epoch 16 -- Batch 826/ 842, training loss 0.36005252599716187\n",
      "Epoch 16 -- Batch 827/ 842, training loss 0.35532331466674805\n",
      "Epoch 16 -- Batch 828/ 842, training loss 0.33884257078170776\n",
      "Epoch 16 -- Batch 829/ 842, training loss 0.345654159784317\n",
      "Epoch 16 -- Batch 830/ 842, training loss 0.34241944551467896\n",
      "Epoch 16 -- Batch 831/ 842, training loss 0.3423198163509369\n",
      "Epoch 16 -- Batch 832/ 842, training loss 0.3527580499649048\n",
      "Epoch 16 -- Batch 833/ 842, training loss 0.35327327251434326\n",
      "Epoch 16 -- Batch 834/ 842, training loss 0.3568360209465027\n",
      "Epoch 16 -- Batch 835/ 842, training loss 0.35977640748023987\n",
      "Epoch 16 -- Batch 836/ 842, training loss 0.3510321378707886\n",
      "Epoch 16 -- Batch 837/ 842, training loss 0.3335072994232178\n",
      "Epoch 16 -- Batch 838/ 842, training loss 0.3397430181503296\n",
      "Epoch 16 -- Batch 839/ 842, training loss 0.3499925434589386\n",
      "Epoch 16 -- Batch 840/ 842, training loss 0.3488590121269226\n",
      "Epoch 16 -- Batch 841/ 842, training loss 0.3429607152938843\n",
      "Epoch 16 -- Batch 842/ 842, training loss 0.3349304497241974\n",
      "----------------------------------------------------------------------\n",
      "Epoch 16 -- Batch 1/ 94, validation loss 0.33455830812454224\n",
      "Epoch 16 -- Batch 2/ 94, validation loss 0.3343605697154999\n",
      "Epoch 16 -- Batch 3/ 94, validation loss 0.3370390236377716\n",
      "Epoch 16 -- Batch 4/ 94, validation loss 0.3657744824886322\n",
      "Epoch 16 -- Batch 5/ 94, validation loss 0.34689953923225403\n",
      "Epoch 16 -- Batch 6/ 94, validation loss 0.34564563632011414\n",
      "Epoch 16 -- Batch 7/ 94, validation loss 0.3431699872016907\n",
      "Epoch 16 -- Batch 8/ 94, validation loss 0.3382796049118042\n",
      "Epoch 16 -- Batch 9/ 94, validation loss 0.3513612449169159\n",
      "Epoch 16 -- Batch 10/ 94, validation loss 0.3517252802848816\n",
      "Epoch 16 -- Batch 11/ 94, validation loss 0.3354547917842865\n",
      "Epoch 16 -- Batch 12/ 94, validation loss 0.3595271110534668\n",
      "Epoch 16 -- Batch 13/ 94, validation loss 0.33518823981285095\n",
      "Epoch 16 -- Batch 14/ 94, validation loss 0.3351840376853943\n",
      "Epoch 16 -- Batch 15/ 94, validation loss 0.33341753482818604\n",
      "Epoch 16 -- Batch 16/ 94, validation loss 0.35047975182533264\n",
      "Epoch 16 -- Batch 17/ 94, validation loss 0.34228941798210144\n",
      "Epoch 16 -- Batch 18/ 94, validation loss 0.34431707859039307\n",
      "Epoch 16 -- Batch 19/ 94, validation loss 0.3330053687095642\n",
      "Epoch 16 -- Batch 20/ 94, validation loss 0.3504600524902344\n",
      "Epoch 16 -- Batch 21/ 94, validation loss 0.35134974122047424\n",
      "Epoch 16 -- Batch 22/ 94, validation loss 0.34657174348831177\n",
      "Epoch 16 -- Batch 23/ 94, validation loss 0.3436981439590454\n",
      "Epoch 16 -- Batch 24/ 94, validation loss 0.35531145334243774\n",
      "Epoch 16 -- Batch 25/ 94, validation loss 0.351909339427948\n",
      "Epoch 16 -- Batch 26/ 94, validation loss 0.34191465377807617\n",
      "Epoch 16 -- Batch 27/ 94, validation loss 0.3395356833934784\n",
      "Epoch 16 -- Batch 28/ 94, validation loss 0.34751370549201965\n",
      "Epoch 16 -- Batch 29/ 94, validation loss 0.3456968367099762\n",
      "Epoch 16 -- Batch 30/ 94, validation loss 0.3340758979320526\n",
      "Epoch 16 -- Batch 31/ 94, validation loss 0.3307240307331085\n",
      "Epoch 16 -- Batch 32/ 94, validation loss 0.3488885164260864\n",
      "Epoch 16 -- Batch 33/ 94, validation loss 0.3335556089878082\n",
      "Epoch 16 -- Batch 34/ 94, validation loss 0.33972078561782837\n",
      "Epoch 16 -- Batch 35/ 94, validation loss 0.3328852951526642\n",
      "Epoch 16 -- Batch 36/ 94, validation loss 0.3321763575077057\n",
      "Epoch 16 -- Batch 37/ 94, validation loss 0.3368953466415405\n",
      "Epoch 16 -- Batch 38/ 94, validation loss 0.33479106426239014\n",
      "Epoch 16 -- Batch 39/ 94, validation loss 0.3367375135421753\n",
      "Epoch 16 -- Batch 40/ 94, validation loss 0.3292180895805359\n",
      "Epoch 16 -- Batch 41/ 94, validation loss 0.3489750027656555\n",
      "Epoch 16 -- Batch 42/ 94, validation loss 0.35446426272392273\n",
      "Epoch 16 -- Batch 43/ 94, validation loss 0.3403249979019165\n",
      "Epoch 16 -- Batch 44/ 94, validation loss 0.3462732136249542\n",
      "Epoch 16 -- Batch 45/ 94, validation loss 0.33487260341644287\n",
      "Epoch 16 -- Batch 46/ 94, validation loss 0.33897721767425537\n",
      "Epoch 16 -- Batch 47/ 94, validation loss 0.34131842851638794\n",
      "Epoch 16 -- Batch 48/ 94, validation loss 0.3413409888744354\n",
      "Epoch 16 -- Batch 49/ 94, validation loss 0.32513856887817383\n",
      "Epoch 16 -- Batch 50/ 94, validation loss 0.33819758892059326\n",
      "Epoch 16 -- Batch 51/ 94, validation loss 0.33558738231658936\n",
      "Epoch 16 -- Batch 52/ 94, validation loss 0.3372824490070343\n",
      "Epoch 16 -- Batch 53/ 94, validation loss 0.33777496218681335\n",
      "Epoch 16 -- Batch 54/ 94, validation loss 0.3215177059173584\n",
      "Epoch 16 -- Batch 55/ 94, validation loss 0.35055986046791077\n",
      "Epoch 16 -- Batch 56/ 94, validation loss 0.3511418402194977\n",
      "Epoch 16 -- Batch 57/ 94, validation loss 0.3490268588066101\n",
      "Epoch 16 -- Batch 58/ 94, validation loss 0.3273395895957947\n",
      "Epoch 16 -- Batch 59/ 94, validation loss 0.33265137672424316\n",
      "Epoch 16 -- Batch 60/ 94, validation loss 0.34757572412490845\n",
      "Epoch 16 -- Batch 61/ 94, validation loss 0.34132251143455505\n",
      "Epoch 16 -- Batch 62/ 94, validation loss 0.3471667468547821\n",
      "Epoch 16 -- Batch 63/ 94, validation loss 0.33174821734428406\n",
      "Epoch 16 -- Batch 64/ 94, validation loss 0.35478752851486206\n",
      "Epoch 16 -- Batch 65/ 94, validation loss 0.34335818886756897\n",
      "Epoch 16 -- Batch 66/ 94, validation loss 0.37629690766334534\n",
      "Epoch 16 -- Batch 67/ 94, validation loss 0.35098448395729065\n",
      "Epoch 16 -- Batch 68/ 94, validation loss 0.3431726098060608\n",
      "Epoch 16 -- Batch 69/ 94, validation loss 0.340570867061615\n",
      "Epoch 16 -- Batch 70/ 94, validation loss 0.33558765053749084\n",
      "Epoch 16 -- Batch 71/ 94, validation loss 0.3367375433444977\n",
      "Epoch 16 -- Batch 72/ 94, validation loss 0.32922905683517456\n",
      "Epoch 16 -- Batch 73/ 94, validation loss 0.34169140458106995\n",
      "Epoch 16 -- Batch 74/ 94, validation loss 0.34098514914512634\n",
      "Epoch 16 -- Batch 75/ 94, validation loss 0.33856454491615295\n",
      "Epoch 16 -- Batch 76/ 94, validation loss 0.35451236367225647\n",
      "Epoch 16 -- Batch 77/ 94, validation loss 0.33033424615859985\n",
      "Epoch 16 -- Batch 78/ 94, validation loss 0.34091436862945557\n",
      "Epoch 16 -- Batch 79/ 94, validation loss 0.3277013301849365\n",
      "Epoch 16 -- Batch 80/ 94, validation loss 0.3403448164463043\n",
      "Epoch 16 -- Batch 81/ 94, validation loss 0.33716464042663574\n",
      "Epoch 16 -- Batch 82/ 94, validation loss 0.32467833161354065\n",
      "Epoch 16 -- Batch 83/ 94, validation loss 0.33608946204185486\n",
      "Epoch 16 -- Batch 84/ 94, validation loss 0.33639177680015564\n",
      "Epoch 16 -- Batch 85/ 94, validation loss 0.3371511995792389\n",
      "Epoch 16 -- Batch 86/ 94, validation loss 0.3229546844959259\n",
      "Epoch 16 -- Batch 87/ 94, validation loss 0.3454876244068146\n",
      "Epoch 16 -- Batch 88/ 94, validation loss 0.3548695743083954\n",
      "Epoch 16 -- Batch 89/ 94, validation loss 0.33622705936431885\n",
      "Epoch 16 -- Batch 90/ 94, validation loss 0.33270496129989624\n",
      "Epoch 16 -- Batch 91/ 94, validation loss 0.3355885148048401\n",
      "Epoch 16 -- Batch 92/ 94, validation loss 0.33988621830940247\n",
      "Epoch 16 -- Batch 93/ 94, validation loss 0.3398055136203766\n",
      "Epoch 16 -- Batch 94/ 94, validation loss 0.37035587430000305\n",
      "----------------------------------------------------------------------\n",
      "Epoch 16 loss: Training 0.34752193093299866, Validation 0.37035587430000305\n",
      "----------------------------------------------------------------------\n",
      "Epoch 17/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 3 4 8 9 21 22 23\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 6 13 15 26 27 28 31\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CC1(C)Cc2c(sc(NC(=O)c3ccccc3Cl)c2=O)C2(C)C'\n",
      "[19:06:29] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(C(=O)Oc2ccc(/C=C3/SC(=S)N(CC(=O)O)C3=O)cc2=O'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 13 14 15\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 19\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'O=C1CC(c2ccccc2)Nc2nc3cc4ccccc-n3c22'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 11 13 14 19 20 21 22\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 22 23\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 16 29\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'Cc1csc2nc(CNS(=O)(=O)c3ccc4c4c(cccc35)CC3)cn12'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CN[C@H]2C[C@@H](C(=O)NC3CCCC3)N(Cc3ccc4c(c3)OCO4)C23)n1'\n",
      "[19:06:29] SMILES Parse Error: extra open parentheses for input: 'O=C(NC(C(=O)OCC(=O)N1CCN(C(=O)c2ccco2)CC1)c1ccc(F)cc1'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCNC(=O)C2CCCN(c3ncnc4c3nc3n4CCCCC4)C2)cc1OC'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 1 2 3 23 27\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 10 11 12 17 18\n",
      "[19:06:29] SMILES Parse Error: extra open parentheses for input: 'CCOC(=O)C(C#NC(C)=O)(C(=O)OCC(=O)c1c(N)n(C)c(=O)n(C)c1=O'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'O=C1COc2ccc(C(=O)c3ccc(Oc4ccccc4)cc3[N+]22)cc1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 8 12 19 20\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'c1coc(CNc2ccnc3c(-c4ccc5[nH]ccc55)cnc23)c1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 7 8 10 11 12 17 18 19 22\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'N=C1/C(=C/c2ccc(SCc3ccccc3)c([N+](=O)[O-])c2)C(=O)N=C2SC1CN=Cc1ccc(c2ccccc2)o1'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'O=S(=O)(c1ccccc1)N1CCSC1c1cccc(-c2cccc3c2)c1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 27\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CN1C[C@H]c2[C@@H](n1c1ccc(C#N)cc13)[C@@H]2CO'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 2 7 8 15 16\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CC(C)CCn1c(C(=O)O)c(C2CC2)nc1ccc(F)cc1'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CCCS(=O)(=O)N1Cc(cc(OC)nc1-c2cccc(C(F)(F)F)c2)c2ccccc12'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 23 24 25 26 27 28 35 36 37\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 7 21\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'O=c1c2c3c(sc2ncn1Cc1cc(Cl)ccc1Cl)C1C(=O)c2ccccc2CN1Cc1ccccc1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 8 9 10 23 24\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'O=C(N/N=C/c1ccc(F)cc1)C(CSc1ccccc1)N2C'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 30 31 32 33 34\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c3c([nH]c2c1)[C@@H](CO)N(Cc1ccccn1)CC31CCN(C(=O)[C@H]C2)C1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 13 14 15 17 22 23 26\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CN1c2ccc(C#Cc3cccnc3)cc2[C@@H]2[C@H](CCN3C(=O)CN2CCOCC2)[C@@H]1CO'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 8 9 10 12 13 14 15 16 17\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 15 18 19\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(NC(=O)C2c3ccccc3C(=O)N3c2cccnc2)cc1'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CCOCCCNc1nc2c(O1)ccccc1C(=O)N(C)C'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 5 6 21 22 27\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 3 4 9 16 23\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 14 15 16 17 18 19 20\n",
      "[19:06:29] SMILES Parse Error: syntax error while parsing: CCc1ccc(N/C(SCc2cccc(Cl)=)Cl)[N+](=O)[O-])cc1\n",
      "[19:06:29] SMILES Parse Error: Failed parsing SMILES 'CCc1ccc(N/C(SCc2cccc(Cl)=)Cl)[N+](=O)[O-])cc1' for input: 'CCc1ccc(N/C(SCc2cccc(Cl)=)Cl)[N+](=O)[O-])cc1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 1 2 16 17 18 19 20\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(-c2cn3nc(-c4ccc(Br)cc4)n4c3c2ncn3C2CCCCC2)cc1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 2 3 18 28 29\n",
      "[19:06:29] SMILES Parse Error: ring closure 6 duplicates bond between atom 20 and atom 21 for input: 'Cc1ncn(-c2ccc(Nc3ncc4cccc(-c5cc6c6nnn6-c6ccccc5)c4n3)cc2)n1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 6 7 20\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)[C@H]2[C@H](C3CC3)C3c4ccc(-c5cccc(F)c5)cc4C(=O)[C@@H]3[C@@H]1C4'\n",
      "[19:06:29] SMILES Parse Error: unclosed ring for input: 'Cc1csc(NC(=O)CSc2nnc3n(C(C)C)c3ccccc32)n1'\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 31 32\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 9 10 11\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 16 17 18\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 10 11 22 23 24 25 26\n",
      "[19:06:29] Can't kekulize mol.  Unkekulized atoms: 4 9 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 -- Batch 1/ 842, training loss 0.34768131375312805\n",
      "Epoch 17 -- Batch 2/ 842, training loss 0.35069727897644043\n",
      "Epoch 17 -- Batch 3/ 842, training loss 0.3368029296398163\n",
      "Epoch 17 -- Batch 4/ 842, training loss 0.35167649388313293\n",
      "Epoch 17 -- Batch 5/ 842, training loss 0.342470645904541\n",
      "Epoch 17 -- Batch 6/ 842, training loss 0.3417949378490448\n",
      "Epoch 17 -- Batch 7/ 842, training loss 0.3506258726119995\n",
      "Epoch 17 -- Batch 8/ 842, training loss 0.3395083546638489\n",
      "Epoch 17 -- Batch 9/ 842, training loss 0.3436502516269684\n",
      "Epoch 17 -- Batch 10/ 842, training loss 0.3340004086494446\n",
      "Epoch 17 -- Batch 11/ 842, training loss 0.3476896584033966\n",
      "Epoch 17 -- Batch 12/ 842, training loss 0.33771029114723206\n",
      "Epoch 17 -- Batch 13/ 842, training loss 0.34038203954696655\n",
      "Epoch 17 -- Batch 14/ 842, training loss 0.34130024909973145\n",
      "Epoch 17 -- Batch 15/ 842, training loss 0.3444267511367798\n",
      "Epoch 17 -- Batch 16/ 842, training loss 0.3422892093658447\n",
      "Epoch 17 -- Batch 17/ 842, training loss 0.33488571643829346\n",
      "Epoch 17 -- Batch 18/ 842, training loss 0.3562471568584442\n",
      "Epoch 17 -- Batch 19/ 842, training loss 0.33514657616615295\n",
      "Epoch 17 -- Batch 20/ 842, training loss 0.33305391669273376\n",
      "Epoch 17 -- Batch 21/ 842, training loss 0.31316936016082764\n",
      "Epoch 17 -- Batch 22/ 842, training loss 0.34026896953582764\n",
      "Epoch 17 -- Batch 23/ 842, training loss 0.33526358008384705\n",
      "Epoch 17 -- Batch 24/ 842, training loss 0.34672534465789795\n",
      "Epoch 17 -- Batch 25/ 842, training loss 0.3364444375038147\n",
      "Epoch 17 -- Batch 26/ 842, training loss 0.3562745451927185\n",
      "Epoch 17 -- Batch 27/ 842, training loss 0.34938129782676697\n",
      "Epoch 17 -- Batch 28/ 842, training loss 0.33587825298309326\n",
      "Epoch 17 -- Batch 29/ 842, training loss 0.33663082122802734\n",
      "Epoch 17 -- Batch 30/ 842, training loss 0.3271441161632538\n",
      "Epoch 17 -- Batch 31/ 842, training loss 0.32808926701545715\n",
      "Epoch 17 -- Batch 32/ 842, training loss 0.3412599265575409\n",
      "Epoch 17 -- Batch 33/ 842, training loss 0.33819881081581116\n",
      "Epoch 17 -- Batch 34/ 842, training loss 0.3291035592556\n",
      "Epoch 17 -- Batch 35/ 842, training loss 0.3441520929336548\n",
      "Epoch 17 -- Batch 36/ 842, training loss 0.34786224365234375\n",
      "Epoch 17 -- Batch 37/ 842, training loss 0.33845922350883484\n",
      "Epoch 17 -- Batch 38/ 842, training loss 0.34024664759635925\n",
      "Epoch 17 -- Batch 39/ 842, training loss 0.34214717149734497\n",
      "Epoch 17 -- Batch 40/ 842, training loss 0.34010857343673706\n",
      "Epoch 17 -- Batch 41/ 842, training loss 0.3542149066925049\n",
      "Epoch 17 -- Batch 42/ 842, training loss 0.34477001428604126\n",
      "Epoch 17 -- Batch 43/ 842, training loss 0.333702027797699\n",
      "Epoch 17 -- Batch 44/ 842, training loss 0.3522522449493408\n",
      "Epoch 17 -- Batch 45/ 842, training loss 0.35179975628852844\n",
      "Epoch 17 -- Batch 46/ 842, training loss 0.33850187063217163\n",
      "Epoch 17 -- Batch 47/ 842, training loss 0.3260965347290039\n",
      "Epoch 17 -- Batch 48/ 842, training loss 0.3400510251522064\n",
      "Epoch 17 -- Batch 49/ 842, training loss 0.34648022055625916\n",
      "Epoch 17 -- Batch 50/ 842, training loss 0.3568892180919647\n",
      "Epoch 17 -- Batch 51/ 842, training loss 0.33281436562538147\n",
      "Epoch 17 -- Batch 52/ 842, training loss 0.328080415725708\n",
      "Epoch 17 -- Batch 53/ 842, training loss 0.3364242911338806\n",
      "Epoch 17 -- Batch 54/ 842, training loss 0.3375258445739746\n",
      "Epoch 17 -- Batch 55/ 842, training loss 0.3328794240951538\n",
      "Epoch 17 -- Batch 56/ 842, training loss 0.3349563777446747\n",
      "Epoch 17 -- Batch 57/ 842, training loss 0.3400692641735077\n",
      "Epoch 17 -- Batch 58/ 842, training loss 0.3416305184364319\n",
      "Epoch 17 -- Batch 59/ 842, training loss 0.3480062484741211\n",
      "Epoch 17 -- Batch 60/ 842, training loss 0.3570643961429596\n",
      "Epoch 17 -- Batch 61/ 842, training loss 0.3376285135746002\n",
      "Epoch 17 -- Batch 62/ 842, training loss 0.3385135233402252\n",
      "Epoch 17 -- Batch 63/ 842, training loss 0.3450780212879181\n",
      "Epoch 17 -- Batch 64/ 842, training loss 0.34539565443992615\n",
      "Epoch 17 -- Batch 65/ 842, training loss 0.33799415826797485\n",
      "Epoch 17 -- Batch 66/ 842, training loss 0.3426491618156433\n",
      "Epoch 17 -- Batch 67/ 842, training loss 0.3380507528781891\n",
      "Epoch 17 -- Batch 68/ 842, training loss 0.3372482657432556\n",
      "Epoch 17 -- Batch 69/ 842, training loss 0.3294282555580139\n",
      "Epoch 17 -- Batch 70/ 842, training loss 0.34372976422309875\n",
      "Epoch 17 -- Batch 71/ 842, training loss 0.33656612038612366\n",
      "Epoch 17 -- Batch 72/ 842, training loss 0.34316492080688477\n",
      "Epoch 17 -- Batch 73/ 842, training loss 0.3372490406036377\n",
      "Epoch 17 -- Batch 74/ 842, training loss 0.32500940561294556\n",
      "Epoch 17 -- Batch 75/ 842, training loss 0.3414264917373657\n",
      "Epoch 17 -- Batch 76/ 842, training loss 0.33908456563949585\n",
      "Epoch 17 -- Batch 77/ 842, training loss 0.3497540056705475\n",
      "Epoch 17 -- Batch 78/ 842, training loss 0.3401884436607361\n",
      "Epoch 17 -- Batch 79/ 842, training loss 0.3538791239261627\n",
      "Epoch 17 -- Batch 80/ 842, training loss 0.3423346281051636\n",
      "Epoch 17 -- Batch 81/ 842, training loss 0.34277406334877014\n",
      "Epoch 17 -- Batch 82/ 842, training loss 0.33072352409362793\n",
      "Epoch 17 -- Batch 83/ 842, training loss 0.3270167112350464\n",
      "Epoch 17 -- Batch 84/ 842, training loss 0.3365705609321594\n",
      "Epoch 17 -- Batch 85/ 842, training loss 0.3464047312736511\n",
      "Epoch 17 -- Batch 86/ 842, training loss 0.32320156693458557\n",
      "Epoch 17 -- Batch 87/ 842, training loss 0.33589300513267517\n",
      "Epoch 17 -- Batch 88/ 842, training loss 0.3495999574661255\n",
      "Epoch 17 -- Batch 89/ 842, training loss 0.35169732570648193\n",
      "Epoch 17 -- Batch 90/ 842, training loss 0.3413446843624115\n",
      "Epoch 17 -- Batch 91/ 842, training loss 0.3397442102432251\n",
      "Epoch 17 -- Batch 92/ 842, training loss 0.33936700224876404\n",
      "Epoch 17 -- Batch 93/ 842, training loss 0.3444889783859253\n",
      "Epoch 17 -- Batch 94/ 842, training loss 0.3489338159561157\n",
      "Epoch 17 -- Batch 95/ 842, training loss 0.34077391028404236\n",
      "Epoch 17 -- Batch 96/ 842, training loss 0.34471479058265686\n",
      "Epoch 17 -- Batch 97/ 842, training loss 0.34784114360809326\n",
      "Epoch 17 -- Batch 98/ 842, training loss 0.3436790108680725\n",
      "Epoch 17 -- Batch 99/ 842, training loss 0.3288632035255432\n",
      "Epoch 17 -- Batch 100/ 842, training loss 0.33223605155944824\n",
      "Epoch 17 -- Batch 101/ 842, training loss 0.3424793779850006\n",
      "Epoch 17 -- Batch 102/ 842, training loss 0.34148839116096497\n",
      "Epoch 17 -- Batch 103/ 842, training loss 0.34868159890174866\n",
      "Epoch 17 -- Batch 104/ 842, training loss 0.35204842686653137\n",
      "Epoch 17 -- Batch 105/ 842, training loss 0.328377366065979\n",
      "Epoch 17 -- Batch 106/ 842, training loss 0.33792027831077576\n",
      "Epoch 17 -- Batch 107/ 842, training loss 0.35423216223716736\n",
      "Epoch 17 -- Batch 108/ 842, training loss 0.3395034372806549\n",
      "Epoch 17 -- Batch 109/ 842, training loss 0.33831915259361267\n",
      "Epoch 17 -- Batch 110/ 842, training loss 0.3438793420791626\n",
      "Epoch 17 -- Batch 111/ 842, training loss 0.34129849076271057\n",
      "Epoch 17 -- Batch 112/ 842, training loss 0.3462040424346924\n",
      "Epoch 17 -- Batch 113/ 842, training loss 0.3379818797111511\n",
      "Epoch 17 -- Batch 114/ 842, training loss 0.3434896767139435\n",
      "Epoch 17 -- Batch 115/ 842, training loss 0.3428404629230499\n",
      "Epoch 17 -- Batch 116/ 842, training loss 0.34036287665367126\n",
      "Epoch 17 -- Batch 117/ 842, training loss 0.34150922298431396\n",
      "Epoch 17 -- Batch 118/ 842, training loss 0.33933544158935547\n",
      "Epoch 17 -- Batch 119/ 842, training loss 0.33923226594924927\n",
      "Epoch 17 -- Batch 120/ 842, training loss 0.3419410288333893\n",
      "Epoch 17 -- Batch 121/ 842, training loss 0.3452981412410736\n",
      "Epoch 17 -- Batch 122/ 842, training loss 0.33556434512138367\n",
      "Epoch 17 -- Batch 123/ 842, training loss 0.3467932343482971\n",
      "Epoch 17 -- Batch 124/ 842, training loss 0.3472902476787567\n",
      "Epoch 17 -- Batch 125/ 842, training loss 0.3310473561286926\n",
      "Epoch 17 -- Batch 126/ 842, training loss 0.32969728112220764\n",
      "Epoch 17 -- Batch 127/ 842, training loss 0.3447652757167816\n",
      "Epoch 17 -- Batch 128/ 842, training loss 0.336405485868454\n",
      "Epoch 17 -- Batch 129/ 842, training loss 0.3448534905910492\n",
      "Epoch 17 -- Batch 130/ 842, training loss 0.33931565284729004\n",
      "Epoch 17 -- Batch 131/ 842, training loss 0.34966906905174255\n",
      "Epoch 17 -- Batch 132/ 842, training loss 0.34486356377601624\n",
      "Epoch 17 -- Batch 133/ 842, training loss 0.34472525119781494\n",
      "Epoch 17 -- Batch 134/ 842, training loss 0.3429569900035858\n",
      "Epoch 17 -- Batch 135/ 842, training loss 0.3427831828594208\n",
      "Epoch 17 -- Batch 136/ 842, training loss 0.3317192792892456\n",
      "Epoch 17 -- Batch 137/ 842, training loss 0.3368290066719055\n",
      "Epoch 17 -- Batch 138/ 842, training loss 0.3374333679676056\n",
      "Epoch 17 -- Batch 139/ 842, training loss 0.33689844608306885\n",
      "Epoch 17 -- Batch 140/ 842, training loss 0.33311688899993896\n",
      "Epoch 17 -- Batch 141/ 842, training loss 0.34383952617645264\n",
      "Epoch 17 -- Batch 142/ 842, training loss 0.3355560004711151\n",
      "Epoch 17 -- Batch 143/ 842, training loss 0.3528969883918762\n",
      "Epoch 17 -- Batch 144/ 842, training loss 0.33815646171569824\n",
      "Epoch 17 -- Batch 145/ 842, training loss 0.3606177568435669\n",
      "Epoch 17 -- Batch 146/ 842, training loss 0.33896955847740173\n",
      "Epoch 17 -- Batch 147/ 842, training loss 0.33273863792419434\n",
      "Epoch 17 -- Batch 148/ 842, training loss 0.34826934337615967\n",
      "Epoch 17 -- Batch 149/ 842, training loss 0.3390100598335266\n",
      "Epoch 17 -- Batch 150/ 842, training loss 0.3363852798938751\n",
      "Epoch 17 -- Batch 151/ 842, training loss 0.3313833773136139\n",
      "Epoch 17 -- Batch 152/ 842, training loss 0.3391929864883423\n",
      "Epoch 17 -- Batch 153/ 842, training loss 0.33628737926483154\n",
      "Epoch 17 -- Batch 154/ 842, training loss 0.3245854079723358\n",
      "Epoch 17 -- Batch 155/ 842, training loss 0.3528705835342407\n",
      "Epoch 17 -- Batch 156/ 842, training loss 0.34583407640457153\n",
      "Epoch 17 -- Batch 157/ 842, training loss 0.3470402657985687\n",
      "Epoch 17 -- Batch 158/ 842, training loss 0.3235383927822113\n",
      "Epoch 17 -- Batch 159/ 842, training loss 0.3335447609424591\n",
      "Epoch 17 -- Batch 160/ 842, training loss 0.3312477171421051\n",
      "Epoch 17 -- Batch 161/ 842, training loss 0.3323995769023895\n",
      "Epoch 17 -- Batch 162/ 842, training loss 0.3369133174419403\n",
      "Epoch 17 -- Batch 163/ 842, training loss 0.3402421176433563\n",
      "Epoch 17 -- Batch 164/ 842, training loss 0.3454209864139557\n",
      "Epoch 17 -- Batch 165/ 842, training loss 0.3338242471218109\n",
      "Epoch 17 -- Batch 166/ 842, training loss 0.3375821113586426\n",
      "Epoch 17 -- Batch 167/ 842, training loss 0.33871546387672424\n",
      "Epoch 17 -- Batch 168/ 842, training loss 0.35134050250053406\n",
      "Epoch 17 -- Batch 169/ 842, training loss 0.34050655364990234\n",
      "Epoch 17 -- Batch 170/ 842, training loss 0.345488578081131\n",
      "Epoch 17 -- Batch 171/ 842, training loss 0.34534725546836853\n",
      "Epoch 17 -- Batch 172/ 842, training loss 0.3389691710472107\n",
      "Epoch 17 -- Batch 173/ 842, training loss 0.3327850103378296\n",
      "Epoch 17 -- Batch 174/ 842, training loss 0.3427589535713196\n",
      "Epoch 17 -- Batch 175/ 842, training loss 0.3327904939651489\n",
      "Epoch 17 -- Batch 176/ 842, training loss 0.34503495693206787\n",
      "Epoch 17 -- Batch 177/ 842, training loss 0.33048179745674133\n",
      "Epoch 17 -- Batch 178/ 842, training loss 0.3448540270328522\n",
      "Epoch 17 -- Batch 179/ 842, training loss 0.34644195437431335\n",
      "Epoch 17 -- Batch 180/ 842, training loss 0.33624184131622314\n",
      "Epoch 17 -- Batch 181/ 842, training loss 0.3429553508758545\n",
      "Epoch 17 -- Batch 182/ 842, training loss 0.3391517102718353\n",
      "Epoch 17 -- Batch 183/ 842, training loss 0.336635947227478\n",
      "Epoch 17 -- Batch 184/ 842, training loss 0.3410334885120392\n",
      "Epoch 17 -- Batch 185/ 842, training loss 0.3351656198501587\n",
      "Epoch 17 -- Batch 186/ 842, training loss 0.3443343937397003\n",
      "Epoch 17 -- Batch 187/ 842, training loss 0.3484254777431488\n",
      "Epoch 17 -- Batch 188/ 842, training loss 0.32714492082595825\n",
      "Epoch 17 -- Batch 189/ 842, training loss 0.3347543179988861\n",
      "Epoch 17 -- Batch 190/ 842, training loss 0.33790042996406555\n",
      "Epoch 17 -- Batch 191/ 842, training loss 0.33977028727531433\n",
      "Epoch 17 -- Batch 192/ 842, training loss 0.3349589705467224\n",
      "Epoch 17 -- Batch 193/ 842, training loss 0.3512890636920929\n",
      "Epoch 17 -- Batch 194/ 842, training loss 0.3444826602935791\n",
      "Epoch 17 -- Batch 195/ 842, training loss 0.34414535760879517\n",
      "Epoch 17 -- Batch 196/ 842, training loss 0.3384193778038025\n",
      "Epoch 17 -- Batch 197/ 842, training loss 0.3289639353752136\n",
      "Epoch 17 -- Batch 198/ 842, training loss 0.34574976563453674\n",
      "Epoch 17 -- Batch 199/ 842, training loss 0.34005528688430786\n",
      "Epoch 17 -- Batch 200/ 842, training loss 0.3355102241039276\n",
      "Epoch 17 -- Batch 201/ 842, training loss 0.34893715381622314\n",
      "Epoch 17 -- Batch 202/ 842, training loss 0.35349491238594055\n",
      "Epoch 17 -- Batch 203/ 842, training loss 0.3589383661746979\n",
      "Epoch 17 -- Batch 204/ 842, training loss 0.35034066438674927\n",
      "Epoch 17 -- Batch 205/ 842, training loss 0.3427612781524658\n",
      "Epoch 17 -- Batch 206/ 842, training loss 0.32645758986473083\n",
      "Epoch 17 -- Batch 207/ 842, training loss 0.3484230935573578\n",
      "Epoch 17 -- Batch 208/ 842, training loss 0.35116609930992126\n",
      "Epoch 17 -- Batch 209/ 842, training loss 0.32906508445739746\n",
      "Epoch 17 -- Batch 210/ 842, training loss 0.35464033484458923\n",
      "Epoch 17 -- Batch 211/ 842, training loss 0.3293691873550415\n",
      "Epoch 17 -- Batch 212/ 842, training loss 0.341178834438324\n",
      "Epoch 17 -- Batch 213/ 842, training loss 0.3460390567779541\n",
      "Epoch 17 -- Batch 214/ 842, training loss 0.3436499834060669\n",
      "Epoch 17 -- Batch 215/ 842, training loss 0.34366267919540405\n",
      "Epoch 17 -- Batch 216/ 842, training loss 0.34237679839134216\n",
      "Epoch 17 -- Batch 217/ 842, training loss 0.3301745653152466\n",
      "Epoch 17 -- Batch 218/ 842, training loss 0.3467307388782501\n",
      "Epoch 17 -- Batch 219/ 842, training loss 0.33993494510650635\n",
      "Epoch 17 -- Batch 220/ 842, training loss 0.3342445194721222\n",
      "Epoch 17 -- Batch 221/ 842, training loss 0.3420998454093933\n",
      "Epoch 17 -- Batch 222/ 842, training loss 0.33553943037986755\n",
      "Epoch 17 -- Batch 223/ 842, training loss 0.3492670953273773\n",
      "Epoch 17 -- Batch 224/ 842, training loss 0.33871832489967346\n",
      "Epoch 17 -- Batch 225/ 842, training loss 0.3357999622821808\n",
      "Epoch 17 -- Batch 226/ 842, training loss 0.34485751390457153\n",
      "Epoch 17 -- Batch 227/ 842, training loss 0.33264032006263733\n",
      "Epoch 17 -- Batch 228/ 842, training loss 0.3397015333175659\n",
      "Epoch 17 -- Batch 229/ 842, training loss 0.33275991678237915\n",
      "Epoch 17 -- Batch 230/ 842, training loss 0.3390561044216156\n",
      "Epoch 17 -- Batch 231/ 842, training loss 0.34597328305244446\n",
      "Epoch 17 -- Batch 232/ 842, training loss 0.3343689739704132\n",
      "Epoch 17 -- Batch 233/ 842, training loss 0.33803510665893555\n",
      "Epoch 17 -- Batch 234/ 842, training loss 0.34060904383659363\n",
      "Epoch 17 -- Batch 235/ 842, training loss 0.34230831265449524\n",
      "Epoch 17 -- Batch 236/ 842, training loss 0.34268441796302795\n",
      "Epoch 17 -- Batch 237/ 842, training loss 0.34233811497688293\n",
      "Epoch 17 -- Batch 238/ 842, training loss 0.33386626839637756\n",
      "Epoch 17 -- Batch 239/ 842, training loss 0.34070682525634766\n",
      "Epoch 17 -- Batch 240/ 842, training loss 0.33880043029785156\n",
      "Epoch 17 -- Batch 241/ 842, training loss 0.3487299382686615\n",
      "Epoch 17 -- Batch 242/ 842, training loss 0.3432372510433197\n",
      "Epoch 17 -- Batch 243/ 842, training loss 0.33762580156326294\n",
      "Epoch 17 -- Batch 244/ 842, training loss 0.3370976448059082\n",
      "Epoch 17 -- Batch 245/ 842, training loss 0.35429662466049194\n",
      "Epoch 17 -- Batch 246/ 842, training loss 0.33832553029060364\n",
      "Epoch 17 -- Batch 247/ 842, training loss 0.34513163566589355\n",
      "Epoch 17 -- Batch 248/ 842, training loss 0.34694960713386536\n",
      "Epoch 17 -- Batch 249/ 842, training loss 0.34468576312065125\n",
      "Epoch 17 -- Batch 250/ 842, training loss 0.3444366157054901\n",
      "Epoch 17 -- Batch 251/ 842, training loss 0.3299843370914459\n",
      "Epoch 17 -- Batch 252/ 842, training loss 0.3401362895965576\n",
      "Epoch 17 -- Batch 253/ 842, training loss 0.34536948800086975\n",
      "Epoch 17 -- Batch 254/ 842, training loss 0.32549455761909485\n",
      "Epoch 17 -- Batch 255/ 842, training loss 0.3265060484409332\n",
      "Epoch 17 -- Batch 256/ 842, training loss 0.3389626443386078\n",
      "Epoch 17 -- Batch 257/ 842, training loss 0.3303866684436798\n",
      "Epoch 17 -- Batch 258/ 842, training loss 0.32687875628471375\n",
      "Epoch 17 -- Batch 259/ 842, training loss 0.33732256293296814\n",
      "Epoch 17 -- Batch 260/ 842, training loss 0.33570972084999084\n",
      "Epoch 17 -- Batch 261/ 842, training loss 0.34766900539398193\n",
      "Epoch 17 -- Batch 262/ 842, training loss 0.32331612706184387\n",
      "Epoch 17 -- Batch 263/ 842, training loss 0.3378294110298157\n",
      "Epoch 17 -- Batch 264/ 842, training loss 0.33760660886764526\n",
      "Epoch 17 -- Batch 265/ 842, training loss 0.34543490409851074\n",
      "Epoch 17 -- Batch 266/ 842, training loss 0.3428986072540283\n",
      "Epoch 17 -- Batch 267/ 842, training loss 0.3366755247116089\n",
      "Epoch 17 -- Batch 268/ 842, training loss 0.35123372077941895\n",
      "Epoch 17 -- Batch 269/ 842, training loss 0.34159308671951294\n",
      "Epoch 17 -- Batch 270/ 842, training loss 0.33413857221603394\n",
      "Epoch 17 -- Batch 271/ 842, training loss 0.3283129930496216\n",
      "Epoch 17 -- Batch 272/ 842, training loss 0.32975640892982483\n",
      "Epoch 17 -- Batch 273/ 842, training loss 0.33141157031059265\n",
      "Epoch 17 -- Batch 274/ 842, training loss 0.341122031211853\n",
      "Epoch 17 -- Batch 275/ 842, training loss 0.3483986556529999\n",
      "Epoch 17 -- Batch 276/ 842, training loss 0.3414607048034668\n",
      "Epoch 17 -- Batch 277/ 842, training loss 0.35850071907043457\n",
      "Epoch 17 -- Batch 278/ 842, training loss 0.33433374762535095\n",
      "Epoch 17 -- Batch 279/ 842, training loss 0.3343116044998169\n",
      "Epoch 17 -- Batch 280/ 842, training loss 0.3299885392189026\n",
      "Epoch 17 -- Batch 281/ 842, training loss 0.3462887108325958\n",
      "Epoch 17 -- Batch 282/ 842, training loss 0.33157414197921753\n",
      "Epoch 17 -- Batch 283/ 842, training loss 0.3469448387622833\n",
      "Epoch 17 -- Batch 284/ 842, training loss 0.3517373502254486\n",
      "Epoch 17 -- Batch 285/ 842, training loss 0.3550983965396881\n",
      "Epoch 17 -- Batch 286/ 842, training loss 0.34870532155036926\n",
      "Epoch 17 -- Batch 287/ 842, training loss 0.3478986620903015\n",
      "Epoch 17 -- Batch 288/ 842, training loss 0.34217846393585205\n",
      "Epoch 17 -- Batch 289/ 842, training loss 0.3381469249725342\n",
      "Epoch 17 -- Batch 290/ 842, training loss 0.34585148096084595\n",
      "Epoch 17 -- Batch 291/ 842, training loss 0.34049907326698303\n",
      "Epoch 17 -- Batch 292/ 842, training loss 0.33478304743766785\n",
      "Epoch 17 -- Batch 293/ 842, training loss 0.34192541241645813\n",
      "Epoch 17 -- Batch 294/ 842, training loss 0.35517820715904236\n",
      "Epoch 17 -- Batch 295/ 842, training loss 0.3415498733520508\n",
      "Epoch 17 -- Batch 296/ 842, training loss 0.3447483479976654\n",
      "Epoch 17 -- Batch 297/ 842, training loss 0.35185790061950684\n",
      "Epoch 17 -- Batch 298/ 842, training loss 0.3524068593978882\n",
      "Epoch 17 -- Batch 299/ 842, training loss 0.3524473309516907\n",
      "Epoch 17 -- Batch 300/ 842, training loss 0.33468005061149597\n",
      "Epoch 17 -- Batch 301/ 842, training loss 0.35032936930656433\n",
      "Epoch 17 -- Batch 302/ 842, training loss 0.3317341208457947\n",
      "Epoch 17 -- Batch 303/ 842, training loss 0.34897762537002563\n",
      "Epoch 17 -- Batch 304/ 842, training loss 0.32884442806243896\n",
      "Epoch 17 -- Batch 305/ 842, training loss 0.33352240920066833\n",
      "Epoch 17 -- Batch 306/ 842, training loss 0.33512985706329346\n",
      "Epoch 17 -- Batch 307/ 842, training loss 0.3493395745754242\n",
      "Epoch 17 -- Batch 308/ 842, training loss 0.33481815457344055\n",
      "Epoch 17 -- Batch 309/ 842, training loss 0.3385278582572937\n",
      "Epoch 17 -- Batch 310/ 842, training loss 0.34457775950431824\n",
      "Epoch 17 -- Batch 311/ 842, training loss 0.3482450842857361\n",
      "Epoch 17 -- Batch 312/ 842, training loss 0.34786346554756165\n",
      "Epoch 17 -- Batch 313/ 842, training loss 0.3466190695762634\n",
      "Epoch 17 -- Batch 314/ 842, training loss 0.3470454812049866\n",
      "Epoch 17 -- Batch 315/ 842, training loss 0.347135990858078\n",
      "Epoch 17 -- Batch 316/ 842, training loss 0.3501501977443695\n",
      "Epoch 17 -- Batch 317/ 842, training loss 0.33045971393585205\n",
      "Epoch 17 -- Batch 318/ 842, training loss 0.3419601321220398\n",
      "Epoch 17 -- Batch 319/ 842, training loss 0.3163996636867523\n",
      "Epoch 17 -- Batch 320/ 842, training loss 0.3429100513458252\n",
      "Epoch 17 -- Batch 321/ 842, training loss 0.34434080123901367\n",
      "Epoch 17 -- Batch 322/ 842, training loss 0.32897427678108215\n",
      "Epoch 17 -- Batch 323/ 842, training loss 0.33790209889411926\n",
      "Epoch 17 -- Batch 324/ 842, training loss 0.35324206948280334\n",
      "Epoch 17 -- Batch 325/ 842, training loss 0.32987499237060547\n",
      "Epoch 17 -- Batch 326/ 842, training loss 0.34421366453170776\n",
      "Epoch 17 -- Batch 327/ 842, training loss 0.3469599187374115\n",
      "Epoch 17 -- Batch 328/ 842, training loss 0.3489651679992676\n",
      "Epoch 17 -- Batch 329/ 842, training loss 0.322452187538147\n",
      "Epoch 17 -- Batch 330/ 842, training loss 0.35727953910827637\n",
      "Epoch 17 -- Batch 331/ 842, training loss 0.3388090133666992\n",
      "Epoch 17 -- Batch 332/ 842, training loss 0.33753398060798645\n",
      "Epoch 17 -- Batch 333/ 842, training loss 0.34711724519729614\n",
      "Epoch 17 -- Batch 334/ 842, training loss 0.34817472100257874\n",
      "Epoch 17 -- Batch 335/ 842, training loss 0.3459603488445282\n",
      "Epoch 17 -- Batch 336/ 842, training loss 0.33480507135391235\n",
      "Epoch 17 -- Batch 337/ 842, training loss 0.33875328302383423\n",
      "Epoch 17 -- Batch 338/ 842, training loss 0.34161871671676636\n",
      "Epoch 17 -- Batch 339/ 842, training loss 0.33001285791397095\n",
      "Epoch 17 -- Batch 340/ 842, training loss 0.35360437631607056\n",
      "Epoch 17 -- Batch 341/ 842, training loss 0.3361172378063202\n",
      "Epoch 17 -- Batch 342/ 842, training loss 0.3432532548904419\n",
      "Epoch 17 -- Batch 343/ 842, training loss 0.3340050280094147\n",
      "Epoch 17 -- Batch 344/ 842, training loss 0.34722259640693665\n",
      "Epoch 17 -- Batch 345/ 842, training loss 0.3505140244960785\n",
      "Epoch 17 -- Batch 346/ 842, training loss 0.34295016527175903\n",
      "Epoch 17 -- Batch 347/ 842, training loss 0.3370504677295685\n",
      "Epoch 17 -- Batch 348/ 842, training loss 0.34158122539520264\n",
      "Epoch 17 -- Batch 349/ 842, training loss 0.34352102875709534\n",
      "Epoch 17 -- Batch 350/ 842, training loss 0.33855369687080383\n",
      "Epoch 17 -- Batch 351/ 842, training loss 0.34206992387771606\n",
      "Epoch 17 -- Batch 352/ 842, training loss 0.3277985751628876\n",
      "Epoch 17 -- Batch 353/ 842, training loss 0.3445781171321869\n",
      "Epoch 17 -- Batch 354/ 842, training loss 0.3403356075286865\n",
      "Epoch 17 -- Batch 355/ 842, training loss 0.34870436787605286\n",
      "Epoch 17 -- Batch 356/ 842, training loss 0.3343166708946228\n",
      "Epoch 17 -- Batch 357/ 842, training loss 0.35001859068870544\n",
      "Epoch 17 -- Batch 358/ 842, training loss 0.33830124139785767\n",
      "Epoch 17 -- Batch 359/ 842, training loss 0.3405326008796692\n",
      "Epoch 17 -- Batch 360/ 842, training loss 0.3493534028530121\n",
      "Epoch 17 -- Batch 361/ 842, training loss 0.3389546573162079\n",
      "Epoch 17 -- Batch 362/ 842, training loss 0.353145956993103\n",
      "Epoch 17 -- Batch 363/ 842, training loss 0.3253736197948456\n",
      "Epoch 17 -- Batch 364/ 842, training loss 0.3246893286705017\n",
      "Epoch 17 -- Batch 365/ 842, training loss 0.34358879923820496\n",
      "Epoch 17 -- Batch 366/ 842, training loss 0.3382807970046997\n",
      "Epoch 17 -- Batch 367/ 842, training loss 0.34566712379455566\n",
      "Epoch 17 -- Batch 368/ 842, training loss 0.344366192817688\n",
      "Epoch 17 -- Batch 369/ 842, training loss 0.3435206711292267\n",
      "Epoch 17 -- Batch 370/ 842, training loss 0.3401072323322296\n",
      "Epoch 17 -- Batch 371/ 842, training loss 0.3476705551147461\n",
      "Epoch 17 -- Batch 372/ 842, training loss 0.34418368339538574\n",
      "Epoch 17 -- Batch 373/ 842, training loss 0.35689857602119446\n",
      "Epoch 17 -- Batch 374/ 842, training loss 0.34080299735069275\n",
      "Epoch 17 -- Batch 375/ 842, training loss 0.34658509492874146\n",
      "Epoch 17 -- Batch 376/ 842, training loss 0.34405744075775146\n",
      "Epoch 17 -- Batch 377/ 842, training loss 0.3422858715057373\n",
      "Epoch 17 -- Batch 378/ 842, training loss 0.3382168710231781\n",
      "Epoch 17 -- Batch 379/ 842, training loss 0.3465845584869385\n",
      "Epoch 17 -- Batch 380/ 842, training loss 0.3530566990375519\n",
      "Epoch 17 -- Batch 381/ 842, training loss 0.3425700068473816\n",
      "Epoch 17 -- Batch 382/ 842, training loss 0.32479193806648254\n",
      "Epoch 17 -- Batch 383/ 842, training loss 0.33466416597366333\n",
      "Epoch 17 -- Batch 384/ 842, training loss 0.33440107107162476\n",
      "Epoch 17 -- Batch 385/ 842, training loss 0.3309144675731659\n",
      "Epoch 17 -- Batch 386/ 842, training loss 0.347907692193985\n",
      "Epoch 17 -- Batch 387/ 842, training loss 0.349639356136322\n",
      "Epoch 17 -- Batch 388/ 842, training loss 0.3275773525238037\n",
      "Epoch 17 -- Batch 389/ 842, training loss 0.3419535458087921\n",
      "Epoch 17 -- Batch 390/ 842, training loss 0.34188663959503174\n",
      "Epoch 17 -- Batch 391/ 842, training loss 0.3468354344367981\n",
      "Epoch 17 -- Batch 392/ 842, training loss 0.3508976399898529\n",
      "Epoch 17 -- Batch 393/ 842, training loss 0.34333857893943787\n",
      "Epoch 17 -- Batch 394/ 842, training loss 0.33057981729507446\n",
      "Epoch 17 -- Batch 395/ 842, training loss 0.34959718585014343\n",
      "Epoch 17 -- Batch 396/ 842, training loss 0.3453254997730255\n",
      "Epoch 17 -- Batch 397/ 842, training loss 0.33972206711769104\n",
      "Epoch 17 -- Batch 398/ 842, training loss 0.3384568393230438\n",
      "Epoch 17 -- Batch 399/ 842, training loss 0.33746758103370667\n",
      "Epoch 17 -- Batch 400/ 842, training loss 0.34234902262687683\n",
      "Epoch 17 -- Batch 401/ 842, training loss 0.34782007336616516\n",
      "Epoch 17 -- Batch 402/ 842, training loss 0.3362937867641449\n",
      "Epoch 17 -- Batch 403/ 842, training loss 0.3443650007247925\n",
      "Epoch 17 -- Batch 404/ 842, training loss 0.3290087580680847\n",
      "Epoch 17 -- Batch 405/ 842, training loss 0.3626633882522583\n",
      "Epoch 17 -- Batch 406/ 842, training loss 0.3430054485797882\n",
      "Epoch 17 -- Batch 407/ 842, training loss 0.3252085745334625\n",
      "Epoch 17 -- Batch 408/ 842, training loss 0.3399104177951813\n",
      "Epoch 17 -- Batch 409/ 842, training loss 0.3565777540206909\n",
      "Epoch 17 -- Batch 410/ 842, training loss 0.33677226305007935\n",
      "Epoch 17 -- Batch 411/ 842, training loss 0.34497368335723877\n",
      "Epoch 17 -- Batch 412/ 842, training loss 0.34315723180770874\n",
      "Epoch 17 -- Batch 413/ 842, training loss 0.355735719203949\n",
      "Epoch 17 -- Batch 414/ 842, training loss 0.3474942147731781\n",
      "Epoch 17 -- Batch 415/ 842, training loss 0.35363632440567017\n",
      "Epoch 17 -- Batch 416/ 842, training loss 0.3401416838169098\n",
      "Epoch 17 -- Batch 417/ 842, training loss 0.35232242941856384\n",
      "Epoch 17 -- Batch 418/ 842, training loss 0.34013062715530396\n",
      "Epoch 17 -- Batch 419/ 842, training loss 0.33170071244239807\n",
      "Epoch 17 -- Batch 420/ 842, training loss 0.3399229049682617\n",
      "Epoch 17 -- Batch 421/ 842, training loss 0.3501349687576294\n",
      "Epoch 17 -- Batch 422/ 842, training loss 0.34503376483917236\n",
      "Epoch 17 -- Batch 423/ 842, training loss 0.3444702625274658\n",
      "Epoch 17 -- Batch 424/ 842, training loss 0.3367801010608673\n",
      "Epoch 17 -- Batch 425/ 842, training loss 0.3428349494934082\n",
      "Epoch 17 -- Batch 426/ 842, training loss 0.3491052985191345\n",
      "Epoch 17 -- Batch 427/ 842, training loss 0.33484944701194763\n",
      "Epoch 17 -- Batch 428/ 842, training loss 0.33997485041618347\n",
      "Epoch 17 -- Batch 429/ 842, training loss 0.3383055627346039\n",
      "Epoch 17 -- Batch 430/ 842, training loss 0.3421443700790405\n",
      "Epoch 17 -- Batch 431/ 842, training loss 0.3423443138599396\n",
      "Epoch 17 -- Batch 432/ 842, training loss 0.338254451751709\n",
      "Epoch 17 -- Batch 433/ 842, training loss 0.3552507758140564\n",
      "Epoch 17 -- Batch 434/ 842, training loss 0.34517842531204224\n",
      "Epoch 17 -- Batch 435/ 842, training loss 0.3364515006542206\n",
      "Epoch 17 -- Batch 436/ 842, training loss 0.3344835340976715\n",
      "Epoch 17 -- Batch 437/ 842, training loss 0.3499559760093689\n",
      "Epoch 17 -- Batch 438/ 842, training loss 0.3404974043369293\n",
      "Epoch 17 -- Batch 439/ 842, training loss 0.35624685883522034\n",
      "Epoch 17 -- Batch 440/ 842, training loss 0.3426382839679718\n",
      "Epoch 17 -- Batch 441/ 842, training loss 0.3379371762275696\n",
      "Epoch 17 -- Batch 442/ 842, training loss 0.33550047874450684\n",
      "Epoch 17 -- Batch 443/ 842, training loss 0.33658990263938904\n",
      "Epoch 17 -- Batch 444/ 842, training loss 0.3521147072315216\n",
      "Epoch 17 -- Batch 445/ 842, training loss 0.33632349967956543\n",
      "Epoch 17 -- Batch 446/ 842, training loss 0.33802151679992676\n",
      "Epoch 17 -- Batch 447/ 842, training loss 0.3411523103713989\n",
      "Epoch 17 -- Batch 448/ 842, training loss 0.35151925683021545\n",
      "Epoch 17 -- Batch 449/ 842, training loss 0.33190903067588806\n",
      "Epoch 17 -- Batch 450/ 842, training loss 0.3426295518875122\n",
      "Epoch 17 -- Batch 451/ 842, training loss 0.3220546543598175\n",
      "Epoch 17 -- Batch 452/ 842, training loss 0.3360716998577118\n",
      "Epoch 17 -- Batch 453/ 842, training loss 0.33012261986732483\n",
      "Epoch 17 -- Batch 454/ 842, training loss 0.3400084972381592\n",
      "Epoch 17 -- Batch 455/ 842, training loss 0.3435704708099365\n",
      "Epoch 17 -- Batch 456/ 842, training loss 0.33507075905799866\n",
      "Epoch 17 -- Batch 457/ 842, training loss 0.3359836935997009\n",
      "Epoch 17 -- Batch 458/ 842, training loss 0.34548985958099365\n",
      "Epoch 17 -- Batch 459/ 842, training loss 0.33155789971351624\n",
      "Epoch 17 -- Batch 460/ 842, training loss 0.3443879783153534\n",
      "Epoch 17 -- Batch 461/ 842, training loss 0.3301449120044708\n",
      "Epoch 17 -- Batch 462/ 842, training loss 0.3540489673614502\n",
      "Epoch 17 -- Batch 463/ 842, training loss 0.34936049580574036\n",
      "Epoch 17 -- Batch 464/ 842, training loss 0.3396458327770233\n",
      "Epoch 17 -- Batch 465/ 842, training loss 0.3459031283855438\n",
      "Epoch 17 -- Batch 466/ 842, training loss 0.3534746468067169\n",
      "Epoch 17 -- Batch 467/ 842, training loss 0.34284508228302\n",
      "Epoch 17 -- Batch 468/ 842, training loss 0.3445403277873993\n",
      "Epoch 17 -- Batch 469/ 842, training loss 0.3371425271034241\n",
      "Epoch 17 -- Batch 470/ 842, training loss 0.3608565330505371\n",
      "Epoch 17 -- Batch 471/ 842, training loss 0.3499726951122284\n",
      "Epoch 17 -- Batch 472/ 842, training loss 0.3426479995250702\n",
      "Epoch 17 -- Batch 473/ 842, training loss 0.34829890727996826\n",
      "Epoch 17 -- Batch 474/ 842, training loss 0.34343811869621277\n",
      "Epoch 17 -- Batch 475/ 842, training loss 0.3491775691509247\n",
      "Epoch 17 -- Batch 476/ 842, training loss 0.3431742787361145\n",
      "Epoch 17 -- Batch 477/ 842, training loss 0.35083845257759094\n",
      "Epoch 17 -- Batch 478/ 842, training loss 0.3386284112930298\n",
      "Epoch 17 -- Batch 479/ 842, training loss 0.35012710094451904\n",
      "Epoch 17 -- Batch 480/ 842, training loss 0.3286687731742859\n",
      "Epoch 17 -- Batch 481/ 842, training loss 0.3326435089111328\n",
      "Epoch 17 -- Batch 482/ 842, training loss 0.34074726700782776\n",
      "Epoch 17 -- Batch 483/ 842, training loss 0.33824774622917175\n",
      "Epoch 17 -- Batch 484/ 842, training loss 0.339546263217926\n",
      "Epoch 17 -- Batch 485/ 842, training loss 0.337942510843277\n",
      "Epoch 17 -- Batch 486/ 842, training loss 0.3357878625392914\n",
      "Epoch 17 -- Batch 487/ 842, training loss 0.3360196650028229\n",
      "Epoch 17 -- Batch 488/ 842, training loss 0.34798988699913025\n",
      "Epoch 17 -- Batch 489/ 842, training loss 0.33971700072288513\n",
      "Epoch 17 -- Batch 490/ 842, training loss 0.3463563621044159\n",
      "Epoch 17 -- Batch 491/ 842, training loss 0.33385688066482544\n",
      "Epoch 17 -- Batch 492/ 842, training loss 0.3358526825904846\n",
      "Epoch 17 -- Batch 493/ 842, training loss 0.34224027395248413\n",
      "Epoch 17 -- Batch 494/ 842, training loss 0.34153613448143005\n",
      "Epoch 17 -- Batch 495/ 842, training loss 0.35191479325294495\n",
      "Epoch 17 -- Batch 496/ 842, training loss 0.3638380467891693\n",
      "Epoch 17 -- Batch 497/ 842, training loss 0.3419383466243744\n",
      "Epoch 17 -- Batch 498/ 842, training loss 0.348552405834198\n",
      "Epoch 17 -- Batch 499/ 842, training loss 0.3527336120605469\n",
      "Epoch 17 -- Batch 500/ 842, training loss 0.3460460603237152\n",
      "Epoch 17 -- Batch 501/ 842, training loss 0.33950451016426086\n",
      "Epoch 17 -- Batch 502/ 842, training loss 0.32823026180267334\n",
      "Epoch 17 -- Batch 503/ 842, training loss 0.3364962041378021\n",
      "Epoch 17 -- Batch 504/ 842, training loss 0.3434551954269409\n",
      "Epoch 17 -- Batch 505/ 842, training loss 0.3415893018245697\n",
      "Epoch 17 -- Batch 506/ 842, training loss 0.3556453287601471\n",
      "Epoch 17 -- Batch 507/ 842, training loss 0.3382863700389862\n",
      "Epoch 17 -- Batch 508/ 842, training loss 0.35640281438827515\n",
      "Epoch 17 -- Batch 509/ 842, training loss 0.3414314091205597\n",
      "Epoch 17 -- Batch 510/ 842, training loss 0.34636130928993225\n",
      "Epoch 17 -- Batch 511/ 842, training loss 0.3316207230091095\n",
      "Epoch 17 -- Batch 512/ 842, training loss 0.33953598141670227\n",
      "Epoch 17 -- Batch 513/ 842, training loss 0.3527372181415558\n",
      "Epoch 17 -- Batch 514/ 842, training loss 0.32998859882354736\n",
      "Epoch 17 -- Batch 515/ 842, training loss 0.33523058891296387\n",
      "Epoch 17 -- Batch 516/ 842, training loss 0.3454747200012207\n",
      "Epoch 17 -- Batch 517/ 842, training loss 0.35934197902679443\n",
      "Epoch 17 -- Batch 518/ 842, training loss 0.34540069103240967\n",
      "Epoch 17 -- Batch 519/ 842, training loss 0.3548356294631958\n",
      "Epoch 17 -- Batch 520/ 842, training loss 0.3442201316356659\n",
      "Epoch 17 -- Batch 521/ 842, training loss 0.3269336521625519\n",
      "Epoch 17 -- Batch 522/ 842, training loss 0.33601823449134827\n",
      "Epoch 17 -- Batch 523/ 842, training loss 0.35049787163734436\n",
      "Epoch 17 -- Batch 524/ 842, training loss 0.35050874948501587\n",
      "Epoch 17 -- Batch 525/ 842, training loss 0.34423723816871643\n",
      "Epoch 17 -- Batch 526/ 842, training loss 0.33615773916244507\n",
      "Epoch 17 -- Batch 527/ 842, training loss 0.3336358666419983\n",
      "Epoch 17 -- Batch 528/ 842, training loss 0.344065397977829\n",
      "Epoch 17 -- Batch 529/ 842, training loss 0.3333398699760437\n",
      "Epoch 17 -- Batch 530/ 842, training loss 0.3475075960159302\n",
      "Epoch 17 -- Batch 531/ 842, training loss 0.3425411880016327\n",
      "Epoch 17 -- Batch 532/ 842, training loss 0.3473999500274658\n",
      "Epoch 17 -- Batch 533/ 842, training loss 0.3413771986961365\n",
      "Epoch 17 -- Batch 534/ 842, training loss 0.35182228684425354\n",
      "Epoch 17 -- Batch 535/ 842, training loss 0.3534950613975525\n",
      "Epoch 17 -- Batch 536/ 842, training loss 0.3400794565677643\n",
      "Epoch 17 -- Batch 537/ 842, training loss 0.34720203280448914\n",
      "Epoch 17 -- Batch 538/ 842, training loss 0.34598854184150696\n",
      "Epoch 17 -- Batch 539/ 842, training loss 0.34365910291671753\n",
      "Epoch 17 -- Batch 540/ 842, training loss 0.3583224415779114\n",
      "Epoch 17 -- Batch 541/ 842, training loss 0.3374945819377899\n",
      "Epoch 17 -- Batch 542/ 842, training loss 0.339712530374527\n",
      "Epoch 17 -- Batch 543/ 842, training loss 0.33897408843040466\n",
      "Epoch 17 -- Batch 544/ 842, training loss 0.342900812625885\n",
      "Epoch 17 -- Batch 545/ 842, training loss 0.3480408191680908\n",
      "Epoch 17 -- Batch 546/ 842, training loss 0.352399617433548\n",
      "Epoch 17 -- Batch 547/ 842, training loss 0.3428446054458618\n",
      "Epoch 17 -- Batch 548/ 842, training loss 0.3528171479701996\n",
      "Epoch 17 -- Batch 549/ 842, training loss 0.34459784626960754\n",
      "Epoch 17 -- Batch 550/ 842, training loss 0.356732040643692\n",
      "Epoch 17 -- Batch 551/ 842, training loss 0.3554520308971405\n",
      "Epoch 17 -- Batch 552/ 842, training loss 0.3452381193637848\n",
      "Epoch 17 -- Batch 553/ 842, training loss 0.3639405071735382\n",
      "Epoch 17 -- Batch 554/ 842, training loss 0.338996559381485\n",
      "Epoch 17 -- Batch 555/ 842, training loss 0.34855523705482483\n",
      "Epoch 17 -- Batch 556/ 842, training loss 0.3400718867778778\n",
      "Epoch 17 -- Batch 557/ 842, training loss 0.32718974351882935\n",
      "Epoch 17 -- Batch 558/ 842, training loss 0.34060171246528625\n",
      "Epoch 17 -- Batch 559/ 842, training loss 0.3378002941608429\n",
      "Epoch 17 -- Batch 560/ 842, training loss 0.3524816334247589\n",
      "Epoch 17 -- Batch 561/ 842, training loss 0.3450542390346527\n",
      "Epoch 17 -- Batch 562/ 842, training loss 0.3441741466522217\n",
      "Epoch 17 -- Batch 563/ 842, training loss 0.3424580693244934\n",
      "Epoch 17 -- Batch 564/ 842, training loss 0.3537653386592865\n",
      "Epoch 17 -- Batch 565/ 842, training loss 0.35218510031700134\n",
      "Epoch 17 -- Batch 566/ 842, training loss 0.34897154569625854\n",
      "Epoch 17 -- Batch 567/ 842, training loss 0.3369053602218628\n",
      "Epoch 17 -- Batch 568/ 842, training loss 0.3344031274318695\n",
      "Epoch 17 -- Batch 569/ 842, training loss 0.3483283221721649\n",
      "Epoch 17 -- Batch 570/ 842, training loss 0.34863293170928955\n",
      "Epoch 17 -- Batch 571/ 842, training loss 0.34169575572013855\n",
      "Epoch 17 -- Batch 572/ 842, training loss 0.3370128870010376\n",
      "Epoch 17 -- Batch 573/ 842, training loss 0.34592434763908386\n",
      "Epoch 17 -- Batch 574/ 842, training loss 0.33702579140663147\n",
      "Epoch 17 -- Batch 575/ 842, training loss 0.3454485833644867\n",
      "Epoch 17 -- Batch 576/ 842, training loss 0.3420587480068207\n",
      "Epoch 17 -- Batch 577/ 842, training loss 0.35027316212654114\n",
      "Epoch 17 -- Batch 578/ 842, training loss 0.3520669639110565\n",
      "Epoch 17 -- Batch 579/ 842, training loss 0.3366757929325104\n",
      "Epoch 17 -- Batch 580/ 842, training loss 0.3356929421424866\n",
      "Epoch 17 -- Batch 581/ 842, training loss 0.3488585650920868\n",
      "Epoch 17 -- Batch 582/ 842, training loss 0.3294825851917267\n",
      "Epoch 17 -- Batch 583/ 842, training loss 0.3395562767982483\n",
      "Epoch 17 -- Batch 584/ 842, training loss 0.3442947566509247\n",
      "Epoch 17 -- Batch 585/ 842, training loss 0.34151795506477356\n",
      "Epoch 17 -- Batch 586/ 842, training loss 0.32706665992736816\n",
      "Epoch 17 -- Batch 587/ 842, training loss 0.3433828055858612\n",
      "Epoch 17 -- Batch 588/ 842, training loss 0.33457037806510925\n",
      "Epoch 17 -- Batch 589/ 842, training loss 0.3448573350906372\n",
      "Epoch 17 -- Batch 590/ 842, training loss 0.3522432744503021\n",
      "Epoch 17 -- Batch 591/ 842, training loss 0.34581661224365234\n",
      "Epoch 17 -- Batch 592/ 842, training loss 0.34271669387817383\n",
      "Epoch 17 -- Batch 593/ 842, training loss 0.3417797386646271\n",
      "Epoch 17 -- Batch 594/ 842, training loss 0.3355070948600769\n",
      "Epoch 17 -- Batch 595/ 842, training loss 0.34349483251571655\n",
      "Epoch 17 -- Batch 596/ 842, training loss 0.3371886610984802\n",
      "Epoch 17 -- Batch 597/ 842, training loss 0.3439728915691376\n",
      "Epoch 17 -- Batch 598/ 842, training loss 0.3375871181488037\n",
      "Epoch 17 -- Batch 599/ 842, training loss 0.3418391942977905\n",
      "Epoch 17 -- Batch 600/ 842, training loss 0.33998724818229675\n",
      "Epoch 17 -- Batch 601/ 842, training loss 0.34034988284111023\n",
      "Epoch 17 -- Batch 602/ 842, training loss 0.3411717712879181\n",
      "Epoch 17 -- Batch 603/ 842, training loss 0.3362554609775543\n",
      "Epoch 17 -- Batch 604/ 842, training loss 0.3325498700141907\n",
      "Epoch 17 -- Batch 605/ 842, training loss 0.3597109019756317\n",
      "Epoch 17 -- Batch 606/ 842, training loss 0.34295031428337097\n",
      "Epoch 17 -- Batch 607/ 842, training loss 0.34959426522254944\n",
      "Epoch 17 -- Batch 608/ 842, training loss 0.3359115719795227\n",
      "Epoch 17 -- Batch 609/ 842, training loss 0.3402077555656433\n",
      "Epoch 17 -- Batch 610/ 842, training loss 0.3317820131778717\n",
      "Epoch 17 -- Batch 611/ 842, training loss 0.3424201011657715\n",
      "Epoch 17 -- Batch 612/ 842, training loss 0.34212788939476013\n",
      "Epoch 17 -- Batch 613/ 842, training loss 0.34896814823150635\n",
      "Epoch 17 -- Batch 614/ 842, training loss 0.3465871214866638\n",
      "Epoch 17 -- Batch 615/ 842, training loss 0.3479219675064087\n",
      "Epoch 17 -- Batch 616/ 842, training loss 0.34632667899131775\n",
      "Epoch 17 -- Batch 617/ 842, training loss 0.33720576763153076\n",
      "Epoch 17 -- Batch 618/ 842, training loss 0.340529203414917\n",
      "Epoch 17 -- Batch 619/ 842, training loss 0.3578200042247772\n",
      "Epoch 17 -- Batch 620/ 842, training loss 0.33858034014701843\n",
      "Epoch 17 -- Batch 621/ 842, training loss 0.33118611574172974\n",
      "Epoch 17 -- Batch 622/ 842, training loss 0.34980762004852295\n",
      "Epoch 17 -- Batch 623/ 842, training loss 0.34916675090789795\n",
      "Epoch 17 -- Batch 624/ 842, training loss 0.34644147753715515\n",
      "Epoch 17 -- Batch 625/ 842, training loss 0.3400762677192688\n",
      "Epoch 17 -- Batch 626/ 842, training loss 0.33777251839637756\n",
      "Epoch 17 -- Batch 627/ 842, training loss 0.34002500772476196\n",
      "Epoch 17 -- Batch 628/ 842, training loss 0.340869665145874\n",
      "Epoch 17 -- Batch 629/ 842, training loss 0.34772637486457825\n",
      "Epoch 17 -- Batch 630/ 842, training loss 0.3346421420574188\n",
      "Epoch 17 -- Batch 631/ 842, training loss 0.3441692888736725\n",
      "Epoch 17 -- Batch 632/ 842, training loss 0.34073033928871155\n",
      "Epoch 17 -- Batch 633/ 842, training loss 0.342195987701416\n",
      "Epoch 17 -- Batch 634/ 842, training loss 0.3298599123954773\n",
      "Epoch 17 -- Batch 635/ 842, training loss 0.3529779314994812\n",
      "Epoch 17 -- Batch 636/ 842, training loss 0.3436264395713806\n",
      "Epoch 17 -- Batch 637/ 842, training loss 0.3435109555721283\n",
      "Epoch 17 -- Batch 638/ 842, training loss 0.3373345732688904\n",
      "Epoch 17 -- Batch 639/ 842, training loss 0.3533782958984375\n",
      "Epoch 17 -- Batch 640/ 842, training loss 0.34221622347831726\n",
      "Epoch 17 -- Batch 641/ 842, training loss 0.3525119125843048\n",
      "Epoch 17 -- Batch 642/ 842, training loss 0.3412564694881439\n",
      "Epoch 17 -- Batch 643/ 842, training loss 0.3426671326160431\n",
      "Epoch 17 -- Batch 644/ 842, training loss 0.3501262068748474\n",
      "Epoch 17 -- Batch 645/ 842, training loss 0.3367592692375183\n",
      "Epoch 17 -- Batch 646/ 842, training loss 0.3353763222694397\n",
      "Epoch 17 -- Batch 647/ 842, training loss 0.3401687741279602\n",
      "Epoch 17 -- Batch 648/ 842, training loss 0.34202438592910767\n",
      "Epoch 17 -- Batch 649/ 842, training loss 0.34172388911247253\n",
      "Epoch 17 -- Batch 650/ 842, training loss 0.3326854109764099\n",
      "Epoch 17 -- Batch 651/ 842, training loss 0.34491968154907227\n",
      "Epoch 17 -- Batch 652/ 842, training loss 0.33640316128730774\n",
      "Epoch 17 -- Batch 653/ 842, training loss 0.34923404455184937\n",
      "Epoch 17 -- Batch 654/ 842, training loss 0.3406665325164795\n",
      "Epoch 17 -- Batch 655/ 842, training loss 0.3389497697353363\n",
      "Epoch 17 -- Batch 656/ 842, training loss 0.3259999752044678\n",
      "Epoch 17 -- Batch 657/ 842, training loss 0.3385581374168396\n",
      "Epoch 17 -- Batch 658/ 842, training loss 0.3450709283351898\n",
      "Epoch 17 -- Batch 659/ 842, training loss 0.3306691646575928\n",
      "Epoch 17 -- Batch 660/ 842, training loss 0.346327006816864\n",
      "Epoch 17 -- Batch 661/ 842, training loss 0.34277403354644775\n",
      "Epoch 17 -- Batch 662/ 842, training loss 0.3534160256385803\n",
      "Epoch 17 -- Batch 663/ 842, training loss 0.33366191387176514\n",
      "Epoch 17 -- Batch 664/ 842, training loss 0.34741514921188354\n",
      "Epoch 17 -- Batch 665/ 842, training loss 0.35265880823135376\n",
      "Epoch 17 -- Batch 666/ 842, training loss 0.3513195812702179\n",
      "Epoch 17 -- Batch 667/ 842, training loss 0.35251384973526\n",
      "Epoch 17 -- Batch 668/ 842, training loss 0.34398847818374634\n",
      "Epoch 17 -- Batch 669/ 842, training loss 0.3282921612262726\n",
      "Epoch 17 -- Batch 670/ 842, training loss 0.35101014375686646\n",
      "Epoch 17 -- Batch 671/ 842, training loss 0.3394763469696045\n",
      "Epoch 17 -- Batch 672/ 842, training loss 0.3496098518371582\n",
      "Epoch 17 -- Batch 673/ 842, training loss 0.35248643159866333\n",
      "Epoch 17 -- Batch 674/ 842, training loss 0.3454038202762604\n",
      "Epoch 17 -- Batch 675/ 842, training loss 0.33859002590179443\n",
      "Epoch 17 -- Batch 676/ 842, training loss 0.34281763434410095\n",
      "Epoch 17 -- Batch 677/ 842, training loss 0.33017033338546753\n",
      "Epoch 17 -- Batch 678/ 842, training loss 0.35478490591049194\n",
      "Epoch 17 -- Batch 679/ 842, training loss 0.3396771252155304\n",
      "Epoch 17 -- Batch 680/ 842, training loss 0.35187244415283203\n",
      "Epoch 17 -- Batch 681/ 842, training loss 0.3488149046897888\n",
      "Epoch 17 -- Batch 682/ 842, training loss 0.3405933678150177\n",
      "Epoch 17 -- Batch 683/ 842, training loss 0.351171612739563\n",
      "Epoch 17 -- Batch 684/ 842, training loss 0.3413728177547455\n",
      "Epoch 17 -- Batch 685/ 842, training loss 0.3510695695877075\n",
      "Epoch 17 -- Batch 686/ 842, training loss 0.34753409028053284\n",
      "Epoch 17 -- Batch 687/ 842, training loss 0.35397565364837646\n",
      "Epoch 17 -- Batch 688/ 842, training loss 0.33731746673583984\n",
      "Epoch 17 -- Batch 689/ 842, training loss 0.34683477878570557\n",
      "Epoch 17 -- Batch 690/ 842, training loss 0.3609013855457306\n",
      "Epoch 17 -- Batch 691/ 842, training loss 0.3313286006450653\n",
      "Epoch 17 -- Batch 692/ 842, training loss 0.34519872069358826\n",
      "Epoch 17 -- Batch 693/ 842, training loss 0.3409459590911865\n",
      "Epoch 17 -- Batch 694/ 842, training loss 0.35084208846092224\n",
      "Epoch 17 -- Batch 695/ 842, training loss 0.33931830525398254\n",
      "Epoch 17 -- Batch 696/ 842, training loss 0.3384280502796173\n",
      "Epoch 17 -- Batch 697/ 842, training loss 0.33577167987823486\n",
      "Epoch 17 -- Batch 698/ 842, training loss 0.3368169069290161\n",
      "Epoch 17 -- Batch 699/ 842, training loss 0.35323286056518555\n",
      "Epoch 17 -- Batch 700/ 842, training loss 0.33526870608329773\n",
      "Epoch 17 -- Batch 701/ 842, training loss 0.33599886298179626\n",
      "Epoch 17 -- Batch 702/ 842, training loss 0.3375299870967865\n",
      "Epoch 17 -- Batch 703/ 842, training loss 0.33948659896850586\n",
      "Epoch 17 -- Batch 704/ 842, training loss 0.3538637161254883\n",
      "Epoch 17 -- Batch 705/ 842, training loss 0.3491859436035156\n",
      "Epoch 17 -- Batch 706/ 842, training loss 0.34048113226890564\n",
      "Epoch 17 -- Batch 707/ 842, training loss 0.3359261751174927\n",
      "Epoch 17 -- Batch 708/ 842, training loss 0.354837030172348\n",
      "Epoch 17 -- Batch 709/ 842, training loss 0.35512301325798035\n",
      "Epoch 17 -- Batch 710/ 842, training loss 0.3339453637599945\n",
      "Epoch 17 -- Batch 711/ 842, training loss 0.33251282572746277\n",
      "Epoch 17 -- Batch 712/ 842, training loss 0.3473740220069885\n",
      "Epoch 17 -- Batch 713/ 842, training loss 0.34157636761665344\n",
      "Epoch 17 -- Batch 714/ 842, training loss 0.345386803150177\n",
      "Epoch 17 -- Batch 715/ 842, training loss 0.3460003435611725\n",
      "Epoch 17 -- Batch 716/ 842, training loss 0.3407902717590332\n",
      "Epoch 17 -- Batch 717/ 842, training loss 0.3430268168449402\n",
      "Epoch 17 -- Batch 718/ 842, training loss 0.34112748503685\n",
      "Epoch 17 -- Batch 719/ 842, training loss 0.3302340507507324\n",
      "Epoch 17 -- Batch 720/ 842, training loss 0.3395291268825531\n",
      "Epoch 17 -- Batch 721/ 842, training loss 0.35655859112739563\n",
      "Epoch 17 -- Batch 722/ 842, training loss 0.36014026403427124\n",
      "Epoch 17 -- Batch 723/ 842, training loss 0.3536892533302307\n",
      "Epoch 17 -- Batch 724/ 842, training loss 0.3475117087364197\n",
      "Epoch 17 -- Batch 725/ 842, training loss 0.3429437577724457\n",
      "Epoch 17 -- Batch 726/ 842, training loss 0.3412033021450043\n",
      "Epoch 17 -- Batch 727/ 842, training loss 0.3485417366027832\n",
      "Epoch 17 -- Batch 728/ 842, training loss 0.34048110246658325\n",
      "Epoch 17 -- Batch 729/ 842, training loss 0.3338419795036316\n",
      "Epoch 17 -- Batch 730/ 842, training loss 0.34232768416404724\n",
      "Epoch 17 -- Batch 731/ 842, training loss 0.3426215648651123\n",
      "Epoch 17 -- Batch 732/ 842, training loss 0.34558340907096863\n",
      "Epoch 17 -- Batch 733/ 842, training loss 0.34810036420822144\n",
      "Epoch 17 -- Batch 734/ 842, training loss 0.3407677114009857\n",
      "Epoch 17 -- Batch 735/ 842, training loss 0.3410864472389221\n",
      "Epoch 17 -- Batch 736/ 842, training loss 0.3450600504875183\n",
      "Epoch 17 -- Batch 737/ 842, training loss 0.33433857560157776\n",
      "Epoch 17 -- Batch 738/ 842, training loss 0.3364049792289734\n",
      "Epoch 17 -- Batch 739/ 842, training loss 0.3366052210330963\n",
      "Epoch 17 -- Batch 740/ 842, training loss 0.34163644909858704\n",
      "Epoch 17 -- Batch 741/ 842, training loss 0.3457241356372833\n",
      "Epoch 17 -- Batch 742/ 842, training loss 0.3498585820198059\n",
      "Epoch 17 -- Batch 743/ 842, training loss 0.34496769309043884\n",
      "Epoch 17 -- Batch 744/ 842, training loss 0.3463154733181\n",
      "Epoch 17 -- Batch 745/ 842, training loss 0.33822163939476013\n",
      "Epoch 17 -- Batch 746/ 842, training loss 0.34839901328086853\n",
      "Epoch 17 -- Batch 747/ 842, training loss 0.3406746983528137\n",
      "Epoch 17 -- Batch 748/ 842, training loss 0.3522574305534363\n",
      "Epoch 17 -- Batch 749/ 842, training loss 0.34024810791015625\n",
      "Epoch 17 -- Batch 750/ 842, training loss 0.34743940830230713\n",
      "Epoch 17 -- Batch 751/ 842, training loss 0.34809479117393494\n",
      "Epoch 17 -- Batch 752/ 842, training loss 0.339237242937088\n",
      "Epoch 17 -- Batch 753/ 842, training loss 0.3574552834033966\n",
      "Epoch 17 -- Batch 754/ 842, training loss 0.33424636721611023\n",
      "Epoch 17 -- Batch 755/ 842, training loss 0.3412950932979584\n",
      "Epoch 17 -- Batch 756/ 842, training loss 0.335048109292984\n",
      "Epoch 17 -- Batch 757/ 842, training loss 0.32762831449508667\n",
      "Epoch 17 -- Batch 758/ 842, training loss 0.3416616916656494\n",
      "Epoch 17 -- Batch 759/ 842, training loss 0.34531715512275696\n",
      "Epoch 17 -- Batch 760/ 842, training loss 0.34682151675224304\n",
      "Epoch 17 -- Batch 761/ 842, training loss 0.35826653242111206\n",
      "Epoch 17 -- Batch 762/ 842, training loss 0.3410918712615967\n",
      "Epoch 17 -- Batch 763/ 842, training loss 0.35059791803359985\n",
      "Epoch 17 -- Batch 764/ 842, training loss 0.3329850137233734\n",
      "Epoch 17 -- Batch 765/ 842, training loss 0.3526804447174072\n",
      "Epoch 17 -- Batch 766/ 842, training loss 0.33826684951782227\n",
      "Epoch 17 -- Batch 767/ 842, training loss 0.35262879729270935\n",
      "Epoch 17 -- Batch 768/ 842, training loss 0.3451952636241913\n",
      "Epoch 17 -- Batch 769/ 842, training loss 0.3316510319709778\n",
      "Epoch 17 -- Batch 770/ 842, training loss 0.3504068851470947\n",
      "Epoch 17 -- Batch 771/ 842, training loss 0.3377636969089508\n",
      "Epoch 17 -- Batch 772/ 842, training loss 0.3382880389690399\n",
      "Epoch 17 -- Batch 773/ 842, training loss 0.3401048183441162\n",
      "Epoch 17 -- Batch 774/ 842, training loss 0.34865647554397583\n",
      "Epoch 17 -- Batch 775/ 842, training loss 0.341251015663147\n",
      "Epoch 17 -- Batch 776/ 842, training loss 0.34445980191230774\n",
      "Epoch 17 -- Batch 777/ 842, training loss 0.34109291434288025\n",
      "Epoch 17 -- Batch 778/ 842, training loss 0.3518548309803009\n",
      "Epoch 17 -- Batch 779/ 842, training loss 0.3463227450847626\n",
      "Epoch 17 -- Batch 780/ 842, training loss 0.34639307856559753\n",
      "Epoch 17 -- Batch 781/ 842, training loss 0.34846189618110657\n",
      "Epoch 17 -- Batch 782/ 842, training loss 0.34324488043785095\n",
      "Epoch 17 -- Batch 783/ 842, training loss 0.3392898738384247\n",
      "Epoch 17 -- Batch 784/ 842, training loss 0.33785364031791687\n",
      "Epoch 17 -- Batch 785/ 842, training loss 0.3420296907424927\n",
      "Epoch 17 -- Batch 786/ 842, training loss 0.3428536057472229\n",
      "Epoch 17 -- Batch 787/ 842, training loss 0.3583456575870514\n",
      "Epoch 17 -- Batch 788/ 842, training loss 0.35416164994239807\n",
      "Epoch 17 -- Batch 789/ 842, training loss 0.3484356701374054\n",
      "Epoch 17 -- Batch 790/ 842, training loss 0.34665998816490173\n",
      "Epoch 17 -- Batch 791/ 842, training loss 0.3465367555618286\n",
      "Epoch 17 -- Batch 792/ 842, training loss 0.34982359409332275\n",
      "Epoch 17 -- Batch 793/ 842, training loss 0.3566629886627197\n",
      "Epoch 17 -- Batch 794/ 842, training loss 0.34687116742134094\n",
      "Epoch 17 -- Batch 795/ 842, training loss 0.3436603844165802\n",
      "Epoch 17 -- Batch 796/ 842, training loss 0.33802881836891174\n",
      "Epoch 17 -- Batch 797/ 842, training loss 0.3416435420513153\n",
      "Epoch 17 -- Batch 798/ 842, training loss 0.341872900724411\n",
      "Epoch 17 -- Batch 799/ 842, training loss 0.3470149636268616\n",
      "Epoch 17 -- Batch 800/ 842, training loss 0.34209442138671875\n",
      "Epoch 17 -- Batch 801/ 842, training loss 0.34392011165618896\n",
      "Epoch 17 -- Batch 802/ 842, training loss 0.33705729246139526\n",
      "Epoch 17 -- Batch 803/ 842, training loss 0.3469848036766052\n",
      "Epoch 17 -- Batch 804/ 842, training loss 0.3487832844257355\n",
      "Epoch 17 -- Batch 805/ 842, training loss 0.34043899178504944\n",
      "Epoch 17 -- Batch 806/ 842, training loss 0.3329433798789978\n",
      "Epoch 17 -- Batch 807/ 842, training loss 0.34860607981681824\n",
      "Epoch 17 -- Batch 808/ 842, training loss 0.34337350726127625\n",
      "Epoch 17 -- Batch 809/ 842, training loss 0.33288732171058655\n",
      "Epoch 17 -- Batch 810/ 842, training loss 0.3495180606842041\n",
      "Epoch 17 -- Batch 811/ 842, training loss 0.34501710534095764\n",
      "Epoch 17 -- Batch 812/ 842, training loss 0.3439762592315674\n",
      "Epoch 17 -- Batch 813/ 842, training loss 0.3382994532585144\n",
      "Epoch 17 -- Batch 814/ 842, training loss 0.3499058783054352\n",
      "Epoch 17 -- Batch 815/ 842, training loss 0.3440053462982178\n",
      "Epoch 17 -- Batch 816/ 842, training loss 0.34120139479637146\n",
      "Epoch 17 -- Batch 817/ 842, training loss 0.34815558791160583\n",
      "Epoch 17 -- Batch 818/ 842, training loss 0.34293854236602783\n",
      "Epoch 17 -- Batch 819/ 842, training loss 0.3530685603618622\n",
      "Epoch 17 -- Batch 820/ 842, training loss 0.33991894125938416\n",
      "Epoch 17 -- Batch 821/ 842, training loss 0.34266790747642517\n",
      "Epoch 17 -- Batch 822/ 842, training loss 0.33507367968559265\n",
      "Epoch 17 -- Batch 823/ 842, training loss 0.34621214866638184\n",
      "Epoch 17 -- Batch 824/ 842, training loss 0.34363114833831787\n",
      "Epoch 17 -- Batch 825/ 842, training loss 0.33986756205558777\n",
      "Epoch 17 -- Batch 826/ 842, training loss 0.3425910770893097\n",
      "Epoch 17 -- Batch 827/ 842, training loss 0.3398456573486328\n",
      "Epoch 17 -- Batch 828/ 842, training loss 0.350839763879776\n",
      "Epoch 17 -- Batch 829/ 842, training loss 0.33264589309692383\n",
      "Epoch 17 -- Batch 830/ 842, training loss 0.3560555577278137\n",
      "Epoch 17 -- Batch 831/ 842, training loss 0.3425865173339844\n",
      "Epoch 17 -- Batch 832/ 842, training loss 0.33019036054611206\n",
      "Epoch 17 -- Batch 833/ 842, training loss 0.3430401086807251\n",
      "Epoch 17 -- Batch 834/ 842, training loss 0.34964725375175476\n",
      "Epoch 17 -- Batch 835/ 842, training loss 0.3368486166000366\n",
      "Epoch 17 -- Batch 836/ 842, training loss 0.3447543680667877\n",
      "Epoch 17 -- Batch 837/ 842, training loss 0.3480285406112671\n",
      "Epoch 17 -- Batch 838/ 842, training loss 0.35681137442588806\n",
      "Epoch 17 -- Batch 839/ 842, training loss 0.3302796483039856\n",
      "Epoch 17 -- Batch 840/ 842, training loss 0.34478679299354553\n",
      "Epoch 17 -- Batch 841/ 842, training loss 0.34467294812202454\n",
      "Epoch 17 -- Batch 842/ 842, training loss 0.3285396099090576\n",
      "----------------------------------------------------------------------\n",
      "Epoch 17 -- Batch 1/ 94, validation loss 0.33840009570121765\n",
      "Epoch 17 -- Batch 2/ 94, validation loss 0.321053147315979\n",
      "Epoch 17 -- Batch 3/ 94, validation loss 0.3295648694038391\n",
      "Epoch 17 -- Batch 4/ 94, validation loss 0.3391023278236389\n",
      "Epoch 17 -- Batch 5/ 94, validation loss 0.333126962184906\n",
      "Epoch 17 -- Batch 6/ 94, validation loss 0.3273113965988159\n",
      "Epoch 17 -- Batch 7/ 94, validation loss 0.3338737189769745\n",
      "Epoch 17 -- Batch 8/ 94, validation loss 0.3460826277732849\n",
      "Epoch 17 -- Batch 9/ 94, validation loss 0.338949590921402\n",
      "Epoch 17 -- Batch 10/ 94, validation loss 0.33459407091140747\n",
      "Epoch 17 -- Batch 11/ 94, validation loss 0.3339802324771881\n",
      "Epoch 17 -- Batch 12/ 94, validation loss 0.33599671721458435\n",
      "Epoch 17 -- Batch 13/ 94, validation loss 0.3386040925979614\n",
      "Epoch 17 -- Batch 14/ 94, validation loss 0.3354620337486267\n",
      "Epoch 17 -- Batch 15/ 94, validation loss 0.34708371758461\n",
      "Epoch 17 -- Batch 16/ 94, validation loss 0.3435882329940796\n",
      "Epoch 17 -- Batch 17/ 94, validation loss 0.3280593752861023\n",
      "Epoch 17 -- Batch 18/ 94, validation loss 0.33754420280456543\n",
      "Epoch 17 -- Batch 19/ 94, validation loss 0.3396760821342468\n",
      "Epoch 17 -- Batch 20/ 94, validation loss 0.32819700241088867\n",
      "Epoch 17 -- Batch 21/ 94, validation loss 0.3209157884120941\n",
      "Epoch 17 -- Batch 22/ 94, validation loss 0.3264811038970947\n",
      "Epoch 17 -- Batch 23/ 94, validation loss 0.3384763300418854\n",
      "Epoch 17 -- Batch 24/ 94, validation loss 0.329392671585083\n",
      "Epoch 17 -- Batch 25/ 94, validation loss 0.334046334028244\n",
      "Epoch 17 -- Batch 26/ 94, validation loss 0.32758501172065735\n",
      "Epoch 17 -- Batch 27/ 94, validation loss 0.3335955739021301\n",
      "Epoch 17 -- Batch 28/ 94, validation loss 0.33194392919540405\n",
      "Epoch 17 -- Batch 29/ 94, validation loss 0.3284543752670288\n",
      "Epoch 17 -- Batch 30/ 94, validation loss 0.33077681064605713\n",
      "Epoch 17 -- Batch 31/ 94, validation loss 0.32585322856903076\n",
      "Epoch 17 -- Batch 32/ 94, validation loss 0.3483855724334717\n",
      "Epoch 17 -- Batch 33/ 94, validation loss 0.32828661799430847\n",
      "Epoch 17 -- Batch 34/ 94, validation loss 0.3335099220275879\n",
      "Epoch 17 -- Batch 35/ 94, validation loss 0.34027257561683655\n",
      "Epoch 17 -- Batch 36/ 94, validation loss 0.32366541028022766\n",
      "Epoch 17 -- Batch 37/ 94, validation loss 0.33841079473495483\n",
      "Epoch 17 -- Batch 38/ 94, validation loss 0.3269270062446594\n",
      "Epoch 17 -- Batch 39/ 94, validation loss 0.3288251459598541\n",
      "Epoch 17 -- Batch 40/ 94, validation loss 0.3343418836593628\n",
      "Epoch 17 -- Batch 41/ 94, validation loss 0.33108481764793396\n",
      "Epoch 17 -- Batch 42/ 94, validation loss 0.340120792388916\n",
      "Epoch 17 -- Batch 43/ 94, validation loss 0.3457624018192291\n",
      "Epoch 17 -- Batch 44/ 94, validation loss 0.34321099519729614\n",
      "Epoch 17 -- Batch 45/ 94, validation loss 0.3246626853942871\n",
      "Epoch 17 -- Batch 46/ 94, validation loss 0.32862699031829834\n",
      "Epoch 17 -- Batch 47/ 94, validation loss 0.33435821533203125\n",
      "Epoch 17 -- Batch 48/ 94, validation loss 0.33333712816238403\n",
      "Epoch 17 -- Batch 49/ 94, validation loss 0.33257776498794556\n",
      "Epoch 17 -- Batch 50/ 94, validation loss 0.34606269001960754\n",
      "Epoch 17 -- Batch 51/ 94, validation loss 0.33835211396217346\n",
      "Epoch 17 -- Batch 52/ 94, validation loss 0.34886622428894043\n",
      "Epoch 17 -- Batch 53/ 94, validation loss 0.338839590549469\n",
      "Epoch 17 -- Batch 54/ 94, validation loss 0.35938510298728943\n",
      "Epoch 17 -- Batch 55/ 94, validation loss 0.33882004022598267\n",
      "Epoch 17 -- Batch 56/ 94, validation loss 0.3339114487171173\n",
      "Epoch 17 -- Batch 57/ 94, validation loss 0.34103065729141235\n",
      "Epoch 17 -- Batch 58/ 94, validation loss 0.32158416509628296\n",
      "Epoch 17 -- Batch 59/ 94, validation loss 0.32695385813713074\n",
      "Epoch 17 -- Batch 60/ 94, validation loss 0.3284023106098175\n",
      "Epoch 17 -- Batch 61/ 94, validation loss 0.3629888892173767\n",
      "Epoch 17 -- Batch 62/ 94, validation loss 0.3372158706188202\n",
      "Epoch 17 -- Batch 63/ 94, validation loss 0.3340640068054199\n",
      "Epoch 17 -- Batch 64/ 94, validation loss 0.3264029324054718\n",
      "Epoch 17 -- Batch 65/ 94, validation loss 0.331373393535614\n",
      "Epoch 17 -- Batch 66/ 94, validation loss 0.32991206645965576\n",
      "Epoch 17 -- Batch 67/ 94, validation loss 0.3509087860584259\n",
      "Epoch 17 -- Batch 68/ 94, validation loss 0.3265564739704132\n",
      "Epoch 17 -- Batch 69/ 94, validation loss 0.32813042402267456\n",
      "Epoch 17 -- Batch 70/ 94, validation loss 0.33190080523490906\n",
      "Epoch 17 -- Batch 71/ 94, validation loss 0.32281285524368286\n",
      "Epoch 17 -- Batch 72/ 94, validation loss 0.328125923871994\n",
      "Epoch 17 -- Batch 73/ 94, validation loss 0.35626745223999023\n",
      "Epoch 17 -- Batch 74/ 94, validation loss 0.32379862666130066\n",
      "Epoch 17 -- Batch 75/ 94, validation loss 0.3346122205257416\n",
      "Epoch 17 -- Batch 76/ 94, validation loss 0.32764387130737305\n",
      "Epoch 17 -- Batch 77/ 94, validation loss 0.3399101793766022\n",
      "Epoch 17 -- Batch 78/ 94, validation loss 0.3343387246131897\n",
      "Epoch 17 -- Batch 79/ 94, validation loss 0.32871463894844055\n",
      "Epoch 17 -- Batch 80/ 94, validation loss 0.34028366208076477\n",
      "Epoch 17 -- Batch 81/ 94, validation loss 0.3289632201194763\n",
      "Epoch 17 -- Batch 82/ 94, validation loss 0.3302839398384094\n",
      "Epoch 17 -- Batch 83/ 94, validation loss 0.33237627148628235\n",
      "Epoch 17 -- Batch 84/ 94, validation loss 0.32727187871932983\n",
      "Epoch 17 -- Batch 85/ 94, validation loss 0.33532851934432983\n",
      "Epoch 17 -- Batch 86/ 94, validation loss 0.33384257555007935\n",
      "Epoch 17 -- Batch 87/ 94, validation loss 0.3283836841583252\n",
      "Epoch 17 -- Batch 88/ 94, validation loss 0.3338736891746521\n",
      "Epoch 17 -- Batch 89/ 94, validation loss 0.3331967890262604\n",
      "Epoch 17 -- Batch 90/ 94, validation loss 0.34201478958129883\n",
      "Epoch 17 -- Batch 91/ 94, validation loss 0.333495169878006\n",
      "Epoch 17 -- Batch 92/ 94, validation loss 0.3292442858219147\n",
      "Epoch 17 -- Batch 93/ 94, validation loss 0.3326549828052521\n",
      "Epoch 17 -- Batch 94/ 94, validation loss 0.34134817123413086\n",
      "----------------------------------------------------------------------\n",
      "Epoch 17 loss: Training 0.3420950472354889, Validation 0.34134817123413086\n",
      "----------------------------------------------------------------------\n",
      "Epoch 18/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:06:39] Can't kekulize mol.  Unkekulized atoms: 8\n",
      "[19:06:39] Can't kekulize mol.  Unkekulized atoms: 9\n",
      "[19:06:39] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CC[C@@]2(OC1)OC1CC3C4CC=C5C[C@@H](O)CC[C@]5(C)C4CC[C@]1(C)C2'\n",
      "[19:06:39] Can't kekulize mol.  Unkekulized atoms: 13 14 29\n",
      "[19:06:39] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNC(=O)CSc2nnc(Cn3c(=O)sc4ccccc33)n2C)cc1'\n",
      "[19:06:39] SMILES Parse Error: extra open parentheses for input: 'c1ccc2c(c1)CCOC2c1nccc([nH]c2ccccc12'\n",
      "[19:06:39] SMILES Parse Error: unclosed ring for input: 'CC1=NCC(C(=O)N2CCN(C(=O)c3cc4cc(OC)c(OC)cc4sc3c3)CCC2)C1'\n",
      "[19:06:39] SMILES Parse Error: syntax error while parsing: CCOc1ccc(-n2c(C)cc(/)=C(\\C#N)C2c2cc3c(cc2Br)OCO3)cc1C\n",
      "[19:06:39] SMILES Parse Error: Failed parsing SMILES 'CCOc1ccc(-n2c(C)cc(/)=C(\\C#N)C2c2cc3c(cc2Br)OCO3)cc1C' for input: 'CCOc1ccc(-n2c(C)cc(/)=C(\\C#N)C2c2cc3c(cc2Br)OCO3)cc1C'\n",
      "[19:06:40] SMILES Parse Error: extra open parentheses for input: 'Brc1cccc(Oc2ccc3c4c(oc3c2)C1CCCC4C1C4'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'Cc1noc(C)c1S(=O)(=O)Nc1ccc2c(c1)CC(=O)N3[C@@H](C)CO[C@@H](C)C[C@@H](C)O[C@@H](C)CO2'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 12\n",
      "[19:06:40] SMILES Parse Error: ring closure 3 duplicates bond between atom 17 and atom 24 for input: 'O=C1C=C(c2cccc(NS(C)(=O)=O)c2)C[C@@H]2c3(N4CCCCC4)c3ccccc3[C@@H][C@@H]2N1C'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cc(N2CCOCC2)cc(NC(=O)NCc3ccccc3F)c2)c1[N+]cnc12'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)CN2C(=O)[C@@H]3CCCN3C(=O)c2ccccc2C)cc1'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'COc1cccc(Cn2cnc3c(N)c(C)c(C)cc31)c1'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CC2CCN(C(=O)c3cc4c(s2)CCCC4)CC2)cc1OC'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'O=S(=O)(NCCn1ncnc2-c3ccccc3Sc3ccccc31)c1ccccc1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 1 2 4\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 20 21 31 32 33\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 4 22 23 24 25\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 3 20 21 22 23 24 25 26 27\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 20\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'C=CCn1c([C@@H]2C[C@@H]3[C@H](O)[C@H](O)[C@@H](O)[C@@H]3O)[C@@H]c2ccc([N+](=O)[O-])cc21'\n",
      "[19:06:40] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1ccccc1C(=O)N1CCOCC1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 8 10 25 26\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'c1ccc(-c2nn3nc(C4-c5ccc(Cl)cc5)c(Cl)cc3n3)cc2'\n",
      "[19:06:40] Explicit valence for atom # 8 Cl, 2, is greater than permitted\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CCOc1cc(C2CC(=O)N(CC(=O)OCC)C(=O)NC2=C2C(=O)CC(C)(C)C3)ccc1O'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 3 9 10 15 16 17 19 21 22\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'Clc1cccc(-c2ccc(NC3CCCN(Cc4nc(C5CC4)no4)C3)cc2)c1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 19 20\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)C(=O)OC1CCC2(C)C(CC(=O)N3Cc3ccccc3)CCN13'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 1 2 20 21 22\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 2 14\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 20 21 22 23 24 30 31\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 8 14 15 16 17 18 19\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 8\n",
      "[19:06:40] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 23 24 25 26 27\n",
      "[19:06:40] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2[nH]c(=O)c(CN(C)c3nc(C(C)(C)C)no3)c(=O)[nH]2)c1\n",
      "[19:06:40] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2[nH]c(=O)c(CN(C)c3nc(C(C)(C)C)no3)c(=O)[nH]2)c1' for input: 'Cc1ccc2[nH]c(=O)c(CN(C)c3nc(C(C)(C)C)no3)c(=O)[nH]2)c1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 9 11 18\n",
      "[19:06:40] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2nc(N3CCC(NC4C5CC6)CC4)CC3)nc(C)c2c1\n",
      "[19:06:40] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2nc(N3CCC(NC4C5CC6)CC4)CC3)nc(C)c2c1' for input: 'COc1ccc2nc(N3CCC(NC4C5CC6)CC4)CC3)nc(C)c2c1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 10 11 12\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 2 4 5 6 20\n",
      "[19:06:40] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[19:06:40] Explicit valence for atom # 14 O, 3, is greater than permitted\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 2\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CC(=O)OC1CCC(C(=O)OC)(C(=O)OC23CCCC(C)(C(=O)OC)C3C)CC1'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'O=C(c1cccs1)N1CCN(C(=O)C2CSC3(C)CN(CC(C)C)C2)CC1'\n",
      "[19:06:40] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(C(=O)Nc2ccc3c(c2)C(=O)N(C)C[C@@H](OC)[C@H](C)CN(CC2CC2)[C@@H](C)CO3'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 1 5 6 7 8 9 10 20\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(N3CCC(NC4C5CCOCC4)CC4)nc(N)c3c2c1'\n",
      "[19:06:40] Explicit valence for atom # 14 C, 5, is greater than permitted\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'c1ccc2c3c([nH]c2c1)C1C2CCC4CC12CC(CO1C(=O)CCC4)C1'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CCn1cccc(CN2CCN(C)C(=O)C2(C)C)c1cnnn1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 5 6 8\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CC(C)N(C(=O)CSc1nnc(CN2CCOCC1)n1C)C(C)C'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CN(C)c1cccc(Oc2cc(C(F)(F)F)cc3c(cnn4CCNC(=O)c4cccs4)c23)c1'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 27\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'Cc1c(C#N)c(N)c2c3c(sc1n1)CCCC3'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)c1ccc(S(=O)(=O)N2CCC[C@H]3CN(Cc4ccc(F)cc4)CC2)cc1'\n",
      "[19:06:40] Explicit valence for atom # 8 O, 3, is greater than permitted\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 3 4 6 7 8\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)Nc2cccc(-c3cc4cc5c(cc3OC(N)=O)ccc3n2)cc2)cc1'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(C)oc2c1c(CN(C)C)c1c(=O)n(C)c(=O)n2C'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 25 26 27\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(Cl)cc1)Nc1nnc(CSs2)s1'\n",
      "[19:06:40] SMILES Parse Error: unclosed ring for input: 'O=C(C1CCC1)N1CCc2ccc(NC(=O)C3(c4cccc(F)c4)CCOC(=O)C3CCCCC3)cc21'\n",
      "[19:06:40] Can't kekulize mol.  Unkekulized atoms: 13 14 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 -- Batch 1/ 842, training loss 0.3246118128299713\n",
      "Epoch 18 -- Batch 2/ 842, training loss 0.3254129886627197\n",
      "Epoch 18 -- Batch 3/ 842, training loss 0.33914270997047424\n",
      "Epoch 18 -- Batch 4/ 842, training loss 0.33431270718574524\n",
      "Epoch 18 -- Batch 5/ 842, training loss 0.3470187187194824\n",
      "Epoch 18 -- Batch 6/ 842, training loss 0.3404253423213959\n",
      "Epoch 18 -- Batch 7/ 842, training loss 0.3414013981819153\n",
      "Epoch 18 -- Batch 8/ 842, training loss 0.34155553579330444\n",
      "Epoch 18 -- Batch 9/ 842, training loss 0.33105385303497314\n",
      "Epoch 18 -- Batch 10/ 842, training loss 0.34139955043792725\n",
      "Epoch 18 -- Batch 11/ 842, training loss 0.33808761835098267\n",
      "Epoch 18 -- Batch 12/ 842, training loss 0.3519282639026642\n",
      "Epoch 18 -- Batch 13/ 842, training loss 0.34202778339385986\n",
      "Epoch 18 -- Batch 14/ 842, training loss 0.3400056064128876\n",
      "Epoch 18 -- Batch 15/ 842, training loss 0.3301893472671509\n",
      "Epoch 18 -- Batch 16/ 842, training loss 0.3318544030189514\n",
      "Epoch 18 -- Batch 17/ 842, training loss 0.33836832642555237\n",
      "Epoch 18 -- Batch 18/ 842, training loss 0.3452981114387512\n",
      "Epoch 18 -- Batch 19/ 842, training loss 0.34712842106819153\n",
      "Epoch 18 -- Batch 20/ 842, training loss 0.32779091596603394\n",
      "Epoch 18 -- Batch 21/ 842, training loss 0.3312038481235504\n",
      "Epoch 18 -- Batch 22/ 842, training loss 0.34404200315475464\n",
      "Epoch 18 -- Batch 23/ 842, training loss 0.32892829179763794\n",
      "Epoch 18 -- Batch 24/ 842, training loss 0.33094725012779236\n",
      "Epoch 18 -- Batch 25/ 842, training loss 0.331805020570755\n",
      "Epoch 18 -- Batch 26/ 842, training loss 0.33608579635620117\n",
      "Epoch 18 -- Batch 27/ 842, training loss 0.335784912109375\n",
      "Epoch 18 -- Batch 28/ 842, training loss 0.3256699740886688\n",
      "Epoch 18 -- Batch 29/ 842, training loss 0.33159103989601135\n",
      "Epoch 18 -- Batch 30/ 842, training loss 0.3207527697086334\n",
      "Epoch 18 -- Batch 31/ 842, training loss 0.32873615622520447\n",
      "Epoch 18 -- Batch 32/ 842, training loss 0.34099259972572327\n",
      "Epoch 18 -- Batch 33/ 842, training loss 0.3390496075153351\n",
      "Epoch 18 -- Batch 34/ 842, training loss 0.33399131894111633\n",
      "Epoch 18 -- Batch 35/ 842, training loss 0.33096230030059814\n",
      "Epoch 18 -- Batch 36/ 842, training loss 0.3393622636795044\n",
      "Epoch 18 -- Batch 37/ 842, training loss 0.32673436403274536\n",
      "Epoch 18 -- Batch 38/ 842, training loss 0.33567872643470764\n",
      "Epoch 18 -- Batch 39/ 842, training loss 0.33340054750442505\n",
      "Epoch 18 -- Batch 40/ 842, training loss 0.3381780683994293\n",
      "Epoch 18 -- Batch 41/ 842, training loss 0.3390856087207794\n",
      "Epoch 18 -- Batch 42/ 842, training loss 0.3396459221839905\n",
      "Epoch 18 -- Batch 43/ 842, training loss 0.3237614333629608\n",
      "Epoch 18 -- Batch 44/ 842, training loss 0.33935847878456116\n",
      "Epoch 18 -- Batch 45/ 842, training loss 0.34290066361427307\n",
      "Epoch 18 -- Batch 46/ 842, training loss 0.3325181007385254\n",
      "Epoch 18 -- Batch 47/ 842, training loss 0.3318284749984741\n",
      "Epoch 18 -- Batch 48/ 842, training loss 0.3386358916759491\n",
      "Epoch 18 -- Batch 49/ 842, training loss 0.31033799052238464\n",
      "Epoch 18 -- Batch 50/ 842, training loss 0.327629953622818\n",
      "Epoch 18 -- Batch 51/ 842, training loss 0.3272243142127991\n",
      "Epoch 18 -- Batch 52/ 842, training loss 0.31425541639328003\n",
      "Epoch 18 -- Batch 53/ 842, training loss 0.34374523162841797\n",
      "Epoch 18 -- Batch 54/ 842, training loss 0.31823229789733887\n",
      "Epoch 18 -- Batch 55/ 842, training loss 0.3319043815135956\n",
      "Epoch 18 -- Batch 56/ 842, training loss 0.34097009897232056\n",
      "Epoch 18 -- Batch 57/ 842, training loss 0.32888153195381165\n",
      "Epoch 18 -- Batch 58/ 842, training loss 0.32976073026657104\n",
      "Epoch 18 -- Batch 59/ 842, training loss 0.3363766074180603\n",
      "Epoch 18 -- Batch 60/ 842, training loss 0.32481706142425537\n",
      "Epoch 18 -- Batch 61/ 842, training loss 0.3312486708164215\n",
      "Epoch 18 -- Batch 62/ 842, training loss 0.3391135036945343\n",
      "Epoch 18 -- Batch 63/ 842, training loss 0.34143203496932983\n",
      "Epoch 18 -- Batch 64/ 842, training loss 0.3445185124874115\n",
      "Epoch 18 -- Batch 65/ 842, training loss 0.3403261601924896\n",
      "Epoch 18 -- Batch 66/ 842, training loss 0.3342318534851074\n",
      "Epoch 18 -- Batch 67/ 842, training loss 0.33725523948669434\n",
      "Epoch 18 -- Batch 68/ 842, training loss 0.34608325362205505\n",
      "Epoch 18 -- Batch 69/ 842, training loss 0.33438554406166077\n",
      "Epoch 18 -- Batch 70/ 842, training loss 0.3299618661403656\n",
      "Epoch 18 -- Batch 71/ 842, training loss 0.3459436297416687\n",
      "Epoch 18 -- Batch 72/ 842, training loss 0.33155205845832825\n",
      "Epoch 18 -- Batch 73/ 842, training loss 0.32915428280830383\n",
      "Epoch 18 -- Batch 74/ 842, training loss 0.3290098309516907\n",
      "Epoch 18 -- Batch 75/ 842, training loss 0.340282142162323\n",
      "Epoch 18 -- Batch 76/ 842, training loss 0.3316608667373657\n",
      "Epoch 18 -- Batch 77/ 842, training loss 0.3486664891242981\n",
      "Epoch 18 -- Batch 78/ 842, training loss 0.3294687271118164\n",
      "Epoch 18 -- Batch 79/ 842, training loss 0.3402350842952728\n",
      "Epoch 18 -- Batch 80/ 842, training loss 0.32358071208000183\n",
      "Epoch 18 -- Batch 81/ 842, training loss 0.34949764609336853\n",
      "Epoch 18 -- Batch 82/ 842, training loss 0.3316943049430847\n",
      "Epoch 18 -- Batch 83/ 842, training loss 0.33447760343551636\n",
      "Epoch 18 -- Batch 84/ 842, training loss 0.327659010887146\n",
      "Epoch 18 -- Batch 85/ 842, training loss 0.3360215723514557\n",
      "Epoch 18 -- Batch 86/ 842, training loss 0.33464664220809937\n",
      "Epoch 18 -- Batch 87/ 842, training loss 0.32188260555267334\n",
      "Epoch 18 -- Batch 88/ 842, training loss 0.32592830061912537\n",
      "Epoch 18 -- Batch 89/ 842, training loss 0.334261029958725\n",
      "Epoch 18 -- Batch 90/ 842, training loss 0.3389575183391571\n",
      "Epoch 18 -- Batch 91/ 842, training loss 0.3399084806442261\n",
      "Epoch 18 -- Batch 92/ 842, training loss 0.34417447447776794\n",
      "Epoch 18 -- Batch 93/ 842, training loss 0.3376595973968506\n",
      "Epoch 18 -- Batch 94/ 842, training loss 0.3351350724697113\n",
      "Epoch 18 -- Batch 95/ 842, training loss 0.3422222137451172\n",
      "Epoch 18 -- Batch 96/ 842, training loss 0.33032065629959106\n",
      "Epoch 18 -- Batch 97/ 842, training loss 0.32582199573516846\n",
      "Epoch 18 -- Batch 98/ 842, training loss 0.3448677361011505\n",
      "Epoch 18 -- Batch 99/ 842, training loss 0.3397972881793976\n",
      "Epoch 18 -- Batch 100/ 842, training loss 0.33412113785743713\n",
      "Epoch 18 -- Batch 101/ 842, training loss 0.33979010581970215\n",
      "Epoch 18 -- Batch 102/ 842, training loss 0.3247981369495392\n",
      "Epoch 18 -- Batch 103/ 842, training loss 0.3357505202293396\n",
      "Epoch 18 -- Batch 104/ 842, training loss 0.342095285654068\n",
      "Epoch 18 -- Batch 105/ 842, training loss 0.33282822370529175\n",
      "Epoch 18 -- Batch 106/ 842, training loss 0.34108445048332214\n",
      "Epoch 18 -- Batch 107/ 842, training loss 0.326580286026001\n",
      "Epoch 18 -- Batch 108/ 842, training loss 0.3329543471336365\n",
      "Epoch 18 -- Batch 109/ 842, training loss 0.3265467584133148\n",
      "Epoch 18 -- Batch 110/ 842, training loss 0.3492392301559448\n",
      "Epoch 18 -- Batch 111/ 842, training loss 0.3342067301273346\n",
      "Epoch 18 -- Batch 112/ 842, training loss 0.3177942633628845\n",
      "Epoch 18 -- Batch 113/ 842, training loss 0.34356847405433655\n",
      "Epoch 18 -- Batch 114/ 842, training loss 0.32820865511894226\n",
      "Epoch 18 -- Batch 115/ 842, training loss 0.32295161485671997\n",
      "Epoch 18 -- Batch 116/ 842, training loss 0.3279069662094116\n",
      "Epoch 18 -- Batch 117/ 842, training loss 0.3303993344306946\n",
      "Epoch 18 -- Batch 118/ 842, training loss 0.33089518547058105\n",
      "Epoch 18 -- Batch 119/ 842, training loss 0.3142375349998474\n",
      "Epoch 18 -- Batch 120/ 842, training loss 0.32032138109207153\n",
      "Epoch 18 -- Batch 121/ 842, training loss 0.3337509036064148\n",
      "Epoch 18 -- Batch 122/ 842, training loss 0.33930453658103943\n",
      "Epoch 18 -- Batch 123/ 842, training loss 0.331760972738266\n",
      "Epoch 18 -- Batch 124/ 842, training loss 0.3328680992126465\n",
      "Epoch 18 -- Batch 125/ 842, training loss 0.33972305059432983\n",
      "Epoch 18 -- Batch 126/ 842, training loss 0.32356706261634827\n",
      "Epoch 18 -- Batch 127/ 842, training loss 0.3432474732398987\n",
      "Epoch 18 -- Batch 128/ 842, training loss 0.3400106728076935\n",
      "Epoch 18 -- Batch 129/ 842, training loss 0.33535581827163696\n",
      "Epoch 18 -- Batch 130/ 842, training loss 0.3429635167121887\n",
      "Epoch 18 -- Batch 131/ 842, training loss 0.3303894102573395\n",
      "Epoch 18 -- Batch 132/ 842, training loss 0.33652257919311523\n",
      "Epoch 18 -- Batch 133/ 842, training loss 0.3368277847766876\n",
      "Epoch 18 -- Batch 134/ 842, training loss 0.3248848617076874\n",
      "Epoch 18 -- Batch 135/ 842, training loss 0.3363414406776428\n",
      "Epoch 18 -- Batch 136/ 842, training loss 0.329645574092865\n",
      "Epoch 18 -- Batch 137/ 842, training loss 0.33572497963905334\n",
      "Epoch 18 -- Batch 138/ 842, training loss 0.3339068591594696\n",
      "Epoch 18 -- Batch 139/ 842, training loss 0.3296688199043274\n",
      "Epoch 18 -- Batch 140/ 842, training loss 0.3390218913555145\n",
      "Epoch 18 -- Batch 141/ 842, training loss 0.33745789527893066\n",
      "Epoch 18 -- Batch 142/ 842, training loss 0.3374268710613251\n",
      "Epoch 18 -- Batch 143/ 842, training loss 0.3288626968860626\n",
      "Epoch 18 -- Batch 144/ 842, training loss 0.32980766892433167\n",
      "Epoch 18 -- Batch 145/ 842, training loss 0.3323197066783905\n",
      "Epoch 18 -- Batch 146/ 842, training loss 0.34861060976982117\n",
      "Epoch 18 -- Batch 147/ 842, training loss 0.3299507796764374\n",
      "Epoch 18 -- Batch 148/ 842, training loss 0.32914185523986816\n",
      "Epoch 18 -- Batch 149/ 842, training loss 0.3097383379936218\n",
      "Epoch 18 -- Batch 150/ 842, training loss 0.3438006043434143\n",
      "Epoch 18 -- Batch 151/ 842, training loss 0.31747984886169434\n",
      "Epoch 18 -- Batch 152/ 842, training loss 0.3331511914730072\n",
      "Epoch 18 -- Batch 153/ 842, training loss 0.33773186802864075\n",
      "Epoch 18 -- Batch 154/ 842, training loss 0.32409337162971497\n",
      "Epoch 18 -- Batch 155/ 842, training loss 0.3339923918247223\n",
      "Epoch 18 -- Batch 156/ 842, training loss 0.3362937867641449\n",
      "Epoch 18 -- Batch 157/ 842, training loss 0.3279363512992859\n",
      "Epoch 18 -- Batch 158/ 842, training loss 0.3265898525714874\n",
      "Epoch 18 -- Batch 159/ 842, training loss 0.3277485966682434\n",
      "Epoch 18 -- Batch 160/ 842, training loss 0.3450569212436676\n",
      "Epoch 18 -- Batch 161/ 842, training loss 0.3409968316555023\n",
      "Epoch 18 -- Batch 162/ 842, training loss 0.333234965801239\n",
      "Epoch 18 -- Batch 163/ 842, training loss 0.3432207703590393\n",
      "Epoch 18 -- Batch 164/ 842, training loss 0.3418632745742798\n",
      "Epoch 18 -- Batch 165/ 842, training loss 0.3288338780403137\n",
      "Epoch 18 -- Batch 166/ 842, training loss 0.3310949206352234\n",
      "Epoch 18 -- Batch 167/ 842, training loss 0.3432031273841858\n",
      "Epoch 18 -- Batch 168/ 842, training loss 0.3221602737903595\n",
      "Epoch 18 -- Batch 169/ 842, training loss 0.3403928577899933\n",
      "Epoch 18 -- Batch 170/ 842, training loss 0.3336293399333954\n",
      "Epoch 18 -- Batch 171/ 842, training loss 0.3384401500225067\n",
      "Epoch 18 -- Batch 172/ 842, training loss 0.3236805498600006\n",
      "Epoch 18 -- Batch 173/ 842, training loss 0.331465482711792\n",
      "Epoch 18 -- Batch 174/ 842, training loss 0.330501526594162\n",
      "Epoch 18 -- Batch 175/ 842, training loss 0.32433196902275085\n",
      "Epoch 18 -- Batch 176/ 842, training loss 0.3270931839942932\n",
      "Epoch 18 -- Batch 177/ 842, training loss 0.32894593477249146\n",
      "Epoch 18 -- Batch 178/ 842, training loss 0.33772096037864685\n",
      "Epoch 18 -- Batch 179/ 842, training loss 0.3434481918811798\n",
      "Epoch 18 -- Batch 180/ 842, training loss 0.3335477411746979\n",
      "Epoch 18 -- Batch 181/ 842, training loss 0.3432231843471527\n",
      "Epoch 18 -- Batch 182/ 842, training loss 0.34282445907592773\n",
      "Epoch 18 -- Batch 183/ 842, training loss 0.3508407771587372\n",
      "Epoch 18 -- Batch 184/ 842, training loss 0.33658552169799805\n",
      "Epoch 18 -- Batch 185/ 842, training loss 0.32792410254478455\n",
      "Epoch 18 -- Batch 186/ 842, training loss 0.3298611044883728\n",
      "Epoch 18 -- Batch 187/ 842, training loss 0.3326922059059143\n",
      "Epoch 18 -- Batch 188/ 842, training loss 0.3199143707752228\n",
      "Epoch 18 -- Batch 189/ 842, training loss 0.3314836323261261\n",
      "Epoch 18 -- Batch 190/ 842, training loss 0.3307384252548218\n",
      "Epoch 18 -- Batch 191/ 842, training loss 0.3313566744327545\n",
      "Epoch 18 -- Batch 192/ 842, training loss 0.3319697380065918\n",
      "Epoch 18 -- Batch 193/ 842, training loss 0.3503838777542114\n",
      "Epoch 18 -- Batch 194/ 842, training loss 0.3336716890335083\n",
      "Epoch 18 -- Batch 195/ 842, training loss 0.33608853816986084\n",
      "Epoch 18 -- Batch 196/ 842, training loss 0.3414126932621002\n",
      "Epoch 18 -- Batch 197/ 842, training loss 0.3269249200820923\n",
      "Epoch 18 -- Batch 198/ 842, training loss 0.3310418426990509\n",
      "Epoch 18 -- Batch 199/ 842, training loss 0.32489895820617676\n",
      "Epoch 18 -- Batch 200/ 842, training loss 0.32457372546195984\n",
      "Epoch 18 -- Batch 201/ 842, training loss 0.33624401688575745\n",
      "Epoch 18 -- Batch 202/ 842, training loss 0.3378849923610687\n",
      "Epoch 18 -- Batch 203/ 842, training loss 0.3326193392276764\n",
      "Epoch 18 -- Batch 204/ 842, training loss 0.33791860938072205\n",
      "Epoch 18 -- Batch 205/ 842, training loss 0.3416776955127716\n",
      "Epoch 18 -- Batch 206/ 842, training loss 0.3208971321582794\n",
      "Epoch 18 -- Batch 207/ 842, training loss 0.3206040561199188\n",
      "Epoch 18 -- Batch 208/ 842, training loss 0.33128276467323303\n",
      "Epoch 18 -- Batch 209/ 842, training loss 0.34315451979637146\n",
      "Epoch 18 -- Batch 210/ 842, training loss 0.3376238942146301\n",
      "Epoch 18 -- Batch 211/ 842, training loss 0.34893760085105896\n",
      "Epoch 18 -- Batch 212/ 842, training loss 0.33681750297546387\n",
      "Epoch 18 -- Batch 213/ 842, training loss 0.3409912884235382\n",
      "Epoch 18 -- Batch 214/ 842, training loss 0.3262444734573364\n",
      "Epoch 18 -- Batch 215/ 842, training loss 0.3186541199684143\n",
      "Epoch 18 -- Batch 216/ 842, training loss 0.3255389928817749\n",
      "Epoch 18 -- Batch 217/ 842, training loss 0.3338794410228729\n",
      "Epoch 18 -- Batch 218/ 842, training loss 0.34385794401168823\n",
      "Epoch 18 -- Batch 219/ 842, training loss 0.33745360374450684\n",
      "Epoch 18 -- Batch 220/ 842, training loss 0.3315456509590149\n",
      "Epoch 18 -- Batch 221/ 842, training loss 0.3490217328071594\n",
      "Epoch 18 -- Batch 222/ 842, training loss 0.3388206958770752\n",
      "Epoch 18 -- Batch 223/ 842, training loss 0.33378273248672485\n",
      "Epoch 18 -- Batch 224/ 842, training loss 0.33014893531799316\n",
      "Epoch 18 -- Batch 225/ 842, training loss 0.3363755941390991\n",
      "Epoch 18 -- Batch 226/ 842, training loss 0.3293361961841583\n",
      "Epoch 18 -- Batch 227/ 842, training loss 0.3207308053970337\n",
      "Epoch 18 -- Batch 228/ 842, training loss 0.3176634907722473\n",
      "Epoch 18 -- Batch 229/ 842, training loss 0.3461979925632477\n",
      "Epoch 18 -- Batch 230/ 842, training loss 0.33220699429512024\n",
      "Epoch 18 -- Batch 231/ 842, training loss 0.32839110493659973\n",
      "Epoch 18 -- Batch 232/ 842, training loss 0.34681427478790283\n",
      "Epoch 18 -- Batch 233/ 842, training loss 0.33143624663352966\n",
      "Epoch 18 -- Batch 234/ 842, training loss 0.34019333124160767\n",
      "Epoch 18 -- Batch 235/ 842, training loss 0.33385175466537476\n",
      "Epoch 18 -- Batch 236/ 842, training loss 0.3355861008167267\n",
      "Epoch 18 -- Batch 237/ 842, training loss 0.33214274048805237\n",
      "Epoch 18 -- Batch 238/ 842, training loss 0.3261440396308899\n",
      "Epoch 18 -- Batch 239/ 842, training loss 0.3333008587360382\n",
      "Epoch 18 -- Batch 240/ 842, training loss 0.33377766609191895\n",
      "Epoch 18 -- Batch 241/ 842, training loss 0.3448704481124878\n",
      "Epoch 18 -- Batch 242/ 842, training loss 0.32800132036209106\n",
      "Epoch 18 -- Batch 243/ 842, training loss 0.3400503396987915\n",
      "Epoch 18 -- Batch 244/ 842, training loss 0.33268293738365173\n",
      "Epoch 18 -- Batch 245/ 842, training loss 0.34324774146080017\n",
      "Epoch 18 -- Batch 246/ 842, training loss 0.3376963436603546\n",
      "Epoch 18 -- Batch 247/ 842, training loss 0.33260735869407654\n",
      "Epoch 18 -- Batch 248/ 842, training loss 0.3270000219345093\n",
      "Epoch 18 -- Batch 249/ 842, training loss 0.33900174498558044\n",
      "Epoch 18 -- Batch 250/ 842, training loss 0.332888126373291\n",
      "Epoch 18 -- Batch 251/ 842, training loss 0.331815242767334\n",
      "Epoch 18 -- Batch 252/ 842, training loss 0.3300238251686096\n",
      "Epoch 18 -- Batch 253/ 842, training loss 0.3379330635070801\n",
      "Epoch 18 -- Batch 254/ 842, training loss 0.34690237045288086\n",
      "Epoch 18 -- Batch 255/ 842, training loss 0.3342262804508209\n",
      "Epoch 18 -- Batch 256/ 842, training loss 0.3509463965892792\n",
      "Epoch 18 -- Batch 257/ 842, training loss 0.3416982591152191\n",
      "Epoch 18 -- Batch 258/ 842, training loss 0.34522145986557007\n",
      "Epoch 18 -- Batch 259/ 842, training loss 0.3320100009441376\n",
      "Epoch 18 -- Batch 260/ 842, training loss 0.34067171812057495\n",
      "Epoch 18 -- Batch 261/ 842, training loss 0.33329230546951294\n",
      "Epoch 18 -- Batch 262/ 842, training loss 0.32428231835365295\n",
      "Epoch 18 -- Batch 263/ 842, training loss 0.33500736951828003\n",
      "Epoch 18 -- Batch 264/ 842, training loss 0.3227238357067108\n",
      "Epoch 18 -- Batch 265/ 842, training loss 0.33975592255592346\n",
      "Epoch 18 -- Batch 266/ 842, training loss 0.3446703553199768\n",
      "Epoch 18 -- Batch 267/ 842, training loss 0.3401675522327423\n",
      "Epoch 18 -- Batch 268/ 842, training loss 0.33168676495552063\n",
      "Epoch 18 -- Batch 269/ 842, training loss 0.34080246090888977\n",
      "Epoch 18 -- Batch 270/ 842, training loss 0.340746134519577\n",
      "Epoch 18 -- Batch 271/ 842, training loss 0.33569830656051636\n",
      "Epoch 18 -- Batch 272/ 842, training loss 0.3294203281402588\n",
      "Epoch 18 -- Batch 273/ 842, training loss 0.3210403025150299\n",
      "Epoch 18 -- Batch 274/ 842, training loss 0.3388659954071045\n",
      "Epoch 18 -- Batch 275/ 842, training loss 0.33997562527656555\n",
      "Epoch 18 -- Batch 276/ 842, training loss 0.33181360363960266\n",
      "Epoch 18 -- Batch 277/ 842, training loss 0.33267003297805786\n",
      "Epoch 18 -- Batch 278/ 842, training loss 0.3309974670410156\n",
      "Epoch 18 -- Batch 279/ 842, training loss 0.3360617458820343\n",
      "Epoch 18 -- Batch 280/ 842, training loss 0.34112951159477234\n",
      "Epoch 18 -- Batch 281/ 842, training loss 0.3312327265739441\n",
      "Epoch 18 -- Batch 282/ 842, training loss 0.3389975130558014\n",
      "Epoch 18 -- Batch 283/ 842, training loss 0.33449578285217285\n",
      "Epoch 18 -- Batch 284/ 842, training loss 0.341031938791275\n",
      "Epoch 18 -- Batch 285/ 842, training loss 0.35082218050956726\n",
      "Epoch 18 -- Batch 286/ 842, training loss 0.3210121989250183\n",
      "Epoch 18 -- Batch 287/ 842, training loss 0.3407312035560608\n",
      "Epoch 18 -- Batch 288/ 842, training loss 0.3317365348339081\n",
      "Epoch 18 -- Batch 289/ 842, training loss 0.3354845345020294\n",
      "Epoch 18 -- Batch 290/ 842, training loss 0.3487771451473236\n",
      "Epoch 18 -- Batch 291/ 842, training loss 0.3391425311565399\n",
      "Epoch 18 -- Batch 292/ 842, training loss 0.3441644012928009\n",
      "Epoch 18 -- Batch 293/ 842, training loss 0.3472861051559448\n",
      "Epoch 18 -- Batch 294/ 842, training loss 0.32859882712364197\n",
      "Epoch 18 -- Batch 295/ 842, training loss 0.3501856327056885\n",
      "Epoch 18 -- Batch 296/ 842, training loss 0.3229065239429474\n",
      "Epoch 18 -- Batch 297/ 842, training loss 0.34298017621040344\n",
      "Epoch 18 -- Batch 298/ 842, training loss 0.34182706475257874\n",
      "Epoch 18 -- Batch 299/ 842, training loss 0.34605762362480164\n",
      "Epoch 18 -- Batch 300/ 842, training loss 0.33498039841651917\n",
      "Epoch 18 -- Batch 301/ 842, training loss 0.33634909987449646\n",
      "Epoch 18 -- Batch 302/ 842, training loss 0.3394869267940521\n",
      "Epoch 18 -- Batch 303/ 842, training loss 0.3373587727546692\n",
      "Epoch 18 -- Batch 304/ 842, training loss 0.34568876028060913\n",
      "Epoch 18 -- Batch 305/ 842, training loss 0.3435753285884857\n",
      "Epoch 18 -- Batch 306/ 842, training loss 0.33968403935432434\n",
      "Epoch 18 -- Batch 307/ 842, training loss 0.33545494079589844\n",
      "Epoch 18 -- Batch 308/ 842, training loss 0.33412760496139526\n",
      "Epoch 18 -- Batch 309/ 842, training loss 0.33882641792297363\n",
      "Epoch 18 -- Batch 310/ 842, training loss 0.3400907516479492\n",
      "Epoch 18 -- Batch 311/ 842, training loss 0.33607396483421326\n",
      "Epoch 18 -- Batch 312/ 842, training loss 0.32992756366729736\n",
      "Epoch 18 -- Batch 313/ 842, training loss 0.3368738293647766\n",
      "Epoch 18 -- Batch 314/ 842, training loss 0.34137046337127686\n",
      "Epoch 18 -- Batch 315/ 842, training loss 0.33136582374572754\n",
      "Epoch 18 -- Batch 316/ 842, training loss 0.3445942997932434\n",
      "Epoch 18 -- Batch 317/ 842, training loss 0.3267483115196228\n",
      "Epoch 18 -- Batch 318/ 842, training loss 0.3376707434654236\n",
      "Epoch 18 -- Batch 319/ 842, training loss 0.34172555804252625\n",
      "Epoch 18 -- Batch 320/ 842, training loss 0.3386850357055664\n",
      "Epoch 18 -- Batch 321/ 842, training loss 0.3347230553627014\n",
      "Epoch 18 -- Batch 322/ 842, training loss 0.3214709162712097\n",
      "Epoch 18 -- Batch 323/ 842, training loss 0.33355432748794556\n",
      "Epoch 18 -- Batch 324/ 842, training loss 0.3326900005340576\n",
      "Epoch 18 -- Batch 325/ 842, training loss 0.33467838168144226\n",
      "Epoch 18 -- Batch 326/ 842, training loss 0.3311651647090912\n",
      "Epoch 18 -- Batch 327/ 842, training loss 0.3423052430152893\n",
      "Epoch 18 -- Batch 328/ 842, training loss 0.3457893133163452\n",
      "Epoch 18 -- Batch 329/ 842, training loss 0.3465408384799957\n",
      "Epoch 18 -- Batch 330/ 842, training loss 0.3384416699409485\n",
      "Epoch 18 -- Batch 331/ 842, training loss 0.33673012256622314\n",
      "Epoch 18 -- Batch 332/ 842, training loss 0.34386560320854187\n",
      "Epoch 18 -- Batch 333/ 842, training loss 0.3414680063724518\n",
      "Epoch 18 -- Batch 334/ 842, training loss 0.33270522952079773\n",
      "Epoch 18 -- Batch 335/ 842, training loss 0.341686487197876\n",
      "Epoch 18 -- Batch 336/ 842, training loss 0.3390900790691376\n",
      "Epoch 18 -- Batch 337/ 842, training loss 0.3233875036239624\n",
      "Epoch 18 -- Batch 338/ 842, training loss 0.3366224765777588\n",
      "Epoch 18 -- Batch 339/ 842, training loss 0.3406126797199249\n",
      "Epoch 18 -- Batch 340/ 842, training loss 0.330147385597229\n",
      "Epoch 18 -- Batch 341/ 842, training loss 0.3322063982486725\n",
      "Epoch 18 -- Batch 342/ 842, training loss 0.35024163126945496\n",
      "Epoch 18 -- Batch 343/ 842, training loss 0.3410826325416565\n",
      "Epoch 18 -- Batch 344/ 842, training loss 0.328351765871048\n",
      "Epoch 18 -- Batch 345/ 842, training loss 0.3317152261734009\n",
      "Epoch 18 -- Batch 346/ 842, training loss 0.3281748294830322\n",
      "Epoch 18 -- Batch 347/ 842, training loss 0.3279009759426117\n",
      "Epoch 18 -- Batch 348/ 842, training loss 0.330016165971756\n",
      "Epoch 18 -- Batch 349/ 842, training loss 0.3458905518054962\n",
      "Epoch 18 -- Batch 350/ 842, training loss 0.34032055735588074\n",
      "Epoch 18 -- Batch 351/ 842, training loss 0.33588093519210815\n",
      "Epoch 18 -- Batch 352/ 842, training loss 0.3442980945110321\n",
      "Epoch 18 -- Batch 353/ 842, training loss 0.3341991603374481\n",
      "Epoch 18 -- Batch 354/ 842, training loss 0.32720765471458435\n",
      "Epoch 18 -- Batch 355/ 842, training loss 0.3362112045288086\n",
      "Epoch 18 -- Batch 356/ 842, training loss 0.3358501195907593\n",
      "Epoch 18 -- Batch 357/ 842, training loss 0.34188950061798096\n",
      "Epoch 18 -- Batch 358/ 842, training loss 0.33623531460762024\n",
      "Epoch 18 -- Batch 359/ 842, training loss 0.34858012199401855\n",
      "Epoch 18 -- Batch 360/ 842, training loss 0.34613344073295593\n",
      "Epoch 18 -- Batch 361/ 842, training loss 0.3281664252281189\n",
      "Epoch 18 -- Batch 362/ 842, training loss 0.34819117188453674\n",
      "Epoch 18 -- Batch 363/ 842, training loss 0.32162895798683167\n",
      "Epoch 18 -- Batch 364/ 842, training loss 0.34774529933929443\n",
      "Epoch 18 -- Batch 365/ 842, training loss 0.3345617651939392\n",
      "Epoch 18 -- Batch 366/ 842, training loss 0.33434104919433594\n",
      "Epoch 18 -- Batch 367/ 842, training loss 0.34392932057380676\n",
      "Epoch 18 -- Batch 368/ 842, training loss 0.33755576610565186\n",
      "Epoch 18 -- Batch 369/ 842, training loss 0.33589231967926025\n",
      "Epoch 18 -- Batch 370/ 842, training loss 0.35152846574783325\n",
      "Epoch 18 -- Batch 371/ 842, training loss 0.33362525701522827\n",
      "Epoch 18 -- Batch 372/ 842, training loss 0.3234303593635559\n",
      "Epoch 18 -- Batch 373/ 842, training loss 0.32851579785346985\n",
      "Epoch 18 -- Batch 374/ 842, training loss 0.3314264416694641\n",
      "Epoch 18 -- Batch 375/ 842, training loss 0.32661521434783936\n",
      "Epoch 18 -- Batch 376/ 842, training loss 0.33337658643722534\n",
      "Epoch 18 -- Batch 377/ 842, training loss 0.3543773889541626\n",
      "Epoch 18 -- Batch 378/ 842, training loss 0.33095356822013855\n",
      "Epoch 18 -- Batch 379/ 842, training loss 0.3227453827857971\n",
      "Epoch 18 -- Batch 380/ 842, training loss 0.33874037861824036\n",
      "Epoch 18 -- Batch 381/ 842, training loss 0.3352309465408325\n",
      "Epoch 18 -- Batch 382/ 842, training loss 0.32377171516418457\n",
      "Epoch 18 -- Batch 383/ 842, training loss 0.3412160575389862\n",
      "Epoch 18 -- Batch 384/ 842, training loss 0.3391213119029999\n",
      "Epoch 18 -- Batch 385/ 842, training loss 0.332478791475296\n",
      "Epoch 18 -- Batch 386/ 842, training loss 0.33674874901771545\n",
      "Epoch 18 -- Batch 387/ 842, training loss 0.3251934051513672\n",
      "Epoch 18 -- Batch 388/ 842, training loss 0.33958205580711365\n",
      "Epoch 18 -- Batch 389/ 842, training loss 0.345065712928772\n",
      "Epoch 18 -- Batch 390/ 842, training loss 0.34459367394447327\n",
      "Epoch 18 -- Batch 391/ 842, training loss 0.3351387083530426\n",
      "Epoch 18 -- Batch 392/ 842, training loss 0.3212040066719055\n",
      "Epoch 18 -- Batch 393/ 842, training loss 0.33900952339172363\n",
      "Epoch 18 -- Batch 394/ 842, training loss 0.33403506875038147\n",
      "Epoch 18 -- Batch 395/ 842, training loss 0.3352715075016022\n",
      "Epoch 18 -- Batch 396/ 842, training loss 0.35332733392715454\n",
      "Epoch 18 -- Batch 397/ 842, training loss 0.3341137766838074\n",
      "Epoch 18 -- Batch 398/ 842, training loss 0.32371070981025696\n",
      "Epoch 18 -- Batch 399/ 842, training loss 0.3349258303642273\n",
      "Epoch 18 -- Batch 400/ 842, training loss 0.32943737506866455\n",
      "Epoch 18 -- Batch 401/ 842, training loss 0.3402766287326813\n",
      "Epoch 18 -- Batch 402/ 842, training loss 0.3374743163585663\n",
      "Epoch 18 -- Batch 403/ 842, training loss 0.34822219610214233\n",
      "Epoch 18 -- Batch 404/ 842, training loss 0.34022197127342224\n",
      "Epoch 18 -- Batch 405/ 842, training loss 0.331961065530777\n",
      "Epoch 18 -- Batch 406/ 842, training loss 0.338154673576355\n",
      "Epoch 18 -- Batch 407/ 842, training loss 0.33682820200920105\n",
      "Epoch 18 -- Batch 408/ 842, training loss 0.3423275053501129\n",
      "Epoch 18 -- Batch 409/ 842, training loss 0.33471617102622986\n",
      "Epoch 18 -- Batch 410/ 842, training loss 0.3257729411125183\n",
      "Epoch 18 -- Batch 411/ 842, training loss 0.33387982845306396\n",
      "Epoch 18 -- Batch 412/ 842, training loss 0.33415624499320984\n",
      "Epoch 18 -- Batch 413/ 842, training loss 0.3333771228790283\n",
      "Epoch 18 -- Batch 414/ 842, training loss 0.3308541178703308\n",
      "Epoch 18 -- Batch 415/ 842, training loss 0.338508665561676\n",
      "Epoch 18 -- Batch 416/ 842, training loss 0.33421096205711365\n",
      "Epoch 18 -- Batch 417/ 842, training loss 0.32756930589675903\n",
      "Epoch 18 -- Batch 418/ 842, training loss 0.33855873346328735\n",
      "Epoch 18 -- Batch 419/ 842, training loss 0.3367668092250824\n",
      "Epoch 18 -- Batch 420/ 842, training loss 0.3273959457874298\n",
      "Epoch 18 -- Batch 421/ 842, training loss 0.33374738693237305\n",
      "Epoch 18 -- Batch 422/ 842, training loss 0.33864375948905945\n",
      "Epoch 18 -- Batch 423/ 842, training loss 0.3440822660923004\n",
      "Epoch 18 -- Batch 424/ 842, training loss 0.3511078655719757\n",
      "Epoch 18 -- Batch 425/ 842, training loss 0.34242185950279236\n",
      "Epoch 18 -- Batch 426/ 842, training loss 0.34925806522369385\n",
      "Epoch 18 -- Batch 427/ 842, training loss 0.33231794834136963\n",
      "Epoch 18 -- Batch 428/ 842, training loss 0.3384318947792053\n",
      "Epoch 18 -- Batch 429/ 842, training loss 0.32989734411239624\n",
      "Epoch 18 -- Batch 430/ 842, training loss 0.3417150378227234\n",
      "Epoch 18 -- Batch 431/ 842, training loss 0.33532530069351196\n",
      "Epoch 18 -- Batch 432/ 842, training loss 0.3420126438140869\n",
      "Epoch 18 -- Batch 433/ 842, training loss 0.34092745184898376\n",
      "Epoch 18 -- Batch 434/ 842, training loss 0.3373625874519348\n",
      "Epoch 18 -- Batch 435/ 842, training loss 0.33724960684776306\n",
      "Epoch 18 -- Batch 436/ 842, training loss 0.32449978590011597\n",
      "Epoch 18 -- Batch 437/ 842, training loss 0.3336622714996338\n",
      "Epoch 18 -- Batch 438/ 842, training loss 0.34067562222480774\n",
      "Epoch 18 -- Batch 439/ 842, training loss 0.3427408039569855\n",
      "Epoch 18 -- Batch 440/ 842, training loss 0.3277018070220947\n",
      "Epoch 18 -- Batch 441/ 842, training loss 0.33972328901290894\n",
      "Epoch 18 -- Batch 442/ 842, training loss 0.3368733823299408\n",
      "Epoch 18 -- Batch 443/ 842, training loss 0.326256662607193\n",
      "Epoch 18 -- Batch 444/ 842, training loss 0.3244919180870056\n",
      "Epoch 18 -- Batch 445/ 842, training loss 0.3263101577758789\n",
      "Epoch 18 -- Batch 446/ 842, training loss 0.34319332242012024\n",
      "Epoch 18 -- Batch 447/ 842, training loss 0.33154281973838806\n",
      "Epoch 18 -- Batch 448/ 842, training loss 0.35127952694892883\n",
      "Epoch 18 -- Batch 449/ 842, training loss 0.33914893865585327\n",
      "Epoch 18 -- Batch 450/ 842, training loss 0.3361923396587372\n",
      "Epoch 18 -- Batch 451/ 842, training loss 0.3276280462741852\n",
      "Epoch 18 -- Batch 452/ 842, training loss 0.33887729048728943\n",
      "Epoch 18 -- Batch 453/ 842, training loss 0.3322891294956207\n",
      "Epoch 18 -- Batch 454/ 842, training loss 0.33648982644081116\n",
      "Epoch 18 -- Batch 455/ 842, training loss 0.3484252393245697\n",
      "Epoch 18 -- Batch 456/ 842, training loss 0.3419591188430786\n",
      "Epoch 18 -- Batch 457/ 842, training loss 0.3417612612247467\n",
      "Epoch 18 -- Batch 458/ 842, training loss 0.3349877595901489\n",
      "Epoch 18 -- Batch 459/ 842, training loss 0.3421301543712616\n",
      "Epoch 18 -- Batch 460/ 842, training loss 0.3286677300930023\n",
      "Epoch 18 -- Batch 461/ 842, training loss 0.3370105028152466\n",
      "Epoch 18 -- Batch 462/ 842, training loss 0.32016223669052124\n",
      "Epoch 18 -- Batch 463/ 842, training loss 0.33230310678482056\n",
      "Epoch 18 -- Batch 464/ 842, training loss 0.34092217683792114\n",
      "Epoch 18 -- Batch 465/ 842, training loss 0.339762419462204\n",
      "Epoch 18 -- Batch 466/ 842, training loss 0.33217525482177734\n",
      "Epoch 18 -- Batch 467/ 842, training loss 0.33559370040893555\n",
      "Epoch 18 -- Batch 468/ 842, training loss 0.3554694950580597\n",
      "Epoch 18 -- Batch 469/ 842, training loss 0.3362935483455658\n",
      "Epoch 18 -- Batch 470/ 842, training loss 0.3265474736690521\n",
      "Epoch 18 -- Batch 471/ 842, training loss 0.33823442459106445\n",
      "Epoch 18 -- Batch 472/ 842, training loss 0.3320024013519287\n",
      "Epoch 18 -- Batch 473/ 842, training loss 0.3325081765651703\n",
      "Epoch 18 -- Batch 474/ 842, training loss 0.3438575267791748\n",
      "Epoch 18 -- Batch 475/ 842, training loss 0.3336672782897949\n",
      "Epoch 18 -- Batch 476/ 842, training loss 0.33609068393707275\n",
      "Epoch 18 -- Batch 477/ 842, training loss 0.3321581780910492\n",
      "Epoch 18 -- Batch 478/ 842, training loss 0.33859702944755554\n",
      "Epoch 18 -- Batch 479/ 842, training loss 0.34019550681114197\n",
      "Epoch 18 -- Batch 480/ 842, training loss 0.340403288602829\n",
      "Epoch 18 -- Batch 481/ 842, training loss 0.348111093044281\n",
      "Epoch 18 -- Batch 482/ 842, training loss 0.32739248871803284\n",
      "Epoch 18 -- Batch 483/ 842, training loss 0.34214821457862854\n",
      "Epoch 18 -- Batch 484/ 842, training loss 0.33273783326148987\n",
      "Epoch 18 -- Batch 485/ 842, training loss 0.33064478635787964\n",
      "Epoch 18 -- Batch 486/ 842, training loss 0.3432598114013672\n",
      "Epoch 18 -- Batch 487/ 842, training loss 0.33777734637260437\n",
      "Epoch 18 -- Batch 488/ 842, training loss 0.32762986421585083\n",
      "Epoch 18 -- Batch 489/ 842, training loss 0.32781386375427246\n",
      "Epoch 18 -- Batch 490/ 842, training loss 0.3340684771537781\n",
      "Epoch 18 -- Batch 491/ 842, training loss 0.3366669714450836\n",
      "Epoch 18 -- Batch 492/ 842, training loss 0.34520629048347473\n",
      "Epoch 18 -- Batch 493/ 842, training loss 0.33957988023757935\n",
      "Epoch 18 -- Batch 494/ 842, training loss 0.33885762095451355\n",
      "Epoch 18 -- Batch 495/ 842, training loss 0.3270453214645386\n",
      "Epoch 18 -- Batch 496/ 842, training loss 0.3320097029209137\n",
      "Epoch 18 -- Batch 497/ 842, training loss 0.35204559564590454\n",
      "Epoch 18 -- Batch 498/ 842, training loss 0.3358828127384186\n",
      "Epoch 18 -- Batch 499/ 842, training loss 0.32668212056159973\n",
      "Epoch 18 -- Batch 500/ 842, training loss 0.3439622223377228\n",
      "Epoch 18 -- Batch 501/ 842, training loss 0.3503108322620392\n",
      "Epoch 18 -- Batch 502/ 842, training loss 0.33496302366256714\n",
      "Epoch 18 -- Batch 503/ 842, training loss 0.3398798406124115\n",
      "Epoch 18 -- Batch 504/ 842, training loss 0.33332496881484985\n",
      "Epoch 18 -- Batch 505/ 842, training loss 0.3401881754398346\n",
      "Epoch 18 -- Batch 506/ 842, training loss 0.3501805067062378\n",
      "Epoch 18 -- Batch 507/ 842, training loss 0.33334702253341675\n",
      "Epoch 18 -- Batch 508/ 842, training loss 0.353013813495636\n",
      "Epoch 18 -- Batch 509/ 842, training loss 0.3380795419216156\n",
      "Epoch 18 -- Batch 510/ 842, training loss 0.32678890228271484\n",
      "Epoch 18 -- Batch 511/ 842, training loss 0.33450713753700256\n",
      "Epoch 18 -- Batch 512/ 842, training loss 0.3370964229106903\n",
      "Epoch 18 -- Batch 513/ 842, training loss 0.3324928879737854\n",
      "Epoch 18 -- Batch 514/ 842, training loss 0.3286880850791931\n",
      "Epoch 18 -- Batch 515/ 842, training loss 0.33716440200805664\n",
      "Epoch 18 -- Batch 516/ 842, training loss 0.34345245361328125\n",
      "Epoch 18 -- Batch 517/ 842, training loss 0.33709004521369934\n",
      "Epoch 18 -- Batch 518/ 842, training loss 0.333438515663147\n",
      "Epoch 18 -- Batch 519/ 842, training loss 0.334218829870224\n",
      "Epoch 18 -- Batch 520/ 842, training loss 0.3326011300086975\n",
      "Epoch 18 -- Batch 521/ 842, training loss 0.3486291766166687\n",
      "Epoch 18 -- Batch 522/ 842, training loss 0.3325676918029785\n",
      "Epoch 18 -- Batch 523/ 842, training loss 0.3407275080680847\n",
      "Epoch 18 -- Batch 524/ 842, training loss 0.3319077491760254\n",
      "Epoch 18 -- Batch 525/ 842, training loss 0.3253995180130005\n",
      "Epoch 18 -- Batch 526/ 842, training loss 0.34186792373657227\n",
      "Epoch 18 -- Batch 527/ 842, training loss 0.3481385111808777\n",
      "Epoch 18 -- Batch 528/ 842, training loss 0.346670538187027\n",
      "Epoch 18 -- Batch 529/ 842, training loss 0.3441653549671173\n",
      "Epoch 18 -- Batch 530/ 842, training loss 0.3409237265586853\n",
      "Epoch 18 -- Batch 531/ 842, training loss 0.33758753538131714\n",
      "Epoch 18 -- Batch 532/ 842, training loss 0.3467503786087036\n",
      "Epoch 18 -- Batch 533/ 842, training loss 0.3386791944503784\n",
      "Epoch 18 -- Batch 534/ 842, training loss 0.33761581778526306\n",
      "Epoch 18 -- Batch 535/ 842, training loss 0.34015220403671265\n",
      "Epoch 18 -- Batch 536/ 842, training loss 0.3378126621246338\n",
      "Epoch 18 -- Batch 537/ 842, training loss 0.35140055418014526\n",
      "Epoch 18 -- Batch 538/ 842, training loss 0.33437395095825195\n",
      "Epoch 18 -- Batch 539/ 842, training loss 0.3483078181743622\n",
      "Epoch 18 -- Batch 540/ 842, training loss 0.341871976852417\n",
      "Epoch 18 -- Batch 541/ 842, training loss 0.33423754572868347\n",
      "Epoch 18 -- Batch 542/ 842, training loss 0.339562326669693\n",
      "Epoch 18 -- Batch 543/ 842, training loss 0.3362421691417694\n",
      "Epoch 18 -- Batch 544/ 842, training loss 0.336917519569397\n",
      "Epoch 18 -- Batch 545/ 842, training loss 0.34044715762138367\n",
      "Epoch 18 -- Batch 546/ 842, training loss 0.3339625895023346\n",
      "Epoch 18 -- Batch 547/ 842, training loss 0.3445562720298767\n",
      "Epoch 18 -- Batch 548/ 842, training loss 0.3446670174598694\n",
      "Epoch 18 -- Batch 549/ 842, training loss 0.338813453912735\n",
      "Epoch 18 -- Batch 550/ 842, training loss 0.3460792899131775\n",
      "Epoch 18 -- Batch 551/ 842, training loss 0.3495389223098755\n",
      "Epoch 18 -- Batch 552/ 842, training loss 0.34151288866996765\n",
      "Epoch 18 -- Batch 553/ 842, training loss 0.3343731164932251\n",
      "Epoch 18 -- Batch 554/ 842, training loss 0.330807089805603\n",
      "Epoch 18 -- Batch 555/ 842, training loss 0.3262532651424408\n",
      "Epoch 18 -- Batch 556/ 842, training loss 0.3204127550125122\n",
      "Epoch 18 -- Batch 557/ 842, training loss 0.3495951294898987\n",
      "Epoch 18 -- Batch 558/ 842, training loss 0.3486121594905853\n",
      "Epoch 18 -- Batch 559/ 842, training loss 0.3424190878868103\n",
      "Epoch 18 -- Batch 560/ 842, training loss 0.3276017904281616\n",
      "Epoch 18 -- Batch 561/ 842, training loss 0.34874799847602844\n",
      "Epoch 18 -- Batch 562/ 842, training loss 0.3302244246006012\n",
      "Epoch 18 -- Batch 563/ 842, training loss 0.32243165373802185\n",
      "Epoch 18 -- Batch 564/ 842, training loss 0.33447572588920593\n",
      "Epoch 18 -- Batch 565/ 842, training loss 0.3470734655857086\n",
      "Epoch 18 -- Batch 566/ 842, training loss 0.3355972468852997\n",
      "Epoch 18 -- Batch 567/ 842, training loss 0.34628909826278687\n",
      "Epoch 18 -- Batch 568/ 842, training loss 0.3350558876991272\n",
      "Epoch 18 -- Batch 569/ 842, training loss 0.3409968614578247\n",
      "Epoch 18 -- Batch 570/ 842, training loss 0.3494247496128082\n",
      "Epoch 18 -- Batch 571/ 842, training loss 0.33654922246932983\n",
      "Epoch 18 -- Batch 572/ 842, training loss 0.3426303565502167\n",
      "Epoch 18 -- Batch 573/ 842, training loss 0.32703083753585815\n",
      "Epoch 18 -- Batch 574/ 842, training loss 0.34758007526397705\n",
      "Epoch 18 -- Batch 575/ 842, training loss 0.3506726920604706\n",
      "Epoch 18 -- Batch 576/ 842, training loss 0.323964387178421\n",
      "Epoch 18 -- Batch 577/ 842, training loss 0.32976728677749634\n",
      "Epoch 18 -- Batch 578/ 842, training loss 0.32831957936286926\n",
      "Epoch 18 -- Batch 579/ 842, training loss 0.34176504611968994\n",
      "Epoch 18 -- Batch 580/ 842, training loss 0.3472854793071747\n",
      "Epoch 18 -- Batch 581/ 842, training loss 0.33624744415283203\n",
      "Epoch 18 -- Batch 582/ 842, training loss 0.32861924171447754\n",
      "Epoch 18 -- Batch 583/ 842, training loss 0.33780449628829956\n",
      "Epoch 18 -- Batch 584/ 842, training loss 0.332008421421051\n",
      "Epoch 18 -- Batch 585/ 842, training loss 0.3359263837337494\n",
      "Epoch 18 -- Batch 586/ 842, training loss 0.3310200572013855\n",
      "Epoch 18 -- Batch 587/ 842, training loss 0.3315541446208954\n",
      "Epoch 18 -- Batch 588/ 842, training loss 0.33683261275291443\n",
      "Epoch 18 -- Batch 589/ 842, training loss 0.3384847044944763\n",
      "Epoch 18 -- Batch 590/ 842, training loss 0.33755218982696533\n",
      "Epoch 18 -- Batch 591/ 842, training loss 0.3350752294063568\n",
      "Epoch 18 -- Batch 592/ 842, training loss 0.3339497745037079\n",
      "Epoch 18 -- Batch 593/ 842, training loss 0.34575197100639343\n",
      "Epoch 18 -- Batch 594/ 842, training loss 0.3398382365703583\n",
      "Epoch 18 -- Batch 595/ 842, training loss 0.3345392942428589\n",
      "Epoch 18 -- Batch 596/ 842, training loss 0.3207089900970459\n",
      "Epoch 18 -- Batch 597/ 842, training loss 0.3427412211894989\n",
      "Epoch 18 -- Batch 598/ 842, training loss 0.3353831171989441\n",
      "Epoch 18 -- Batch 599/ 842, training loss 0.3397868573665619\n",
      "Epoch 18 -- Batch 600/ 842, training loss 0.32503950595855713\n",
      "Epoch 18 -- Batch 601/ 842, training loss 0.328581839799881\n",
      "Epoch 18 -- Batch 602/ 842, training loss 0.35073530673980713\n",
      "Epoch 18 -- Batch 603/ 842, training loss 0.32727473974227905\n",
      "Epoch 18 -- Batch 604/ 842, training loss 0.330685555934906\n",
      "Epoch 18 -- Batch 605/ 842, training loss 0.3281574249267578\n",
      "Epoch 18 -- Batch 606/ 842, training loss 0.3229960501194\n",
      "Epoch 18 -- Batch 607/ 842, training loss 0.3370603621006012\n",
      "Epoch 18 -- Batch 608/ 842, training loss 0.34222379326820374\n",
      "Epoch 18 -- Batch 609/ 842, training loss 0.33736103773117065\n",
      "Epoch 18 -- Batch 610/ 842, training loss 0.3363092839717865\n",
      "Epoch 18 -- Batch 611/ 842, training loss 0.34917813539505005\n",
      "Epoch 18 -- Batch 612/ 842, training loss 0.3350377678871155\n",
      "Epoch 18 -- Batch 613/ 842, training loss 0.34710824489593506\n",
      "Epoch 18 -- Batch 614/ 842, training loss 0.35583293437957764\n",
      "Epoch 18 -- Batch 615/ 842, training loss 0.3393733501434326\n",
      "Epoch 18 -- Batch 616/ 842, training loss 0.34408530592918396\n",
      "Epoch 18 -- Batch 617/ 842, training loss 0.33884871006011963\n",
      "Epoch 18 -- Batch 618/ 842, training loss 0.3273066580295563\n",
      "Epoch 18 -- Batch 619/ 842, training loss 0.34222257137298584\n",
      "Epoch 18 -- Batch 620/ 842, training loss 0.3342341184616089\n",
      "Epoch 18 -- Batch 621/ 842, training loss 0.3406173884868622\n",
      "Epoch 18 -- Batch 622/ 842, training loss 0.34309449791908264\n",
      "Epoch 18 -- Batch 623/ 842, training loss 0.3403558135032654\n",
      "Epoch 18 -- Batch 624/ 842, training loss 0.34551727771759033\n",
      "Epoch 18 -- Batch 625/ 842, training loss 0.3446829617023468\n",
      "Epoch 18 -- Batch 626/ 842, training loss 0.3489607870578766\n",
      "Epoch 18 -- Batch 627/ 842, training loss 0.3336392641067505\n",
      "Epoch 18 -- Batch 628/ 842, training loss 0.3286064565181732\n",
      "Epoch 18 -- Batch 629/ 842, training loss 0.3486405909061432\n",
      "Epoch 18 -- Batch 630/ 842, training loss 0.3337158262729645\n",
      "Epoch 18 -- Batch 631/ 842, training loss 0.3314892649650574\n",
      "Epoch 18 -- Batch 632/ 842, training loss 0.3351542353630066\n",
      "Epoch 18 -- Batch 633/ 842, training loss 0.3536033034324646\n",
      "Epoch 18 -- Batch 634/ 842, training loss 0.33264806866645813\n",
      "Epoch 18 -- Batch 635/ 842, training loss 0.344416081905365\n",
      "Epoch 18 -- Batch 636/ 842, training loss 0.3354644179344177\n",
      "Epoch 18 -- Batch 637/ 842, training loss 0.34009411931037903\n",
      "Epoch 18 -- Batch 638/ 842, training loss 0.35696786642074585\n",
      "Epoch 18 -- Batch 639/ 842, training loss 0.34925875067710876\n",
      "Epoch 18 -- Batch 640/ 842, training loss 0.3280649781227112\n",
      "Epoch 18 -- Batch 641/ 842, training loss 0.33193033933639526\n",
      "Epoch 18 -- Batch 642/ 842, training loss 0.34061869978904724\n",
      "Epoch 18 -- Batch 643/ 842, training loss 0.33913475275039673\n",
      "Epoch 18 -- Batch 644/ 842, training loss 0.34478673338890076\n",
      "Epoch 18 -- Batch 645/ 842, training loss 0.3272596299648285\n",
      "Epoch 18 -- Batch 646/ 842, training loss 0.33197832107543945\n",
      "Epoch 18 -- Batch 647/ 842, training loss 0.3352150321006775\n",
      "Epoch 18 -- Batch 648/ 842, training loss 0.33502137660980225\n",
      "Epoch 18 -- Batch 649/ 842, training loss 0.3430752158164978\n",
      "Epoch 18 -- Batch 650/ 842, training loss 0.33707404136657715\n",
      "Epoch 18 -- Batch 651/ 842, training loss 0.3437088131904602\n",
      "Epoch 18 -- Batch 652/ 842, training loss 0.33764442801475525\n",
      "Epoch 18 -- Batch 653/ 842, training loss 0.3321068584918976\n",
      "Epoch 18 -- Batch 654/ 842, training loss 0.3514632284641266\n",
      "Epoch 18 -- Batch 655/ 842, training loss 0.33011898398399353\n",
      "Epoch 18 -- Batch 656/ 842, training loss 0.3396100401878357\n",
      "Epoch 18 -- Batch 657/ 842, training loss 0.33511239290237427\n",
      "Epoch 18 -- Batch 658/ 842, training loss 0.3268173336982727\n",
      "Epoch 18 -- Batch 659/ 842, training loss 0.3304439187049866\n",
      "Epoch 18 -- Batch 660/ 842, training loss 0.3368968069553375\n",
      "Epoch 18 -- Batch 661/ 842, training loss 0.33479440212249756\n",
      "Epoch 18 -- Batch 662/ 842, training loss 0.33677253127098083\n",
      "Epoch 18 -- Batch 663/ 842, training loss 0.3431687355041504\n",
      "Epoch 18 -- Batch 664/ 842, training loss 0.34406840801239014\n",
      "Epoch 18 -- Batch 665/ 842, training loss 0.34434041380882263\n",
      "Epoch 18 -- Batch 666/ 842, training loss 0.35508960485458374\n",
      "Epoch 18 -- Batch 667/ 842, training loss 0.35065585374832153\n",
      "Epoch 18 -- Batch 668/ 842, training loss 0.33283814787864685\n",
      "Epoch 18 -- Batch 669/ 842, training loss 0.3512369990348816\n",
      "Epoch 18 -- Batch 670/ 842, training loss 0.3486554026603699\n",
      "Epoch 18 -- Batch 671/ 842, training loss 0.3247267007827759\n",
      "Epoch 18 -- Batch 672/ 842, training loss 0.3358089029788971\n",
      "Epoch 18 -- Batch 673/ 842, training loss 0.3311997652053833\n",
      "Epoch 18 -- Batch 674/ 842, training loss 0.32885628938674927\n",
      "Epoch 18 -- Batch 675/ 842, training loss 0.3276227116584778\n",
      "Epoch 18 -- Batch 676/ 842, training loss 0.33976754546165466\n",
      "Epoch 18 -- Batch 677/ 842, training loss 0.3360445201396942\n",
      "Epoch 18 -- Batch 678/ 842, training loss 0.33687418699264526\n",
      "Epoch 18 -- Batch 679/ 842, training loss 0.34009623527526855\n",
      "Epoch 18 -- Batch 680/ 842, training loss 0.33953428268432617\n",
      "Epoch 18 -- Batch 681/ 842, training loss 0.33842700719833374\n",
      "Epoch 18 -- Batch 682/ 842, training loss 0.36713799834251404\n",
      "Epoch 18 -- Batch 683/ 842, training loss 0.3370733857154846\n",
      "Epoch 18 -- Batch 684/ 842, training loss 0.33814170956611633\n",
      "Epoch 18 -- Batch 685/ 842, training loss 0.33400213718414307\n",
      "Epoch 18 -- Batch 686/ 842, training loss 0.3262760639190674\n",
      "Epoch 18 -- Batch 687/ 842, training loss 0.3346676826477051\n",
      "Epoch 18 -- Batch 688/ 842, training loss 0.3470957279205322\n",
      "Epoch 18 -- Batch 689/ 842, training loss 0.33393701910972595\n",
      "Epoch 18 -- Batch 690/ 842, training loss 0.35249441862106323\n",
      "Epoch 18 -- Batch 691/ 842, training loss 0.3260926902294159\n",
      "Epoch 18 -- Batch 692/ 842, training loss 0.3179386854171753\n",
      "Epoch 18 -- Batch 693/ 842, training loss 0.34615111351013184\n",
      "Epoch 18 -- Batch 694/ 842, training loss 0.3328595459461212\n",
      "Epoch 18 -- Batch 695/ 842, training loss 0.33118557929992676\n",
      "Epoch 18 -- Batch 696/ 842, training loss 0.3368179202079773\n",
      "Epoch 18 -- Batch 697/ 842, training loss 0.3472539186477661\n",
      "Epoch 18 -- Batch 698/ 842, training loss 0.3434724807739258\n",
      "Epoch 18 -- Batch 699/ 842, training loss 0.3206729590892792\n",
      "Epoch 18 -- Batch 700/ 842, training loss 0.3429141640663147\n",
      "Epoch 18 -- Batch 701/ 842, training loss 0.3343886137008667\n",
      "Epoch 18 -- Batch 702/ 842, training loss 0.3336946666240692\n",
      "Epoch 18 -- Batch 703/ 842, training loss 0.34479624032974243\n",
      "Epoch 18 -- Batch 704/ 842, training loss 0.35017237067222595\n",
      "Epoch 18 -- Batch 705/ 842, training loss 0.33861812949180603\n",
      "Epoch 18 -- Batch 706/ 842, training loss 0.35430002212524414\n",
      "Epoch 18 -- Batch 707/ 842, training loss 0.3513895869255066\n",
      "Epoch 18 -- Batch 708/ 842, training loss 0.3456655442714691\n",
      "Epoch 18 -- Batch 709/ 842, training loss 0.33305349946022034\n",
      "Epoch 18 -- Batch 710/ 842, training loss 0.3322485089302063\n",
      "Epoch 18 -- Batch 711/ 842, training loss 0.3434757590293884\n",
      "Epoch 18 -- Batch 712/ 842, training loss 0.3352009952068329\n",
      "Epoch 18 -- Batch 713/ 842, training loss 0.3445291221141815\n",
      "Epoch 18 -- Batch 714/ 842, training loss 0.338093638420105\n",
      "Epoch 18 -- Batch 715/ 842, training loss 0.3432977795600891\n",
      "Epoch 18 -- Batch 716/ 842, training loss 0.3369835615158081\n",
      "Epoch 18 -- Batch 717/ 842, training loss 0.35322391986846924\n",
      "Epoch 18 -- Batch 718/ 842, training loss 0.33879294991493225\n",
      "Epoch 18 -- Batch 719/ 842, training loss 0.3335576057434082\n",
      "Epoch 18 -- Batch 720/ 842, training loss 0.3421785235404968\n",
      "Epoch 18 -- Batch 721/ 842, training loss 0.34356850385665894\n",
      "Epoch 18 -- Batch 722/ 842, training loss 0.33359330892562866\n",
      "Epoch 18 -- Batch 723/ 842, training loss 0.3282815217971802\n",
      "Epoch 18 -- Batch 724/ 842, training loss 0.3470228910446167\n",
      "Epoch 18 -- Batch 725/ 842, training loss 0.3406294584274292\n",
      "Epoch 18 -- Batch 726/ 842, training loss 0.3339519500732422\n",
      "Epoch 18 -- Batch 727/ 842, training loss 0.33676931262016296\n",
      "Epoch 18 -- Batch 728/ 842, training loss 0.3508017957210541\n",
      "Epoch 18 -- Batch 729/ 842, training loss 0.3465455174446106\n",
      "Epoch 18 -- Batch 730/ 842, training loss 0.33765462040901184\n",
      "Epoch 18 -- Batch 731/ 842, training loss 0.3470839858055115\n",
      "Epoch 18 -- Batch 732/ 842, training loss 0.33115535974502563\n",
      "Epoch 18 -- Batch 733/ 842, training loss 0.33432477712631226\n",
      "Epoch 18 -- Batch 734/ 842, training loss 0.3327792286872864\n",
      "Epoch 18 -- Batch 735/ 842, training loss 0.3287944793701172\n",
      "Epoch 18 -- Batch 736/ 842, training loss 0.34473559260368347\n",
      "Epoch 18 -- Batch 737/ 842, training loss 0.3408007323741913\n",
      "Epoch 18 -- Batch 738/ 842, training loss 0.3291657567024231\n",
      "Epoch 18 -- Batch 739/ 842, training loss 0.3574899733066559\n",
      "Epoch 18 -- Batch 740/ 842, training loss 0.33903440833091736\n",
      "Epoch 18 -- Batch 741/ 842, training loss 0.3304692506790161\n",
      "Epoch 18 -- Batch 742/ 842, training loss 0.33766162395477295\n",
      "Epoch 18 -- Batch 743/ 842, training loss 0.33492204546928406\n",
      "Epoch 18 -- Batch 744/ 842, training loss 0.3342675566673279\n",
      "Epoch 18 -- Batch 745/ 842, training loss 0.33845219016075134\n",
      "Epoch 18 -- Batch 746/ 842, training loss 0.3291463553905487\n",
      "Epoch 18 -- Batch 747/ 842, training loss 0.32833144068717957\n",
      "Epoch 18 -- Batch 748/ 842, training loss 0.3493713140487671\n",
      "Epoch 18 -- Batch 749/ 842, training loss 0.3479738235473633\n",
      "Epoch 18 -- Batch 750/ 842, training loss 0.32932019233703613\n",
      "Epoch 18 -- Batch 751/ 842, training loss 0.3271045684814453\n",
      "Epoch 18 -- Batch 752/ 842, training loss 0.33170926570892334\n",
      "Epoch 18 -- Batch 753/ 842, training loss 0.34244003891944885\n",
      "Epoch 18 -- Batch 754/ 842, training loss 0.3303056061267853\n",
      "Epoch 18 -- Batch 755/ 842, training loss 0.34315240383148193\n",
      "Epoch 18 -- Batch 756/ 842, training loss 0.3431403934955597\n",
      "Epoch 18 -- Batch 757/ 842, training loss 0.32844430208206177\n",
      "Epoch 18 -- Batch 758/ 842, training loss 0.3322533667087555\n",
      "Epoch 18 -- Batch 759/ 842, training loss 0.344120591878891\n",
      "Epoch 18 -- Batch 760/ 842, training loss 0.3341442942619324\n",
      "Epoch 18 -- Batch 761/ 842, training loss 0.34203869104385376\n",
      "Epoch 18 -- Batch 762/ 842, training loss 0.32925212383270264\n",
      "Epoch 18 -- Batch 763/ 842, training loss 0.34307917952537537\n",
      "Epoch 18 -- Batch 764/ 842, training loss 0.3423022925853729\n",
      "Epoch 18 -- Batch 765/ 842, training loss 0.346219927072525\n",
      "Epoch 18 -- Batch 766/ 842, training loss 0.35385018587112427\n",
      "Epoch 18 -- Batch 767/ 842, training loss 0.33901360630989075\n",
      "Epoch 18 -- Batch 768/ 842, training loss 0.3329496383666992\n",
      "Epoch 18 -- Batch 769/ 842, training loss 0.3352756202220917\n",
      "Epoch 18 -- Batch 770/ 842, training loss 0.3387952446937561\n",
      "Epoch 18 -- Batch 771/ 842, training loss 0.33215031027793884\n",
      "Epoch 18 -- Batch 772/ 842, training loss 0.33979278802871704\n",
      "Epoch 18 -- Batch 773/ 842, training loss 0.33887097239494324\n",
      "Epoch 18 -- Batch 774/ 842, training loss 0.34189167618751526\n",
      "Epoch 18 -- Batch 775/ 842, training loss 0.34402933716773987\n",
      "Epoch 18 -- Batch 776/ 842, training loss 0.33304205536842346\n",
      "Epoch 18 -- Batch 777/ 842, training loss 0.34064584970474243\n",
      "Epoch 18 -- Batch 778/ 842, training loss 0.33716893196105957\n",
      "Epoch 18 -- Batch 779/ 842, training loss 0.34811466932296753\n",
      "Epoch 18 -- Batch 780/ 842, training loss 0.3398017883300781\n",
      "Epoch 18 -- Batch 781/ 842, training loss 0.3588138222694397\n",
      "Epoch 18 -- Batch 782/ 842, training loss 0.32887208461761475\n",
      "Epoch 18 -- Batch 783/ 842, training loss 0.3418392837047577\n",
      "Epoch 18 -- Batch 784/ 842, training loss 0.33545079827308655\n",
      "Epoch 18 -- Batch 785/ 842, training loss 0.3484719693660736\n",
      "Epoch 18 -- Batch 786/ 842, training loss 0.33709216117858887\n",
      "Epoch 18 -- Batch 787/ 842, training loss 0.33147498965263367\n",
      "Epoch 18 -- Batch 788/ 842, training loss 0.3417179584503174\n",
      "Epoch 18 -- Batch 789/ 842, training loss 0.33611834049224854\n",
      "Epoch 18 -- Batch 790/ 842, training loss 0.34185972809791565\n",
      "Epoch 18 -- Batch 791/ 842, training loss 0.33669912815093994\n",
      "Epoch 18 -- Batch 792/ 842, training loss 0.3275570869445801\n",
      "Epoch 18 -- Batch 793/ 842, training loss 0.33787015080451965\n",
      "Epoch 18 -- Batch 794/ 842, training loss 0.33266013860702515\n",
      "Epoch 18 -- Batch 795/ 842, training loss 0.3314720690250397\n",
      "Epoch 18 -- Batch 796/ 842, training loss 0.33154600858688354\n",
      "Epoch 18 -- Batch 797/ 842, training loss 0.33821508288383484\n",
      "Epoch 18 -- Batch 798/ 842, training loss 0.33052346110343933\n",
      "Epoch 18 -- Batch 799/ 842, training loss 0.3303214907646179\n",
      "Epoch 18 -- Batch 800/ 842, training loss 0.33040159940719604\n",
      "Epoch 18 -- Batch 801/ 842, training loss 0.33030545711517334\n",
      "Epoch 18 -- Batch 802/ 842, training loss 0.3318600058555603\n",
      "Epoch 18 -- Batch 803/ 842, training loss 0.345847487449646\n",
      "Epoch 18 -- Batch 804/ 842, training loss 0.3375793695449829\n",
      "Epoch 18 -- Batch 805/ 842, training loss 0.3416801989078522\n",
      "Epoch 18 -- Batch 806/ 842, training loss 0.34069305658340454\n",
      "Epoch 18 -- Batch 807/ 842, training loss 0.33333659172058105\n",
      "Epoch 18 -- Batch 808/ 842, training loss 0.34092798829078674\n",
      "Epoch 18 -- Batch 809/ 842, training loss 0.34917518496513367\n",
      "Epoch 18 -- Batch 810/ 842, training loss 0.3482508957386017\n",
      "Epoch 18 -- Batch 811/ 842, training loss 0.3294920325279236\n",
      "Epoch 18 -- Batch 812/ 842, training loss 0.3472379744052887\n",
      "Epoch 18 -- Batch 813/ 842, training loss 0.33125656843185425\n",
      "Epoch 18 -- Batch 814/ 842, training loss 0.3364679217338562\n",
      "Epoch 18 -- Batch 815/ 842, training loss 0.34785714745521545\n",
      "Epoch 18 -- Batch 816/ 842, training loss 0.33250918984413147\n",
      "Epoch 18 -- Batch 817/ 842, training loss 0.3452475368976593\n",
      "Epoch 18 -- Batch 818/ 842, training loss 0.34261059761047363\n",
      "Epoch 18 -- Batch 819/ 842, training loss 0.3396936058998108\n",
      "Epoch 18 -- Batch 820/ 842, training loss 0.3348354399204254\n",
      "Epoch 18 -- Batch 821/ 842, training loss 0.3370582163333893\n",
      "Epoch 18 -- Batch 822/ 842, training loss 0.3373037576675415\n",
      "Epoch 18 -- Batch 823/ 842, training loss 0.3411520719528198\n",
      "Epoch 18 -- Batch 824/ 842, training loss 0.3336474895477295\n",
      "Epoch 18 -- Batch 825/ 842, training loss 0.3349176049232483\n",
      "Epoch 18 -- Batch 826/ 842, training loss 0.3463846445083618\n",
      "Epoch 18 -- Batch 827/ 842, training loss 0.35195818543434143\n",
      "Epoch 18 -- Batch 828/ 842, training loss 0.342763215303421\n",
      "Epoch 18 -- Batch 829/ 842, training loss 0.3439367413520813\n",
      "Epoch 18 -- Batch 830/ 842, training loss 0.3436417877674103\n",
      "Epoch 18 -- Batch 831/ 842, training loss 0.3410928547382355\n",
      "Epoch 18 -- Batch 832/ 842, training loss 0.3289080560207367\n",
      "Epoch 18 -- Batch 833/ 842, training loss 0.3328703045845032\n",
      "Epoch 18 -- Batch 834/ 842, training loss 0.3482615351676941\n",
      "Epoch 18 -- Batch 835/ 842, training loss 0.3269970715045929\n",
      "Epoch 18 -- Batch 836/ 842, training loss 0.33302783966064453\n",
      "Epoch 18 -- Batch 837/ 842, training loss 0.3310028612613678\n",
      "Epoch 18 -- Batch 838/ 842, training loss 0.3304465413093567\n",
      "Epoch 18 -- Batch 839/ 842, training loss 0.32472261786460876\n",
      "Epoch 18 -- Batch 840/ 842, training loss 0.3305460214614868\n",
      "Epoch 18 -- Batch 841/ 842, training loss 0.32995253801345825\n",
      "Epoch 18 -- Batch 842/ 842, training loss 0.34990185499191284\n",
      "----------------------------------------------------------------------\n",
      "Epoch 18 -- Batch 1/ 94, validation loss 0.32241132855415344\n",
      "Epoch 18 -- Batch 2/ 94, validation loss 0.3404140770435333\n",
      "Epoch 18 -- Batch 3/ 94, validation loss 0.3300086557865143\n",
      "Epoch 18 -- Batch 4/ 94, validation loss 0.35909736156463623\n",
      "Epoch 18 -- Batch 5/ 94, validation loss 0.3453487455844879\n",
      "Epoch 18 -- Batch 6/ 94, validation loss 0.32761725783348083\n",
      "Epoch 18 -- Batch 7/ 94, validation loss 0.3225143253803253\n",
      "Epoch 18 -- Batch 8/ 94, validation loss 0.31918713450431824\n",
      "Epoch 18 -- Batch 9/ 94, validation loss 0.3139272630214691\n",
      "Epoch 18 -- Batch 10/ 94, validation loss 0.3247784376144409\n",
      "Epoch 18 -- Batch 11/ 94, validation loss 0.31607067584991455\n",
      "Epoch 18 -- Batch 12/ 94, validation loss 0.333984375\n",
      "Epoch 18 -- Batch 13/ 94, validation loss 0.33368557691574097\n",
      "Epoch 18 -- Batch 14/ 94, validation loss 0.3314065933227539\n",
      "Epoch 18 -- Batch 15/ 94, validation loss 0.3309450149536133\n",
      "Epoch 18 -- Batch 16/ 94, validation loss 0.3434089124202728\n",
      "Epoch 18 -- Batch 17/ 94, validation loss 0.3230118155479431\n",
      "Epoch 18 -- Batch 18/ 94, validation loss 0.33275389671325684\n",
      "Epoch 18 -- Batch 19/ 94, validation loss 0.32369571924209595\n",
      "Epoch 18 -- Batch 20/ 94, validation loss 0.33107468485832214\n",
      "Epoch 18 -- Batch 21/ 94, validation loss 0.3288727402687073\n",
      "Epoch 18 -- Batch 22/ 94, validation loss 0.31585705280303955\n",
      "Epoch 18 -- Batch 23/ 94, validation loss 0.3250245153903961\n",
      "Epoch 18 -- Batch 24/ 94, validation loss 0.3348712623119354\n",
      "Epoch 18 -- Batch 25/ 94, validation loss 0.331288605928421\n",
      "Epoch 18 -- Batch 26/ 94, validation loss 0.37496834993362427\n",
      "Epoch 18 -- Batch 27/ 94, validation loss 0.31807002425193787\n",
      "Epoch 18 -- Batch 28/ 94, validation loss 0.32178449630737305\n",
      "Epoch 18 -- Batch 29/ 94, validation loss 0.3278818428516388\n",
      "Epoch 18 -- Batch 30/ 94, validation loss 0.3161377012729645\n",
      "Epoch 18 -- Batch 31/ 94, validation loss 0.3305138945579529\n",
      "Epoch 18 -- Batch 32/ 94, validation loss 0.3298931419849396\n",
      "Epoch 18 -- Batch 33/ 94, validation loss 0.3304567337036133\n",
      "Epoch 18 -- Batch 34/ 94, validation loss 0.32095038890838623\n",
      "Epoch 18 -- Batch 35/ 94, validation loss 0.34286096692085266\n",
      "Epoch 18 -- Batch 36/ 94, validation loss 0.3274529278278351\n",
      "Epoch 18 -- Batch 37/ 94, validation loss 0.3375316262245178\n",
      "Epoch 18 -- Batch 38/ 94, validation loss 0.33829766511917114\n",
      "Epoch 18 -- Batch 39/ 94, validation loss 0.3232989013195038\n",
      "Epoch 18 -- Batch 40/ 94, validation loss 0.3450484573841095\n",
      "Epoch 18 -- Batch 41/ 94, validation loss 0.32569217681884766\n",
      "Epoch 18 -- Batch 42/ 94, validation loss 0.3554723262786865\n",
      "Epoch 18 -- Batch 43/ 94, validation loss 0.338835209608078\n",
      "Epoch 18 -- Batch 44/ 94, validation loss 0.3272441625595093\n",
      "Epoch 18 -- Batch 45/ 94, validation loss 0.34136465191841125\n",
      "Epoch 18 -- Batch 46/ 94, validation loss 0.33430397510528564\n",
      "Epoch 18 -- Batch 47/ 94, validation loss 0.3239631950855255\n",
      "Epoch 18 -- Batch 48/ 94, validation loss 0.3234994113445282\n",
      "Epoch 18 -- Batch 49/ 94, validation loss 0.33817052841186523\n",
      "Epoch 18 -- Batch 50/ 94, validation loss 0.33368444442749023\n",
      "Epoch 18 -- Batch 51/ 94, validation loss 0.33023473620414734\n",
      "Epoch 18 -- Batch 52/ 94, validation loss 0.32408007979393005\n",
      "Epoch 18 -- Batch 53/ 94, validation loss 0.32160326838493347\n",
      "Epoch 18 -- Batch 54/ 94, validation loss 0.3304457664489746\n",
      "Epoch 18 -- Batch 55/ 94, validation loss 0.3470044434070587\n",
      "Epoch 18 -- Batch 56/ 94, validation loss 0.3271368145942688\n",
      "Epoch 18 -- Batch 57/ 94, validation loss 0.3398948311805725\n",
      "Epoch 18 -- Batch 58/ 94, validation loss 0.3267192542552948\n",
      "Epoch 18 -- Batch 59/ 94, validation loss 0.3251873254776001\n",
      "Epoch 18 -- Batch 60/ 94, validation loss 0.33273690938949585\n",
      "Epoch 18 -- Batch 61/ 94, validation loss 0.3502424955368042\n",
      "Epoch 18 -- Batch 62/ 94, validation loss 0.33029672503471375\n",
      "Epoch 18 -- Batch 63/ 94, validation loss 0.3277057111263275\n",
      "Epoch 18 -- Batch 64/ 94, validation loss 0.32916998863220215\n",
      "Epoch 18 -- Batch 65/ 94, validation loss 0.33574149012565613\n",
      "Epoch 18 -- Batch 66/ 94, validation loss 0.32590192556381226\n",
      "Epoch 18 -- Batch 67/ 94, validation loss 0.31752803921699524\n",
      "Epoch 18 -- Batch 68/ 94, validation loss 0.32622960209846497\n",
      "Epoch 18 -- Batch 69/ 94, validation loss 0.3235660493373871\n",
      "Epoch 18 -- Batch 70/ 94, validation loss 0.3347201347351074\n",
      "Epoch 18 -- Batch 71/ 94, validation loss 0.33813315629959106\n",
      "Epoch 18 -- Batch 72/ 94, validation loss 0.3266879618167877\n",
      "Epoch 18 -- Batch 73/ 94, validation loss 0.3252553641796112\n",
      "Epoch 18 -- Batch 74/ 94, validation loss 0.33787986636161804\n",
      "Epoch 18 -- Batch 75/ 94, validation loss 0.341242253780365\n",
      "Epoch 18 -- Batch 76/ 94, validation loss 0.3219015896320343\n",
      "Epoch 18 -- Batch 77/ 94, validation loss 0.33673813939094543\n",
      "Epoch 18 -- Batch 78/ 94, validation loss 0.3200819790363312\n",
      "Epoch 18 -- Batch 79/ 94, validation loss 0.3375682234764099\n",
      "Epoch 18 -- Batch 80/ 94, validation loss 0.3368729054927826\n",
      "Epoch 18 -- Batch 81/ 94, validation loss 0.328504741191864\n",
      "Epoch 18 -- Batch 82/ 94, validation loss 0.32784155011177063\n",
      "Epoch 18 -- Batch 83/ 94, validation loss 0.3387316167354584\n",
      "Epoch 18 -- Batch 84/ 94, validation loss 0.322736531496048\n",
      "Epoch 18 -- Batch 85/ 94, validation loss 0.33516618609428406\n",
      "Epoch 18 -- Batch 86/ 94, validation loss 0.3237742483615875\n",
      "Epoch 18 -- Batch 87/ 94, validation loss 0.33579710125923157\n",
      "Epoch 18 -- Batch 88/ 94, validation loss 0.3299093246459961\n",
      "Epoch 18 -- Batch 89/ 94, validation loss 0.3242028057575226\n",
      "Epoch 18 -- Batch 90/ 94, validation loss 0.3265136480331421\n",
      "Epoch 18 -- Batch 91/ 94, validation loss 0.32257577776908875\n",
      "Epoch 18 -- Batch 92/ 94, validation loss 0.31455591320991516\n",
      "Epoch 18 -- Batch 93/ 94, validation loss 0.3379758596420288\n",
      "Epoch 18 -- Batch 94/ 94, validation loss 0.3089485466480255\n",
      "----------------------------------------------------------------------\n",
      "Epoch 18 loss: Training 0.33651265501976013, Validation 0.3089485466480255\n",
      "----------------------------------------------------------------------\n",
      "Epoch 19/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:06:50] SMILES Parse Error: syntax error while parsing: O=C(NCc1n[nH]c(=S)n1-c1cccc()c1)c1ccccc1\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'O=C(NCc1n[nH]c(=S)n1-c1cccc()c1)c1ccccc1' for input: 'O=C(NCc1n[nH]c(=S)n1-c1cccc()c1)c1ccccc1'\n",
      "[19:06:50] SMILES Parse Error: syntax error while parsing: CCCCNC(=O)/(=C/c1ccco1)NC(=O)c1ccc(C)cc1\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'CCCCNC(=O)/(=C/c1ccco1)NC(=O)c1ccc(C)cc1' for input: 'CCCCNC(=O)/(=C/c1ccco1)NC(=O)c1ccc(C)cc1'\n",
      "[19:06:50] SMILES Parse Error: syntax error while parsing: Cc1ccc(-n2c(=O)[nH]cc(C(=O)Nc3c(C)cc(Br)cc3C)c2=)cc1\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(-n2c(=O)[nH]cc(C(=O)Nc3c(C)cc(Br)cc3C)c2=)cc1' for input: 'Cc1ccc(-n2c(=O)[nH]cc(C(=O)Nc3c(C)cc(Br)cc3C)c2=)cc1'\n",
      "[19:06:50] SMILES Parse Error: ring closure 2 duplicates bond between atom 4 and atom 12 for input: 'C[C@@H]1C[C@@H](C2(c3ccc(F)cc3)C2)n(C)n1'\n",
      "[19:06:50] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCCCCCCCCCC(=O)NCC[N+]SC)CO=O\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCCCCCC(=O)NCC[N+]SC)CO=O' for input: 'CCCCCCCCCCCCCCCC(=O)NCC[N+]SC)CO=O'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)CCC(=O)Nc1ccc(C2SCC)cc2ccccc12'\n",
      "[19:06:50] Explicit valence for atom # 1 F, 2, is greater than permitted\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 20 21 24\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 1 2 3 9 11 12 13\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(C(=O)O)C1CCC2(C)OC(=O)CC13'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'COC(=O)C1(CCC(=O)O)Cc2cc(-c3ccc5c(c3)OCO4)ccc2O1'\n",
      "[19:06:50] non-ring atom 18 marked aromatic\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CCC12CC3(CC1)CC(C(=O)NCCCN1CCN(C)CC1)O2'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 22\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 15 16\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCN2C(=O)CCC2CCNCc2cc3c(cc23)OCO3)cc1OC'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CCc1ccc2[nH]c(O)c(Nc3ccc4c(c3)OCO4)C2c1cccnc1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CC12C(=O)OC(CO)(C(F)(F)F)C1(C)O'\n",
      "[19:06:50] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(-c2ccc3c(c2)-c(ccccc2NS(C)(=O)=O)ccc23'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 9 10 11 12 21 22 27\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CCc1csc(CCNC(=O)C2(CC)CC[C@@H]2C(=O)N(C)Cc3ccccc32)c1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1cccc(NC(=O)c2sc3nc4c(c(-c3ccco3)c2ccccc24)CC(C)(C)C)c1'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 11 12 25 26 27\n",
      "[19:06:50] SMILES Parse Error: syntax error while parsing: Cc1cc(C)cc(NC(=O)CCCn2c(SCC(=O)O)nc3c(sc4cccc(4)c42)c2=O)c1\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C)cc(NC(=O)CCCn2c(SCC(=O)O)nc3c(sc4cccc(4)c42)c2=O)c1' for input: 'Cc1cc(C)cc(NC(=O)CCCn2c(SCC(=O)O)nc3c(sc4cccc(4)c42)c2=O)c1'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 17 18 19 20 22 23 24 25 26\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 3 4 5 7 8 14 23\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 20 23\n",
      "[19:06:50] SMILES Parse Error: syntax error while parsing: CCn1c(-c2ccc(OC)c()cc2)csc1=Nc1cccc(C(F)(F)F)c1\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'CCn1c(-c2ccc(OC)c()cc2)csc1=Nc1cccc(C(F)(F)F)c1' for input: 'CCn1c(-c2ccc(OC)c()cc2)csc1=Nc1cccc(C(F)(F)F)c1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)COc1c3oc4ccccc4c(=O)n2c2CO1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)cc(CN(Cc1ccccc1)C(=O)COC(=O)C1CC(C)(C)NC(C)(C)C1)C(=O)NC1CCCC1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N2C(=O)C3C4c5ccc(C(=O)c5ccccc5)C(=O)C5C3C2=O)c1'\n",
      "[19:06:50] SMILES Parse Error: extra close parentheses while parsing: C1C2(C(O)BrNN-2ccc2c2cn[nH]c2)CO3)ccc1Cl\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'C1C2(C(O)BrNN-2ccc2c2cn[nH]c2)CO3)ccc1Cl' for input: 'C1C2(C(O)BrNN-2ccc2c2cn[nH]c2)CO3)ccc1Cl'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CCc1onc(C)c1C(=O)Nc1nccn#Cc1cc1ccccc12'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'O=C(c1cccc(F)c1)N(C)CC(=O)N1[C@@H]2CC[C@H]1CN1Cc1cccc(-c2ccccc2)c1'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 6 7 8 19 20 22 23 24 25\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'O=C(O1CCCC1=O)C(c1ccccc1)N1'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 1 2 3 23 24 25 26 27 29\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CN(C)c1ccc(/C=N/N2C(=O)C3C4C=CC(C5)C3C2=O)cc1'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 24 25 31\n",
      "[19:06:50] SMILES Parse Error: extra close parentheses while parsing: Cc1ncn(-c2ccc(N=c3oc4cc(Cl)cc(Cl)c4C3=O)cc2)c1)c1ccccc1\n",
      "[19:06:50] SMILES Parse Error: Failed parsing SMILES 'Cc1ncn(-c2ccc(N=c3oc4cc(Cl)cc(Cl)c4C3=O)cc2)c1)c1ccccc1' for input: 'Cc1ncn(-c2ccc(N=c3oc4cc(Cl)cc(Cl)c4C3=O)cc2)c1)c1ccccc1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(N2C(N)=C(C#N)C(c3cc(Br)ccc3OC)C3=C2C)sc2c1CCC(C)C2'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 3 11 12\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'Cc1ccn2c(NC(=O)C3(C)CC3CCC(C)(C)C3)c(=O)[nH]c2c1'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'COCCNC(CC(=O)Nc1c(C)cc(C)cc2C)c1C'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'COc1cccc(-c2nc3sc4c(c3c(=O)n2CCC#N)C(C)CC)c1'\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 8 17 18 19 20 21 22\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 20 21 22\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 3 5 6 7 14 15 24\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 28\n",
      "[19:06:50] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 10 11 29 30\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Cl)cc1-n1c(C)nc2cc=C(c3ccc([N+](=O)[O-])cc3)=Nc2ccccc21'\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1Nc1c(C)nn2C1CC2CCC1C2'\n",
      "[19:06:50] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:06:50] SMILES Parse Error: unclosed ring for input: 'Cn1c(=O)C(CC(=O)N2CCC3(CC2)OCCO3)c2c[nH]c3ccccc22'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 -- Batch 1/ 842, training loss 0.3267415761947632\n",
      "Epoch 19 -- Batch 2/ 842, training loss 0.3274000883102417\n",
      "Epoch 19 -- Batch 3/ 842, training loss 0.3315780460834503\n",
      "Epoch 19 -- Batch 4/ 842, training loss 0.3226020336151123\n",
      "Epoch 19 -- Batch 5/ 842, training loss 0.34827181696891785\n",
      "Epoch 19 -- Batch 6/ 842, training loss 0.32821935415267944\n",
      "Epoch 19 -- Batch 7/ 842, training loss 0.33390310406684875\n",
      "Epoch 19 -- Batch 8/ 842, training loss 0.3423732817173004\n",
      "Epoch 19 -- Batch 9/ 842, training loss 0.32637739181518555\n",
      "Epoch 19 -- Batch 10/ 842, training loss 0.34051331877708435\n",
      "Epoch 19 -- Batch 11/ 842, training loss 0.33092352747917175\n",
      "Epoch 19 -- Batch 12/ 842, training loss 0.3239806592464447\n",
      "Epoch 19 -- Batch 13/ 842, training loss 0.33943215012550354\n",
      "Epoch 19 -- Batch 14/ 842, training loss 0.3362945020198822\n",
      "Epoch 19 -- Batch 15/ 842, training loss 0.32654133439064026\n",
      "Epoch 19 -- Batch 16/ 842, training loss 0.33544081449508667\n",
      "Epoch 19 -- Batch 17/ 842, training loss 0.33531010150909424\n",
      "Epoch 19 -- Batch 18/ 842, training loss 0.33093589544296265\n",
      "Epoch 19 -- Batch 19/ 842, training loss 0.318827360868454\n",
      "Epoch 19 -- Batch 20/ 842, training loss 0.33540791273117065\n",
      "Epoch 19 -- Batch 21/ 842, training loss 0.32737216353416443\n",
      "Epoch 19 -- Batch 22/ 842, training loss 0.32238471508026123\n",
      "Epoch 19 -- Batch 23/ 842, training loss 0.3344936668872833\n",
      "Epoch 19 -- Batch 24/ 842, training loss 0.33080244064331055\n",
      "Epoch 19 -- Batch 25/ 842, training loss 0.31756240129470825\n",
      "Epoch 19 -- Batch 26/ 842, training loss 0.3269117474555969\n",
      "Epoch 19 -- Batch 27/ 842, training loss 0.3312661647796631\n",
      "Epoch 19 -- Batch 28/ 842, training loss 0.3410482406616211\n",
      "Epoch 19 -- Batch 29/ 842, training loss 0.3267698287963867\n",
      "Epoch 19 -- Batch 30/ 842, training loss 0.3380385935306549\n",
      "Epoch 19 -- Batch 31/ 842, training loss 0.3246704638004303\n",
      "Epoch 19 -- Batch 32/ 842, training loss 0.3233181834220886\n",
      "Epoch 19 -- Batch 33/ 842, training loss 0.31651079654693604\n",
      "Epoch 19 -- Batch 34/ 842, training loss 0.32421889901161194\n",
      "Epoch 19 -- Batch 35/ 842, training loss 0.34407350420951843\n",
      "Epoch 19 -- Batch 36/ 842, training loss 0.33093053102493286\n",
      "Epoch 19 -- Batch 37/ 842, training loss 0.3320176899433136\n",
      "Epoch 19 -- Batch 38/ 842, training loss 0.3331235349178314\n",
      "Epoch 19 -- Batch 39/ 842, training loss 0.3323327302932739\n",
      "Epoch 19 -- Batch 40/ 842, training loss 0.319221556186676\n",
      "Epoch 19 -- Batch 41/ 842, training loss 0.31375592947006226\n",
      "Epoch 19 -- Batch 42/ 842, training loss 0.33677101135253906\n",
      "Epoch 19 -- Batch 43/ 842, training loss 0.32903867959976196\n",
      "Epoch 19 -- Batch 44/ 842, training loss 0.3429626226425171\n",
      "Epoch 19 -- Batch 45/ 842, training loss 0.3282465636730194\n",
      "Epoch 19 -- Batch 46/ 842, training loss 0.32461148500442505\n",
      "Epoch 19 -- Batch 47/ 842, training loss 0.31824731826782227\n",
      "Epoch 19 -- Batch 48/ 842, training loss 0.32876524329185486\n",
      "Epoch 19 -- Batch 49/ 842, training loss 0.3346307575702667\n",
      "Epoch 19 -- Batch 50/ 842, training loss 0.3393225371837616\n",
      "Epoch 19 -- Batch 51/ 842, training loss 0.33105477690696716\n",
      "Epoch 19 -- Batch 52/ 842, training loss 0.31929081678390503\n",
      "Epoch 19 -- Batch 53/ 842, training loss 0.32220086455345154\n",
      "Epoch 19 -- Batch 54/ 842, training loss 0.3250505328178406\n",
      "Epoch 19 -- Batch 55/ 842, training loss 0.3270350992679596\n",
      "Epoch 19 -- Batch 56/ 842, training loss 0.3405017554759979\n",
      "Epoch 19 -- Batch 57/ 842, training loss 0.33895233273506165\n",
      "Epoch 19 -- Batch 58/ 842, training loss 0.3335280418395996\n",
      "Epoch 19 -- Batch 59/ 842, training loss 0.33292296528816223\n",
      "Epoch 19 -- Batch 60/ 842, training loss 0.31639039516448975\n",
      "Epoch 19 -- Batch 61/ 842, training loss 0.33414310216903687\n",
      "Epoch 19 -- Batch 62/ 842, training loss 0.33387863636016846\n",
      "Epoch 19 -- Batch 63/ 842, training loss 0.33072957396507263\n",
      "Epoch 19 -- Batch 64/ 842, training loss 0.32478731870651245\n",
      "Epoch 19 -- Batch 65/ 842, training loss 0.32449400424957275\n",
      "Epoch 19 -- Batch 66/ 842, training loss 0.32736557722091675\n",
      "Epoch 19 -- Batch 67/ 842, training loss 0.31744855642318726\n",
      "Epoch 19 -- Batch 68/ 842, training loss 0.33934327960014343\n",
      "Epoch 19 -- Batch 69/ 842, training loss 0.3351113796234131\n",
      "Epoch 19 -- Batch 70/ 842, training loss 0.32766446471214294\n",
      "Epoch 19 -- Batch 71/ 842, training loss 0.35199084877967834\n",
      "Epoch 19 -- Batch 72/ 842, training loss 0.33085814118385315\n",
      "Epoch 19 -- Batch 73/ 842, training loss 0.32461485266685486\n",
      "Epoch 19 -- Batch 74/ 842, training loss 0.3451039493083954\n",
      "Epoch 19 -- Batch 75/ 842, training loss 0.33177298307418823\n",
      "Epoch 19 -- Batch 76/ 842, training loss 0.3352987766265869\n",
      "Epoch 19 -- Batch 77/ 842, training loss 0.3372223973274231\n",
      "Epoch 19 -- Batch 78/ 842, training loss 0.3276350498199463\n",
      "Epoch 19 -- Batch 79/ 842, training loss 0.32745006680488586\n",
      "Epoch 19 -- Batch 80/ 842, training loss 0.33650442957878113\n",
      "Epoch 19 -- Batch 81/ 842, training loss 0.33090507984161377\n",
      "Epoch 19 -- Batch 82/ 842, training loss 0.3299923539161682\n",
      "Epoch 19 -- Batch 83/ 842, training loss 0.32228967547416687\n",
      "Epoch 19 -- Batch 84/ 842, training loss 0.3395918607711792\n",
      "Epoch 19 -- Batch 85/ 842, training loss 0.3186284303665161\n",
      "Epoch 19 -- Batch 86/ 842, training loss 0.32266029715538025\n",
      "Epoch 19 -- Batch 87/ 842, training loss 0.34039705991744995\n",
      "Epoch 19 -- Batch 88/ 842, training loss 0.3282637596130371\n",
      "Epoch 19 -- Batch 89/ 842, training loss 0.33372652530670166\n",
      "Epoch 19 -- Batch 90/ 842, training loss 0.327718585729599\n",
      "Epoch 19 -- Batch 91/ 842, training loss 0.33026719093322754\n",
      "Epoch 19 -- Batch 92/ 842, training loss 0.3294597864151001\n",
      "Epoch 19 -- Batch 93/ 842, training loss 0.3313736617565155\n",
      "Epoch 19 -- Batch 94/ 842, training loss 0.32599198818206787\n",
      "Epoch 19 -- Batch 95/ 842, training loss 0.3263607621192932\n",
      "Epoch 19 -- Batch 96/ 842, training loss 0.3458905816078186\n",
      "Epoch 19 -- Batch 97/ 842, training loss 0.3316403329372406\n",
      "Epoch 19 -- Batch 98/ 842, training loss 0.33881086111068726\n",
      "Epoch 19 -- Batch 99/ 842, training loss 0.3378649055957794\n",
      "Epoch 19 -- Batch 100/ 842, training loss 0.3202314078807831\n",
      "Epoch 19 -- Batch 101/ 842, training loss 0.33102232217788696\n",
      "Epoch 19 -- Batch 102/ 842, training loss 0.34097975492477417\n",
      "Epoch 19 -- Batch 103/ 842, training loss 0.3289075493812561\n",
      "Epoch 19 -- Batch 104/ 842, training loss 0.32493850588798523\n",
      "Epoch 19 -- Batch 105/ 842, training loss 0.33046627044677734\n",
      "Epoch 19 -- Batch 106/ 842, training loss 0.33971863985061646\n",
      "Epoch 19 -- Batch 107/ 842, training loss 0.32412534952163696\n",
      "Epoch 19 -- Batch 108/ 842, training loss 0.3370969891548157\n",
      "Epoch 19 -- Batch 109/ 842, training loss 0.3395030200481415\n",
      "Epoch 19 -- Batch 110/ 842, training loss 0.34040048718452454\n",
      "Epoch 19 -- Batch 111/ 842, training loss 0.3332459330558777\n",
      "Epoch 19 -- Batch 112/ 842, training loss 0.32281360030174255\n",
      "Epoch 19 -- Batch 113/ 842, training loss 0.33773764967918396\n",
      "Epoch 19 -- Batch 114/ 842, training loss 0.3264088034629822\n",
      "Epoch 19 -- Batch 115/ 842, training loss 0.3414318561553955\n",
      "Epoch 19 -- Batch 116/ 842, training loss 0.34206637740135193\n",
      "Epoch 19 -- Batch 117/ 842, training loss 0.32967206835746765\n",
      "Epoch 19 -- Batch 118/ 842, training loss 0.32605716586112976\n",
      "Epoch 19 -- Batch 119/ 842, training loss 0.3416061997413635\n",
      "Epoch 19 -- Batch 120/ 842, training loss 0.32329609990119934\n",
      "Epoch 19 -- Batch 121/ 842, training loss 0.34518882632255554\n",
      "Epoch 19 -- Batch 122/ 842, training loss 0.3389354348182678\n",
      "Epoch 19 -- Batch 123/ 842, training loss 0.3248741328716278\n",
      "Epoch 19 -- Batch 124/ 842, training loss 0.33619940280914307\n",
      "Epoch 19 -- Batch 125/ 842, training loss 0.33019495010375977\n",
      "Epoch 19 -- Batch 126/ 842, training loss 0.3340745270252228\n",
      "Epoch 19 -- Batch 127/ 842, training loss 0.3345740735530853\n",
      "Epoch 19 -- Batch 128/ 842, training loss 0.3188285827636719\n",
      "Epoch 19 -- Batch 129/ 842, training loss 0.3200257122516632\n",
      "Epoch 19 -- Batch 130/ 842, training loss 0.3407008945941925\n",
      "Epoch 19 -- Batch 131/ 842, training loss 0.3285893201828003\n",
      "Epoch 19 -- Batch 132/ 842, training loss 0.3220036029815674\n",
      "Epoch 19 -- Batch 133/ 842, training loss 0.31704282760620117\n",
      "Epoch 19 -- Batch 134/ 842, training loss 0.33594921231269836\n",
      "Epoch 19 -- Batch 135/ 842, training loss 0.3299703598022461\n",
      "Epoch 19 -- Batch 136/ 842, training loss 0.3390210270881653\n",
      "Epoch 19 -- Batch 137/ 842, training loss 0.3241997957229614\n",
      "Epoch 19 -- Batch 138/ 842, training loss 0.3266776502132416\n",
      "Epoch 19 -- Batch 139/ 842, training loss 0.3310754895210266\n",
      "Epoch 19 -- Batch 140/ 842, training loss 0.32383960485458374\n",
      "Epoch 19 -- Batch 141/ 842, training loss 0.3284745514392853\n",
      "Epoch 19 -- Batch 142/ 842, training loss 0.3313910961151123\n",
      "Epoch 19 -- Batch 143/ 842, training loss 0.3323415517807007\n",
      "Epoch 19 -- Batch 144/ 842, training loss 0.3373563885688782\n",
      "Epoch 19 -- Batch 145/ 842, training loss 0.32120656967163086\n",
      "Epoch 19 -- Batch 146/ 842, training loss 0.32313260436058044\n",
      "Epoch 19 -- Batch 147/ 842, training loss 0.318771630525589\n",
      "Epoch 19 -- Batch 148/ 842, training loss 0.33120298385620117\n",
      "Epoch 19 -- Batch 149/ 842, training loss 0.34286946058273315\n",
      "Epoch 19 -- Batch 150/ 842, training loss 0.33151209354400635\n",
      "Epoch 19 -- Batch 151/ 842, training loss 0.3227490782737732\n",
      "Epoch 19 -- Batch 152/ 842, training loss 0.32365086674690247\n",
      "Epoch 19 -- Batch 153/ 842, training loss 0.3196753263473511\n",
      "Epoch 19 -- Batch 154/ 842, training loss 0.33034053444862366\n",
      "Epoch 19 -- Batch 155/ 842, training loss 0.3261008560657501\n",
      "Epoch 19 -- Batch 156/ 842, training loss 0.33225470781326294\n",
      "Epoch 19 -- Batch 157/ 842, training loss 0.3334805369377136\n",
      "Epoch 19 -- Batch 158/ 842, training loss 0.3270978033542633\n",
      "Epoch 19 -- Batch 159/ 842, training loss 0.3310145139694214\n",
      "Epoch 19 -- Batch 160/ 842, training loss 0.327985942363739\n",
      "Epoch 19 -- Batch 161/ 842, training loss 0.33508339524269104\n",
      "Epoch 19 -- Batch 162/ 842, training loss 0.3393368124961853\n",
      "Epoch 19 -- Batch 163/ 842, training loss 0.3304153382778168\n",
      "Epoch 19 -- Batch 164/ 842, training loss 0.32376930117607117\n",
      "Epoch 19 -- Batch 165/ 842, training loss 0.33137810230255127\n",
      "Epoch 19 -- Batch 166/ 842, training loss 0.33965206146240234\n",
      "Epoch 19 -- Batch 167/ 842, training loss 0.33309686183929443\n",
      "Epoch 19 -- Batch 168/ 842, training loss 0.3394436240196228\n",
      "Epoch 19 -- Batch 169/ 842, training loss 0.31598398089408875\n",
      "Epoch 19 -- Batch 170/ 842, training loss 0.32080456614494324\n",
      "Epoch 19 -- Batch 171/ 842, training loss 0.32630667090415955\n",
      "Epoch 19 -- Batch 172/ 842, training loss 0.32849785685539246\n",
      "Epoch 19 -- Batch 173/ 842, training loss 0.32270902395248413\n",
      "Epoch 19 -- Batch 174/ 842, training loss 0.3371800482273102\n",
      "Epoch 19 -- Batch 175/ 842, training loss 0.32550930976867676\n",
      "Epoch 19 -- Batch 176/ 842, training loss 0.3202698230743408\n",
      "Epoch 19 -- Batch 177/ 842, training loss 0.32838839292526245\n",
      "Epoch 19 -- Batch 178/ 842, training loss 0.33872127532958984\n",
      "Epoch 19 -- Batch 179/ 842, training loss 0.3306180238723755\n",
      "Epoch 19 -- Batch 180/ 842, training loss 0.3360171914100647\n",
      "Epoch 19 -- Batch 181/ 842, training loss 0.33404722809791565\n",
      "Epoch 19 -- Batch 182/ 842, training loss 0.3263613283634186\n",
      "Epoch 19 -- Batch 183/ 842, training loss 0.3345155417919159\n",
      "Epoch 19 -- Batch 184/ 842, training loss 0.33074918389320374\n",
      "Epoch 19 -- Batch 185/ 842, training loss 0.3378152549266815\n",
      "Epoch 19 -- Batch 186/ 842, training loss 0.3357568085193634\n",
      "Epoch 19 -- Batch 187/ 842, training loss 0.32982704043388367\n",
      "Epoch 19 -- Batch 188/ 842, training loss 0.31804853677749634\n",
      "Epoch 19 -- Batch 189/ 842, training loss 0.33213919401168823\n",
      "Epoch 19 -- Batch 190/ 842, training loss 0.3447614014148712\n",
      "Epoch 19 -- Batch 191/ 842, training loss 0.33716267347335815\n",
      "Epoch 19 -- Batch 192/ 842, training loss 0.33710598945617676\n",
      "Epoch 19 -- Batch 193/ 842, training loss 0.32396814227104187\n",
      "Epoch 19 -- Batch 194/ 842, training loss 0.33754849433898926\n",
      "Epoch 19 -- Batch 195/ 842, training loss 0.31649401783943176\n",
      "Epoch 19 -- Batch 196/ 842, training loss 0.3259817063808441\n",
      "Epoch 19 -- Batch 197/ 842, training loss 0.31312277913093567\n",
      "Epoch 19 -- Batch 198/ 842, training loss 0.3387463092803955\n",
      "Epoch 19 -- Batch 199/ 842, training loss 0.33896398544311523\n",
      "Epoch 19 -- Batch 200/ 842, training loss 0.31635576486587524\n",
      "Epoch 19 -- Batch 201/ 842, training loss 0.343847393989563\n",
      "Epoch 19 -- Batch 202/ 842, training loss 0.31153449416160583\n",
      "Epoch 19 -- Batch 203/ 842, training loss 0.32809948921203613\n",
      "Epoch 19 -- Batch 204/ 842, training loss 0.3323172926902771\n",
      "Epoch 19 -- Batch 205/ 842, training loss 0.3444768190383911\n",
      "Epoch 19 -- Batch 206/ 842, training loss 0.3230471909046173\n",
      "Epoch 19 -- Batch 207/ 842, training loss 0.3266950845718384\n",
      "Epoch 19 -- Batch 208/ 842, training loss 0.31547582149505615\n",
      "Epoch 19 -- Batch 209/ 842, training loss 0.3289620578289032\n",
      "Epoch 19 -- Batch 210/ 842, training loss 0.34262216091156006\n",
      "Epoch 19 -- Batch 211/ 842, training loss 0.3309812843799591\n",
      "Epoch 19 -- Batch 212/ 842, training loss 0.3303888440132141\n",
      "Epoch 19 -- Batch 213/ 842, training loss 0.3393349051475525\n",
      "Epoch 19 -- Batch 214/ 842, training loss 0.3234667479991913\n",
      "Epoch 19 -- Batch 215/ 842, training loss 0.34505152702331543\n",
      "Epoch 19 -- Batch 216/ 842, training loss 0.33164891600608826\n",
      "Epoch 19 -- Batch 217/ 842, training loss 0.32717111706733704\n",
      "Epoch 19 -- Batch 218/ 842, training loss 0.3230225741863251\n",
      "Epoch 19 -- Batch 219/ 842, training loss 0.33172014355659485\n",
      "Epoch 19 -- Batch 220/ 842, training loss 0.31607457995414734\n",
      "Epoch 19 -- Batch 221/ 842, training loss 0.3309553265571594\n",
      "Epoch 19 -- Batch 222/ 842, training loss 0.3255957365036011\n",
      "Epoch 19 -- Batch 223/ 842, training loss 0.3333478271961212\n",
      "Epoch 19 -- Batch 224/ 842, training loss 0.33050793409347534\n",
      "Epoch 19 -- Batch 225/ 842, training loss 0.3278084397315979\n",
      "Epoch 19 -- Batch 226/ 842, training loss 0.3320651948451996\n",
      "Epoch 19 -- Batch 227/ 842, training loss 0.3465255796909332\n",
      "Epoch 19 -- Batch 228/ 842, training loss 0.3289262652397156\n",
      "Epoch 19 -- Batch 229/ 842, training loss 0.33479973673820496\n",
      "Epoch 19 -- Batch 230/ 842, training loss 0.32780921459198\n",
      "Epoch 19 -- Batch 231/ 842, training loss 0.33275106549263\n",
      "Epoch 19 -- Batch 232/ 842, training loss 0.34277141094207764\n",
      "Epoch 19 -- Batch 233/ 842, training loss 0.325427770614624\n",
      "Epoch 19 -- Batch 234/ 842, training loss 0.337441086769104\n",
      "Epoch 19 -- Batch 235/ 842, training loss 0.33284318447113037\n",
      "Epoch 19 -- Batch 236/ 842, training loss 0.3306146562099457\n",
      "Epoch 19 -- Batch 237/ 842, training loss 0.3281387388706207\n",
      "Epoch 19 -- Batch 238/ 842, training loss 0.33914798498153687\n",
      "Epoch 19 -- Batch 239/ 842, training loss 0.3319148123264313\n",
      "Epoch 19 -- Batch 240/ 842, training loss 0.3578997254371643\n",
      "Epoch 19 -- Batch 241/ 842, training loss 0.33394163846969604\n",
      "Epoch 19 -- Batch 242/ 842, training loss 0.32170534133911133\n",
      "Epoch 19 -- Batch 243/ 842, training loss 0.3459778428077698\n",
      "Epoch 19 -- Batch 244/ 842, training loss 0.3349058926105499\n",
      "Epoch 19 -- Batch 245/ 842, training loss 0.3244161903858185\n",
      "Epoch 19 -- Batch 246/ 842, training loss 0.3339020013809204\n",
      "Epoch 19 -- Batch 247/ 842, training loss 0.33494991064071655\n",
      "Epoch 19 -- Batch 248/ 842, training loss 0.33034810423851013\n",
      "Epoch 19 -- Batch 249/ 842, training loss 0.33498892188072205\n",
      "Epoch 19 -- Batch 250/ 842, training loss 0.32058966159820557\n",
      "Epoch 19 -- Batch 251/ 842, training loss 0.3307672142982483\n",
      "Epoch 19 -- Batch 252/ 842, training loss 0.32313072681427\n",
      "Epoch 19 -- Batch 253/ 842, training loss 0.3256124258041382\n",
      "Epoch 19 -- Batch 254/ 842, training loss 0.3311930298805237\n",
      "Epoch 19 -- Batch 255/ 842, training loss 0.3287975490093231\n",
      "Epoch 19 -- Batch 256/ 842, training loss 0.3264487385749817\n",
      "Epoch 19 -- Batch 257/ 842, training loss 0.32713383436203003\n",
      "Epoch 19 -- Batch 258/ 842, training loss 0.3445270359516144\n",
      "Epoch 19 -- Batch 259/ 842, training loss 0.33538830280303955\n",
      "Epoch 19 -- Batch 260/ 842, training loss 0.32116010785102844\n",
      "Epoch 19 -- Batch 261/ 842, training loss 0.33808398246765137\n",
      "Epoch 19 -- Batch 262/ 842, training loss 0.3340873420238495\n",
      "Epoch 19 -- Batch 263/ 842, training loss 0.3192674219608307\n",
      "Epoch 19 -- Batch 264/ 842, training loss 0.33866453170776367\n",
      "Epoch 19 -- Batch 265/ 842, training loss 0.33666425943374634\n",
      "Epoch 19 -- Batch 266/ 842, training loss 0.337311327457428\n",
      "Epoch 19 -- Batch 267/ 842, training loss 0.3410104513168335\n",
      "Epoch 19 -- Batch 268/ 842, training loss 0.3479512333869934\n",
      "Epoch 19 -- Batch 269/ 842, training loss 0.32306087017059326\n",
      "Epoch 19 -- Batch 270/ 842, training loss 0.33352503180503845\n",
      "Epoch 19 -- Batch 271/ 842, training loss 0.32197850942611694\n",
      "Epoch 19 -- Batch 272/ 842, training loss 0.33130592107772827\n",
      "Epoch 19 -- Batch 273/ 842, training loss 0.3305797874927521\n",
      "Epoch 19 -- Batch 274/ 842, training loss 0.33324185013771057\n",
      "Epoch 19 -- Batch 275/ 842, training loss 0.3316943347454071\n",
      "Epoch 19 -- Batch 276/ 842, training loss 0.3292377293109894\n",
      "Epoch 19 -- Batch 277/ 842, training loss 0.3220036029815674\n",
      "Epoch 19 -- Batch 278/ 842, training loss 0.3255549371242523\n",
      "Epoch 19 -- Batch 279/ 842, training loss 0.33314409852027893\n",
      "Epoch 19 -- Batch 280/ 842, training loss 0.3303486406803131\n",
      "Epoch 19 -- Batch 281/ 842, training loss 0.33332589268684387\n",
      "Epoch 19 -- Batch 282/ 842, training loss 0.32108432054519653\n",
      "Epoch 19 -- Batch 283/ 842, training loss 0.3406257927417755\n",
      "Epoch 19 -- Batch 284/ 842, training loss 0.3241857886314392\n",
      "Epoch 19 -- Batch 285/ 842, training loss 0.3381117284297943\n",
      "Epoch 19 -- Batch 286/ 842, training loss 0.32437726855278015\n",
      "Epoch 19 -- Batch 287/ 842, training loss 0.32185742259025574\n",
      "Epoch 19 -- Batch 288/ 842, training loss 0.33199793100357056\n",
      "Epoch 19 -- Batch 289/ 842, training loss 0.3423875868320465\n",
      "Epoch 19 -- Batch 290/ 842, training loss 0.32992690801620483\n",
      "Epoch 19 -- Batch 291/ 842, training loss 0.33291497826576233\n",
      "Epoch 19 -- Batch 292/ 842, training loss 0.33071526885032654\n",
      "Epoch 19 -- Batch 293/ 842, training loss 0.32837510108947754\n",
      "Epoch 19 -- Batch 294/ 842, training loss 0.3217732012271881\n",
      "Epoch 19 -- Batch 295/ 842, training loss 0.3288148045539856\n",
      "Epoch 19 -- Batch 296/ 842, training loss 0.33813124895095825\n",
      "Epoch 19 -- Batch 297/ 842, training loss 0.3342321515083313\n",
      "Epoch 19 -- Batch 298/ 842, training loss 0.3370114862918854\n",
      "Epoch 19 -- Batch 299/ 842, training loss 0.3229735195636749\n",
      "Epoch 19 -- Batch 300/ 842, training loss 0.3430994749069214\n",
      "Epoch 19 -- Batch 301/ 842, training loss 0.32069650292396545\n",
      "Epoch 19 -- Batch 302/ 842, training loss 0.3298356235027313\n",
      "Epoch 19 -- Batch 303/ 842, training loss 0.3327684700489044\n",
      "Epoch 19 -- Batch 304/ 842, training loss 0.34179192781448364\n",
      "Epoch 19 -- Batch 305/ 842, training loss 0.3188052177429199\n",
      "Epoch 19 -- Batch 306/ 842, training loss 0.3360714316368103\n",
      "Epoch 19 -- Batch 307/ 842, training loss 0.3285445272922516\n",
      "Epoch 19 -- Batch 308/ 842, training loss 0.3428650498390198\n",
      "Epoch 19 -- Batch 309/ 842, training loss 0.34081995487213135\n",
      "Epoch 19 -- Batch 310/ 842, training loss 0.34736573696136475\n",
      "Epoch 19 -- Batch 311/ 842, training loss 0.33105120062828064\n",
      "Epoch 19 -- Batch 312/ 842, training loss 0.3373452425003052\n",
      "Epoch 19 -- Batch 313/ 842, training loss 0.32955455780029297\n",
      "Epoch 19 -- Batch 314/ 842, training loss 0.3369142711162567\n",
      "Epoch 19 -- Batch 315/ 842, training loss 0.3327990472316742\n",
      "Epoch 19 -- Batch 316/ 842, training loss 0.3263441324234009\n",
      "Epoch 19 -- Batch 317/ 842, training loss 0.3289031982421875\n",
      "Epoch 19 -- Batch 318/ 842, training loss 0.3332022726535797\n",
      "Epoch 19 -- Batch 319/ 842, training loss 0.33571764826774597\n",
      "Epoch 19 -- Batch 320/ 842, training loss 0.33028125762939453\n",
      "Epoch 19 -- Batch 321/ 842, training loss 0.32845041155815125\n",
      "Epoch 19 -- Batch 322/ 842, training loss 0.32577475905418396\n",
      "Epoch 19 -- Batch 323/ 842, training loss 0.325108140707016\n",
      "Epoch 19 -- Batch 324/ 842, training loss 0.3335936367511749\n",
      "Epoch 19 -- Batch 325/ 842, training loss 0.32473376393318176\n",
      "Epoch 19 -- Batch 326/ 842, training loss 0.3284081220626831\n",
      "Epoch 19 -- Batch 327/ 842, training loss 0.3363608717918396\n",
      "Epoch 19 -- Batch 328/ 842, training loss 0.31694963574409485\n",
      "Epoch 19 -- Batch 329/ 842, training loss 0.3592866361141205\n",
      "Epoch 19 -- Batch 330/ 842, training loss 0.3359890580177307\n",
      "Epoch 19 -- Batch 331/ 842, training loss 0.3407447934150696\n",
      "Epoch 19 -- Batch 332/ 842, training loss 0.32798847556114197\n",
      "Epoch 19 -- Batch 333/ 842, training loss 0.3317221999168396\n",
      "Epoch 19 -- Batch 334/ 842, training loss 0.33803197741508484\n",
      "Epoch 19 -- Batch 335/ 842, training loss 0.32435646653175354\n",
      "Epoch 19 -- Batch 336/ 842, training loss 0.34327638149261475\n",
      "Epoch 19 -- Batch 337/ 842, training loss 0.3434249460697174\n",
      "Epoch 19 -- Batch 338/ 842, training loss 0.34185704588890076\n",
      "Epoch 19 -- Batch 339/ 842, training loss 0.33179742097854614\n",
      "Epoch 19 -- Batch 340/ 842, training loss 0.34851571917533875\n",
      "Epoch 19 -- Batch 341/ 842, training loss 0.33270248770713806\n",
      "Epoch 19 -- Batch 342/ 842, training loss 0.34100863337516785\n",
      "Epoch 19 -- Batch 343/ 842, training loss 0.3274463713169098\n",
      "Epoch 19 -- Batch 344/ 842, training loss 0.32899802923202515\n",
      "Epoch 19 -- Batch 345/ 842, training loss 0.32411837577819824\n",
      "Epoch 19 -- Batch 346/ 842, training loss 0.3389011025428772\n",
      "Epoch 19 -- Batch 347/ 842, training loss 0.33889344334602356\n",
      "Epoch 19 -- Batch 348/ 842, training loss 0.334212064743042\n",
      "Epoch 19 -- Batch 349/ 842, training loss 0.3393242061138153\n",
      "Epoch 19 -- Batch 350/ 842, training loss 0.3361985385417938\n",
      "Epoch 19 -- Batch 351/ 842, training loss 0.3342585861682892\n",
      "Epoch 19 -- Batch 352/ 842, training loss 0.3253202438354492\n",
      "Epoch 19 -- Batch 353/ 842, training loss 0.33530646562576294\n",
      "Epoch 19 -- Batch 354/ 842, training loss 0.33805039525032043\n",
      "Epoch 19 -- Batch 355/ 842, training loss 0.33728349208831787\n",
      "Epoch 19 -- Batch 356/ 842, training loss 0.33576861023902893\n",
      "Epoch 19 -- Batch 357/ 842, training loss 0.3271411657333374\n",
      "Epoch 19 -- Batch 358/ 842, training loss 0.32347139716148376\n",
      "Epoch 19 -- Batch 359/ 842, training loss 0.3262642025947571\n",
      "Epoch 19 -- Batch 360/ 842, training loss 0.3478993773460388\n",
      "Epoch 19 -- Batch 361/ 842, training loss 0.3279642164707184\n",
      "Epoch 19 -- Batch 362/ 842, training loss 0.34395161271095276\n",
      "Epoch 19 -- Batch 363/ 842, training loss 0.32585835456848145\n",
      "Epoch 19 -- Batch 364/ 842, training loss 0.32830294966697693\n",
      "Epoch 19 -- Batch 365/ 842, training loss 0.3222229480743408\n",
      "Epoch 19 -- Batch 366/ 842, training loss 0.33745336532592773\n",
      "Epoch 19 -- Batch 367/ 842, training loss 0.3258177936077118\n",
      "Epoch 19 -- Batch 368/ 842, training loss 0.31476137042045593\n",
      "Epoch 19 -- Batch 369/ 842, training loss 0.3276461362838745\n",
      "Epoch 19 -- Batch 370/ 842, training loss 0.34339627623558044\n",
      "Epoch 19 -- Batch 371/ 842, training loss 0.33012622594833374\n",
      "Epoch 19 -- Batch 372/ 842, training loss 0.3162315785884857\n",
      "Epoch 19 -- Batch 373/ 842, training loss 0.324625700712204\n",
      "Epoch 19 -- Batch 374/ 842, training loss 0.3285076916217804\n",
      "Epoch 19 -- Batch 375/ 842, training loss 0.3354702293872833\n",
      "Epoch 19 -- Batch 376/ 842, training loss 0.33853721618652344\n",
      "Epoch 19 -- Batch 377/ 842, training loss 0.330009788274765\n",
      "Epoch 19 -- Batch 378/ 842, training loss 0.33204004168510437\n",
      "Epoch 19 -- Batch 379/ 842, training loss 0.3417019546031952\n",
      "Epoch 19 -- Batch 380/ 842, training loss 0.31405431032180786\n",
      "Epoch 19 -- Batch 381/ 842, training loss 0.3271494507789612\n",
      "Epoch 19 -- Batch 382/ 842, training loss 0.32976990938186646\n",
      "Epoch 19 -- Batch 383/ 842, training loss 0.3403593897819519\n",
      "Epoch 19 -- Batch 384/ 842, training loss 0.3365359902381897\n",
      "Epoch 19 -- Batch 385/ 842, training loss 0.33289772272109985\n",
      "Epoch 19 -- Batch 386/ 842, training loss 0.33397042751312256\n",
      "Epoch 19 -- Batch 387/ 842, training loss 0.3320778012275696\n",
      "Epoch 19 -- Batch 388/ 842, training loss 0.33153218030929565\n",
      "Epoch 19 -- Batch 389/ 842, training loss 0.3346080482006073\n",
      "Epoch 19 -- Batch 390/ 842, training loss 0.34584906697273254\n",
      "Epoch 19 -- Batch 391/ 842, training loss 0.32754912972450256\n",
      "Epoch 19 -- Batch 392/ 842, training loss 0.3323558270931244\n",
      "Epoch 19 -- Batch 393/ 842, training loss 0.33386898040771484\n",
      "Epoch 19 -- Batch 394/ 842, training loss 0.3244403302669525\n",
      "Epoch 19 -- Batch 395/ 842, training loss 0.3391883373260498\n",
      "Epoch 19 -- Batch 396/ 842, training loss 0.33710166811943054\n",
      "Epoch 19 -- Batch 397/ 842, training loss 0.3309585154056549\n",
      "Epoch 19 -- Batch 398/ 842, training loss 0.32767805457115173\n",
      "Epoch 19 -- Batch 399/ 842, training loss 0.34519046545028687\n",
      "Epoch 19 -- Batch 400/ 842, training loss 0.33871155977249146\n",
      "Epoch 19 -- Batch 401/ 842, training loss 0.33205437660217285\n",
      "Epoch 19 -- Batch 402/ 842, training loss 0.33749303221702576\n",
      "Epoch 19 -- Batch 403/ 842, training loss 0.32706883549690247\n",
      "Epoch 19 -- Batch 404/ 842, training loss 0.3381061255931854\n",
      "Epoch 19 -- Batch 405/ 842, training loss 0.33474841713905334\n",
      "Epoch 19 -- Batch 406/ 842, training loss 0.34146666526794434\n",
      "Epoch 19 -- Batch 407/ 842, training loss 0.3268478214740753\n",
      "Epoch 19 -- Batch 408/ 842, training loss 0.32837149500846863\n",
      "Epoch 19 -- Batch 409/ 842, training loss 0.3273337483406067\n",
      "Epoch 19 -- Batch 410/ 842, training loss 0.3316027522087097\n",
      "Epoch 19 -- Batch 411/ 842, training loss 0.34585872292518616\n",
      "Epoch 19 -- Batch 412/ 842, training loss 0.32958513498306274\n",
      "Epoch 19 -- Batch 413/ 842, training loss 0.3283037543296814\n",
      "Epoch 19 -- Batch 414/ 842, training loss 0.3203895092010498\n",
      "Epoch 19 -- Batch 415/ 842, training loss 0.33039212226867676\n",
      "Epoch 19 -- Batch 416/ 842, training loss 0.3347214460372925\n",
      "Epoch 19 -- Batch 417/ 842, training loss 0.31575074791908264\n",
      "Epoch 19 -- Batch 418/ 842, training loss 0.3350411057472229\n",
      "Epoch 19 -- Batch 419/ 842, training loss 0.32404661178588867\n",
      "Epoch 19 -- Batch 420/ 842, training loss 0.3223295211791992\n",
      "Epoch 19 -- Batch 421/ 842, training loss 0.3356093466281891\n",
      "Epoch 19 -- Batch 422/ 842, training loss 0.33228766918182373\n",
      "Epoch 19 -- Batch 423/ 842, training loss 0.3242231011390686\n",
      "Epoch 19 -- Batch 424/ 842, training loss 0.3277917802333832\n",
      "Epoch 19 -- Batch 425/ 842, training loss 0.32495391368865967\n",
      "Epoch 19 -- Batch 426/ 842, training loss 0.33241137862205505\n",
      "Epoch 19 -- Batch 427/ 842, training loss 0.3292122483253479\n",
      "Epoch 19 -- Batch 428/ 842, training loss 0.3399190604686737\n",
      "Epoch 19 -- Batch 429/ 842, training loss 0.3322998583316803\n",
      "Epoch 19 -- Batch 430/ 842, training loss 0.32980823516845703\n",
      "Epoch 19 -- Batch 431/ 842, training loss 0.3283676505088806\n",
      "Epoch 19 -- Batch 432/ 842, training loss 0.3252740800380707\n",
      "Epoch 19 -- Batch 433/ 842, training loss 0.3340282440185547\n",
      "Epoch 19 -- Batch 434/ 842, training loss 0.33575868606567383\n",
      "Epoch 19 -- Batch 435/ 842, training loss 0.33143532276153564\n",
      "Epoch 19 -- Batch 436/ 842, training loss 0.33127135038375854\n",
      "Epoch 19 -- Batch 437/ 842, training loss 0.3376637101173401\n",
      "Epoch 19 -- Batch 438/ 842, training loss 0.32370322942733765\n",
      "Epoch 19 -- Batch 439/ 842, training loss 0.3321585953235626\n",
      "Epoch 19 -- Batch 440/ 842, training loss 0.34012550115585327\n",
      "Epoch 19 -- Batch 441/ 842, training loss 0.32453957200050354\n",
      "Epoch 19 -- Batch 442/ 842, training loss 0.3311622440814972\n",
      "Epoch 19 -- Batch 443/ 842, training loss 0.3444039523601532\n",
      "Epoch 19 -- Batch 444/ 842, training loss 0.3257978856563568\n",
      "Epoch 19 -- Batch 445/ 842, training loss 0.32520756125450134\n",
      "Epoch 19 -- Batch 446/ 842, training loss 0.34110480546951294\n",
      "Epoch 19 -- Batch 447/ 842, training loss 0.3411591053009033\n",
      "Epoch 19 -- Batch 448/ 842, training loss 0.3359638750553131\n",
      "Epoch 19 -- Batch 449/ 842, training loss 0.34419870376586914\n",
      "Epoch 19 -- Batch 450/ 842, training loss 0.31641918420791626\n",
      "Epoch 19 -- Batch 451/ 842, training loss 0.3317028880119324\n",
      "Epoch 19 -- Batch 452/ 842, training loss 0.3264668583869934\n",
      "Epoch 19 -- Batch 453/ 842, training loss 0.3240405023097992\n",
      "Epoch 19 -- Batch 454/ 842, training loss 0.3372878134250641\n",
      "Epoch 19 -- Batch 455/ 842, training loss 0.33154308795928955\n",
      "Epoch 19 -- Batch 456/ 842, training loss 0.3384789228439331\n",
      "Epoch 19 -- Batch 457/ 842, training loss 0.3328779637813568\n",
      "Epoch 19 -- Batch 458/ 842, training loss 0.3258984088897705\n",
      "Epoch 19 -- Batch 459/ 842, training loss 0.32584789395332336\n",
      "Epoch 19 -- Batch 460/ 842, training loss 0.3271552324295044\n",
      "Epoch 19 -- Batch 461/ 842, training loss 0.33571621775627136\n",
      "Epoch 19 -- Batch 462/ 842, training loss 0.3218950927257538\n",
      "Epoch 19 -- Batch 463/ 842, training loss 0.33363789319992065\n",
      "Epoch 19 -- Batch 464/ 842, training loss 0.3449234664440155\n",
      "Epoch 19 -- Batch 465/ 842, training loss 0.33135995268821716\n",
      "Epoch 19 -- Batch 466/ 842, training loss 0.3390200734138489\n",
      "Epoch 19 -- Batch 467/ 842, training loss 0.3372170031070709\n",
      "Epoch 19 -- Batch 468/ 842, training loss 0.3257504105567932\n",
      "Epoch 19 -- Batch 469/ 842, training loss 0.334400475025177\n",
      "Epoch 19 -- Batch 470/ 842, training loss 0.3246769905090332\n",
      "Epoch 19 -- Batch 471/ 842, training loss 0.3217458724975586\n",
      "Epoch 19 -- Batch 472/ 842, training loss 0.3427935540676117\n",
      "Epoch 19 -- Batch 473/ 842, training loss 0.32267487049102783\n",
      "Epoch 19 -- Batch 474/ 842, training loss 0.3253016173839569\n",
      "Epoch 19 -- Batch 475/ 842, training loss 0.334920734167099\n",
      "Epoch 19 -- Batch 476/ 842, training loss 0.32981470227241516\n",
      "Epoch 19 -- Batch 477/ 842, training loss 0.3294191062450409\n",
      "Epoch 19 -- Batch 478/ 842, training loss 0.3273322284221649\n",
      "Epoch 19 -- Batch 479/ 842, training loss 0.32925692200660706\n",
      "Epoch 19 -- Batch 480/ 842, training loss 0.3355420231819153\n",
      "Epoch 19 -- Batch 481/ 842, training loss 0.3416312634944916\n",
      "Epoch 19 -- Batch 482/ 842, training loss 0.32601016759872437\n",
      "Epoch 19 -- Batch 483/ 842, training loss 0.345558762550354\n",
      "Epoch 19 -- Batch 484/ 842, training loss 0.34034034609794617\n",
      "Epoch 19 -- Batch 485/ 842, training loss 0.33555203676223755\n",
      "Epoch 19 -- Batch 486/ 842, training loss 0.33762046694755554\n",
      "Epoch 19 -- Batch 487/ 842, training loss 0.3398968279361725\n",
      "Epoch 19 -- Batch 488/ 842, training loss 0.31916534900665283\n",
      "Epoch 19 -- Batch 489/ 842, training loss 0.33747240900993347\n",
      "Epoch 19 -- Batch 490/ 842, training loss 0.33433347940444946\n",
      "Epoch 19 -- Batch 491/ 842, training loss 0.33662471175193787\n",
      "Epoch 19 -- Batch 492/ 842, training loss 0.3371845781803131\n",
      "Epoch 19 -- Batch 493/ 842, training loss 0.3276905417442322\n",
      "Epoch 19 -- Batch 494/ 842, training loss 0.3230542540550232\n",
      "Epoch 19 -- Batch 495/ 842, training loss 0.32705119252204895\n",
      "Epoch 19 -- Batch 496/ 842, training loss 0.3218790888786316\n",
      "Epoch 19 -- Batch 497/ 842, training loss 0.33109062910079956\n",
      "Epoch 19 -- Batch 498/ 842, training loss 0.3279968500137329\n",
      "Epoch 19 -- Batch 499/ 842, training loss 0.32881301641464233\n",
      "Epoch 19 -- Batch 500/ 842, training loss 0.3435014486312866\n",
      "Epoch 19 -- Batch 501/ 842, training loss 0.3211756944656372\n",
      "Epoch 19 -- Batch 502/ 842, training loss 0.33869850635528564\n",
      "Epoch 19 -- Batch 503/ 842, training loss 0.32462868094444275\n",
      "Epoch 19 -- Batch 504/ 842, training loss 0.3463248014450073\n",
      "Epoch 19 -- Batch 505/ 842, training loss 0.3246442377567291\n",
      "Epoch 19 -- Batch 506/ 842, training loss 0.3296087980270386\n",
      "Epoch 19 -- Batch 507/ 842, training loss 0.34138360619544983\n",
      "Epoch 19 -- Batch 508/ 842, training loss 0.3420255184173584\n",
      "Epoch 19 -- Batch 509/ 842, training loss 0.33261579275131226\n",
      "Epoch 19 -- Batch 510/ 842, training loss 0.33225107192993164\n",
      "Epoch 19 -- Batch 511/ 842, training loss 0.34518909454345703\n",
      "Epoch 19 -- Batch 512/ 842, training loss 0.32767659425735474\n",
      "Epoch 19 -- Batch 513/ 842, training loss 0.32743579149246216\n",
      "Epoch 19 -- Batch 514/ 842, training loss 0.3319244682788849\n",
      "Epoch 19 -- Batch 515/ 842, training loss 0.33309972286224365\n",
      "Epoch 19 -- Batch 516/ 842, training loss 0.3363281190395355\n",
      "Epoch 19 -- Batch 517/ 842, training loss 0.33861204981803894\n",
      "Epoch 19 -- Batch 518/ 842, training loss 0.331860214471817\n",
      "Epoch 19 -- Batch 519/ 842, training loss 0.33080241084098816\n",
      "Epoch 19 -- Batch 520/ 842, training loss 0.3323601484298706\n",
      "Epoch 19 -- Batch 521/ 842, training loss 0.3384164273738861\n",
      "Epoch 19 -- Batch 522/ 842, training loss 0.3437694013118744\n",
      "Epoch 19 -- Batch 523/ 842, training loss 0.3209311366081238\n",
      "Epoch 19 -- Batch 524/ 842, training loss 0.3314273953437805\n",
      "Epoch 19 -- Batch 525/ 842, training loss 0.33422887325286865\n",
      "Epoch 19 -- Batch 526/ 842, training loss 0.3291498124599457\n",
      "Epoch 19 -- Batch 527/ 842, training loss 0.33687731623649597\n",
      "Epoch 19 -- Batch 528/ 842, training loss 0.34763962030410767\n",
      "Epoch 19 -- Batch 529/ 842, training loss 0.3422873914241791\n",
      "Epoch 19 -- Batch 530/ 842, training loss 0.3392696678638458\n",
      "Epoch 19 -- Batch 531/ 842, training loss 0.33319467306137085\n",
      "Epoch 19 -- Batch 532/ 842, training loss 0.31920063495635986\n",
      "Epoch 19 -- Batch 533/ 842, training loss 0.32023921608924866\n",
      "Epoch 19 -- Batch 534/ 842, training loss 0.3359757959842682\n",
      "Epoch 19 -- Batch 535/ 842, training loss 0.338560551404953\n",
      "Epoch 19 -- Batch 536/ 842, training loss 0.3378002643585205\n",
      "Epoch 19 -- Batch 537/ 842, training loss 0.3281541168689728\n",
      "Epoch 19 -- Batch 538/ 842, training loss 0.32369694113731384\n",
      "Epoch 19 -- Batch 539/ 842, training loss 0.33769944310188293\n",
      "Epoch 19 -- Batch 540/ 842, training loss 0.32992178201675415\n",
      "Epoch 19 -- Batch 541/ 842, training loss 0.3438604772090912\n",
      "Epoch 19 -- Batch 542/ 842, training loss 0.3286517262458801\n",
      "Epoch 19 -- Batch 543/ 842, training loss 0.3356753885746002\n",
      "Epoch 19 -- Batch 544/ 842, training loss 0.3377569019794464\n",
      "Epoch 19 -- Batch 545/ 842, training loss 0.3328225314617157\n",
      "Epoch 19 -- Batch 546/ 842, training loss 0.32840496301651\n",
      "Epoch 19 -- Batch 547/ 842, training loss 0.33887559175491333\n",
      "Epoch 19 -- Batch 548/ 842, training loss 0.34595781564712524\n",
      "Epoch 19 -- Batch 549/ 842, training loss 0.3339467942714691\n",
      "Epoch 19 -- Batch 550/ 842, training loss 0.3373105227947235\n",
      "Epoch 19 -- Batch 551/ 842, training loss 0.3356620669364929\n",
      "Epoch 19 -- Batch 552/ 842, training loss 0.31837141513824463\n",
      "Epoch 19 -- Batch 553/ 842, training loss 0.3470591604709625\n",
      "Epoch 19 -- Batch 554/ 842, training loss 0.32555291056632996\n",
      "Epoch 19 -- Batch 555/ 842, training loss 0.334338903427124\n",
      "Epoch 19 -- Batch 556/ 842, training loss 0.33328777551651\n",
      "Epoch 19 -- Batch 557/ 842, training loss 0.34995537996292114\n",
      "Epoch 19 -- Batch 558/ 842, training loss 0.32651224732398987\n",
      "Epoch 19 -- Batch 559/ 842, training loss 0.33312171697616577\n",
      "Epoch 19 -- Batch 560/ 842, training loss 0.3312453329563141\n",
      "Epoch 19 -- Batch 561/ 842, training loss 0.3372064232826233\n",
      "Epoch 19 -- Batch 562/ 842, training loss 0.330302894115448\n",
      "Epoch 19 -- Batch 563/ 842, training loss 0.3354566693305969\n",
      "Epoch 19 -- Batch 564/ 842, training loss 0.3288559019565582\n",
      "Epoch 19 -- Batch 565/ 842, training loss 0.3609919846057892\n",
      "Epoch 19 -- Batch 566/ 842, training loss 0.3326520621776581\n",
      "Epoch 19 -- Batch 567/ 842, training loss 0.3145868182182312\n",
      "Epoch 19 -- Batch 568/ 842, training loss 0.3303133547306061\n",
      "Epoch 19 -- Batch 569/ 842, training loss 0.3307139277458191\n",
      "Epoch 19 -- Batch 570/ 842, training loss 0.32701507210731506\n",
      "Epoch 19 -- Batch 571/ 842, training loss 0.32878294587135315\n",
      "Epoch 19 -- Batch 572/ 842, training loss 0.32068008184432983\n",
      "Epoch 19 -- Batch 573/ 842, training loss 0.33354437351226807\n",
      "Epoch 19 -- Batch 574/ 842, training loss 0.3392074704170227\n",
      "Epoch 19 -- Batch 575/ 842, training loss 0.32717111706733704\n",
      "Epoch 19 -- Batch 576/ 842, training loss 0.3340654969215393\n",
      "Epoch 19 -- Batch 577/ 842, training loss 0.33538001775741577\n",
      "Epoch 19 -- Batch 578/ 842, training loss 0.3283739984035492\n",
      "Epoch 19 -- Batch 579/ 842, training loss 0.3335180878639221\n",
      "Epoch 19 -- Batch 580/ 842, training loss 0.3365902900695801\n",
      "Epoch 19 -- Batch 581/ 842, training loss 0.326043963432312\n",
      "Epoch 19 -- Batch 582/ 842, training loss 0.33598753809928894\n",
      "Epoch 19 -- Batch 583/ 842, training loss 0.3425876200199127\n",
      "Epoch 19 -- Batch 584/ 842, training loss 0.33619534969329834\n",
      "Epoch 19 -- Batch 585/ 842, training loss 0.3398513197898865\n",
      "Epoch 19 -- Batch 586/ 842, training loss 0.3428255319595337\n",
      "Epoch 19 -- Batch 587/ 842, training loss 0.33200976252555847\n",
      "Epoch 19 -- Batch 588/ 842, training loss 0.3222682774066925\n",
      "Epoch 19 -- Batch 589/ 842, training loss 0.32765549421310425\n",
      "Epoch 19 -- Batch 590/ 842, training loss 0.33814218640327454\n",
      "Epoch 19 -- Batch 591/ 842, training loss 0.34704163670539856\n",
      "Epoch 19 -- Batch 592/ 842, training loss 0.324952095746994\n",
      "Epoch 19 -- Batch 593/ 842, training loss 0.3360879123210907\n",
      "Epoch 19 -- Batch 594/ 842, training loss 0.3344907760620117\n",
      "Epoch 19 -- Batch 595/ 842, training loss 0.3447705805301666\n",
      "Epoch 19 -- Batch 596/ 842, training loss 0.33407357335090637\n",
      "Epoch 19 -- Batch 597/ 842, training loss 0.3335379362106323\n",
      "Epoch 19 -- Batch 598/ 842, training loss 0.3325124680995941\n",
      "Epoch 19 -- Batch 599/ 842, training loss 0.3384612500667572\n",
      "Epoch 19 -- Batch 600/ 842, training loss 0.3358924090862274\n",
      "Epoch 19 -- Batch 601/ 842, training loss 0.33476582169532776\n",
      "Epoch 19 -- Batch 602/ 842, training loss 0.3217897117137909\n",
      "Epoch 19 -- Batch 603/ 842, training loss 0.33236026763916016\n",
      "Epoch 19 -- Batch 604/ 842, training loss 0.3460201621055603\n",
      "Epoch 19 -- Batch 605/ 842, training loss 0.33125314116477966\n",
      "Epoch 19 -- Batch 606/ 842, training loss 0.3458649516105652\n",
      "Epoch 19 -- Batch 607/ 842, training loss 0.3360919952392578\n",
      "Epoch 19 -- Batch 608/ 842, training loss 0.333035945892334\n",
      "Epoch 19 -- Batch 609/ 842, training loss 0.34119030833244324\n",
      "Epoch 19 -- Batch 610/ 842, training loss 0.34137991070747375\n",
      "Epoch 19 -- Batch 611/ 842, training loss 0.3381827473640442\n",
      "Epoch 19 -- Batch 612/ 842, training loss 0.34381482005119324\n",
      "Epoch 19 -- Batch 613/ 842, training loss 0.3383826017379761\n",
      "Epoch 19 -- Batch 614/ 842, training loss 0.3293461203575134\n",
      "Epoch 19 -- Batch 615/ 842, training loss 0.32830509543418884\n",
      "Epoch 19 -- Batch 616/ 842, training loss 0.3349605202674866\n",
      "Epoch 19 -- Batch 617/ 842, training loss 0.3379763960838318\n",
      "Epoch 19 -- Batch 618/ 842, training loss 0.33012211322784424\n",
      "Epoch 19 -- Batch 619/ 842, training loss 0.33447355031967163\n",
      "Epoch 19 -- Batch 620/ 842, training loss 0.3429732024669647\n",
      "Epoch 19 -- Batch 621/ 842, training loss 0.341698557138443\n",
      "Epoch 19 -- Batch 622/ 842, training loss 0.3381856083869934\n",
      "Epoch 19 -- Batch 623/ 842, training loss 0.32531052827835083\n",
      "Epoch 19 -- Batch 624/ 842, training loss 0.3420044481754303\n",
      "Epoch 19 -- Batch 625/ 842, training loss 0.3330632150173187\n",
      "Epoch 19 -- Batch 626/ 842, training loss 0.3273344933986664\n",
      "Epoch 19 -- Batch 627/ 842, training loss 0.3380880653858185\n",
      "Epoch 19 -- Batch 628/ 842, training loss 0.32750949263572693\n",
      "Epoch 19 -- Batch 629/ 842, training loss 0.33624622225761414\n",
      "Epoch 19 -- Batch 630/ 842, training loss 0.3337074816226959\n",
      "Epoch 19 -- Batch 631/ 842, training loss 0.33195409178733826\n",
      "Epoch 19 -- Batch 632/ 842, training loss 0.33235955238342285\n",
      "Epoch 19 -- Batch 633/ 842, training loss 0.32542017102241516\n",
      "Epoch 19 -- Batch 634/ 842, training loss 0.3497481048107147\n",
      "Epoch 19 -- Batch 635/ 842, training loss 0.3333538770675659\n",
      "Epoch 19 -- Batch 636/ 842, training loss 0.32915282249450684\n",
      "Epoch 19 -- Batch 637/ 842, training loss 0.3432656228542328\n",
      "Epoch 19 -- Batch 638/ 842, training loss 0.3344275653362274\n",
      "Epoch 19 -- Batch 639/ 842, training loss 0.34324032068252563\n",
      "Epoch 19 -- Batch 640/ 842, training loss 0.3212287724018097\n",
      "Epoch 19 -- Batch 641/ 842, training loss 0.33019351959228516\n",
      "Epoch 19 -- Batch 642/ 842, training loss 0.3303614556789398\n",
      "Epoch 19 -- Batch 643/ 842, training loss 0.32429757714271545\n",
      "Epoch 19 -- Batch 644/ 842, training loss 0.34061717987060547\n",
      "Epoch 19 -- Batch 645/ 842, training loss 0.3316267430782318\n",
      "Epoch 19 -- Batch 646/ 842, training loss 0.33574607968330383\n",
      "Epoch 19 -- Batch 647/ 842, training loss 0.32999369502067566\n",
      "Epoch 19 -- Batch 648/ 842, training loss 0.32639673352241516\n",
      "Epoch 19 -- Batch 649/ 842, training loss 0.33713600039482117\n",
      "Epoch 19 -- Batch 650/ 842, training loss 0.3379996716976166\n",
      "Epoch 19 -- Batch 651/ 842, training loss 0.3354208171367645\n",
      "Epoch 19 -- Batch 652/ 842, training loss 0.33569955825805664\n",
      "Epoch 19 -- Batch 653/ 842, training loss 0.33618077635765076\n",
      "Epoch 19 -- Batch 654/ 842, training loss 0.3324619233608246\n",
      "Epoch 19 -- Batch 655/ 842, training loss 0.3334750533103943\n",
      "Epoch 19 -- Batch 656/ 842, training loss 0.3368324935436249\n",
      "Epoch 19 -- Batch 657/ 842, training loss 0.31984320282936096\n",
      "Epoch 19 -- Batch 658/ 842, training loss 0.3283722698688507\n",
      "Epoch 19 -- Batch 659/ 842, training loss 0.3360828161239624\n",
      "Epoch 19 -- Batch 660/ 842, training loss 0.32175523042678833\n",
      "Epoch 19 -- Batch 661/ 842, training loss 0.32984402775764465\n",
      "Epoch 19 -- Batch 662/ 842, training loss 0.3219817280769348\n",
      "Epoch 19 -- Batch 663/ 842, training loss 0.33627939224243164\n",
      "Epoch 19 -- Batch 664/ 842, training loss 0.33369630575180054\n",
      "Epoch 19 -- Batch 665/ 842, training loss 0.32578617334365845\n",
      "Epoch 19 -- Batch 666/ 842, training loss 0.3299318253993988\n",
      "Epoch 19 -- Batch 667/ 842, training loss 0.33529171347618103\n",
      "Epoch 19 -- Batch 668/ 842, training loss 0.33494943380355835\n",
      "Epoch 19 -- Batch 669/ 842, training loss 0.34345853328704834\n",
      "Epoch 19 -- Batch 670/ 842, training loss 0.3366106152534485\n",
      "Epoch 19 -- Batch 671/ 842, training loss 0.34699732065200806\n",
      "Epoch 19 -- Batch 672/ 842, training loss 0.3331570327281952\n",
      "Epoch 19 -- Batch 673/ 842, training loss 0.3154870867729187\n",
      "Epoch 19 -- Batch 674/ 842, training loss 0.32307565212249756\n",
      "Epoch 19 -- Batch 675/ 842, training loss 0.32378649711608887\n",
      "Epoch 19 -- Batch 676/ 842, training loss 0.3269523084163666\n",
      "Epoch 19 -- Batch 677/ 842, training loss 0.31955403089523315\n",
      "Epoch 19 -- Batch 678/ 842, training loss 0.3293493092060089\n",
      "Epoch 19 -- Batch 679/ 842, training loss 0.3381575047969818\n",
      "Epoch 19 -- Batch 680/ 842, training loss 0.3449885845184326\n",
      "Epoch 19 -- Batch 681/ 842, training loss 0.33876219391822815\n",
      "Epoch 19 -- Batch 682/ 842, training loss 0.33488038182258606\n",
      "Epoch 19 -- Batch 683/ 842, training loss 0.3464931547641754\n",
      "Epoch 19 -- Batch 684/ 842, training loss 0.332047700881958\n",
      "Epoch 19 -- Batch 685/ 842, training loss 0.33092978596687317\n",
      "Epoch 19 -- Batch 686/ 842, training loss 0.331594318151474\n",
      "Epoch 19 -- Batch 687/ 842, training loss 0.33383142948150635\n",
      "Epoch 19 -- Batch 688/ 842, training loss 0.3282206058502197\n",
      "Epoch 19 -- Batch 689/ 842, training loss 0.3377643823623657\n",
      "Epoch 19 -- Batch 690/ 842, training loss 0.3407532274723053\n",
      "Epoch 19 -- Batch 691/ 842, training loss 0.3381827473640442\n",
      "Epoch 19 -- Batch 692/ 842, training loss 0.33304378390312195\n",
      "Epoch 19 -- Batch 693/ 842, training loss 0.3173469305038452\n",
      "Epoch 19 -- Batch 694/ 842, training loss 0.32805195450782776\n",
      "Epoch 19 -- Batch 695/ 842, training loss 0.3356000781059265\n",
      "Epoch 19 -- Batch 696/ 842, training loss 0.3413234055042267\n",
      "Epoch 19 -- Batch 697/ 842, training loss 0.3369556963443756\n",
      "Epoch 19 -- Batch 698/ 842, training loss 0.33235451579093933\n",
      "Epoch 19 -- Batch 699/ 842, training loss 0.32944414019584656\n",
      "Epoch 19 -- Batch 700/ 842, training loss 0.33546850085258484\n",
      "Epoch 19 -- Batch 701/ 842, training loss 0.32911187410354614\n",
      "Epoch 19 -- Batch 702/ 842, training loss 0.32875633239746094\n",
      "Epoch 19 -- Batch 703/ 842, training loss 0.33847305178642273\n",
      "Epoch 19 -- Batch 704/ 842, training loss 0.3407747149467468\n",
      "Epoch 19 -- Batch 705/ 842, training loss 0.33522889018058777\n",
      "Epoch 19 -- Batch 706/ 842, training loss 0.3344675302505493\n",
      "Epoch 19 -- Batch 707/ 842, training loss 0.3330267667770386\n",
      "Epoch 19 -- Batch 708/ 842, training loss 0.3333151340484619\n",
      "Epoch 19 -- Batch 709/ 842, training loss 0.3427947461605072\n",
      "Epoch 19 -- Batch 710/ 842, training loss 0.32961374521255493\n",
      "Epoch 19 -- Batch 711/ 842, training loss 0.34043774008750916\n",
      "Epoch 19 -- Batch 712/ 842, training loss 0.33919432759284973\n",
      "Epoch 19 -- Batch 713/ 842, training loss 0.3157839775085449\n",
      "Epoch 19 -- Batch 714/ 842, training loss 0.34826862812042236\n",
      "Epoch 19 -- Batch 715/ 842, training loss 0.3365682065486908\n",
      "Epoch 19 -- Batch 716/ 842, training loss 0.3341272175312042\n",
      "Epoch 19 -- Batch 717/ 842, training loss 0.34329667687416077\n",
      "Epoch 19 -- Batch 718/ 842, training loss 0.3432524502277374\n",
      "Epoch 19 -- Batch 719/ 842, training loss 0.33214810490608215\n",
      "Epoch 19 -- Batch 720/ 842, training loss 0.3260650336742401\n",
      "Epoch 19 -- Batch 721/ 842, training loss 0.32791006565093994\n",
      "Epoch 19 -- Batch 722/ 842, training loss 0.33390867710113525\n",
      "Epoch 19 -- Batch 723/ 842, training loss 0.3509882390499115\n",
      "Epoch 19 -- Batch 724/ 842, training loss 0.3348953127861023\n",
      "Epoch 19 -- Batch 725/ 842, training loss 0.32790976762771606\n",
      "Epoch 19 -- Batch 726/ 842, training loss 0.33411023020744324\n",
      "Epoch 19 -- Batch 727/ 842, training loss 0.34096601605415344\n",
      "Epoch 19 -- Batch 728/ 842, training loss 0.3335445523262024\n",
      "Epoch 19 -- Batch 729/ 842, training loss 0.3356887698173523\n",
      "Epoch 19 -- Batch 730/ 842, training loss 0.32749420404434204\n",
      "Epoch 19 -- Batch 731/ 842, training loss 0.333529531955719\n",
      "Epoch 19 -- Batch 732/ 842, training loss 0.3270001709461212\n",
      "Epoch 19 -- Batch 733/ 842, training loss 0.33575770258903503\n",
      "Epoch 19 -- Batch 734/ 842, training loss 0.33798813819885254\n",
      "Epoch 19 -- Batch 735/ 842, training loss 0.3278810679912567\n",
      "Epoch 19 -- Batch 736/ 842, training loss 0.3347199261188507\n",
      "Epoch 19 -- Batch 737/ 842, training loss 0.33471065759658813\n",
      "Epoch 19 -- Batch 738/ 842, training loss 0.3272397518157959\n",
      "Epoch 19 -- Batch 739/ 842, training loss 0.3426506221294403\n",
      "Epoch 19 -- Batch 740/ 842, training loss 0.3473338484764099\n",
      "Epoch 19 -- Batch 741/ 842, training loss 0.3389592468738556\n",
      "Epoch 19 -- Batch 742/ 842, training loss 0.35150420665740967\n",
      "Epoch 19 -- Batch 743/ 842, training loss 0.3307080864906311\n",
      "Epoch 19 -- Batch 744/ 842, training loss 0.3345690071582794\n",
      "Epoch 19 -- Batch 745/ 842, training loss 0.33750638365745544\n",
      "Epoch 19 -- Batch 746/ 842, training loss 0.34220683574676514\n",
      "Epoch 19 -- Batch 747/ 842, training loss 0.316663533449173\n",
      "Epoch 19 -- Batch 748/ 842, training loss 0.32699406147003174\n",
      "Epoch 19 -- Batch 749/ 842, training loss 0.3431430757045746\n",
      "Epoch 19 -- Batch 750/ 842, training loss 0.3374161720275879\n",
      "Epoch 19 -- Batch 751/ 842, training loss 0.33440348505973816\n",
      "Epoch 19 -- Batch 752/ 842, training loss 0.3391888737678528\n",
      "Epoch 19 -- Batch 753/ 842, training loss 0.3436819911003113\n",
      "Epoch 19 -- Batch 754/ 842, training loss 0.3393586277961731\n",
      "Epoch 19 -- Batch 755/ 842, training loss 0.34405049681663513\n",
      "Epoch 19 -- Batch 756/ 842, training loss 0.3301694095134735\n",
      "Epoch 19 -- Batch 757/ 842, training loss 0.3433166742324829\n",
      "Epoch 19 -- Batch 758/ 842, training loss 0.33976316452026367\n",
      "Epoch 19 -- Batch 759/ 842, training loss 0.32331833243370056\n",
      "Epoch 19 -- Batch 760/ 842, training loss 0.3488955497741699\n",
      "Epoch 19 -- Batch 761/ 842, training loss 0.3312908411026001\n",
      "Epoch 19 -- Batch 762/ 842, training loss 0.33467042446136475\n",
      "Epoch 19 -- Batch 763/ 842, training loss 0.33420076966285706\n",
      "Epoch 19 -- Batch 764/ 842, training loss 0.33460792899131775\n",
      "Epoch 19 -- Batch 765/ 842, training loss 0.34710368514060974\n",
      "Epoch 19 -- Batch 766/ 842, training loss 0.32367733120918274\n",
      "Epoch 19 -- Batch 767/ 842, training loss 0.3451872766017914\n",
      "Epoch 19 -- Batch 768/ 842, training loss 0.33155736327171326\n",
      "Epoch 19 -- Batch 769/ 842, training loss 0.3364031910896301\n",
      "Epoch 19 -- Batch 770/ 842, training loss 0.32673656940460205\n",
      "Epoch 19 -- Batch 771/ 842, training loss 0.3326420783996582\n",
      "Epoch 19 -- Batch 772/ 842, training loss 0.33625444769859314\n",
      "Epoch 19 -- Batch 773/ 842, training loss 0.34052133560180664\n",
      "Epoch 19 -- Batch 774/ 842, training loss 0.3321970999240875\n",
      "Epoch 19 -- Batch 775/ 842, training loss 0.34107422828674316\n",
      "Epoch 19 -- Batch 776/ 842, training loss 0.3347867727279663\n",
      "Epoch 19 -- Batch 777/ 842, training loss 0.3392693102359772\n",
      "Epoch 19 -- Batch 778/ 842, training loss 0.33678027987480164\n",
      "Epoch 19 -- Batch 779/ 842, training loss 0.3220549523830414\n",
      "Epoch 19 -- Batch 780/ 842, training loss 0.33340680599212646\n",
      "Epoch 19 -- Batch 781/ 842, training loss 0.3348412811756134\n",
      "Epoch 19 -- Batch 782/ 842, training loss 0.324026495218277\n",
      "Epoch 19 -- Batch 783/ 842, training loss 0.3380068242549896\n",
      "Epoch 19 -- Batch 784/ 842, training loss 0.31717151403427124\n",
      "Epoch 19 -- Batch 785/ 842, training loss 0.3264773190021515\n",
      "Epoch 19 -- Batch 786/ 842, training loss 0.3254162669181824\n",
      "Epoch 19 -- Batch 787/ 842, training loss 0.3364550769329071\n",
      "Epoch 19 -- Batch 788/ 842, training loss 0.34002164006233215\n",
      "Epoch 19 -- Batch 789/ 842, training loss 0.3339473307132721\n",
      "Epoch 19 -- Batch 790/ 842, training loss 0.3336663246154785\n",
      "Epoch 19 -- Batch 791/ 842, training loss 0.3400994539260864\n",
      "Epoch 19 -- Batch 792/ 842, training loss 0.32791560888290405\n",
      "Epoch 19 -- Batch 793/ 842, training loss 0.33962613344192505\n",
      "Epoch 19 -- Batch 794/ 842, training loss 0.3488275110721588\n",
      "Epoch 19 -- Batch 795/ 842, training loss 0.33278802037239075\n",
      "Epoch 19 -- Batch 796/ 842, training loss 0.3419562578201294\n",
      "Epoch 19 -- Batch 797/ 842, training loss 0.34039005637168884\n",
      "Epoch 19 -- Batch 798/ 842, training loss 0.34618666768074036\n",
      "Epoch 19 -- Batch 799/ 842, training loss 0.3323706090450287\n",
      "Epoch 19 -- Batch 800/ 842, training loss 0.33961066603660583\n",
      "Epoch 19 -- Batch 801/ 842, training loss 0.3266548812389374\n",
      "Epoch 19 -- Batch 802/ 842, training loss 0.33998018503189087\n",
      "Epoch 19 -- Batch 803/ 842, training loss 0.3319914937019348\n",
      "Epoch 19 -- Batch 804/ 842, training loss 0.32927608489990234\n",
      "Epoch 19 -- Batch 805/ 842, training loss 0.31681588292121887\n",
      "Epoch 19 -- Batch 806/ 842, training loss 0.33421874046325684\n",
      "Epoch 19 -- Batch 807/ 842, training loss 0.35372528433799744\n",
      "Epoch 19 -- Batch 808/ 842, training loss 0.3488467037677765\n",
      "Epoch 19 -- Batch 809/ 842, training loss 0.32600459456443787\n",
      "Epoch 19 -- Batch 810/ 842, training loss 0.34065327048301697\n",
      "Epoch 19 -- Batch 811/ 842, training loss 0.32753464579582214\n",
      "Epoch 19 -- Batch 812/ 842, training loss 0.3354532718658447\n",
      "Epoch 19 -- Batch 813/ 842, training loss 0.32963043451309204\n",
      "Epoch 19 -- Batch 814/ 842, training loss 0.334286630153656\n",
      "Epoch 19 -- Batch 815/ 842, training loss 0.3410741090774536\n",
      "Epoch 19 -- Batch 816/ 842, training loss 0.3415963053703308\n",
      "Epoch 19 -- Batch 817/ 842, training loss 0.3428265452384949\n",
      "Epoch 19 -- Batch 818/ 842, training loss 0.33332744240760803\n",
      "Epoch 19 -- Batch 819/ 842, training loss 0.3305339217185974\n",
      "Epoch 19 -- Batch 820/ 842, training loss 0.33053356409072876\n",
      "Epoch 19 -- Batch 821/ 842, training loss 0.32136762142181396\n",
      "Epoch 19 -- Batch 822/ 842, training loss 0.332574725151062\n",
      "Epoch 19 -- Batch 823/ 842, training loss 0.3386037349700928\n",
      "Epoch 19 -- Batch 824/ 842, training loss 0.3232859969139099\n",
      "Epoch 19 -- Batch 825/ 842, training loss 0.3331053853034973\n",
      "Epoch 19 -- Batch 826/ 842, training loss 0.32942280173301697\n",
      "Epoch 19 -- Batch 827/ 842, training loss 0.32874712347984314\n",
      "Epoch 19 -- Batch 828/ 842, training loss 0.3367760479450226\n",
      "Epoch 19 -- Batch 829/ 842, training loss 0.3306809961795807\n",
      "Epoch 19 -- Batch 830/ 842, training loss 0.3358679413795471\n",
      "Epoch 19 -- Batch 831/ 842, training loss 0.32484686374664307\n",
      "Epoch 19 -- Batch 832/ 842, training loss 0.3233398497104645\n",
      "Epoch 19 -- Batch 833/ 842, training loss 0.33348992466926575\n",
      "Epoch 19 -- Batch 834/ 842, training loss 0.3301989734172821\n",
      "Epoch 19 -- Batch 835/ 842, training loss 0.3269917368888855\n",
      "Epoch 19 -- Batch 836/ 842, training loss 0.33380165696144104\n",
      "Epoch 19 -- Batch 837/ 842, training loss 0.3384798765182495\n",
      "Epoch 19 -- Batch 838/ 842, training loss 0.3379521369934082\n",
      "Epoch 19 -- Batch 839/ 842, training loss 0.3423822820186615\n",
      "Epoch 19 -- Batch 840/ 842, training loss 0.3417161703109741\n",
      "Epoch 19 -- Batch 841/ 842, training loss 0.33443039655685425\n",
      "Epoch 19 -- Batch 842/ 842, training loss 0.3082068860530853\n",
      "----------------------------------------------------------------------\n",
      "Epoch 19 -- Batch 1/ 94, validation loss 0.32546767592430115\n",
      "Epoch 19 -- Batch 2/ 94, validation loss 0.33254092931747437\n",
      "Epoch 19 -- Batch 3/ 94, validation loss 0.3150714337825775\n",
      "Epoch 19 -- Batch 4/ 94, validation loss 0.3414691686630249\n",
      "Epoch 19 -- Batch 5/ 94, validation loss 0.323429137468338\n",
      "Epoch 19 -- Batch 6/ 94, validation loss 0.3235604763031006\n",
      "Epoch 19 -- Batch 7/ 94, validation loss 0.3139324486255646\n",
      "Epoch 19 -- Batch 8/ 94, validation loss 0.3105931282043457\n",
      "Epoch 19 -- Batch 9/ 94, validation loss 0.32483339309692383\n",
      "Epoch 19 -- Batch 10/ 94, validation loss 0.31628766655921936\n",
      "Epoch 19 -- Batch 11/ 94, validation loss 0.324514240026474\n",
      "Epoch 19 -- Batch 12/ 94, validation loss 0.3382885456085205\n",
      "Epoch 19 -- Batch 13/ 94, validation loss 0.3214242458343506\n",
      "Epoch 19 -- Batch 14/ 94, validation loss 0.33565816283226013\n",
      "Epoch 19 -- Batch 15/ 94, validation loss 0.32473477721214294\n",
      "Epoch 19 -- Batch 16/ 94, validation loss 0.3338291347026825\n",
      "Epoch 19 -- Batch 17/ 94, validation loss 0.3141064941883087\n",
      "Epoch 19 -- Batch 18/ 94, validation loss 0.3329596221446991\n",
      "Epoch 19 -- Batch 19/ 94, validation loss 0.31551989912986755\n",
      "Epoch 19 -- Batch 20/ 94, validation loss 0.34792688488960266\n",
      "Epoch 19 -- Batch 21/ 94, validation loss 0.32637038826942444\n",
      "Epoch 19 -- Batch 22/ 94, validation loss 0.3275720775127411\n",
      "Epoch 19 -- Batch 23/ 94, validation loss 0.31125861406326294\n",
      "Epoch 19 -- Batch 24/ 94, validation loss 0.3160240650177002\n",
      "Epoch 19 -- Batch 25/ 94, validation loss 0.3280290365219116\n",
      "Epoch 19 -- Batch 26/ 94, validation loss 0.3212374448776245\n",
      "Epoch 19 -- Batch 27/ 94, validation loss 0.30055391788482666\n",
      "Epoch 19 -- Batch 28/ 94, validation loss 0.31658390164375305\n",
      "Epoch 19 -- Batch 29/ 94, validation loss 0.32881662249565125\n",
      "Epoch 19 -- Batch 30/ 94, validation loss 0.32681238651275635\n",
      "Epoch 19 -- Batch 31/ 94, validation loss 0.3238336741924286\n",
      "Epoch 19 -- Batch 32/ 94, validation loss 0.32989299297332764\n",
      "Epoch 19 -- Batch 33/ 94, validation loss 0.33364471793174744\n",
      "Epoch 19 -- Batch 34/ 94, validation loss 0.3207438886165619\n",
      "Epoch 19 -- Batch 35/ 94, validation loss 0.31760963797569275\n",
      "Epoch 19 -- Batch 36/ 94, validation loss 0.31676602363586426\n",
      "Epoch 19 -- Batch 37/ 94, validation loss 0.3165343999862671\n",
      "Epoch 19 -- Batch 38/ 94, validation loss 0.3305412828922272\n",
      "Epoch 19 -- Batch 39/ 94, validation loss 0.32656705379486084\n",
      "Epoch 19 -- Batch 40/ 94, validation loss 0.3094140887260437\n",
      "Epoch 19 -- Batch 41/ 94, validation loss 0.3160713016986847\n",
      "Epoch 19 -- Batch 42/ 94, validation loss 0.33773088455200195\n",
      "Epoch 19 -- Batch 43/ 94, validation loss 0.32445472478866577\n",
      "Epoch 19 -- Batch 44/ 94, validation loss 0.332613468170166\n",
      "Epoch 19 -- Batch 45/ 94, validation loss 0.3263118267059326\n",
      "Epoch 19 -- Batch 46/ 94, validation loss 0.3282262086868286\n",
      "Epoch 19 -- Batch 47/ 94, validation loss 0.3308103084564209\n",
      "Epoch 19 -- Batch 48/ 94, validation loss 0.3115360140800476\n",
      "Epoch 19 -- Batch 49/ 94, validation loss 0.32916226983070374\n",
      "Epoch 19 -- Batch 50/ 94, validation loss 0.3131924867630005\n",
      "Epoch 19 -- Batch 51/ 94, validation loss 0.3384014368057251\n",
      "Epoch 19 -- Batch 52/ 94, validation loss 0.3305840492248535\n",
      "Epoch 19 -- Batch 53/ 94, validation loss 0.34021082520484924\n",
      "Epoch 19 -- Batch 54/ 94, validation loss 0.33784034848213196\n",
      "Epoch 19 -- Batch 55/ 94, validation loss 0.35025277733802795\n",
      "Epoch 19 -- Batch 56/ 94, validation loss 0.32779088616371155\n",
      "Epoch 19 -- Batch 57/ 94, validation loss 0.3296058177947998\n",
      "Epoch 19 -- Batch 58/ 94, validation loss 0.3342137634754181\n",
      "Epoch 19 -- Batch 59/ 94, validation loss 0.324134886264801\n",
      "Epoch 19 -- Batch 60/ 94, validation loss 0.3310808539390564\n",
      "Epoch 19 -- Batch 61/ 94, validation loss 0.31995823979377747\n",
      "Epoch 19 -- Batch 62/ 94, validation loss 0.3224032521247864\n",
      "Epoch 19 -- Batch 63/ 94, validation loss 0.3250753581523895\n",
      "Epoch 19 -- Batch 64/ 94, validation loss 0.3213757872581482\n",
      "Epoch 19 -- Batch 65/ 94, validation loss 0.3247719407081604\n",
      "Epoch 19 -- Batch 66/ 94, validation loss 0.33866745233535767\n",
      "Epoch 19 -- Batch 67/ 94, validation loss 0.3224029541015625\n",
      "Epoch 19 -- Batch 68/ 94, validation loss 0.33504873514175415\n",
      "Epoch 19 -- Batch 69/ 94, validation loss 0.32902050018310547\n",
      "Epoch 19 -- Batch 70/ 94, validation loss 0.31766417622566223\n",
      "Epoch 19 -- Batch 71/ 94, validation loss 0.3184904158115387\n",
      "Epoch 19 -- Batch 72/ 94, validation loss 0.335462749004364\n",
      "Epoch 19 -- Batch 73/ 94, validation loss 0.3260827362537384\n",
      "Epoch 19 -- Batch 74/ 94, validation loss 0.3187391757965088\n",
      "Epoch 19 -- Batch 75/ 94, validation loss 0.33358675241470337\n",
      "Epoch 19 -- Batch 76/ 94, validation loss 0.3166792094707489\n",
      "Epoch 19 -- Batch 77/ 94, validation loss 0.3188627362251282\n",
      "Epoch 19 -- Batch 78/ 94, validation loss 0.3288628160953522\n",
      "Epoch 19 -- Batch 79/ 94, validation loss 0.3098588287830353\n",
      "Epoch 19 -- Batch 80/ 94, validation loss 0.33519867062568665\n",
      "Epoch 19 -- Batch 81/ 94, validation loss 0.3212132155895233\n",
      "Epoch 19 -- Batch 82/ 94, validation loss 0.33603498339653015\n",
      "Epoch 19 -- Batch 83/ 94, validation loss 0.314632385969162\n",
      "Epoch 19 -- Batch 84/ 94, validation loss 0.3296770751476288\n",
      "Epoch 19 -- Batch 85/ 94, validation loss 0.3561633825302124\n",
      "Epoch 19 -- Batch 86/ 94, validation loss 0.31984126567840576\n",
      "Epoch 19 -- Batch 87/ 94, validation loss 0.32546526193618774\n",
      "Epoch 19 -- Batch 88/ 94, validation loss 0.34436023235321045\n",
      "Epoch 19 -- Batch 89/ 94, validation loss 0.327227383852005\n",
      "Epoch 19 -- Batch 90/ 94, validation loss 0.3230651617050171\n",
      "Epoch 19 -- Batch 91/ 94, validation loss 0.3167732357978821\n",
      "Epoch 19 -- Batch 92/ 94, validation loss 0.33145636320114136\n",
      "Epoch 19 -- Batch 93/ 94, validation loss 0.319324791431427\n",
      "Epoch 19 -- Batch 94/ 94, validation loss 0.33000263571739197\n",
      "----------------------------------------------------------------------\n",
      "Epoch 19 loss: Training 0.3325396478176117, Validation 0.33000263571739197\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 5 6 8\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 2 3 5 7 8 9 10 11 12\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 25 33 34\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'C[C@]12CC=CCC1CC3CCC'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 3 5 10 11 20\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'CNC(=O)[C@@H]1C[C@@H](NCC2=CC[C@H]3C[C@H]2C3(c2ccc(C)cc2)C3)C1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 26\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 5 15 24\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 8 9 10 11\n",
      "[19:07:00] SMILES Parse Error: ring closure 3 duplicates bond between atom 16 and atom 17 for input: 'Cc1sc2nc(CCN)nc(N3C[C@H]4C[C@H]3C3[C@H]3CC[C@H]3N4)c2c1C'\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1c(C)oc2ccc(N(C(=O)Nc3ccccc3Cl)C3=O)c1Cl'\n",
      "[19:07:00] SMILES Parse Error: extra open parentheses for input: 'CC(=O)N1CCc2cc(S(=O)(=O)NCC(C(=O)N3CCN(C(C)=O)CC3)ccc21'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 24 25 26 27 28 29 30\n",
      "[19:07:00] SMILES Parse Error: extra open parentheses for input: 'O=S(=O)(N[C@@H]1CCN(C(=O)c2cn[nH]c2)C[C@H]1Cn1ccnc1'\n",
      "[19:07:00] Explicit valence for atom # 0 O, 3, is greater than permitted\n",
      "[19:07:00] SMILES Parse Error: ring closure 4 duplicates bond between atom 19 and atom 20 for input: 'Cc1cccc(OCCN2CCN(S(=O)(=O)c3ccc4c4c(c3)CCC(=O)N5CCC4)CC2)c1'\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'CCCCS(=O)(=O)N1CCN(c2nc(-c3cc4ccccc4cc(C(F)(F)F)n3)c(C)nc23)CC1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 2 12 13\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'CCCCCCCCCCCC(=O)NC[C@]1(C)O[C@@H](C)C(=O)[C@@H]2CC'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 12 13 14 24 25 27\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 12 13 14 31 32\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C1(COSC(F)F)N=C(c2ccc(C)cc2)NC(=O)c2ccco2)cc1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 4 21 22 23 25 26 27\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'O=c1c2n(nc2o1)CCN(Cc1cn[nH]c1-c1ccc(F)cc1)CC2'\n",
      "[19:07:00] SMILES Parse Error: ring closure 1 duplicates bond between atom 15 and atom 16 for input: 'COC(=O)[C@]12C[C@H](CC(=O)N3)CC(=O)N1[C@H]1c1cc(C)ccc1O2'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 2 3 6 7 8 10 11 26 27\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 2 3 4 9 10 11 13\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1NC(=O)C1NC(=O)NC1N2Cc1ccc(C)cc1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 27\n",
      "[19:07:00] SMILES Parse Error: extra close parentheses while parsing: CCC(C(=O)N1CCc2c(nc(N)[nH]c2=O)C1)C1(C(F)(F)F)C(F)(F)F)C(=O)O1\n",
      "[19:07:00] SMILES Parse Error: Failed parsing SMILES 'CCC(C(=O)N1CCc2c(nc(N)[nH]c2=O)C1)C1(C(F)(F)F)C(F)(F)F)C(=O)O1' for input: 'CCC(C(=O)N1CCc2c(nc(N)[nH]c2=O)C1)C1(C(F)(F)F)C(F)(F)F)C(=O)O1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 20\n",
      "[19:07:00] SMILES Parse Error: extra open parentheses for input: 'COC(=O)CC(NC1CCCN(c2ccc(NC(C)=O)cc2)C1=O'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 25 26\n",
      "[19:07:00] Explicit valence for atom # 4 O, 3, is greater than permitted\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'O=C1OC(c2cc(Cl)ccc2Cl)=N/c1cc(Cl)c2ccccc2c1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 24 25 26\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14 17 19\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'O=Cc1nc(ccccc2=O)c2cc(Cl)ccc2n1'\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)N1CCN(CCN=CC2=O)CC(O)CC1'\n",
      "[19:07:00] SMILES Parse Error: extra close parentheses while parsing: Fc1ccccc1F)c1cnc(N=Nc2cccc(SCc3ccc(F)cc3)c2)[nH]c1=O\n",
      "[19:07:00] SMILES Parse Error: Failed parsing SMILES 'Fc1ccccc1F)c1cnc(N=Nc2cccc(SCc3ccc(F)cc3)c2)[nH]c1=O' for input: 'Fc1ccccc1F)c1cnc(N=Nc2cccc(SCc3ccc(F)cc3)c2)[nH]c1=O'\n",
      "[19:07:00] SMILES Parse Error: extra close parentheses while parsing: Cc1ccccc1C(=O)Nc1nc2cc3nc(NC(=O)C4CC4)sc3c2)s1\n",
      "[19:07:00] SMILES Parse Error: Failed parsing SMILES 'Cc1ccccc1C(=O)Nc1nc2cc3nc(NC(=O)C4CC4)sc3c2)s1' for input: 'Cc1ccccc1C(=O)Nc1nc2cc3nc(NC(=O)C4CC4)sc3c2)s1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 9 10 15 16 17 18 20 21 22\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'N#CCCn1ncc2c3c([nH]c3cccc13)CCC3'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 3 4 5 9 10\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)CN2C(=O)S/C(=C\\c3ccc4c(c3)CCCCO)C2=O)cc1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 5 7 8 9 26 28\n",
      "[19:07:00] SMILES Parse Error: unclosed ring for input: 'O=C1/C(=C\\C2COc3ccccc3C2(O)C(=O)N2Cc2ccccc21)c1ccc(Cl)cc1'\n",
      "[19:07:00] Can't kekulize mol.  Unkekulized atoms: 8 9 17 18 20 22 24 25 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 -- Batch 1/ 842, training loss 0.3206017017364502\n",
      "Epoch 20 -- Batch 2/ 842, training loss 0.3367495834827423\n",
      "Epoch 20 -- Batch 3/ 842, training loss 0.31786760687828064\n",
      "Epoch 20 -- Batch 4/ 842, training loss 0.3202224671840668\n",
      "Epoch 20 -- Batch 5/ 842, training loss 0.32466819882392883\n",
      "Epoch 20 -- Batch 6/ 842, training loss 0.3153238296508789\n",
      "Epoch 20 -- Batch 7/ 842, training loss 0.3328734040260315\n",
      "Epoch 20 -- Batch 8/ 842, training loss 0.340962678194046\n",
      "Epoch 20 -- Batch 9/ 842, training loss 0.32902634143829346\n",
      "Epoch 20 -- Batch 10/ 842, training loss 0.31604287028312683\n",
      "Epoch 20 -- Batch 11/ 842, training loss 0.3225213587284088\n",
      "Epoch 20 -- Batch 12/ 842, training loss 0.32428473234176636\n",
      "Epoch 20 -- Batch 13/ 842, training loss 0.329302579164505\n",
      "Epoch 20 -- Batch 14/ 842, training loss 0.3308044373989105\n",
      "Epoch 20 -- Batch 15/ 842, training loss 0.3404587507247925\n",
      "Epoch 20 -- Batch 16/ 842, training loss 0.3265244960784912\n",
      "Epoch 20 -- Batch 17/ 842, training loss 0.319294810295105\n",
      "Epoch 20 -- Batch 18/ 842, training loss 0.3198070526123047\n",
      "Epoch 20 -- Batch 19/ 842, training loss 0.32127371430397034\n",
      "Epoch 20 -- Batch 20/ 842, training loss 0.3280290961265564\n",
      "Epoch 20 -- Batch 21/ 842, training loss 0.3196505308151245\n",
      "Epoch 20 -- Batch 22/ 842, training loss 0.32486972212791443\n",
      "Epoch 20 -- Batch 23/ 842, training loss 0.32646444439888\n",
      "Epoch 20 -- Batch 24/ 842, training loss 0.3271593451499939\n",
      "Epoch 20 -- Batch 25/ 842, training loss 0.3400402069091797\n",
      "Epoch 20 -- Batch 26/ 842, training loss 0.3197523355484009\n",
      "Epoch 20 -- Batch 27/ 842, training loss 0.33634141087532043\n",
      "Epoch 20 -- Batch 28/ 842, training loss 0.33088842034339905\n",
      "Epoch 20 -- Batch 29/ 842, training loss 0.3239251971244812\n",
      "Epoch 20 -- Batch 30/ 842, training loss 0.3202306032180786\n",
      "Epoch 20 -- Batch 31/ 842, training loss 0.31853166222572327\n",
      "Epoch 20 -- Batch 32/ 842, training loss 0.31387513875961304\n",
      "Epoch 20 -- Batch 33/ 842, training loss 0.34286585450172424\n",
      "Epoch 20 -- Batch 34/ 842, training loss 0.32506704330444336\n",
      "Epoch 20 -- Batch 35/ 842, training loss 0.33542704582214355\n",
      "Epoch 20 -- Batch 36/ 842, training loss 0.3236265182495117\n",
      "Epoch 20 -- Batch 37/ 842, training loss 0.34267112612724304\n",
      "Epoch 20 -- Batch 38/ 842, training loss 0.3191540241241455\n",
      "Epoch 20 -- Batch 39/ 842, training loss 0.33251702785491943\n",
      "Epoch 20 -- Batch 40/ 842, training loss 0.3469352126121521\n",
      "Epoch 20 -- Batch 41/ 842, training loss 0.32580795884132385\n",
      "Epoch 20 -- Batch 42/ 842, training loss 0.3289273679256439\n",
      "Epoch 20 -- Batch 43/ 842, training loss 0.3289869725704193\n",
      "Epoch 20 -- Batch 44/ 842, training loss 0.32105138897895813\n",
      "Epoch 20 -- Batch 45/ 842, training loss 0.3302583396434784\n",
      "Epoch 20 -- Batch 46/ 842, training loss 0.3147100806236267\n",
      "Epoch 20 -- Batch 47/ 842, training loss 0.327874094247818\n",
      "Epoch 20 -- Batch 48/ 842, training loss 0.3246975541114807\n",
      "Epoch 20 -- Batch 49/ 842, training loss 0.3205384612083435\n",
      "Epoch 20 -- Batch 50/ 842, training loss 0.3243986964225769\n",
      "Epoch 20 -- Batch 51/ 842, training loss 0.3247982859611511\n",
      "Epoch 20 -- Batch 52/ 842, training loss 0.31972843408584595\n",
      "Epoch 20 -- Batch 53/ 842, training loss 0.3285384178161621\n",
      "Epoch 20 -- Batch 54/ 842, training loss 0.3215799033641815\n",
      "Epoch 20 -- Batch 55/ 842, training loss 0.32200416922569275\n",
      "Epoch 20 -- Batch 56/ 842, training loss 0.3240877687931061\n",
      "Epoch 20 -- Batch 57/ 842, training loss 0.31894174218177795\n",
      "Epoch 20 -- Batch 58/ 842, training loss 0.3285190165042877\n",
      "Epoch 20 -- Batch 59/ 842, training loss 0.31860682368278503\n",
      "Epoch 20 -- Batch 60/ 842, training loss 0.33089473843574524\n",
      "Epoch 20 -- Batch 61/ 842, training loss 0.32928457856178284\n",
      "Epoch 20 -- Batch 62/ 842, training loss 0.32986676692962646\n",
      "Epoch 20 -- Batch 63/ 842, training loss 0.315705269575119\n",
      "Epoch 20 -- Batch 64/ 842, training loss 0.32774242758750916\n",
      "Epoch 20 -- Batch 65/ 842, training loss 0.3213360011577606\n",
      "Epoch 20 -- Batch 66/ 842, training loss 0.3362182676792145\n",
      "Epoch 20 -- Batch 67/ 842, training loss 0.3152827322483063\n",
      "Epoch 20 -- Batch 68/ 842, training loss 0.3242377042770386\n",
      "Epoch 20 -- Batch 69/ 842, training loss 0.3282821476459503\n",
      "Epoch 20 -- Batch 70/ 842, training loss 0.32346832752227783\n",
      "Epoch 20 -- Batch 71/ 842, training loss 0.3191412091255188\n",
      "Epoch 20 -- Batch 72/ 842, training loss 0.33123838901519775\n",
      "Epoch 20 -- Batch 73/ 842, training loss 0.3341130316257477\n",
      "Epoch 20 -- Batch 74/ 842, training loss 0.33050277829170227\n",
      "Epoch 20 -- Batch 75/ 842, training loss 0.3213752210140228\n",
      "Epoch 20 -- Batch 76/ 842, training loss 0.31771284341812134\n",
      "Epoch 20 -- Batch 77/ 842, training loss 0.32523831725120544\n",
      "Epoch 20 -- Batch 78/ 842, training loss 0.3226354420185089\n",
      "Epoch 20 -- Batch 79/ 842, training loss 0.32632142305374146\n",
      "Epoch 20 -- Batch 80/ 842, training loss 0.32549384236335754\n",
      "Epoch 20 -- Batch 81/ 842, training loss 0.326992928981781\n",
      "Epoch 20 -- Batch 82/ 842, training loss 0.3094196915626526\n",
      "Epoch 20 -- Batch 83/ 842, training loss 0.33102723956108093\n",
      "Epoch 20 -- Batch 84/ 842, training loss 0.3307052254676819\n",
      "Epoch 20 -- Batch 85/ 842, training loss 0.3221493065357208\n",
      "Epoch 20 -- Batch 86/ 842, training loss 0.32955193519592285\n",
      "Epoch 20 -- Batch 87/ 842, training loss 0.3302978575229645\n",
      "Epoch 20 -- Batch 88/ 842, training loss 0.3276328444480896\n",
      "Epoch 20 -- Batch 89/ 842, training loss 0.3201636075973511\n",
      "Epoch 20 -- Batch 90/ 842, training loss 0.3178771734237671\n",
      "Epoch 20 -- Batch 91/ 842, training loss 0.31527215242385864\n",
      "Epoch 20 -- Batch 92/ 842, training loss 0.31553804874420166\n",
      "Epoch 20 -- Batch 93/ 842, training loss 0.3328706622123718\n",
      "Epoch 20 -- Batch 94/ 842, training loss 0.33457040786743164\n",
      "Epoch 20 -- Batch 95/ 842, training loss 0.33161187171936035\n",
      "Epoch 20 -- Batch 96/ 842, training loss 0.3309575915336609\n",
      "Epoch 20 -- Batch 97/ 842, training loss 0.32849055528640747\n",
      "Epoch 20 -- Batch 98/ 842, training loss 0.33907511830329895\n",
      "Epoch 20 -- Batch 99/ 842, training loss 0.32680970430374146\n",
      "Epoch 20 -- Batch 100/ 842, training loss 0.33346325159072876\n",
      "Epoch 20 -- Batch 101/ 842, training loss 0.32574522495269775\n",
      "Epoch 20 -- Batch 102/ 842, training loss 0.3188854455947876\n",
      "Epoch 20 -- Batch 103/ 842, training loss 0.32328540086746216\n",
      "Epoch 20 -- Batch 104/ 842, training loss 0.3194259703159332\n",
      "Epoch 20 -- Batch 105/ 842, training loss 0.32213932275772095\n",
      "Epoch 20 -- Batch 106/ 842, training loss 0.3279803991317749\n",
      "Epoch 20 -- Batch 107/ 842, training loss 0.3195057511329651\n",
      "Epoch 20 -- Batch 108/ 842, training loss 0.3193170130252838\n",
      "Epoch 20 -- Batch 109/ 842, training loss 0.3230645954608917\n",
      "Epoch 20 -- Batch 110/ 842, training loss 0.3228636085987091\n",
      "Epoch 20 -- Batch 111/ 842, training loss 0.33159732818603516\n",
      "Epoch 20 -- Batch 112/ 842, training loss 0.3366610109806061\n",
      "Epoch 20 -- Batch 113/ 842, training loss 0.33242595195770264\n",
      "Epoch 20 -- Batch 114/ 842, training loss 0.3311571478843689\n",
      "Epoch 20 -- Batch 115/ 842, training loss 0.3199614882469177\n",
      "Epoch 20 -- Batch 116/ 842, training loss 0.3149501085281372\n",
      "Epoch 20 -- Batch 117/ 842, training loss 0.3313322067260742\n",
      "Epoch 20 -- Batch 118/ 842, training loss 0.3391503393650055\n",
      "Epoch 20 -- Batch 119/ 842, training loss 0.3408178687095642\n",
      "Epoch 20 -- Batch 120/ 842, training loss 0.3137667775154114\n",
      "Epoch 20 -- Batch 121/ 842, training loss 0.32855847477912903\n",
      "Epoch 20 -- Batch 122/ 842, training loss 0.34255728125572205\n",
      "Epoch 20 -- Batch 123/ 842, training loss 0.31996747851371765\n",
      "Epoch 20 -- Batch 124/ 842, training loss 0.34431979060173035\n",
      "Epoch 20 -- Batch 125/ 842, training loss 0.32046425342559814\n",
      "Epoch 20 -- Batch 126/ 842, training loss 0.33121439814567566\n",
      "Epoch 20 -- Batch 127/ 842, training loss 0.3245067000389099\n",
      "Epoch 20 -- Batch 128/ 842, training loss 0.3379683494567871\n",
      "Epoch 20 -- Batch 129/ 842, training loss 0.33890390396118164\n",
      "Epoch 20 -- Batch 130/ 842, training loss 0.3274778127670288\n",
      "Epoch 20 -- Batch 131/ 842, training loss 0.3220888674259186\n",
      "Epoch 20 -- Batch 132/ 842, training loss 0.3149571418762207\n",
      "Epoch 20 -- Batch 133/ 842, training loss 0.3164415955543518\n",
      "Epoch 20 -- Batch 134/ 842, training loss 0.32467353343963623\n",
      "Epoch 20 -- Batch 135/ 842, training loss 0.33723318576812744\n",
      "Epoch 20 -- Batch 136/ 842, training loss 0.31794485449790955\n",
      "Epoch 20 -- Batch 137/ 842, training loss 0.3318385183811188\n",
      "Epoch 20 -- Batch 138/ 842, training loss 0.3267471492290497\n",
      "Epoch 20 -- Batch 139/ 842, training loss 0.32423728704452515\n",
      "Epoch 20 -- Batch 140/ 842, training loss 0.32176917791366577\n",
      "Epoch 20 -- Batch 141/ 842, training loss 0.32224613428115845\n",
      "Epoch 20 -- Batch 142/ 842, training loss 0.3345770835876465\n",
      "Epoch 20 -- Batch 143/ 842, training loss 0.32487133145332336\n",
      "Epoch 20 -- Batch 144/ 842, training loss 0.324293315410614\n",
      "Epoch 20 -- Batch 145/ 842, training loss 0.3261813819408417\n",
      "Epoch 20 -- Batch 146/ 842, training loss 0.3309493362903595\n",
      "Epoch 20 -- Batch 147/ 842, training loss 0.3251327872276306\n",
      "Epoch 20 -- Batch 148/ 842, training loss 0.3194562494754791\n",
      "Epoch 20 -- Batch 149/ 842, training loss 0.3287305235862732\n",
      "Epoch 20 -- Batch 150/ 842, training loss 0.3266102969646454\n",
      "Epoch 20 -- Batch 151/ 842, training loss 0.32717403769493103\n",
      "Epoch 20 -- Batch 152/ 842, training loss 0.33321359753608704\n",
      "Epoch 20 -- Batch 153/ 842, training loss 0.3157183527946472\n",
      "Epoch 20 -- Batch 154/ 842, training loss 0.32994961738586426\n",
      "Epoch 20 -- Batch 155/ 842, training loss 0.3278350830078125\n",
      "Epoch 20 -- Batch 156/ 842, training loss 0.33510276675224304\n",
      "Epoch 20 -- Batch 157/ 842, training loss 0.3385670781135559\n",
      "Epoch 20 -- Batch 158/ 842, training loss 0.32474285364151\n",
      "Epoch 20 -- Batch 159/ 842, training loss 0.32537758350372314\n",
      "Epoch 20 -- Batch 160/ 842, training loss 0.3160092830657959\n",
      "Epoch 20 -- Batch 161/ 842, training loss 0.3171621263027191\n",
      "Epoch 20 -- Batch 162/ 842, training loss 0.31945449113845825\n",
      "Epoch 20 -- Batch 163/ 842, training loss 0.32275259494781494\n",
      "Epoch 20 -- Batch 164/ 842, training loss 0.3112563490867615\n",
      "Epoch 20 -- Batch 165/ 842, training loss 0.3341220021247864\n",
      "Epoch 20 -- Batch 166/ 842, training loss 0.3334513306617737\n",
      "Epoch 20 -- Batch 167/ 842, training loss 0.3233686089515686\n",
      "Epoch 20 -- Batch 168/ 842, training loss 0.33589139580726624\n",
      "Epoch 20 -- Batch 169/ 842, training loss 0.32439467310905457\n",
      "Epoch 20 -- Batch 170/ 842, training loss 0.3343028426170349\n",
      "Epoch 20 -- Batch 171/ 842, training loss 0.3327483534812927\n",
      "Epoch 20 -- Batch 172/ 842, training loss 0.33234816789627075\n",
      "Epoch 20 -- Batch 173/ 842, training loss 0.3262162506580353\n",
      "Epoch 20 -- Batch 174/ 842, training loss 0.33184540271759033\n",
      "Epoch 20 -- Batch 175/ 842, training loss 0.3433929979801178\n",
      "Epoch 20 -- Batch 176/ 842, training loss 0.3239259719848633\n",
      "Epoch 20 -- Batch 177/ 842, training loss 0.32920679450035095\n",
      "Epoch 20 -- Batch 178/ 842, training loss 0.3132491111755371\n",
      "Epoch 20 -- Batch 179/ 842, training loss 0.34070050716400146\n",
      "Epoch 20 -- Batch 180/ 842, training loss 0.3413473665714264\n",
      "Epoch 20 -- Batch 181/ 842, training loss 0.3327334225177765\n",
      "Epoch 20 -- Batch 182/ 842, training loss 0.32585492730140686\n",
      "Epoch 20 -- Batch 183/ 842, training loss 0.32473111152648926\n",
      "Epoch 20 -- Batch 184/ 842, training loss 0.3297080099582672\n",
      "Epoch 20 -- Batch 185/ 842, training loss 0.3276843726634979\n",
      "Epoch 20 -- Batch 186/ 842, training loss 0.3184588849544525\n",
      "Epoch 20 -- Batch 187/ 842, training loss 0.3430568277835846\n",
      "Epoch 20 -- Batch 188/ 842, training loss 0.3245880603790283\n",
      "Epoch 20 -- Batch 189/ 842, training loss 0.3277727961540222\n",
      "Epoch 20 -- Batch 190/ 842, training loss 0.3302861452102661\n",
      "Epoch 20 -- Batch 191/ 842, training loss 0.31653329730033875\n",
      "Epoch 20 -- Batch 192/ 842, training loss 0.32914766669273376\n",
      "Epoch 20 -- Batch 193/ 842, training loss 0.3207479417324066\n",
      "Epoch 20 -- Batch 194/ 842, training loss 0.3199767470359802\n",
      "Epoch 20 -- Batch 195/ 842, training loss 0.3294251263141632\n",
      "Epoch 20 -- Batch 196/ 842, training loss 0.3256113827228546\n",
      "Epoch 20 -- Batch 197/ 842, training loss 0.3098359704017639\n",
      "Epoch 20 -- Batch 198/ 842, training loss 0.3127371370792389\n",
      "Epoch 20 -- Batch 199/ 842, training loss 0.31773367524147034\n",
      "Epoch 20 -- Batch 200/ 842, training loss 0.31385141611099243\n",
      "Epoch 20 -- Batch 201/ 842, training loss 0.32830989360809326\n",
      "Epoch 20 -- Batch 202/ 842, training loss 0.32386723160743713\n",
      "Epoch 20 -- Batch 203/ 842, training loss 0.34431570768356323\n",
      "Epoch 20 -- Batch 204/ 842, training loss 0.3236532509326935\n",
      "Epoch 20 -- Batch 205/ 842, training loss 0.3277950584888458\n",
      "Epoch 20 -- Batch 206/ 842, training loss 0.3312839865684509\n",
      "Epoch 20 -- Batch 207/ 842, training loss 0.3211734890937805\n",
      "Epoch 20 -- Batch 208/ 842, training loss 0.3327665627002716\n",
      "Epoch 20 -- Batch 209/ 842, training loss 0.3298341929912567\n",
      "Epoch 20 -- Batch 210/ 842, training loss 0.3302764296531677\n",
      "Epoch 20 -- Batch 211/ 842, training loss 0.3118172585964203\n",
      "Epoch 20 -- Batch 212/ 842, training loss 0.3236044645309448\n",
      "Epoch 20 -- Batch 213/ 842, training loss 0.3182520270347595\n",
      "Epoch 20 -- Batch 214/ 842, training loss 0.33883529901504517\n",
      "Epoch 20 -- Batch 215/ 842, training loss 0.3243931233882904\n",
      "Epoch 20 -- Batch 216/ 842, training loss 0.32888931035995483\n",
      "Epoch 20 -- Batch 217/ 842, training loss 0.3188232183456421\n",
      "Epoch 20 -- Batch 218/ 842, training loss 0.3247843086719513\n",
      "Epoch 20 -- Batch 219/ 842, training loss 0.33365359902381897\n",
      "Epoch 20 -- Batch 220/ 842, training loss 0.3147929906845093\n",
      "Epoch 20 -- Batch 221/ 842, training loss 0.32595863938331604\n",
      "Epoch 20 -- Batch 222/ 842, training loss 0.32623764872550964\n",
      "Epoch 20 -- Batch 223/ 842, training loss 0.31880033016204834\n",
      "Epoch 20 -- Batch 224/ 842, training loss 0.33736690878868103\n",
      "Epoch 20 -- Batch 225/ 842, training loss 0.3456338942050934\n",
      "Epoch 20 -- Batch 226/ 842, training loss 0.335808664560318\n",
      "Epoch 20 -- Batch 227/ 842, training loss 0.33335232734680176\n",
      "Epoch 20 -- Batch 228/ 842, training loss 0.3184848129749298\n",
      "Epoch 20 -- Batch 229/ 842, training loss 0.3158332109451294\n",
      "Epoch 20 -- Batch 230/ 842, training loss 0.32648965716362\n",
      "Epoch 20 -- Batch 231/ 842, training loss 0.32917001843452454\n",
      "Epoch 20 -- Batch 232/ 842, training loss 0.3343847393989563\n",
      "Epoch 20 -- Batch 233/ 842, training loss 0.3358238935470581\n",
      "Epoch 20 -- Batch 234/ 842, training loss 0.33513689041137695\n",
      "Epoch 20 -- Batch 235/ 842, training loss 0.3194686472415924\n",
      "Epoch 20 -- Batch 236/ 842, training loss 0.3266851007938385\n",
      "Epoch 20 -- Batch 237/ 842, training loss 0.3443049490451813\n",
      "Epoch 20 -- Batch 238/ 842, training loss 0.3246566355228424\n",
      "Epoch 20 -- Batch 239/ 842, training loss 0.3227970004081726\n",
      "Epoch 20 -- Batch 240/ 842, training loss 0.3284814953804016\n",
      "Epoch 20 -- Batch 241/ 842, training loss 0.3327733278274536\n",
      "Epoch 20 -- Batch 242/ 842, training loss 0.3258398175239563\n",
      "Epoch 20 -- Batch 243/ 842, training loss 0.3226390480995178\n",
      "Epoch 20 -- Batch 244/ 842, training loss 0.3395480811595917\n",
      "Epoch 20 -- Batch 245/ 842, training loss 0.3182610869407654\n",
      "Epoch 20 -- Batch 246/ 842, training loss 0.32184284925460815\n",
      "Epoch 20 -- Batch 247/ 842, training loss 0.31774020195007324\n",
      "Epoch 20 -- Batch 248/ 842, training loss 0.33238181471824646\n",
      "Epoch 20 -- Batch 249/ 842, training loss 0.3228963017463684\n",
      "Epoch 20 -- Batch 250/ 842, training loss 0.3326030373573303\n",
      "Epoch 20 -- Batch 251/ 842, training loss 0.32394206523895264\n",
      "Epoch 20 -- Batch 252/ 842, training loss 0.32460570335388184\n",
      "Epoch 20 -- Batch 253/ 842, training loss 0.32453787326812744\n",
      "Epoch 20 -- Batch 254/ 842, training loss 0.3414083421230316\n",
      "Epoch 20 -- Batch 255/ 842, training loss 0.325463205575943\n",
      "Epoch 20 -- Batch 256/ 842, training loss 0.32727256417274475\n",
      "Epoch 20 -- Batch 257/ 842, training loss 0.3193024694919586\n",
      "Epoch 20 -- Batch 258/ 842, training loss 0.3309406042098999\n",
      "Epoch 20 -- Batch 259/ 842, training loss 0.3245460093021393\n",
      "Epoch 20 -- Batch 260/ 842, training loss 0.33972322940826416\n",
      "Epoch 20 -- Batch 261/ 842, training loss 0.3272356688976288\n",
      "Epoch 20 -- Batch 262/ 842, training loss 0.32839053869247437\n",
      "Epoch 20 -- Batch 263/ 842, training loss 0.3336621820926666\n",
      "Epoch 20 -- Batch 264/ 842, training loss 0.3171921670436859\n",
      "Epoch 20 -- Batch 265/ 842, training loss 0.3210212290287018\n",
      "Epoch 20 -- Batch 266/ 842, training loss 0.34150609374046326\n",
      "Epoch 20 -- Batch 267/ 842, training loss 0.32091185450553894\n",
      "Epoch 20 -- Batch 268/ 842, training loss 0.33885475993156433\n",
      "Epoch 20 -- Batch 269/ 842, training loss 0.3231979012489319\n",
      "Epoch 20 -- Batch 270/ 842, training loss 0.3281627297401428\n",
      "Epoch 20 -- Batch 271/ 842, training loss 0.33562716841697693\n",
      "Epoch 20 -- Batch 272/ 842, training loss 0.32286110520362854\n",
      "Epoch 20 -- Batch 273/ 842, training loss 0.3263075351715088\n",
      "Epoch 20 -- Batch 274/ 842, training loss 0.3219916522502899\n",
      "Epoch 20 -- Batch 275/ 842, training loss 0.3193698823451996\n",
      "Epoch 20 -- Batch 276/ 842, training loss 0.33758544921875\n",
      "Epoch 20 -- Batch 277/ 842, training loss 0.3305528461933136\n",
      "Epoch 20 -- Batch 278/ 842, training loss 0.3211507797241211\n",
      "Epoch 20 -- Batch 279/ 842, training loss 0.3258814513683319\n",
      "Epoch 20 -- Batch 280/ 842, training loss 0.3186834752559662\n",
      "Epoch 20 -- Batch 281/ 842, training loss 0.3236740231513977\n",
      "Epoch 20 -- Batch 282/ 842, training loss 0.31981754302978516\n",
      "Epoch 20 -- Batch 283/ 842, training loss 0.32056427001953125\n",
      "Epoch 20 -- Batch 284/ 842, training loss 0.3308335542678833\n",
      "Epoch 20 -- Batch 285/ 842, training loss 0.32742685079574585\n",
      "Epoch 20 -- Batch 286/ 842, training loss 0.3299270570278168\n",
      "Epoch 20 -- Batch 287/ 842, training loss 0.3312889337539673\n",
      "Epoch 20 -- Batch 288/ 842, training loss 0.3310377299785614\n",
      "Epoch 20 -- Batch 289/ 842, training loss 0.32686784863471985\n",
      "Epoch 20 -- Batch 290/ 842, training loss 0.32284966111183167\n",
      "Epoch 20 -- Batch 291/ 842, training loss 0.32707756757736206\n",
      "Epoch 20 -- Batch 292/ 842, training loss 0.3206741213798523\n",
      "Epoch 20 -- Batch 293/ 842, training loss 0.31963178515434265\n",
      "Epoch 20 -- Batch 294/ 842, training loss 0.3184182941913605\n",
      "Epoch 20 -- Batch 295/ 842, training loss 0.3272486627101898\n",
      "Epoch 20 -- Batch 296/ 842, training loss 0.3139548897743225\n",
      "Epoch 20 -- Batch 297/ 842, training loss 0.3201279640197754\n",
      "Epoch 20 -- Batch 298/ 842, training loss 0.316241592168808\n",
      "Epoch 20 -- Batch 299/ 842, training loss 0.33176037669181824\n",
      "Epoch 20 -- Batch 300/ 842, training loss 0.3264464735984802\n",
      "Epoch 20 -- Batch 301/ 842, training loss 0.3268119692802429\n",
      "Epoch 20 -- Batch 302/ 842, training loss 0.32802847027778625\n",
      "Epoch 20 -- Batch 303/ 842, training loss 0.32480740547180176\n",
      "Epoch 20 -- Batch 304/ 842, training loss 0.32567548751831055\n",
      "Epoch 20 -- Batch 305/ 842, training loss 0.33978378772735596\n",
      "Epoch 20 -- Batch 306/ 842, training loss 0.3271162509918213\n",
      "Epoch 20 -- Batch 307/ 842, training loss 0.320788711309433\n",
      "Epoch 20 -- Batch 308/ 842, training loss 0.3408665955066681\n",
      "Epoch 20 -- Batch 309/ 842, training loss 0.3326423466205597\n",
      "Epoch 20 -- Batch 310/ 842, training loss 0.3364051282405853\n",
      "Epoch 20 -- Batch 311/ 842, training loss 0.32761794328689575\n",
      "Epoch 20 -- Batch 312/ 842, training loss 0.31786221265792847\n",
      "Epoch 20 -- Batch 313/ 842, training loss 0.32367902994155884\n",
      "Epoch 20 -- Batch 314/ 842, training loss 0.336172878742218\n",
      "Epoch 20 -- Batch 315/ 842, training loss 0.3357004225254059\n",
      "Epoch 20 -- Batch 316/ 842, training loss 0.3405362367630005\n",
      "Epoch 20 -- Batch 317/ 842, training loss 0.3403173089027405\n",
      "Epoch 20 -- Batch 318/ 842, training loss 0.3256549537181854\n",
      "Epoch 20 -- Batch 319/ 842, training loss 0.3156655728816986\n",
      "Epoch 20 -- Batch 320/ 842, training loss 0.3175041973590851\n",
      "Epoch 20 -- Batch 321/ 842, training loss 0.3391994535923004\n",
      "Epoch 20 -- Batch 322/ 842, training loss 0.3306356966495514\n",
      "Epoch 20 -- Batch 323/ 842, training loss 0.31635990738868713\n",
      "Epoch 20 -- Batch 324/ 842, training loss 0.3351154029369354\n",
      "Epoch 20 -- Batch 325/ 842, training loss 0.3276388943195343\n",
      "Epoch 20 -- Batch 326/ 842, training loss 0.3232478201389313\n",
      "Epoch 20 -- Batch 327/ 842, training loss 0.3319898843765259\n",
      "Epoch 20 -- Batch 328/ 842, training loss 0.3208087980747223\n",
      "Epoch 20 -- Batch 329/ 842, training loss 0.3344769775867462\n",
      "Epoch 20 -- Batch 330/ 842, training loss 0.32183918356895447\n",
      "Epoch 20 -- Batch 331/ 842, training loss 0.31431734561920166\n",
      "Epoch 20 -- Batch 332/ 842, training loss 0.32597169280052185\n",
      "Epoch 20 -- Batch 333/ 842, training loss 0.33375561237335205\n",
      "Epoch 20 -- Batch 334/ 842, training loss 0.3416743278503418\n",
      "Epoch 20 -- Batch 335/ 842, training loss 0.3226719796657562\n",
      "Epoch 20 -- Batch 336/ 842, training loss 0.3244895339012146\n",
      "Epoch 20 -- Batch 337/ 842, training loss 0.3278041481971741\n",
      "Epoch 20 -- Batch 338/ 842, training loss 0.33342862129211426\n",
      "Epoch 20 -- Batch 339/ 842, training loss 0.3153623938560486\n",
      "Epoch 20 -- Batch 340/ 842, training loss 0.33114486932754517\n",
      "Epoch 20 -- Batch 341/ 842, training loss 0.33382245898246765\n",
      "Epoch 20 -- Batch 342/ 842, training loss 0.32156166434288025\n",
      "Epoch 20 -- Batch 343/ 842, training loss 0.3269990384578705\n",
      "Epoch 20 -- Batch 344/ 842, training loss 0.3294406235218048\n",
      "Epoch 20 -- Batch 345/ 842, training loss 0.32853075861930847\n",
      "Epoch 20 -- Batch 346/ 842, training loss 0.32116761803627014\n",
      "Epoch 20 -- Batch 347/ 842, training loss 0.31592610478401184\n",
      "Epoch 20 -- Batch 348/ 842, training loss 0.330422967672348\n",
      "Epoch 20 -- Batch 349/ 842, training loss 0.3251228928565979\n",
      "Epoch 20 -- Batch 350/ 842, training loss 0.33019328117370605\n",
      "Epoch 20 -- Batch 351/ 842, training loss 0.3325372040271759\n",
      "Epoch 20 -- Batch 352/ 842, training loss 0.31543686985969543\n",
      "Epoch 20 -- Batch 353/ 842, training loss 0.3348190486431122\n",
      "Epoch 20 -- Batch 354/ 842, training loss 0.32878726720809937\n",
      "Epoch 20 -- Batch 355/ 842, training loss 0.34351488947868347\n",
      "Epoch 20 -- Batch 356/ 842, training loss 0.3328774571418762\n",
      "Epoch 20 -- Batch 357/ 842, training loss 0.3367904722690582\n",
      "Epoch 20 -- Batch 358/ 842, training loss 0.32530853152275085\n",
      "Epoch 20 -- Batch 359/ 842, training loss 0.3208121955394745\n",
      "Epoch 20 -- Batch 360/ 842, training loss 0.33640798926353455\n",
      "Epoch 20 -- Batch 361/ 842, training loss 0.3284415006637573\n",
      "Epoch 20 -- Batch 362/ 842, training loss 0.3285566568374634\n",
      "Epoch 20 -- Batch 363/ 842, training loss 0.34190425276756287\n",
      "Epoch 20 -- Batch 364/ 842, training loss 0.31875988841056824\n",
      "Epoch 20 -- Batch 365/ 842, training loss 0.33001646399497986\n",
      "Epoch 20 -- Batch 366/ 842, training loss 0.3227171003818512\n",
      "Epoch 20 -- Batch 367/ 842, training loss 0.3378700315952301\n",
      "Epoch 20 -- Batch 368/ 842, training loss 0.3378954529762268\n",
      "Epoch 20 -- Batch 369/ 842, training loss 0.32312822341918945\n",
      "Epoch 20 -- Batch 370/ 842, training loss 0.32379403710365295\n",
      "Epoch 20 -- Batch 371/ 842, training loss 0.34163227677345276\n",
      "Epoch 20 -- Batch 372/ 842, training loss 0.314704030752182\n",
      "Epoch 20 -- Batch 373/ 842, training loss 0.3245277404785156\n",
      "Epoch 20 -- Batch 374/ 842, training loss 0.32955068349838257\n",
      "Epoch 20 -- Batch 375/ 842, training loss 0.32274651527404785\n",
      "Epoch 20 -- Batch 376/ 842, training loss 0.3337330222129822\n",
      "Epoch 20 -- Batch 377/ 842, training loss 0.32529518008232117\n",
      "Epoch 20 -- Batch 378/ 842, training loss 0.3277927339076996\n",
      "Epoch 20 -- Batch 379/ 842, training loss 0.3247672915458679\n",
      "Epoch 20 -- Batch 380/ 842, training loss 0.319210946559906\n",
      "Epoch 20 -- Batch 381/ 842, training loss 0.34009337425231934\n",
      "Epoch 20 -- Batch 382/ 842, training loss 0.3303702473640442\n",
      "Epoch 20 -- Batch 383/ 842, training loss 0.3382357060909271\n",
      "Epoch 20 -- Batch 384/ 842, training loss 0.3182115852832794\n",
      "Epoch 20 -- Batch 385/ 842, training loss 0.33869242668151855\n",
      "Epoch 20 -- Batch 386/ 842, training loss 0.3281123638153076\n",
      "Epoch 20 -- Batch 387/ 842, training loss 0.3238736093044281\n",
      "Epoch 20 -- Batch 388/ 842, training loss 0.32678496837615967\n",
      "Epoch 20 -- Batch 389/ 842, training loss 0.3235393166542053\n",
      "Epoch 20 -- Batch 390/ 842, training loss 0.325467050075531\n",
      "Epoch 20 -- Batch 391/ 842, training loss 0.3299884796142578\n",
      "Epoch 20 -- Batch 392/ 842, training loss 0.34102028608322144\n",
      "Epoch 20 -- Batch 393/ 842, training loss 0.3290197551250458\n",
      "Epoch 20 -- Batch 394/ 842, training loss 0.3216209411621094\n",
      "Epoch 20 -- Batch 395/ 842, training loss 0.3287214934825897\n",
      "Epoch 20 -- Batch 396/ 842, training loss 0.31774672865867615\n",
      "Epoch 20 -- Batch 397/ 842, training loss 0.3346834182739258\n",
      "Epoch 20 -- Batch 398/ 842, training loss 0.32872122526168823\n",
      "Epoch 20 -- Batch 399/ 842, training loss 0.3168288767337799\n",
      "Epoch 20 -- Batch 400/ 842, training loss 0.3226775527000427\n",
      "Epoch 20 -- Batch 401/ 842, training loss 0.3215104639530182\n",
      "Epoch 20 -- Batch 402/ 842, training loss 0.3285042643547058\n",
      "Epoch 20 -- Batch 403/ 842, training loss 0.33367520570755005\n",
      "Epoch 20 -- Batch 404/ 842, training loss 0.3309136629104614\n",
      "Epoch 20 -- Batch 405/ 842, training loss 0.31352049112319946\n",
      "Epoch 20 -- Batch 406/ 842, training loss 0.333038330078125\n",
      "Epoch 20 -- Batch 407/ 842, training loss 0.3177559971809387\n",
      "Epoch 20 -- Batch 408/ 842, training loss 0.32990503311157227\n",
      "Epoch 20 -- Batch 409/ 842, training loss 0.3125936985015869\n",
      "Epoch 20 -- Batch 410/ 842, training loss 0.32428398728370667\n",
      "Epoch 20 -- Batch 411/ 842, training loss 0.3293159604072571\n",
      "Epoch 20 -- Batch 412/ 842, training loss 0.3225213885307312\n",
      "Epoch 20 -- Batch 413/ 842, training loss 0.32875314354896545\n",
      "Epoch 20 -- Batch 414/ 842, training loss 0.32005444169044495\n",
      "Epoch 20 -- Batch 415/ 842, training loss 0.32811272144317627\n",
      "Epoch 20 -- Batch 416/ 842, training loss 0.32911476492881775\n",
      "Epoch 20 -- Batch 417/ 842, training loss 0.32639002799987793\n",
      "Epoch 20 -- Batch 418/ 842, training loss 0.32718077301979065\n",
      "Epoch 20 -- Batch 419/ 842, training loss 0.33499675989151\n",
      "Epoch 20 -- Batch 420/ 842, training loss 0.33001288771629333\n",
      "Epoch 20 -- Batch 421/ 842, training loss 0.3243723511695862\n",
      "Epoch 20 -- Batch 422/ 842, training loss 0.3399497866630554\n",
      "Epoch 20 -- Batch 423/ 842, training loss 0.3198753893375397\n",
      "Epoch 20 -- Batch 424/ 842, training loss 0.3252410590648651\n",
      "Epoch 20 -- Batch 425/ 842, training loss 0.3310413062572479\n",
      "Epoch 20 -- Batch 426/ 842, training loss 0.3159390091896057\n",
      "Epoch 20 -- Batch 427/ 842, training loss 0.3292017877101898\n",
      "Epoch 20 -- Batch 428/ 842, training loss 0.3310820162296295\n",
      "Epoch 20 -- Batch 429/ 842, training loss 0.33193808794021606\n",
      "Epoch 20 -- Batch 430/ 842, training loss 0.32271355390548706\n",
      "Epoch 20 -- Batch 431/ 842, training loss 0.3224973976612091\n",
      "Epoch 20 -- Batch 432/ 842, training loss 0.32068493962287903\n",
      "Epoch 20 -- Batch 433/ 842, training loss 0.3326609134674072\n",
      "Epoch 20 -- Batch 434/ 842, training loss 0.31932786107063293\n",
      "Epoch 20 -- Batch 435/ 842, training loss 0.3284549117088318\n",
      "Epoch 20 -- Batch 436/ 842, training loss 0.3306424915790558\n",
      "Epoch 20 -- Batch 437/ 842, training loss 0.3260241746902466\n",
      "Epoch 20 -- Batch 438/ 842, training loss 0.3324524462223053\n",
      "Epoch 20 -- Batch 439/ 842, training loss 0.3299846947193146\n",
      "Epoch 20 -- Batch 440/ 842, training loss 0.32511603832244873\n",
      "Epoch 20 -- Batch 441/ 842, training loss 0.3170950412750244\n",
      "Epoch 20 -- Batch 442/ 842, training loss 0.3144265413284302\n",
      "Epoch 20 -- Batch 443/ 842, training loss 0.3266404867172241\n",
      "Epoch 20 -- Batch 444/ 842, training loss 0.32195335626602173\n",
      "Epoch 20 -- Batch 445/ 842, training loss 0.32443761825561523\n",
      "Epoch 20 -- Batch 446/ 842, training loss 0.322325199842453\n",
      "Epoch 20 -- Batch 447/ 842, training loss 0.3386783003807068\n",
      "Epoch 20 -- Batch 448/ 842, training loss 0.326445609331131\n",
      "Epoch 20 -- Batch 449/ 842, training loss 0.33339646458625793\n",
      "Epoch 20 -- Batch 450/ 842, training loss 0.3276046812534332\n",
      "Epoch 20 -- Batch 451/ 842, training loss 0.3413625657558441\n",
      "Epoch 20 -- Batch 452/ 842, training loss 0.32538849115371704\n",
      "Epoch 20 -- Batch 453/ 842, training loss 0.337539941072464\n",
      "Epoch 20 -- Batch 454/ 842, training loss 0.32370680570602417\n",
      "Epoch 20 -- Batch 455/ 842, training loss 0.34398916363716125\n",
      "Epoch 20 -- Batch 456/ 842, training loss 0.3276292681694031\n",
      "Epoch 20 -- Batch 457/ 842, training loss 0.3396323025226593\n",
      "Epoch 20 -- Batch 458/ 842, training loss 0.3310791850090027\n",
      "Epoch 20 -- Batch 459/ 842, training loss 0.33119475841522217\n",
      "Epoch 20 -- Batch 460/ 842, training loss 0.3283717930316925\n",
      "Epoch 20 -- Batch 461/ 842, training loss 0.32934895157814026\n",
      "Epoch 20 -- Batch 462/ 842, training loss 0.32330214977264404\n",
      "Epoch 20 -- Batch 463/ 842, training loss 0.325389564037323\n",
      "Epoch 20 -- Batch 464/ 842, training loss 0.32936814427375793\n",
      "Epoch 20 -- Batch 465/ 842, training loss 0.3226015269756317\n",
      "Epoch 20 -- Batch 466/ 842, training loss 0.3384438455104828\n",
      "Epoch 20 -- Batch 467/ 842, training loss 0.33332452178001404\n",
      "Epoch 20 -- Batch 468/ 842, training loss 0.3283822238445282\n",
      "Epoch 20 -- Batch 469/ 842, training loss 0.3392936587333679\n",
      "Epoch 20 -- Batch 470/ 842, training loss 0.3318592309951782\n",
      "Epoch 20 -- Batch 471/ 842, training loss 0.33819904923439026\n",
      "Epoch 20 -- Batch 472/ 842, training loss 0.3277479410171509\n",
      "Epoch 20 -- Batch 473/ 842, training loss 0.32162976264953613\n",
      "Epoch 20 -- Batch 474/ 842, training loss 0.33324089646339417\n",
      "Epoch 20 -- Batch 475/ 842, training loss 0.32394328713417053\n",
      "Epoch 20 -- Batch 476/ 842, training loss 0.31679418683052063\n",
      "Epoch 20 -- Batch 477/ 842, training loss 0.3356075584888458\n",
      "Epoch 20 -- Batch 478/ 842, training loss 0.3334343433380127\n",
      "Epoch 20 -- Batch 479/ 842, training loss 0.3313474953174591\n",
      "Epoch 20 -- Batch 480/ 842, training loss 0.31870827078819275\n",
      "Epoch 20 -- Batch 481/ 842, training loss 0.33183932304382324\n",
      "Epoch 20 -- Batch 482/ 842, training loss 0.3163672089576721\n",
      "Epoch 20 -- Batch 483/ 842, training loss 0.3377324938774109\n",
      "Epoch 20 -- Batch 484/ 842, training loss 0.33507099747657776\n",
      "Epoch 20 -- Batch 485/ 842, training loss 0.3261754810810089\n",
      "Epoch 20 -- Batch 486/ 842, training loss 0.34876859188079834\n",
      "Epoch 20 -- Batch 487/ 842, training loss 0.3235573470592499\n",
      "Epoch 20 -- Batch 488/ 842, training loss 0.32808732986450195\n",
      "Epoch 20 -- Batch 489/ 842, training loss 0.33913281559944153\n",
      "Epoch 20 -- Batch 490/ 842, training loss 0.3286217749118805\n",
      "Epoch 20 -- Batch 491/ 842, training loss 0.31802794337272644\n",
      "Epoch 20 -- Batch 492/ 842, training loss 0.32481327652931213\n",
      "Epoch 20 -- Batch 493/ 842, training loss 0.32561826705932617\n",
      "Epoch 20 -- Batch 494/ 842, training loss 0.35090112686157227\n",
      "Epoch 20 -- Batch 495/ 842, training loss 0.3407432734966278\n",
      "Epoch 20 -- Batch 496/ 842, training loss 0.3246583938598633\n",
      "Epoch 20 -- Batch 497/ 842, training loss 0.34273386001586914\n",
      "Epoch 20 -- Batch 498/ 842, training loss 0.3440496325492859\n",
      "Epoch 20 -- Batch 499/ 842, training loss 0.3408025801181793\n",
      "Epoch 20 -- Batch 500/ 842, training loss 0.3355933129787445\n",
      "Epoch 20 -- Batch 501/ 842, training loss 0.3296889066696167\n",
      "Epoch 20 -- Batch 502/ 842, training loss 0.34558913111686707\n",
      "Epoch 20 -- Batch 503/ 842, training loss 0.33430957794189453\n",
      "Epoch 20 -- Batch 504/ 842, training loss 0.3132884204387665\n",
      "Epoch 20 -- Batch 505/ 842, training loss 0.32649627327919006\n",
      "Epoch 20 -- Batch 506/ 842, training loss 0.3269138038158417\n",
      "Epoch 20 -- Batch 507/ 842, training loss 0.32735174894332886\n",
      "Epoch 20 -- Batch 508/ 842, training loss 0.3284705877304077\n",
      "Epoch 20 -- Batch 509/ 842, training loss 0.3355543911457062\n",
      "Epoch 20 -- Batch 510/ 842, training loss 0.3293292820453644\n",
      "Epoch 20 -- Batch 511/ 842, training loss 0.32388657331466675\n",
      "Epoch 20 -- Batch 512/ 842, training loss 0.33149656653404236\n",
      "Epoch 20 -- Batch 513/ 842, training loss 0.3173600137233734\n",
      "Epoch 20 -- Batch 514/ 842, training loss 0.3197315037250519\n",
      "Epoch 20 -- Batch 515/ 842, training loss 0.33797505497932434\n",
      "Epoch 20 -- Batch 516/ 842, training loss 0.32737529277801514\n",
      "Epoch 20 -- Batch 517/ 842, training loss 0.32981160283088684\n",
      "Epoch 20 -- Batch 518/ 842, training loss 0.34510090947151184\n",
      "Epoch 20 -- Batch 519/ 842, training loss 0.3329082727432251\n",
      "Epoch 20 -- Batch 520/ 842, training loss 0.3322020471096039\n",
      "Epoch 20 -- Batch 521/ 842, training loss 0.321887731552124\n",
      "Epoch 20 -- Batch 522/ 842, training loss 0.34219980239868164\n",
      "Epoch 20 -- Batch 523/ 842, training loss 0.32460737228393555\n",
      "Epoch 20 -- Batch 524/ 842, training loss 0.32050004601478577\n",
      "Epoch 20 -- Batch 525/ 842, training loss 0.3222644329071045\n",
      "Epoch 20 -- Batch 526/ 842, training loss 0.3254680037498474\n",
      "Epoch 20 -- Batch 527/ 842, training loss 0.3239395022392273\n",
      "Epoch 20 -- Batch 528/ 842, training loss 0.328417032957077\n",
      "Epoch 20 -- Batch 529/ 842, training loss 0.3289741575717926\n",
      "Epoch 20 -- Batch 530/ 842, training loss 0.33250486850738525\n",
      "Epoch 20 -- Batch 531/ 842, training loss 0.33565935492515564\n",
      "Epoch 20 -- Batch 532/ 842, training loss 0.3345627188682556\n",
      "Epoch 20 -- Batch 533/ 842, training loss 0.31145936250686646\n",
      "Epoch 20 -- Batch 534/ 842, training loss 0.3445502817630768\n",
      "Epoch 20 -- Batch 535/ 842, training loss 0.33614253997802734\n",
      "Epoch 20 -- Batch 536/ 842, training loss 0.318429559469223\n",
      "Epoch 20 -- Batch 537/ 842, training loss 0.32384297251701355\n",
      "Epoch 20 -- Batch 538/ 842, training loss 0.33373376727104187\n",
      "Epoch 20 -- Batch 539/ 842, training loss 0.3354145884513855\n",
      "Epoch 20 -- Batch 540/ 842, training loss 0.3354957699775696\n",
      "Epoch 20 -- Batch 541/ 842, training loss 0.3353674113750458\n",
      "Epoch 20 -- Batch 542/ 842, training loss 0.32518458366394043\n",
      "Epoch 20 -- Batch 543/ 842, training loss 0.34148144721984863\n",
      "Epoch 20 -- Batch 544/ 842, training loss 0.31815311312675476\n",
      "Epoch 20 -- Batch 545/ 842, training loss 0.3264162242412567\n",
      "Epoch 20 -- Batch 546/ 842, training loss 0.32489097118377686\n",
      "Epoch 20 -- Batch 547/ 842, training loss 0.3192044794559479\n",
      "Epoch 20 -- Batch 548/ 842, training loss 0.3354508876800537\n",
      "Epoch 20 -- Batch 549/ 842, training loss 0.33682164549827576\n",
      "Epoch 20 -- Batch 550/ 842, training loss 0.3250754475593567\n",
      "Epoch 20 -- Batch 551/ 842, training loss 0.3439709544181824\n",
      "Epoch 20 -- Batch 552/ 842, training loss 0.3256157338619232\n",
      "Epoch 20 -- Batch 553/ 842, training loss 0.33547303080558777\n",
      "Epoch 20 -- Batch 554/ 842, training loss 0.3232317268848419\n",
      "Epoch 20 -- Batch 555/ 842, training loss 0.33131536841392517\n",
      "Epoch 20 -- Batch 556/ 842, training loss 0.3289656639099121\n",
      "Epoch 20 -- Batch 557/ 842, training loss 0.33264943957328796\n",
      "Epoch 20 -- Batch 558/ 842, training loss 0.3215665817260742\n",
      "Epoch 20 -- Batch 559/ 842, training loss 0.327942430973053\n",
      "Epoch 20 -- Batch 560/ 842, training loss 0.32693949341773987\n",
      "Epoch 20 -- Batch 561/ 842, training loss 0.32240378856658936\n",
      "Epoch 20 -- Batch 562/ 842, training loss 0.3255566954612732\n",
      "Epoch 20 -- Batch 563/ 842, training loss 0.33952075242996216\n",
      "Epoch 20 -- Batch 564/ 842, training loss 0.3332291543483734\n",
      "Epoch 20 -- Batch 565/ 842, training loss 0.32201626896858215\n",
      "Epoch 20 -- Batch 566/ 842, training loss 0.330923855304718\n",
      "Epoch 20 -- Batch 567/ 842, training loss 0.3213427662849426\n",
      "Epoch 20 -- Batch 568/ 842, training loss 0.32368040084838867\n",
      "Epoch 20 -- Batch 569/ 842, training loss 0.324257493019104\n",
      "Epoch 20 -- Batch 570/ 842, training loss 0.3339225947856903\n",
      "Epoch 20 -- Batch 571/ 842, training loss 0.32257330417633057\n",
      "Epoch 20 -- Batch 572/ 842, training loss 0.3282848596572876\n",
      "Epoch 20 -- Batch 573/ 842, training loss 0.3273945748806\n",
      "Epoch 20 -- Batch 574/ 842, training loss 0.32133111357688904\n",
      "Epoch 20 -- Batch 575/ 842, training loss 0.3112197816371918\n",
      "Epoch 20 -- Batch 576/ 842, training loss 0.33183786273002625\n",
      "Epoch 20 -- Batch 577/ 842, training loss 0.3175741732120514\n",
      "Epoch 20 -- Batch 578/ 842, training loss 0.33389896154403687\n",
      "Epoch 20 -- Batch 579/ 842, training loss 0.335389643907547\n",
      "Epoch 20 -- Batch 580/ 842, training loss 0.34328413009643555\n",
      "Epoch 20 -- Batch 581/ 842, training loss 0.3331804871559143\n",
      "Epoch 20 -- Batch 582/ 842, training loss 0.3318451941013336\n",
      "Epoch 20 -- Batch 583/ 842, training loss 0.3368571996688843\n",
      "Epoch 20 -- Batch 584/ 842, training loss 0.32715141773223877\n",
      "Epoch 20 -- Batch 585/ 842, training loss 0.3243387043476105\n",
      "Epoch 20 -- Batch 586/ 842, training loss 0.3091888427734375\n",
      "Epoch 20 -- Batch 587/ 842, training loss 0.32944926619529724\n",
      "Epoch 20 -- Batch 588/ 842, training loss 0.3191535174846649\n",
      "Epoch 20 -- Batch 589/ 842, training loss 0.32618048787117004\n",
      "Epoch 20 -- Batch 590/ 842, training loss 0.32695233821868896\n",
      "Epoch 20 -- Batch 591/ 842, training loss 0.3336854875087738\n",
      "Epoch 20 -- Batch 592/ 842, training loss 0.32580330967903137\n",
      "Epoch 20 -- Batch 593/ 842, training loss 0.33895906805992126\n",
      "Epoch 20 -- Batch 594/ 842, training loss 0.32656341791152954\n",
      "Epoch 20 -- Batch 595/ 842, training loss 0.33619165420532227\n",
      "Epoch 20 -- Batch 596/ 842, training loss 0.33645951747894287\n",
      "Epoch 20 -- Batch 597/ 842, training loss 0.3306501805782318\n",
      "Epoch 20 -- Batch 598/ 842, training loss 0.3256714642047882\n",
      "Epoch 20 -- Batch 599/ 842, training loss 0.32182303071022034\n",
      "Epoch 20 -- Batch 600/ 842, training loss 0.33162790536880493\n",
      "Epoch 20 -- Batch 601/ 842, training loss 0.3210340738296509\n",
      "Epoch 20 -- Batch 602/ 842, training loss 0.3265701234340668\n",
      "Epoch 20 -- Batch 603/ 842, training loss 0.32540297508239746\n",
      "Epoch 20 -- Batch 604/ 842, training loss 0.33013734221458435\n",
      "Epoch 20 -- Batch 605/ 842, training loss 0.33225521445274353\n",
      "Epoch 20 -- Batch 606/ 842, training loss 0.31668543815612793\n",
      "Epoch 20 -- Batch 607/ 842, training loss 0.32962194085121155\n",
      "Epoch 20 -- Batch 608/ 842, training loss 0.3344987630844116\n",
      "Epoch 20 -- Batch 609/ 842, training loss 0.32310810685157776\n",
      "Epoch 20 -- Batch 610/ 842, training loss 0.3251569867134094\n",
      "Epoch 20 -- Batch 611/ 842, training loss 0.3242260813713074\n",
      "Epoch 20 -- Batch 612/ 842, training loss 0.3237156867980957\n",
      "Epoch 20 -- Batch 613/ 842, training loss 0.3315276801586151\n",
      "Epoch 20 -- Batch 614/ 842, training loss 0.33644571900367737\n",
      "Epoch 20 -- Batch 615/ 842, training loss 0.33383363485336304\n",
      "Epoch 20 -- Batch 616/ 842, training loss 0.3207121789455414\n",
      "Epoch 20 -- Batch 617/ 842, training loss 0.33781856298446655\n",
      "Epoch 20 -- Batch 618/ 842, training loss 0.3230880796909332\n",
      "Epoch 20 -- Batch 619/ 842, training loss 0.33547160029411316\n",
      "Epoch 20 -- Batch 620/ 842, training loss 0.33501577377319336\n",
      "Epoch 20 -- Batch 621/ 842, training loss 0.32179632782936096\n",
      "Epoch 20 -- Batch 622/ 842, training loss 0.3242166340351105\n",
      "Epoch 20 -- Batch 623/ 842, training loss 0.33113959431648254\n",
      "Epoch 20 -- Batch 624/ 842, training loss 0.324091374874115\n",
      "Epoch 20 -- Batch 625/ 842, training loss 0.32948586344718933\n",
      "Epoch 20 -- Batch 626/ 842, training loss 0.32591694593429565\n",
      "Epoch 20 -- Batch 627/ 842, training loss 0.331645131111145\n",
      "Epoch 20 -- Batch 628/ 842, training loss 0.33185601234436035\n",
      "Epoch 20 -- Batch 629/ 842, training loss 0.3263212740421295\n",
      "Epoch 20 -- Batch 630/ 842, training loss 0.3420279622077942\n",
      "Epoch 20 -- Batch 631/ 842, training loss 0.3267104923725128\n",
      "Epoch 20 -- Batch 632/ 842, training loss 0.32972198724746704\n",
      "Epoch 20 -- Batch 633/ 842, training loss 0.33107510209083557\n",
      "Epoch 20 -- Batch 634/ 842, training loss 0.321383535861969\n",
      "Epoch 20 -- Batch 635/ 842, training loss 0.3358394503593445\n",
      "Epoch 20 -- Batch 636/ 842, training loss 0.33529189229011536\n",
      "Epoch 20 -- Batch 637/ 842, training loss 0.32328593730926514\n",
      "Epoch 20 -- Batch 638/ 842, training loss 0.32466861605644226\n",
      "Epoch 20 -- Batch 639/ 842, training loss 0.32530680298805237\n",
      "Epoch 20 -- Batch 640/ 842, training loss 0.3271743953227997\n",
      "Epoch 20 -- Batch 641/ 842, training loss 0.3329346477985382\n",
      "Epoch 20 -- Batch 642/ 842, training loss 0.3317767083644867\n",
      "Epoch 20 -- Batch 643/ 842, training loss 0.3284735679626465\n",
      "Epoch 20 -- Batch 644/ 842, training loss 0.34375354647636414\n",
      "Epoch 20 -- Batch 645/ 842, training loss 0.32471558451652527\n",
      "Epoch 20 -- Batch 646/ 842, training loss 0.324251651763916\n",
      "Epoch 20 -- Batch 647/ 842, training loss 0.34039202332496643\n",
      "Epoch 20 -- Batch 648/ 842, training loss 0.3353101909160614\n",
      "Epoch 20 -- Batch 649/ 842, training loss 0.31683269143104553\n",
      "Epoch 20 -- Batch 650/ 842, training loss 0.3325679898262024\n",
      "Epoch 20 -- Batch 651/ 842, training loss 0.3284963071346283\n",
      "Epoch 20 -- Batch 652/ 842, training loss 0.3331408202648163\n",
      "Epoch 20 -- Batch 653/ 842, training loss 0.32567527890205383\n",
      "Epoch 20 -- Batch 654/ 842, training loss 0.33317577838897705\n",
      "Epoch 20 -- Batch 655/ 842, training loss 0.31547194719314575\n",
      "Epoch 20 -- Batch 656/ 842, training loss 0.33553197979927063\n",
      "Epoch 20 -- Batch 657/ 842, training loss 0.33529528975486755\n",
      "Epoch 20 -- Batch 658/ 842, training loss 0.33072036504745483\n",
      "Epoch 20 -- Batch 659/ 842, training loss 0.32972779870033264\n",
      "Epoch 20 -- Batch 660/ 842, training loss 0.33005258440971375\n",
      "Epoch 20 -- Batch 661/ 842, training loss 0.34077659249305725\n",
      "Epoch 20 -- Batch 662/ 842, training loss 0.33559325337409973\n",
      "Epoch 20 -- Batch 663/ 842, training loss 0.33771711587905884\n",
      "Epoch 20 -- Batch 664/ 842, training loss 0.32664597034454346\n",
      "Epoch 20 -- Batch 665/ 842, training loss 0.3256160020828247\n",
      "Epoch 20 -- Batch 666/ 842, training loss 0.3186641037464142\n",
      "Epoch 20 -- Batch 667/ 842, training loss 0.34705421328544617\n",
      "Epoch 20 -- Batch 668/ 842, training loss 0.3221210539340973\n",
      "Epoch 20 -- Batch 669/ 842, training loss 0.3320184648036957\n",
      "Epoch 20 -- Batch 670/ 842, training loss 0.34046024084091187\n",
      "Epoch 20 -- Batch 671/ 842, training loss 0.32690057158470154\n",
      "Epoch 20 -- Batch 672/ 842, training loss 0.32611083984375\n",
      "Epoch 20 -- Batch 673/ 842, training loss 0.33124539256095886\n",
      "Epoch 20 -- Batch 674/ 842, training loss 0.3229479193687439\n",
      "Epoch 20 -- Batch 675/ 842, training loss 0.32804256677627563\n",
      "Epoch 20 -- Batch 676/ 842, training loss 0.33817118406295776\n",
      "Epoch 20 -- Batch 677/ 842, training loss 0.3174187242984772\n",
      "Epoch 20 -- Batch 678/ 842, training loss 0.34061500430107117\n",
      "Epoch 20 -- Batch 679/ 842, training loss 0.34685924649238586\n",
      "Epoch 20 -- Batch 680/ 842, training loss 0.33467844128608704\n",
      "Epoch 20 -- Batch 681/ 842, training loss 0.31816646456718445\n",
      "Epoch 20 -- Batch 682/ 842, training loss 0.33544814586639404\n",
      "Epoch 20 -- Batch 683/ 842, training loss 0.3283873498439789\n",
      "Epoch 20 -- Batch 684/ 842, training loss 0.34898078441619873\n",
      "Epoch 20 -- Batch 685/ 842, training loss 0.3237898051738739\n",
      "Epoch 20 -- Batch 686/ 842, training loss 0.32712578773498535\n",
      "Epoch 20 -- Batch 687/ 842, training loss 0.33142465353012085\n",
      "Epoch 20 -- Batch 688/ 842, training loss 0.329133003950119\n",
      "Epoch 20 -- Batch 689/ 842, training loss 0.3237548768520355\n",
      "Epoch 20 -- Batch 690/ 842, training loss 0.32904550433158875\n",
      "Epoch 20 -- Batch 691/ 842, training loss 0.332042932510376\n",
      "Epoch 20 -- Batch 692/ 842, training loss 0.32084786891937256\n",
      "Epoch 20 -- Batch 693/ 842, training loss 0.32378295063972473\n",
      "Epoch 20 -- Batch 694/ 842, training loss 0.33719223737716675\n",
      "Epoch 20 -- Batch 695/ 842, training loss 0.3282010853290558\n",
      "Epoch 20 -- Batch 696/ 842, training loss 0.33456411957740784\n",
      "Epoch 20 -- Batch 697/ 842, training loss 0.33399879932403564\n",
      "Epoch 20 -- Batch 698/ 842, training loss 0.31938421726226807\n",
      "Epoch 20 -- Batch 699/ 842, training loss 0.32648923993110657\n",
      "Epoch 20 -- Batch 700/ 842, training loss 0.3287205696105957\n",
      "Epoch 20 -- Batch 701/ 842, training loss 0.3343322277069092\n",
      "Epoch 20 -- Batch 702/ 842, training loss 0.32472795248031616\n",
      "Epoch 20 -- Batch 703/ 842, training loss 0.3300313949584961\n",
      "Epoch 20 -- Batch 704/ 842, training loss 0.33118826150894165\n",
      "Epoch 20 -- Batch 705/ 842, training loss 0.31145787239074707\n",
      "Epoch 20 -- Batch 706/ 842, training loss 0.3425907790660858\n",
      "Epoch 20 -- Batch 707/ 842, training loss 0.32514604926109314\n",
      "Epoch 20 -- Batch 708/ 842, training loss 0.33884406089782715\n",
      "Epoch 20 -- Batch 709/ 842, training loss 0.32417336106300354\n",
      "Epoch 20 -- Batch 710/ 842, training loss 0.3251709043979645\n",
      "Epoch 20 -- Batch 711/ 842, training loss 0.3298015594482422\n",
      "Epoch 20 -- Batch 712/ 842, training loss 0.3316783308982849\n",
      "Epoch 20 -- Batch 713/ 842, training loss 0.33256956934928894\n",
      "Epoch 20 -- Batch 714/ 842, training loss 0.32527169585227966\n",
      "Epoch 20 -- Batch 715/ 842, training loss 0.32609885931015015\n",
      "Epoch 20 -- Batch 716/ 842, training loss 0.33252206444740295\n",
      "Epoch 20 -- Batch 717/ 842, training loss 0.33404356241226196\n",
      "Epoch 20 -- Batch 718/ 842, training loss 0.3195338845252991\n",
      "Epoch 20 -- Batch 719/ 842, training loss 0.3208051025867462\n",
      "Epoch 20 -- Batch 720/ 842, training loss 0.3287440240383148\n",
      "Epoch 20 -- Batch 721/ 842, training loss 0.31731387972831726\n",
      "Epoch 20 -- Batch 722/ 842, training loss 0.32471245527267456\n",
      "Epoch 20 -- Batch 723/ 842, training loss 0.33467307686805725\n",
      "Epoch 20 -- Batch 724/ 842, training loss 0.3271065652370453\n",
      "Epoch 20 -- Batch 725/ 842, training loss 0.3319230377674103\n",
      "Epoch 20 -- Batch 726/ 842, training loss 0.34501132369041443\n",
      "Epoch 20 -- Batch 727/ 842, training loss 0.32117435336112976\n",
      "Epoch 20 -- Batch 728/ 842, training loss 0.3207157850265503\n",
      "Epoch 20 -- Batch 729/ 842, training loss 0.3277825117111206\n",
      "Epoch 20 -- Batch 730/ 842, training loss 0.3291906714439392\n",
      "Epoch 20 -- Batch 731/ 842, training loss 0.32291004061698914\n",
      "Epoch 20 -- Batch 732/ 842, training loss 0.3235090672969818\n",
      "Epoch 20 -- Batch 733/ 842, training loss 0.32456332445144653\n",
      "Epoch 20 -- Batch 734/ 842, training loss 0.328296959400177\n",
      "Epoch 20 -- Batch 735/ 842, training loss 0.3383038640022278\n",
      "Epoch 20 -- Batch 736/ 842, training loss 0.3328813314437866\n",
      "Epoch 20 -- Batch 737/ 842, training loss 0.3319348394870758\n",
      "Epoch 20 -- Batch 738/ 842, training loss 0.3412769138813019\n",
      "Epoch 20 -- Batch 739/ 842, training loss 0.3252936005592346\n",
      "Epoch 20 -- Batch 740/ 842, training loss 0.32895177602767944\n",
      "Epoch 20 -- Batch 741/ 842, training loss 0.3423505425453186\n",
      "Epoch 20 -- Batch 742/ 842, training loss 0.3282214403152466\n",
      "Epoch 20 -- Batch 743/ 842, training loss 0.32995325326919556\n",
      "Epoch 20 -- Batch 744/ 842, training loss 0.3199370801448822\n",
      "Epoch 20 -- Batch 745/ 842, training loss 0.3275015354156494\n",
      "Epoch 20 -- Batch 746/ 842, training loss 0.346564382314682\n",
      "Epoch 20 -- Batch 747/ 842, training loss 0.3391552269458771\n",
      "Epoch 20 -- Batch 748/ 842, training loss 0.3204118013381958\n",
      "Epoch 20 -- Batch 749/ 842, training loss 0.3463795781135559\n",
      "Epoch 20 -- Batch 750/ 842, training loss 0.3202483355998993\n",
      "Epoch 20 -- Batch 751/ 842, training loss 0.33485719561576843\n",
      "Epoch 20 -- Batch 752/ 842, training loss 0.3281039297580719\n",
      "Epoch 20 -- Batch 753/ 842, training loss 0.325499951839447\n",
      "Epoch 20 -- Batch 754/ 842, training loss 0.32064151763916016\n",
      "Epoch 20 -- Batch 755/ 842, training loss 0.32492902874946594\n",
      "Epoch 20 -- Batch 756/ 842, training loss 0.33372583985328674\n",
      "Epoch 20 -- Batch 757/ 842, training loss 0.32291850447654724\n",
      "Epoch 20 -- Batch 758/ 842, training loss 0.34059765934944153\n",
      "Epoch 20 -- Batch 759/ 842, training loss 0.33513206243515015\n",
      "Epoch 20 -- Batch 760/ 842, training loss 0.33187612891197205\n",
      "Epoch 20 -- Batch 761/ 842, training loss 0.3401030898094177\n",
      "Epoch 20 -- Batch 762/ 842, training loss 0.3432265818119049\n",
      "Epoch 20 -- Batch 763/ 842, training loss 0.33471259474754333\n",
      "Epoch 20 -- Batch 764/ 842, training loss 0.3373875916004181\n",
      "Epoch 20 -- Batch 765/ 842, training loss 0.3178343176841736\n",
      "Epoch 20 -- Batch 766/ 842, training loss 0.32219353318214417\n",
      "Epoch 20 -- Batch 767/ 842, training loss 0.3374040126800537\n",
      "Epoch 20 -- Batch 768/ 842, training loss 0.31334561109542847\n",
      "Epoch 20 -- Batch 769/ 842, training loss 0.32933223247528076\n",
      "Epoch 20 -- Batch 770/ 842, training loss 0.3255326747894287\n",
      "Epoch 20 -- Batch 771/ 842, training loss 0.319819837808609\n",
      "Epoch 20 -- Batch 772/ 842, training loss 0.32415494322776794\n",
      "Epoch 20 -- Batch 773/ 842, training loss 0.33783766627311707\n",
      "Epoch 20 -- Batch 774/ 842, training loss 0.33113306760787964\n",
      "Epoch 20 -- Batch 775/ 842, training loss 0.335146427154541\n",
      "Epoch 20 -- Batch 776/ 842, training loss 0.3353588879108429\n",
      "Epoch 20 -- Batch 777/ 842, training loss 0.32859423756599426\n",
      "Epoch 20 -- Batch 778/ 842, training loss 0.31525933742523193\n",
      "Epoch 20 -- Batch 779/ 842, training loss 0.34483346343040466\n",
      "Epoch 20 -- Batch 780/ 842, training loss 0.33047565817832947\n",
      "Epoch 20 -- Batch 781/ 842, training loss 0.3365314304828644\n",
      "Epoch 20 -- Batch 782/ 842, training loss 0.3402596116065979\n",
      "Epoch 20 -- Batch 783/ 842, training loss 0.3440614640712738\n",
      "Epoch 20 -- Batch 784/ 842, training loss 0.3271220028400421\n",
      "Epoch 20 -- Batch 785/ 842, training loss 0.327402263879776\n",
      "Epoch 20 -- Batch 786/ 842, training loss 0.33429983258247375\n",
      "Epoch 20 -- Batch 787/ 842, training loss 0.3226420283317566\n",
      "Epoch 20 -- Batch 788/ 842, training loss 0.3258122205734253\n",
      "Epoch 20 -- Batch 789/ 842, training loss 0.3350166380405426\n",
      "Epoch 20 -- Batch 790/ 842, training loss 0.32152366638183594\n",
      "Epoch 20 -- Batch 791/ 842, training loss 0.32964012026786804\n",
      "Epoch 20 -- Batch 792/ 842, training loss 0.33070170879364014\n",
      "Epoch 20 -- Batch 793/ 842, training loss 0.33375248312950134\n",
      "Epoch 20 -- Batch 794/ 842, training loss 0.3373017907142639\n",
      "Epoch 20 -- Batch 795/ 842, training loss 0.33256638050079346\n",
      "Epoch 20 -- Batch 796/ 842, training loss 0.3334719240665436\n",
      "Epoch 20 -- Batch 797/ 842, training loss 0.3229750990867615\n",
      "Epoch 20 -- Batch 798/ 842, training loss 0.3307179808616638\n",
      "Epoch 20 -- Batch 799/ 842, training loss 0.32199087738990784\n",
      "Epoch 20 -- Batch 800/ 842, training loss 0.32940754294395447\n",
      "Epoch 20 -- Batch 801/ 842, training loss 0.33087578415870667\n",
      "Epoch 20 -- Batch 802/ 842, training loss 0.3325931131839752\n",
      "Epoch 20 -- Batch 803/ 842, training loss 0.3362182378768921\n",
      "Epoch 20 -- Batch 804/ 842, training loss 0.327163428068161\n",
      "Epoch 20 -- Batch 805/ 842, training loss 0.3291395604610443\n",
      "Epoch 20 -- Batch 806/ 842, training loss 0.3298698365688324\n",
      "Epoch 20 -- Batch 807/ 842, training loss 0.3232118487358093\n",
      "Epoch 20 -- Batch 808/ 842, training loss 0.32430192828178406\n",
      "Epoch 20 -- Batch 809/ 842, training loss 0.33193978667259216\n",
      "Epoch 20 -- Batch 810/ 842, training loss 0.3234708905220032\n",
      "Epoch 20 -- Batch 811/ 842, training loss 0.33986008167266846\n",
      "Epoch 20 -- Batch 812/ 842, training loss 0.3443405032157898\n",
      "Epoch 20 -- Batch 813/ 842, training loss 0.32742106914520264\n",
      "Epoch 20 -- Batch 814/ 842, training loss 0.3286415636539459\n",
      "Epoch 20 -- Batch 815/ 842, training loss 0.3285052180290222\n",
      "Epoch 20 -- Batch 816/ 842, training loss 0.3200705945491791\n",
      "Epoch 20 -- Batch 817/ 842, training loss 0.32845816016197205\n",
      "Epoch 20 -- Batch 818/ 842, training loss 0.3379392623901367\n",
      "Epoch 20 -- Batch 819/ 842, training loss 0.3359934389591217\n",
      "Epoch 20 -- Batch 820/ 842, training loss 0.32714009284973145\n",
      "Epoch 20 -- Batch 821/ 842, training loss 0.3230331540107727\n",
      "Epoch 20 -- Batch 822/ 842, training loss 0.3349646329879761\n",
      "Epoch 20 -- Batch 823/ 842, training loss 0.335301011800766\n",
      "Epoch 20 -- Batch 824/ 842, training loss 0.3289967179298401\n",
      "Epoch 20 -- Batch 825/ 842, training loss 0.32413315773010254\n",
      "Epoch 20 -- Batch 826/ 842, training loss 0.32685935497283936\n",
      "Epoch 20 -- Batch 827/ 842, training loss 0.32335594296455383\n",
      "Epoch 20 -- Batch 828/ 842, training loss 0.32118290662765503\n",
      "Epoch 20 -- Batch 829/ 842, training loss 0.32940059900283813\n",
      "Epoch 20 -- Batch 830/ 842, training loss 0.34133797883987427\n",
      "Epoch 20 -- Batch 831/ 842, training loss 0.32842960953712463\n",
      "Epoch 20 -- Batch 832/ 842, training loss 0.34058618545532227\n",
      "Epoch 20 -- Batch 833/ 842, training loss 0.3435324430465698\n",
      "Epoch 20 -- Batch 834/ 842, training loss 0.3347773849964142\n",
      "Epoch 20 -- Batch 835/ 842, training loss 0.31976833939552307\n",
      "Epoch 20 -- Batch 836/ 842, training loss 0.32828211784362793\n",
      "Epoch 20 -- Batch 837/ 842, training loss 0.3346411883831024\n",
      "Epoch 20 -- Batch 838/ 842, training loss 0.31893664598464966\n",
      "Epoch 20 -- Batch 839/ 842, training loss 0.33773013949394226\n",
      "Epoch 20 -- Batch 840/ 842, training loss 0.3158729076385498\n",
      "Epoch 20 -- Batch 841/ 842, training loss 0.3224011957645416\n",
      "Epoch 20 -- Batch 842/ 842, training loss 0.3939918875694275\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20 -- Batch 1/ 94, validation loss 0.31809189915657043\n",
      "Epoch 20 -- Batch 2/ 94, validation loss 0.3301788568496704\n",
      "Epoch 20 -- Batch 3/ 94, validation loss 0.31451618671417236\n",
      "Epoch 20 -- Batch 4/ 94, validation loss 0.33000245690345764\n",
      "Epoch 20 -- Batch 5/ 94, validation loss 0.3100441098213196\n",
      "Epoch 20 -- Batch 6/ 94, validation loss 0.3253827691078186\n",
      "Epoch 20 -- Batch 7/ 94, validation loss 0.3310694098472595\n",
      "Epoch 20 -- Batch 8/ 94, validation loss 0.3181818127632141\n",
      "Epoch 20 -- Batch 9/ 94, validation loss 0.31834033131599426\n",
      "Epoch 20 -- Batch 10/ 94, validation loss 0.32447806000709534\n",
      "Epoch 20 -- Batch 11/ 94, validation loss 0.32283228635787964\n",
      "Epoch 20 -- Batch 12/ 94, validation loss 0.3227421045303345\n",
      "Epoch 20 -- Batch 13/ 94, validation loss 0.32432281970977783\n",
      "Epoch 20 -- Batch 14/ 94, validation loss 0.322920024394989\n",
      "Epoch 20 -- Batch 15/ 94, validation loss 0.3245628774166107\n",
      "Epoch 20 -- Batch 16/ 94, validation loss 0.31799301505088806\n",
      "Epoch 20 -- Batch 17/ 94, validation loss 0.3358443081378937\n",
      "Epoch 20 -- Batch 18/ 94, validation loss 0.314577579498291\n",
      "Epoch 20 -- Batch 19/ 94, validation loss 0.32897770404815674\n",
      "Epoch 20 -- Batch 20/ 94, validation loss 0.3202570974826813\n",
      "Epoch 20 -- Batch 21/ 94, validation loss 0.3142002522945404\n",
      "Epoch 20 -- Batch 22/ 94, validation loss 0.3083752393722534\n",
      "Epoch 20 -- Batch 23/ 94, validation loss 0.3486938178539276\n",
      "Epoch 20 -- Batch 24/ 94, validation loss 0.32093021273612976\n",
      "Epoch 20 -- Batch 25/ 94, validation loss 0.3230653405189514\n",
      "Epoch 20 -- Batch 26/ 94, validation loss 0.322302907705307\n",
      "Epoch 20 -- Batch 27/ 94, validation loss 0.32944148778915405\n",
      "Epoch 20 -- Batch 28/ 94, validation loss 0.3203366994857788\n",
      "Epoch 20 -- Batch 29/ 94, validation loss 0.3193262815475464\n",
      "Epoch 20 -- Batch 30/ 94, validation loss 0.3112223446369171\n",
      "Epoch 20 -- Batch 31/ 94, validation loss 0.32487791776657104\n",
      "Epoch 20 -- Batch 32/ 94, validation loss 0.3212660551071167\n",
      "Epoch 20 -- Batch 33/ 94, validation loss 0.3465460538864136\n",
      "Epoch 20 -- Batch 34/ 94, validation loss 0.32072967290878296\n",
      "Epoch 20 -- Batch 35/ 94, validation loss 0.3530149459838867\n",
      "Epoch 20 -- Batch 36/ 94, validation loss 0.3213579058647156\n",
      "Epoch 20 -- Batch 37/ 94, validation loss 0.3130408823490143\n",
      "Epoch 20 -- Batch 38/ 94, validation loss 0.30925223231315613\n",
      "Epoch 20 -- Batch 39/ 94, validation loss 0.32586348056793213\n",
      "Epoch 20 -- Batch 40/ 94, validation loss 0.3313349187374115\n",
      "Epoch 20 -- Batch 41/ 94, validation loss 0.3258054554462433\n",
      "Epoch 20 -- Batch 42/ 94, validation loss 0.3253251612186432\n",
      "Epoch 20 -- Batch 43/ 94, validation loss 0.3163078725337982\n",
      "Epoch 20 -- Batch 44/ 94, validation loss 0.3177623152732849\n",
      "Epoch 20 -- Batch 45/ 94, validation loss 0.3255651891231537\n",
      "Epoch 20 -- Batch 46/ 94, validation loss 0.318069189786911\n",
      "Epoch 20 -- Batch 47/ 94, validation loss 0.3097791075706482\n",
      "Epoch 20 -- Batch 48/ 94, validation loss 0.3117136061191559\n",
      "Epoch 20 -- Batch 49/ 94, validation loss 0.3209953308105469\n",
      "Epoch 20 -- Batch 50/ 94, validation loss 0.3229382634162903\n",
      "Epoch 20 -- Batch 51/ 94, validation loss 0.3227253556251526\n",
      "Epoch 20 -- Batch 52/ 94, validation loss 0.31718939542770386\n",
      "Epoch 20 -- Batch 53/ 94, validation loss 0.3398909568786621\n",
      "Epoch 20 -- Batch 54/ 94, validation loss 0.31453242897987366\n",
      "Epoch 20 -- Batch 55/ 94, validation loss 0.33128729462623596\n",
      "Epoch 20 -- Batch 56/ 94, validation loss 0.32227081060409546\n",
      "Epoch 20 -- Batch 57/ 94, validation loss 0.3333820104598999\n",
      "Epoch 20 -- Batch 58/ 94, validation loss 0.3122040927410126\n",
      "Epoch 20 -- Batch 59/ 94, validation loss 0.31704050302505493\n",
      "Epoch 20 -- Batch 60/ 94, validation loss 0.313417911529541\n",
      "Epoch 20 -- Batch 61/ 94, validation loss 0.33208441734313965\n",
      "Epoch 20 -- Batch 62/ 94, validation loss 0.3153012692928314\n",
      "Epoch 20 -- Batch 63/ 94, validation loss 0.32038414478302\n",
      "Epoch 20 -- Batch 64/ 94, validation loss 0.31515592336654663\n",
      "Epoch 20 -- Batch 65/ 94, validation loss 0.30356383323669434\n",
      "Epoch 20 -- Batch 66/ 94, validation loss 0.3286527395248413\n",
      "Epoch 20 -- Batch 67/ 94, validation loss 0.3194590210914612\n",
      "Epoch 20 -- Batch 68/ 94, validation loss 0.3144683241844177\n",
      "Epoch 20 -- Batch 69/ 94, validation loss 0.31978753209114075\n",
      "Epoch 20 -- Batch 70/ 94, validation loss 0.33543860912323\n",
      "Epoch 20 -- Batch 71/ 94, validation loss 0.32199209928512573\n",
      "Epoch 20 -- Batch 72/ 94, validation loss 0.3276093900203705\n",
      "Epoch 20 -- Batch 73/ 94, validation loss 0.31554317474365234\n",
      "Epoch 20 -- Batch 74/ 94, validation loss 0.32396650314331055\n",
      "Epoch 20 -- Batch 75/ 94, validation loss 0.3183808922767639\n",
      "Epoch 20 -- Batch 76/ 94, validation loss 0.3330928683280945\n",
      "Epoch 20 -- Batch 77/ 94, validation loss 0.32202380895614624\n",
      "Epoch 20 -- Batch 78/ 94, validation loss 0.32949528098106384\n",
      "Epoch 20 -- Batch 79/ 94, validation loss 0.3146381080150604\n",
      "Epoch 20 -- Batch 80/ 94, validation loss 0.3248348832130432\n",
      "Epoch 20 -- Batch 81/ 94, validation loss 0.3178415298461914\n",
      "Epoch 20 -- Batch 82/ 94, validation loss 0.3302708566188812\n",
      "Epoch 20 -- Batch 83/ 94, validation loss 0.33595484495162964\n",
      "Epoch 20 -- Batch 84/ 94, validation loss 0.30511391162872314\n",
      "Epoch 20 -- Batch 85/ 94, validation loss 0.32733574509620667\n",
      "Epoch 20 -- Batch 86/ 94, validation loss 0.32707101106643677\n",
      "Epoch 20 -- Batch 87/ 94, validation loss 0.3143577575683594\n",
      "Epoch 20 -- Batch 88/ 94, validation loss 0.3072393834590912\n",
      "Epoch 20 -- Batch 89/ 94, validation loss 0.30894678831100464\n",
      "Epoch 20 -- Batch 90/ 94, validation loss 0.3145381808280945\n",
      "Epoch 20 -- Batch 91/ 94, validation loss 0.3164336383342743\n",
      "Epoch 20 -- Batch 92/ 94, validation loss 0.32070088386535645\n",
      "Epoch 20 -- Batch 93/ 94, validation loss 0.32439422607421875\n",
      "Epoch 20 -- Batch 94/ 94, validation loss 0.32886379957199097\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20 loss: Training 0.32822054624557495, Validation 0.32886379957199097\n",
      "----------------------------------------------------------------------\n",
      "Epoch 21/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: CNC(=O)C12CC3(C))CC(C)(CC(C)(C3)C1)C2\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'CNC(=O)C12CC3(C))CC(C)(CC(C)(C3)C1)C2' for input: 'CNC(=O)C12CC3(C))CC(C)(CC(C)(C3)C1)C2'\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'CC(C)[C@@H]1C2C=CC(OC(=O)C2CC2)C(=O)[C@@H]1CCO'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 6 7 8 28 29\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'c1ccc(-n2ncc(-c3ccc4cn[nH]c3c3CC2C(=O)OC3CCCCC3)cc2)cc1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 22 23 24 25 29 30 31\n",
      "[19:07:11] Explicit valence for atom # 10 N, 5, is greater than permitted\n",
      "[19:07:11] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 9 10 14 24 25 26 27 28\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'CNc1ccc([N+](=O)[O-])cc1-n1c2ccccc3cccc2c2ccccc21'\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1nnc(-c2ccc2c(c2)OCO3)o1)NCC1CCCO1'\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(C)c2nc(NC(=O)CCN3C(=O)c4cccc([N+](=O)[O-])c4C3=O)sc3c2C)c1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C)c2nc(NC(=O)CCN3C(=O)c4cccc([N+](=O)[O-])c4C3=O)sc3c2C)c1' for input: 'Cc1cc(C)c2nc(NC(=O)CCN3C(=O)c4cccc([N+](=O)[O-])c4C3=O)sc3c2C)c1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 24 25\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 10 18 19 20 21 22 23\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CNC(=O)CN(c1cccc2ccccc1)S(C)(=O)=O'\n",
      "[19:07:11] SMILES Parse Error: extra open parentheses for input: 'CCOc1ncccc1CN1CCCC(Cn2cc(C(F)(F)F)nn21'\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: CC(C)(NC(=S)NC(=O)c1cccs1)C(=O)NN)c1ccccc1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'CC(C)(NC(=S)NC(=O)c1cccs1)C(=O)NN)c1ccccc1' for input: 'CC(C)(NC(=S)NC(=O)c1cccs1)C(=O)NN)c1ccccc1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCN2C(=O)CC(C(=O)Nc3ccc(C(=O)O)cc3)SC2=S)NC1=NCC1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 8 9 20 21 22 23 24 25 26\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 8 9 17 18 19 20 27\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1sc(NC(=O)c1ccc3c(c1)OCCO2)c1cnn(C)c1'\n",
      "[19:07:11] Explicit valence for atom # 8 C, 5, is greater than permitted\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'N#Cc1ccc(C2=Nc3ccccc3NC2(c2ccccc2Cl)SC3)cc1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 26 27 28 30\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 5 6 7 19 22 23 25 26 27\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 3 19 20 21 22\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: O=C(Oc1c(Br)cc(Br)cc1Br)c1ccco1)c1ccccc1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'O=C(Oc1c(Br)cc(Br)cc1Br)c1ccco1)c1ccccc1' for input: 'O=C(Oc1c(Br)cc(Br)cc1Br)c1ccco1)c1ccccc1'\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: c1ccc(CCN2CNc3nc(Nc4ccc(-n5ccnc5)cc4)ncn3)CC2)cc1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'c1ccc(CCN2CNc3nc(Nc4ccc(-n5ccnc5)cc4)ncn3)CC2)cc1' for input: 'c1ccc(CCN2CNc3nc(Nc4ccc(-n5ccnc5)cc4)ncn3)CC2)cc1'\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: N#Cc1ccc(C(=O)N(CCN(C)Cc2ccccc2)C2CC2)cc1Cl)cc1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'N#Cc1ccc(C(=O)N(CCN(C)Cc2ccccc2)C2CC2)cc1Cl)cc1' for input: 'N#Cc1ccc(C(=O)N(CCN(C)Cc2ccccc2)C2CC2)cc1Cl)cc1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 26 27 28\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: COc1cc2c(cc1OC)n(CC(=O)Nc1ccc3c(c1)OCCO3)n2C(C)C)c1ccccc1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'COc1cc2c(cc1OC)n(CC(=O)Nc1ccc3c(c1)OCCO3)n2C(C)C)c1ccccc1' for input: 'COc1cc2c(cc1OC)n(CC(=O)Nc1ccc3c(c1)OCCO3)n2C(C)C)c1ccccc1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 28\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 13\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N2C(=O)NC(=O)C3(CCc4ccccc4NC(=O)Nc3cccc(OC(C)=O)c4)C2=O)c1'\n",
      "[19:07:11] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccc2c(=O)n(-c3ccc(F)c(Cl)c3)c(=S)[nH]c2c1)N1CCC(C2)CC1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 18\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 2 3 4 39 40\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 5 6 17 18 19 20 21 22 23\n",
      "[19:07:11] SMILES Parse Error: extra close parentheses while parsing: CCOC(=O)C1CCN(c2cc([N+](=O)[O-])c(OC(Cl)Cl)nc2C)c2ccccc2C)CC1\n",
      "[19:07:11] SMILES Parse Error: Failed parsing SMILES 'CCOC(=O)C1CCN(c2cc([N+](=O)[O-])c(OC(Cl)Cl)nc2C)c2ccccc2C)CC1' for input: 'CCOC(=O)C1CCN(c2cc([N+](=O)[O-])c(OC(Cl)Cl)nc2C)c2ccccc2C)CC1'\n",
      "[19:07:11] Can't kekulize mol.  Unkekulized atoms: 2 3 6 7 8 10 11 18 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 -- Batch 1/ 842, training loss 0.3246091604232788\n",
      "Epoch 21 -- Batch 2/ 842, training loss 0.32173535227775574\n",
      "Epoch 21 -- Batch 3/ 842, training loss 0.3143523633480072\n",
      "Epoch 21 -- Batch 4/ 842, training loss 0.3237268030643463\n",
      "Epoch 21 -- Batch 5/ 842, training loss 0.3294360041618347\n",
      "Epoch 21 -- Batch 6/ 842, training loss 0.33539897203445435\n",
      "Epoch 21 -- Batch 7/ 842, training loss 0.3225676417350769\n",
      "Epoch 21 -- Batch 8/ 842, training loss 0.3319466710090637\n",
      "Epoch 21 -- Batch 9/ 842, training loss 0.3291208744049072\n",
      "Epoch 21 -- Batch 10/ 842, training loss 0.33804795145988464\n",
      "Epoch 21 -- Batch 11/ 842, training loss 0.3358694911003113\n",
      "Epoch 21 -- Batch 12/ 842, training loss 0.32413244247436523\n",
      "Epoch 21 -- Batch 13/ 842, training loss 0.319232314825058\n",
      "Epoch 21 -- Batch 14/ 842, training loss 0.3236576318740845\n",
      "Epoch 21 -- Batch 15/ 842, training loss 0.31051895022392273\n",
      "Epoch 21 -- Batch 16/ 842, training loss 0.31000804901123047\n",
      "Epoch 21 -- Batch 17/ 842, training loss 0.33020296692848206\n",
      "Epoch 21 -- Batch 18/ 842, training loss 0.3323690891265869\n",
      "Epoch 21 -- Batch 19/ 842, training loss 0.3305215537548065\n",
      "Epoch 21 -- Batch 20/ 842, training loss 0.31443408131599426\n",
      "Epoch 21 -- Batch 21/ 842, training loss 0.3341834545135498\n",
      "Epoch 21 -- Batch 22/ 842, training loss 0.3109833002090454\n",
      "Epoch 21 -- Batch 23/ 842, training loss 0.32288792729377747\n",
      "Epoch 21 -- Batch 24/ 842, training loss 0.3165763318538666\n",
      "Epoch 21 -- Batch 25/ 842, training loss 0.31291767954826355\n",
      "Epoch 21 -- Batch 26/ 842, training loss 0.3313959538936615\n",
      "Epoch 21 -- Batch 27/ 842, training loss 0.32613998651504517\n",
      "Epoch 21 -- Batch 28/ 842, training loss 0.3312424123287201\n",
      "Epoch 21 -- Batch 29/ 842, training loss 0.3289570212364197\n",
      "Epoch 21 -- Batch 30/ 842, training loss 0.32200297713279724\n",
      "Epoch 21 -- Batch 31/ 842, training loss 0.33125823736190796\n",
      "Epoch 21 -- Batch 32/ 842, training loss 0.3264201283454895\n",
      "Epoch 21 -- Batch 33/ 842, training loss 0.3215627074241638\n",
      "Epoch 21 -- Batch 34/ 842, training loss 0.32988807559013367\n",
      "Epoch 21 -- Batch 35/ 842, training loss 0.3277455270290375\n",
      "Epoch 21 -- Batch 36/ 842, training loss 0.3320710361003876\n",
      "Epoch 21 -- Batch 37/ 842, training loss 0.32215723395347595\n",
      "Epoch 21 -- Batch 38/ 842, training loss 0.33171018958091736\n",
      "Epoch 21 -- Batch 39/ 842, training loss 0.31883636116981506\n",
      "Epoch 21 -- Batch 40/ 842, training loss 0.32736116647720337\n",
      "Epoch 21 -- Batch 41/ 842, training loss 0.3408318758010864\n",
      "Epoch 21 -- Batch 42/ 842, training loss 0.3251292407512665\n",
      "Epoch 21 -- Batch 43/ 842, training loss 0.3228321671485901\n",
      "Epoch 21 -- Batch 44/ 842, training loss 0.3288373053073883\n",
      "Epoch 21 -- Batch 45/ 842, training loss 0.3187229037284851\n",
      "Epoch 21 -- Batch 46/ 842, training loss 0.31837326288223267\n",
      "Epoch 21 -- Batch 47/ 842, training loss 0.3277674615383148\n",
      "Epoch 21 -- Batch 48/ 842, training loss 0.32005175948143005\n",
      "Epoch 21 -- Batch 49/ 842, training loss 0.32494208216667175\n",
      "Epoch 21 -- Batch 50/ 842, training loss 0.31138962507247925\n",
      "Epoch 21 -- Batch 51/ 842, training loss 0.32501521706581116\n",
      "Epoch 21 -- Batch 52/ 842, training loss 0.3214486241340637\n",
      "Epoch 21 -- Batch 53/ 842, training loss 0.3079351782798767\n",
      "Epoch 21 -- Batch 54/ 842, training loss 0.3194281756877899\n",
      "Epoch 21 -- Batch 55/ 842, training loss 0.32682275772094727\n",
      "Epoch 21 -- Batch 56/ 842, training loss 0.31986960768699646\n",
      "Epoch 21 -- Batch 57/ 842, training loss 0.3236648440361023\n",
      "Epoch 21 -- Batch 58/ 842, training loss 0.3100423514842987\n",
      "Epoch 21 -- Batch 59/ 842, training loss 0.3242947459220886\n",
      "Epoch 21 -- Batch 60/ 842, training loss 0.31719836592674255\n",
      "Epoch 21 -- Batch 61/ 842, training loss 0.3284052014350891\n",
      "Epoch 21 -- Batch 62/ 842, training loss 0.31647998094558716\n",
      "Epoch 21 -- Batch 63/ 842, training loss 0.3252533972263336\n",
      "Epoch 21 -- Batch 64/ 842, training loss 0.3262026011943817\n",
      "Epoch 21 -- Batch 65/ 842, training loss 0.33120664954185486\n",
      "Epoch 21 -- Batch 66/ 842, training loss 0.3234730660915375\n",
      "Epoch 21 -- Batch 67/ 842, training loss 0.31959807872772217\n",
      "Epoch 21 -- Batch 68/ 842, training loss 0.33752429485321045\n",
      "Epoch 21 -- Batch 69/ 842, training loss 0.3307344913482666\n",
      "Epoch 21 -- Batch 70/ 842, training loss 0.31199270486831665\n",
      "Epoch 21 -- Batch 71/ 842, training loss 0.3213663399219513\n",
      "Epoch 21 -- Batch 72/ 842, training loss 0.3265722095966339\n",
      "Epoch 21 -- Batch 73/ 842, training loss 0.3188706934452057\n",
      "Epoch 21 -- Batch 74/ 842, training loss 0.3186201751232147\n",
      "Epoch 21 -- Batch 75/ 842, training loss 0.32632914185523987\n",
      "Epoch 21 -- Batch 76/ 842, training loss 0.32209011912345886\n",
      "Epoch 21 -- Batch 77/ 842, training loss 0.3256259262561798\n",
      "Epoch 21 -- Batch 78/ 842, training loss 0.3344542980194092\n",
      "Epoch 21 -- Batch 79/ 842, training loss 0.32018643617630005\n",
      "Epoch 21 -- Batch 80/ 842, training loss 0.3240300714969635\n",
      "Epoch 21 -- Batch 81/ 842, training loss 0.32649755477905273\n",
      "Epoch 21 -- Batch 82/ 842, training loss 0.3145829737186432\n",
      "Epoch 21 -- Batch 83/ 842, training loss 0.32095399498939514\n",
      "Epoch 21 -- Batch 84/ 842, training loss 0.3237003982067108\n",
      "Epoch 21 -- Batch 85/ 842, training loss 0.31697767972946167\n",
      "Epoch 21 -- Batch 86/ 842, training loss 0.32159337401390076\n",
      "Epoch 21 -- Batch 87/ 842, training loss 0.32589191198349\n",
      "Epoch 21 -- Batch 88/ 842, training loss 0.31054458022117615\n",
      "Epoch 21 -- Batch 89/ 842, training loss 0.31343874335289\n",
      "Epoch 21 -- Batch 90/ 842, training loss 0.31281542778015137\n",
      "Epoch 21 -- Batch 91/ 842, training loss 0.32130488753318787\n",
      "Epoch 21 -- Batch 92/ 842, training loss 0.3233028054237366\n",
      "Epoch 21 -- Batch 93/ 842, training loss 0.3346410095691681\n",
      "Epoch 21 -- Batch 94/ 842, training loss 0.3280467689037323\n",
      "Epoch 21 -- Batch 95/ 842, training loss 0.3168763220310211\n",
      "Epoch 21 -- Batch 96/ 842, training loss 0.31421396136283875\n",
      "Epoch 21 -- Batch 97/ 842, training loss 0.31921350955963135\n",
      "Epoch 21 -- Batch 98/ 842, training loss 0.29977548122406006\n",
      "Epoch 21 -- Batch 99/ 842, training loss 0.31846997141838074\n",
      "Epoch 21 -- Batch 100/ 842, training loss 0.3296288847923279\n",
      "Epoch 21 -- Batch 101/ 842, training loss 0.31667467951774597\n",
      "Epoch 21 -- Batch 102/ 842, training loss 0.31640005111694336\n",
      "Epoch 21 -- Batch 103/ 842, training loss 0.3239039182662964\n",
      "Epoch 21 -- Batch 104/ 842, training loss 0.32185032963752747\n",
      "Epoch 21 -- Batch 105/ 842, training loss 0.33330175280570984\n",
      "Epoch 21 -- Batch 106/ 842, training loss 0.3235115706920624\n",
      "Epoch 21 -- Batch 107/ 842, training loss 0.33022910356521606\n",
      "Epoch 21 -- Batch 108/ 842, training loss 0.32973071932792664\n",
      "Epoch 21 -- Batch 109/ 842, training loss 0.3253955543041229\n",
      "Epoch 21 -- Batch 110/ 842, training loss 0.316008597612381\n",
      "Epoch 21 -- Batch 111/ 842, training loss 0.31898704171180725\n",
      "Epoch 21 -- Batch 112/ 842, training loss 0.31984513998031616\n",
      "Epoch 21 -- Batch 113/ 842, training loss 0.3239052891731262\n",
      "Epoch 21 -- Batch 114/ 842, training loss 0.32337498664855957\n",
      "Epoch 21 -- Batch 115/ 842, training loss 0.322397381067276\n",
      "Epoch 21 -- Batch 116/ 842, training loss 0.3147867023944855\n",
      "Epoch 21 -- Batch 117/ 842, training loss 0.32450151443481445\n",
      "Epoch 21 -- Batch 118/ 842, training loss 0.32580313086509705\n",
      "Epoch 21 -- Batch 119/ 842, training loss 0.3206809461116791\n",
      "Epoch 21 -- Batch 120/ 842, training loss 0.3324829041957855\n",
      "Epoch 21 -- Batch 121/ 842, training loss 0.3205575942993164\n",
      "Epoch 21 -- Batch 122/ 842, training loss 0.31576892733573914\n",
      "Epoch 21 -- Batch 123/ 842, training loss 0.3115089237689972\n",
      "Epoch 21 -- Batch 124/ 842, training loss 0.3261730968952179\n",
      "Epoch 21 -- Batch 125/ 842, training loss 0.3173423409461975\n",
      "Epoch 21 -- Batch 126/ 842, training loss 0.31290698051452637\n",
      "Epoch 21 -- Batch 127/ 842, training loss 0.320942759513855\n",
      "Epoch 21 -- Batch 128/ 842, training loss 0.3168795108795166\n",
      "Epoch 21 -- Batch 129/ 842, training loss 0.3326161205768585\n",
      "Epoch 21 -- Batch 130/ 842, training loss 0.31462037563323975\n",
      "Epoch 21 -- Batch 131/ 842, training loss 0.319947212934494\n",
      "Epoch 21 -- Batch 132/ 842, training loss 0.3264860510826111\n",
      "Epoch 21 -- Batch 133/ 842, training loss 0.31426507234573364\n",
      "Epoch 21 -- Batch 134/ 842, training loss 0.3317440152168274\n",
      "Epoch 21 -- Batch 135/ 842, training loss 0.3122384250164032\n",
      "Epoch 21 -- Batch 136/ 842, training loss 0.33773162961006165\n",
      "Epoch 21 -- Batch 137/ 842, training loss 0.3274136483669281\n",
      "Epoch 21 -- Batch 138/ 842, training loss 0.32474687695503235\n",
      "Epoch 21 -- Batch 139/ 842, training loss 0.3185436725616455\n",
      "Epoch 21 -- Batch 140/ 842, training loss 0.31987854838371277\n",
      "Epoch 21 -- Batch 141/ 842, training loss 0.32239416241645813\n",
      "Epoch 21 -- Batch 142/ 842, training loss 0.3162207007408142\n",
      "Epoch 21 -- Batch 143/ 842, training loss 0.32290688157081604\n",
      "Epoch 21 -- Batch 144/ 842, training loss 0.312126100063324\n",
      "Epoch 21 -- Batch 145/ 842, training loss 0.31792885065078735\n",
      "Epoch 21 -- Batch 146/ 842, training loss 0.31739985942840576\n",
      "Epoch 21 -- Batch 147/ 842, training loss 0.31965911388397217\n",
      "Epoch 21 -- Batch 148/ 842, training loss 0.33294951915740967\n",
      "Epoch 21 -- Batch 149/ 842, training loss 0.31290310621261597\n",
      "Epoch 21 -- Batch 150/ 842, training loss 0.3117828071117401\n",
      "Epoch 21 -- Batch 151/ 842, training loss 0.32857745885849\n",
      "Epoch 21 -- Batch 152/ 842, training loss 0.3213525414466858\n",
      "Epoch 21 -- Batch 153/ 842, training loss 0.3302477300167084\n",
      "Epoch 21 -- Batch 154/ 842, training loss 0.3136802315711975\n",
      "Epoch 21 -- Batch 155/ 842, training loss 0.3128073215484619\n",
      "Epoch 21 -- Batch 156/ 842, training loss 0.3272792100906372\n",
      "Epoch 21 -- Batch 157/ 842, training loss 0.3247711658477783\n",
      "Epoch 21 -- Batch 158/ 842, training loss 0.3246900141239166\n",
      "Epoch 21 -- Batch 159/ 842, training loss 0.32014498114585876\n",
      "Epoch 21 -- Batch 160/ 842, training loss 0.31725239753723145\n",
      "Epoch 21 -- Batch 161/ 842, training loss 0.3190319836139679\n",
      "Epoch 21 -- Batch 162/ 842, training loss 0.32482093572616577\n",
      "Epoch 21 -- Batch 163/ 842, training loss 0.3262568414211273\n",
      "Epoch 21 -- Batch 164/ 842, training loss 0.315359890460968\n",
      "Epoch 21 -- Batch 165/ 842, training loss 0.3204323351383209\n",
      "Epoch 21 -- Batch 166/ 842, training loss 0.3218400478363037\n",
      "Epoch 21 -- Batch 167/ 842, training loss 0.32457640767097473\n",
      "Epoch 21 -- Batch 168/ 842, training loss 0.3139413297176361\n",
      "Epoch 21 -- Batch 169/ 842, training loss 0.31475168466567993\n",
      "Epoch 21 -- Batch 170/ 842, training loss 0.3161163628101349\n",
      "Epoch 21 -- Batch 171/ 842, training loss 0.3233180046081543\n",
      "Epoch 21 -- Batch 172/ 842, training loss 0.31698155403137207\n",
      "Epoch 21 -- Batch 173/ 842, training loss 0.3155592083930969\n",
      "Epoch 21 -- Batch 174/ 842, training loss 0.32027238607406616\n",
      "Epoch 21 -- Batch 175/ 842, training loss 0.330781489610672\n",
      "Epoch 21 -- Batch 176/ 842, training loss 0.317078173160553\n",
      "Epoch 21 -- Batch 177/ 842, training loss 0.32053065299987793\n",
      "Epoch 21 -- Batch 178/ 842, training loss 0.32760393619537354\n",
      "Epoch 21 -- Batch 179/ 842, training loss 0.3252376616001129\n",
      "Epoch 21 -- Batch 180/ 842, training loss 0.32045990228652954\n",
      "Epoch 21 -- Batch 181/ 842, training loss 0.3213273882865906\n",
      "Epoch 21 -- Batch 182/ 842, training loss 0.30994266271591187\n",
      "Epoch 21 -- Batch 183/ 842, training loss 0.32649484276771545\n",
      "Epoch 21 -- Batch 184/ 842, training loss 0.3202994465827942\n",
      "Epoch 21 -- Batch 185/ 842, training loss 0.31639304757118225\n",
      "Epoch 21 -- Batch 186/ 842, training loss 0.3220306932926178\n",
      "Epoch 21 -- Batch 187/ 842, training loss 0.3181200325489044\n",
      "Epoch 21 -- Batch 188/ 842, training loss 0.3227749764919281\n",
      "Epoch 21 -- Batch 189/ 842, training loss 0.31469058990478516\n",
      "Epoch 21 -- Batch 190/ 842, training loss 0.333695650100708\n",
      "Epoch 21 -- Batch 191/ 842, training loss 0.31354525685310364\n",
      "Epoch 21 -- Batch 192/ 842, training loss 0.3302081227302551\n",
      "Epoch 21 -- Batch 193/ 842, training loss 0.32297366857528687\n",
      "Epoch 21 -- Batch 194/ 842, training loss 0.31839901208877563\n",
      "Epoch 21 -- Batch 195/ 842, training loss 0.3231849670410156\n",
      "Epoch 21 -- Batch 196/ 842, training loss 0.32229843735694885\n",
      "Epoch 21 -- Batch 197/ 842, training loss 0.32015976309776306\n",
      "Epoch 21 -- Batch 198/ 842, training loss 0.31951406598091125\n",
      "Epoch 21 -- Batch 199/ 842, training loss 0.3147006332874298\n",
      "Epoch 21 -- Batch 200/ 842, training loss 0.3271145820617676\n",
      "Epoch 21 -- Batch 201/ 842, training loss 0.3122572600841522\n",
      "Epoch 21 -- Batch 202/ 842, training loss 0.3339720666408539\n",
      "Epoch 21 -- Batch 203/ 842, training loss 0.3403241038322449\n",
      "Epoch 21 -- Batch 204/ 842, training loss 0.3345348834991455\n",
      "Epoch 21 -- Batch 205/ 842, training loss 0.3345281779766083\n",
      "Epoch 21 -- Batch 206/ 842, training loss 0.32402071356773376\n",
      "Epoch 21 -- Batch 207/ 842, training loss 0.322630912065506\n",
      "Epoch 21 -- Batch 208/ 842, training loss 0.3273756802082062\n",
      "Epoch 21 -- Batch 209/ 842, training loss 0.31249120831489563\n",
      "Epoch 21 -- Batch 210/ 842, training loss 0.3316037356853485\n",
      "Epoch 21 -- Batch 211/ 842, training loss 0.32778874039649963\n",
      "Epoch 21 -- Batch 212/ 842, training loss 0.3267247974872589\n",
      "Epoch 21 -- Batch 213/ 842, training loss 0.3355921506881714\n",
      "Epoch 21 -- Batch 214/ 842, training loss 0.3380599915981293\n",
      "Epoch 21 -- Batch 215/ 842, training loss 0.32392385601997375\n",
      "Epoch 21 -- Batch 216/ 842, training loss 0.3264019787311554\n",
      "Epoch 21 -- Batch 217/ 842, training loss 0.3235912621021271\n",
      "Epoch 21 -- Batch 218/ 842, training loss 0.3210658133029938\n",
      "Epoch 21 -- Batch 219/ 842, training loss 0.32936400175094604\n",
      "Epoch 21 -- Batch 220/ 842, training loss 0.33118242025375366\n",
      "Epoch 21 -- Batch 221/ 842, training loss 0.31054335832595825\n",
      "Epoch 21 -- Batch 222/ 842, training loss 0.32576683163642883\n",
      "Epoch 21 -- Batch 223/ 842, training loss 0.3215240240097046\n",
      "Epoch 21 -- Batch 224/ 842, training loss 0.3193019926548004\n",
      "Epoch 21 -- Batch 225/ 842, training loss 0.3223462402820587\n",
      "Epoch 21 -- Batch 226/ 842, training loss 0.3287400007247925\n",
      "Epoch 21 -- Batch 227/ 842, training loss 0.34346136450767517\n",
      "Epoch 21 -- Batch 228/ 842, training loss 0.3262396454811096\n",
      "Epoch 21 -- Batch 229/ 842, training loss 0.3266071379184723\n",
      "Epoch 21 -- Batch 230/ 842, training loss 0.3124617040157318\n",
      "Epoch 21 -- Batch 231/ 842, training loss 0.3221842050552368\n",
      "Epoch 21 -- Batch 232/ 842, training loss 0.3155941367149353\n",
      "Epoch 21 -- Batch 233/ 842, training loss 0.320061594247818\n",
      "Epoch 21 -- Batch 234/ 842, training loss 0.3283158540725708\n",
      "Epoch 21 -- Batch 235/ 842, training loss 0.3271593153476715\n",
      "Epoch 21 -- Batch 236/ 842, training loss 0.330491304397583\n",
      "Epoch 21 -- Batch 237/ 842, training loss 0.3255871832370758\n",
      "Epoch 21 -- Batch 238/ 842, training loss 0.3311862349510193\n",
      "Epoch 21 -- Batch 239/ 842, training loss 0.3265691101551056\n",
      "Epoch 21 -- Batch 240/ 842, training loss 0.3125718832015991\n",
      "Epoch 21 -- Batch 241/ 842, training loss 0.3154938817024231\n",
      "Epoch 21 -- Batch 242/ 842, training loss 0.319660484790802\n",
      "Epoch 21 -- Batch 243/ 842, training loss 0.32332366704940796\n",
      "Epoch 21 -- Batch 244/ 842, training loss 0.3214024603366852\n",
      "Epoch 21 -- Batch 245/ 842, training loss 0.3234841227531433\n",
      "Epoch 21 -- Batch 246/ 842, training loss 0.3134503960609436\n",
      "Epoch 21 -- Batch 247/ 842, training loss 0.3179031312465668\n",
      "Epoch 21 -- Batch 248/ 842, training loss 0.32210540771484375\n",
      "Epoch 21 -- Batch 249/ 842, training loss 0.3205290734767914\n",
      "Epoch 21 -- Batch 250/ 842, training loss 0.32160624861717224\n",
      "Epoch 21 -- Batch 251/ 842, training loss 0.3338838219642639\n",
      "Epoch 21 -- Batch 252/ 842, training loss 0.3223249018192291\n",
      "Epoch 21 -- Batch 253/ 842, training loss 0.3205750286579132\n",
      "Epoch 21 -- Batch 254/ 842, training loss 0.32582801580429077\n",
      "Epoch 21 -- Batch 255/ 842, training loss 0.32772940397262573\n",
      "Epoch 21 -- Batch 256/ 842, training loss 0.3223288059234619\n",
      "Epoch 21 -- Batch 257/ 842, training loss 0.3355964720249176\n",
      "Epoch 21 -- Batch 258/ 842, training loss 0.32077446579933167\n",
      "Epoch 21 -- Batch 259/ 842, training loss 0.32512345910072327\n",
      "Epoch 21 -- Batch 260/ 842, training loss 0.3152444064617157\n",
      "Epoch 21 -- Batch 261/ 842, training loss 0.3203989267349243\n",
      "Epoch 21 -- Batch 262/ 842, training loss 0.3162249028682709\n",
      "Epoch 21 -- Batch 263/ 842, training loss 0.31484436988830566\n",
      "Epoch 21 -- Batch 264/ 842, training loss 0.3416937291622162\n",
      "Epoch 21 -- Batch 265/ 842, training loss 0.34316471219062805\n",
      "Epoch 21 -- Batch 266/ 842, training loss 0.31832337379455566\n",
      "Epoch 21 -- Batch 267/ 842, training loss 0.32480165362358093\n",
      "Epoch 21 -- Batch 268/ 842, training loss 0.32042500376701355\n",
      "Epoch 21 -- Batch 269/ 842, training loss 0.3112197816371918\n",
      "Epoch 21 -- Batch 270/ 842, training loss 0.3149956464767456\n",
      "Epoch 21 -- Batch 271/ 842, training loss 0.32849496603012085\n",
      "Epoch 21 -- Batch 272/ 842, training loss 0.32457926869392395\n",
      "Epoch 21 -- Batch 273/ 842, training loss 0.31175753474235535\n",
      "Epoch 21 -- Batch 274/ 842, training loss 0.3024405837059021\n",
      "Epoch 21 -- Batch 275/ 842, training loss 0.3233291208744049\n",
      "Epoch 21 -- Batch 276/ 842, training loss 0.31202366948127747\n",
      "Epoch 21 -- Batch 277/ 842, training loss 0.3226880133152008\n",
      "Epoch 21 -- Batch 278/ 842, training loss 0.3198433816432953\n",
      "Epoch 21 -- Batch 279/ 842, training loss 0.32705157995224\n",
      "Epoch 21 -- Batch 280/ 842, training loss 0.32613494992256165\n",
      "Epoch 21 -- Batch 281/ 842, training loss 0.3384994864463806\n",
      "Epoch 21 -- Batch 282/ 842, training loss 0.336108535528183\n",
      "Epoch 21 -- Batch 283/ 842, training loss 0.32345038652420044\n",
      "Epoch 21 -- Batch 284/ 842, training loss 0.32536035776138306\n",
      "Epoch 21 -- Batch 285/ 842, training loss 0.31743094325065613\n",
      "Epoch 21 -- Batch 286/ 842, training loss 0.3197574019432068\n",
      "Epoch 21 -- Batch 287/ 842, training loss 0.3410128355026245\n",
      "Epoch 21 -- Batch 288/ 842, training loss 0.3320726156234741\n",
      "Epoch 21 -- Batch 289/ 842, training loss 0.3207477033138275\n",
      "Epoch 21 -- Batch 290/ 842, training loss 0.3261621296405792\n",
      "Epoch 21 -- Batch 291/ 842, training loss 0.3349553942680359\n",
      "Epoch 21 -- Batch 292/ 842, training loss 0.32603543996810913\n",
      "Epoch 21 -- Batch 293/ 842, training loss 0.3337726593017578\n",
      "Epoch 21 -- Batch 294/ 842, training loss 0.31955087184906006\n",
      "Epoch 21 -- Batch 295/ 842, training loss 0.33011969923973083\n",
      "Epoch 21 -- Batch 296/ 842, training loss 0.3239086866378784\n",
      "Epoch 21 -- Batch 297/ 842, training loss 0.3302314281463623\n",
      "Epoch 21 -- Batch 298/ 842, training loss 0.31562837958335876\n",
      "Epoch 21 -- Batch 299/ 842, training loss 0.32935965061187744\n",
      "Epoch 21 -- Batch 300/ 842, training loss 0.3222666084766388\n",
      "Epoch 21 -- Batch 301/ 842, training loss 0.32284972071647644\n",
      "Epoch 21 -- Batch 302/ 842, training loss 0.32296621799468994\n",
      "Epoch 21 -- Batch 303/ 842, training loss 0.33393651247024536\n",
      "Epoch 21 -- Batch 304/ 842, training loss 0.3144926428794861\n",
      "Epoch 21 -- Batch 305/ 842, training loss 0.3339638411998749\n",
      "Epoch 21 -- Batch 306/ 842, training loss 0.3321065604686737\n",
      "Epoch 21 -- Batch 307/ 842, training loss 0.319092333316803\n",
      "Epoch 21 -- Batch 308/ 842, training loss 0.3219572901725769\n",
      "Epoch 21 -- Batch 309/ 842, training loss 0.3181968331336975\n",
      "Epoch 21 -- Batch 310/ 842, training loss 0.3179478347301483\n",
      "Epoch 21 -- Batch 311/ 842, training loss 0.32150140404701233\n",
      "Epoch 21 -- Batch 312/ 842, training loss 0.313379168510437\n",
      "Epoch 21 -- Batch 313/ 842, training loss 0.31518471240997314\n",
      "Epoch 21 -- Batch 314/ 842, training loss 0.3216645419597626\n",
      "Epoch 21 -- Batch 315/ 842, training loss 0.326788067817688\n",
      "Epoch 21 -- Batch 316/ 842, training loss 0.3281199038028717\n",
      "Epoch 21 -- Batch 317/ 842, training loss 0.3283730149269104\n",
      "Epoch 21 -- Batch 318/ 842, training loss 0.3259086012840271\n",
      "Epoch 21 -- Batch 319/ 842, training loss 0.3303025960922241\n",
      "Epoch 21 -- Batch 320/ 842, training loss 0.32321909070014954\n",
      "Epoch 21 -- Batch 321/ 842, training loss 0.3134719133377075\n",
      "Epoch 21 -- Batch 322/ 842, training loss 0.3303240239620209\n",
      "Epoch 21 -- Batch 323/ 842, training loss 0.3265051245689392\n",
      "Epoch 21 -- Batch 324/ 842, training loss 0.32064273953437805\n",
      "Epoch 21 -- Batch 325/ 842, training loss 0.31287530064582825\n",
      "Epoch 21 -- Batch 326/ 842, training loss 0.3404296636581421\n",
      "Epoch 21 -- Batch 327/ 842, training loss 0.30724310874938965\n",
      "Epoch 21 -- Batch 328/ 842, training loss 0.30798378586769104\n",
      "Epoch 21 -- Batch 329/ 842, training loss 0.3213399648666382\n",
      "Epoch 21 -- Batch 330/ 842, training loss 0.32868796586990356\n",
      "Epoch 21 -- Batch 331/ 842, training loss 0.33325618505477905\n",
      "Epoch 21 -- Batch 332/ 842, training loss 0.31801939010620117\n",
      "Epoch 21 -- Batch 333/ 842, training loss 0.3178689777851105\n",
      "Epoch 21 -- Batch 334/ 842, training loss 0.3173992931842804\n",
      "Epoch 21 -- Batch 335/ 842, training loss 0.32515978813171387\n",
      "Epoch 21 -- Batch 336/ 842, training loss 0.32127082347869873\n",
      "Epoch 21 -- Batch 337/ 842, training loss 0.31923022866249084\n",
      "Epoch 21 -- Batch 338/ 842, training loss 0.3160471022129059\n",
      "Epoch 21 -- Batch 339/ 842, training loss 0.3266458511352539\n",
      "Epoch 21 -- Batch 340/ 842, training loss 0.3320785164833069\n",
      "Epoch 21 -- Batch 341/ 842, training loss 0.3183489143848419\n",
      "Epoch 21 -- Batch 342/ 842, training loss 0.3117033541202545\n",
      "Epoch 21 -- Batch 343/ 842, training loss 0.309643030166626\n",
      "Epoch 21 -- Batch 344/ 842, training loss 0.32713115215301514\n",
      "Epoch 21 -- Batch 345/ 842, training loss 0.3173966705799103\n",
      "Epoch 21 -- Batch 346/ 842, training loss 0.33288100361824036\n",
      "Epoch 21 -- Batch 347/ 842, training loss 0.32856225967407227\n",
      "Epoch 21 -- Batch 348/ 842, training loss 0.32259276509284973\n",
      "Epoch 21 -- Batch 349/ 842, training loss 0.3229833245277405\n",
      "Epoch 21 -- Batch 350/ 842, training loss 0.3259764015674591\n",
      "Epoch 21 -- Batch 351/ 842, training loss 0.3293710947036743\n",
      "Epoch 21 -- Batch 352/ 842, training loss 0.3180834949016571\n",
      "Epoch 21 -- Batch 353/ 842, training loss 0.3220723271369934\n",
      "Epoch 21 -- Batch 354/ 842, training loss 0.32194387912750244\n",
      "Epoch 21 -- Batch 355/ 842, training loss 0.3110181391239166\n",
      "Epoch 21 -- Batch 356/ 842, training loss 0.3252553939819336\n",
      "Epoch 21 -- Batch 357/ 842, training loss 0.3112674951553345\n",
      "Epoch 21 -- Batch 358/ 842, training loss 0.3310853838920593\n",
      "Epoch 21 -- Batch 359/ 842, training loss 0.3349624276161194\n",
      "Epoch 21 -- Batch 360/ 842, training loss 0.3144127428531647\n",
      "Epoch 21 -- Batch 361/ 842, training loss 0.32975253462791443\n",
      "Epoch 21 -- Batch 362/ 842, training loss 0.32359790802001953\n",
      "Epoch 21 -- Batch 363/ 842, training loss 0.33717280626296997\n",
      "Epoch 21 -- Batch 364/ 842, training loss 0.309719055891037\n",
      "Epoch 21 -- Batch 365/ 842, training loss 0.32954487204551697\n",
      "Epoch 21 -- Batch 366/ 842, training loss 0.318389892578125\n",
      "Epoch 21 -- Batch 367/ 842, training loss 0.3313232958316803\n",
      "Epoch 21 -- Batch 368/ 842, training loss 0.3317948281764984\n",
      "Epoch 21 -- Batch 369/ 842, training loss 0.33655020594596863\n",
      "Epoch 21 -- Batch 370/ 842, training loss 0.3274567127227783\n",
      "Epoch 21 -- Batch 371/ 842, training loss 0.33013060688972473\n",
      "Epoch 21 -- Batch 372/ 842, training loss 0.3355601727962494\n",
      "Epoch 21 -- Batch 373/ 842, training loss 0.32125574350357056\n",
      "Epoch 21 -- Batch 374/ 842, training loss 0.31810134649276733\n",
      "Epoch 21 -- Batch 375/ 842, training loss 0.32154032588005066\n",
      "Epoch 21 -- Batch 376/ 842, training loss 0.31761470437049866\n",
      "Epoch 21 -- Batch 377/ 842, training loss 0.3218938112258911\n",
      "Epoch 21 -- Batch 378/ 842, training loss 0.32522669434547424\n",
      "Epoch 21 -- Batch 379/ 842, training loss 0.3326936960220337\n",
      "Epoch 21 -- Batch 380/ 842, training loss 0.3211323022842407\n",
      "Epoch 21 -- Batch 381/ 842, training loss 0.3311970829963684\n",
      "Epoch 21 -- Batch 382/ 842, training loss 0.32648128271102905\n",
      "Epoch 21 -- Batch 383/ 842, training loss 0.31799769401550293\n",
      "Epoch 21 -- Batch 384/ 842, training loss 0.3210035264492035\n",
      "Epoch 21 -- Batch 385/ 842, training loss 0.3224938213825226\n",
      "Epoch 21 -- Batch 386/ 842, training loss 0.3365573585033417\n",
      "Epoch 21 -- Batch 387/ 842, training loss 0.3343362808227539\n",
      "Epoch 21 -- Batch 388/ 842, training loss 0.3268895149230957\n",
      "Epoch 21 -- Batch 389/ 842, training loss 0.32745420932769775\n",
      "Epoch 21 -- Batch 390/ 842, training loss 0.31952351331710815\n",
      "Epoch 21 -- Batch 391/ 842, training loss 0.3242057263851166\n",
      "Epoch 21 -- Batch 392/ 842, training loss 0.3279023766517639\n",
      "Epoch 21 -- Batch 393/ 842, training loss 0.31449905037879944\n",
      "Epoch 21 -- Batch 394/ 842, training loss 0.32612016797065735\n",
      "Epoch 21 -- Batch 395/ 842, training loss 0.32810819149017334\n",
      "Epoch 21 -- Batch 396/ 842, training loss 0.3197333812713623\n",
      "Epoch 21 -- Batch 397/ 842, training loss 0.3269716203212738\n",
      "Epoch 21 -- Batch 398/ 842, training loss 0.3221037685871124\n",
      "Epoch 21 -- Batch 399/ 842, training loss 0.32046911120414734\n",
      "Epoch 21 -- Batch 400/ 842, training loss 0.3375725746154785\n",
      "Epoch 21 -- Batch 401/ 842, training loss 0.3225281834602356\n",
      "Epoch 21 -- Batch 402/ 842, training loss 0.3321327567100525\n",
      "Epoch 21 -- Batch 403/ 842, training loss 0.33279091119766235\n",
      "Epoch 21 -- Batch 404/ 842, training loss 0.3189574182033539\n",
      "Epoch 21 -- Batch 405/ 842, training loss 0.316720187664032\n",
      "Epoch 21 -- Batch 406/ 842, training loss 0.32151055335998535\n",
      "Epoch 21 -- Batch 407/ 842, training loss 0.3187493085861206\n",
      "Epoch 21 -- Batch 408/ 842, training loss 0.31882786750793457\n",
      "Epoch 21 -- Batch 409/ 842, training loss 0.33988335728645325\n",
      "Epoch 21 -- Batch 410/ 842, training loss 0.3267611265182495\n",
      "Epoch 21 -- Batch 411/ 842, training loss 0.31281778216362\n",
      "Epoch 21 -- Batch 412/ 842, training loss 0.31864026188850403\n",
      "Epoch 21 -- Batch 413/ 842, training loss 0.33571234345436096\n",
      "Epoch 21 -- Batch 414/ 842, training loss 0.325473427772522\n",
      "Epoch 21 -- Batch 415/ 842, training loss 0.31913772225379944\n",
      "Epoch 21 -- Batch 416/ 842, training loss 0.3283492624759674\n",
      "Epoch 21 -- Batch 417/ 842, training loss 0.32572031021118164\n",
      "Epoch 21 -- Batch 418/ 842, training loss 0.3185841143131256\n",
      "Epoch 21 -- Batch 419/ 842, training loss 0.3200133442878723\n",
      "Epoch 21 -- Batch 420/ 842, training loss 0.3268832564353943\n",
      "Epoch 21 -- Batch 421/ 842, training loss 0.3120022416114807\n",
      "Epoch 21 -- Batch 422/ 842, training loss 0.32555776834487915\n",
      "Epoch 21 -- Batch 423/ 842, training loss 0.33213359117507935\n",
      "Epoch 21 -- Batch 424/ 842, training loss 0.32618385553359985\n",
      "Epoch 21 -- Batch 425/ 842, training loss 0.3300867974758148\n",
      "Epoch 21 -- Batch 426/ 842, training loss 0.31439173221588135\n",
      "Epoch 21 -- Batch 427/ 842, training loss 0.336311012506485\n",
      "Epoch 21 -- Batch 428/ 842, training loss 0.3318832218647003\n",
      "Epoch 21 -- Batch 429/ 842, training loss 0.32183918356895447\n",
      "Epoch 21 -- Batch 430/ 842, training loss 0.32103726267814636\n",
      "Epoch 21 -- Batch 431/ 842, training loss 0.3185978829860687\n",
      "Epoch 21 -- Batch 432/ 842, training loss 0.32248109579086304\n",
      "Epoch 21 -- Batch 433/ 842, training loss 0.3251311779022217\n",
      "Epoch 21 -- Batch 434/ 842, training loss 0.3160691261291504\n",
      "Epoch 21 -- Batch 435/ 842, training loss 0.3239293098449707\n",
      "Epoch 21 -- Batch 436/ 842, training loss 0.3246188759803772\n",
      "Epoch 21 -- Batch 437/ 842, training loss 0.31751930713653564\n",
      "Epoch 21 -- Batch 438/ 842, training loss 0.3279620110988617\n",
      "Epoch 21 -- Batch 439/ 842, training loss 0.3282753527164459\n",
      "Epoch 21 -- Batch 440/ 842, training loss 0.32387977838516235\n",
      "Epoch 21 -- Batch 441/ 842, training loss 0.32448622584342957\n",
      "Epoch 21 -- Batch 442/ 842, training loss 0.33467769622802734\n",
      "Epoch 21 -- Batch 443/ 842, training loss 0.3191700875759125\n",
      "Epoch 21 -- Batch 444/ 842, training loss 0.34258216619491577\n",
      "Epoch 21 -- Batch 445/ 842, training loss 0.3210047483444214\n",
      "Epoch 21 -- Batch 446/ 842, training loss 0.32041794061660767\n",
      "Epoch 21 -- Batch 447/ 842, training loss 0.31774264574050903\n",
      "Epoch 21 -- Batch 448/ 842, training loss 0.3266243636608124\n",
      "Epoch 21 -- Batch 449/ 842, training loss 0.33491116762161255\n",
      "Epoch 21 -- Batch 450/ 842, training loss 0.3261255919933319\n",
      "Epoch 21 -- Batch 451/ 842, training loss 0.32001689076423645\n",
      "Epoch 21 -- Batch 452/ 842, training loss 0.3336818218231201\n",
      "Epoch 21 -- Batch 453/ 842, training loss 0.3248996138572693\n",
      "Epoch 21 -- Batch 454/ 842, training loss 0.31999561190605164\n",
      "Epoch 21 -- Batch 455/ 842, training loss 0.31389692425727844\n",
      "Epoch 21 -- Batch 456/ 842, training loss 0.3264307975769043\n",
      "Epoch 21 -- Batch 457/ 842, training loss 0.3148667812347412\n",
      "Epoch 21 -- Batch 458/ 842, training loss 0.32695630192756653\n",
      "Epoch 21 -- Batch 459/ 842, training loss 0.33208420872688293\n",
      "Epoch 21 -- Batch 460/ 842, training loss 0.3278847634792328\n",
      "Epoch 21 -- Batch 461/ 842, training loss 0.32527396082878113\n",
      "Epoch 21 -- Batch 462/ 842, training loss 0.32570382952690125\n",
      "Epoch 21 -- Batch 463/ 842, training loss 0.32570314407348633\n",
      "Epoch 21 -- Batch 464/ 842, training loss 0.3189581632614136\n",
      "Epoch 21 -- Batch 465/ 842, training loss 0.33098354935646057\n",
      "Epoch 21 -- Batch 466/ 842, training loss 0.3248760998249054\n",
      "Epoch 21 -- Batch 467/ 842, training loss 0.3173913061618805\n",
      "Epoch 21 -- Batch 468/ 842, training loss 0.3250185251235962\n",
      "Epoch 21 -- Batch 469/ 842, training loss 0.3250673711299896\n",
      "Epoch 21 -- Batch 470/ 842, training loss 0.3262215554714203\n",
      "Epoch 21 -- Batch 471/ 842, training loss 0.317634642124176\n",
      "Epoch 21 -- Batch 472/ 842, training loss 0.3285139203071594\n",
      "Epoch 21 -- Batch 473/ 842, training loss 0.32089099287986755\n",
      "Epoch 21 -- Batch 474/ 842, training loss 0.3354122042655945\n",
      "Epoch 21 -- Batch 475/ 842, training loss 0.32423004508018494\n",
      "Epoch 21 -- Batch 476/ 842, training loss 0.3271171748638153\n",
      "Epoch 21 -- Batch 477/ 842, training loss 0.3204346001148224\n",
      "Epoch 21 -- Batch 478/ 842, training loss 0.3286854028701782\n",
      "Epoch 21 -- Batch 479/ 842, training loss 0.3363863527774811\n",
      "Epoch 21 -- Batch 480/ 842, training loss 0.3195692002773285\n",
      "Epoch 21 -- Batch 481/ 842, training loss 0.3193662464618683\n",
      "Epoch 21 -- Batch 482/ 842, training loss 0.3203393220901489\n",
      "Epoch 21 -- Batch 483/ 842, training loss 0.3273419439792633\n",
      "Epoch 21 -- Batch 484/ 842, training loss 0.3235447108745575\n",
      "Epoch 21 -- Batch 485/ 842, training loss 0.328307569026947\n",
      "Epoch 21 -- Batch 486/ 842, training loss 0.3196829855442047\n",
      "Epoch 21 -- Batch 487/ 842, training loss 0.3292098939418793\n",
      "Epoch 21 -- Batch 488/ 842, training loss 0.3207019567489624\n",
      "Epoch 21 -- Batch 489/ 842, training loss 0.31279635429382324\n",
      "Epoch 21 -- Batch 490/ 842, training loss 0.33059456944465637\n",
      "Epoch 21 -- Batch 491/ 842, training loss 0.33130142092704773\n",
      "Epoch 21 -- Batch 492/ 842, training loss 0.3247119188308716\n",
      "Epoch 21 -- Batch 493/ 842, training loss 0.3278539776802063\n",
      "Epoch 21 -- Batch 494/ 842, training loss 0.3328022062778473\n",
      "Epoch 21 -- Batch 495/ 842, training loss 0.32093578577041626\n",
      "Epoch 21 -- Batch 496/ 842, training loss 0.31196436285972595\n",
      "Epoch 21 -- Batch 497/ 842, training loss 0.32850515842437744\n",
      "Epoch 21 -- Batch 498/ 842, training loss 0.32858991622924805\n",
      "Epoch 21 -- Batch 499/ 842, training loss 0.31567099690437317\n",
      "Epoch 21 -- Batch 500/ 842, training loss 0.3321376144886017\n",
      "Epoch 21 -- Batch 501/ 842, training loss 0.33328017592430115\n",
      "Epoch 21 -- Batch 502/ 842, training loss 0.31655076146125793\n",
      "Epoch 21 -- Batch 503/ 842, training loss 0.3240664005279541\n",
      "Epoch 21 -- Batch 504/ 842, training loss 0.3172987103462219\n",
      "Epoch 21 -- Batch 505/ 842, training loss 0.3340056240558624\n",
      "Epoch 21 -- Batch 506/ 842, training loss 0.32701388001441956\n",
      "Epoch 21 -- Batch 507/ 842, training loss 0.31361982226371765\n",
      "Epoch 21 -- Batch 508/ 842, training loss 0.3354055881500244\n",
      "Epoch 21 -- Batch 509/ 842, training loss 0.33123740553855896\n",
      "Epoch 21 -- Batch 510/ 842, training loss 0.3316965401172638\n",
      "Epoch 21 -- Batch 511/ 842, training loss 0.3258623480796814\n",
      "Epoch 21 -- Batch 512/ 842, training loss 0.339850515127182\n",
      "Epoch 21 -- Batch 513/ 842, training loss 0.32124418020248413\n",
      "Epoch 21 -- Batch 514/ 842, training loss 0.3297189474105835\n",
      "Epoch 21 -- Batch 515/ 842, training loss 0.31760573387145996\n",
      "Epoch 21 -- Batch 516/ 842, training loss 0.32672369480133057\n",
      "Epoch 21 -- Batch 517/ 842, training loss 0.3332858383655548\n",
      "Epoch 21 -- Batch 518/ 842, training loss 0.3433898091316223\n",
      "Epoch 21 -- Batch 519/ 842, training loss 0.32800567150115967\n",
      "Epoch 21 -- Batch 520/ 842, training loss 0.3271821141242981\n",
      "Epoch 21 -- Batch 521/ 842, training loss 0.3199988007545471\n",
      "Epoch 21 -- Batch 522/ 842, training loss 0.32012343406677246\n",
      "Epoch 21 -- Batch 523/ 842, training loss 0.32079240679740906\n",
      "Epoch 21 -- Batch 524/ 842, training loss 0.31354865431785583\n",
      "Epoch 21 -- Batch 525/ 842, training loss 0.32237309217453003\n",
      "Epoch 21 -- Batch 526/ 842, training loss 0.32641464471817017\n",
      "Epoch 21 -- Batch 527/ 842, training loss 0.31623321771621704\n",
      "Epoch 21 -- Batch 528/ 842, training loss 0.3291378319263458\n",
      "Epoch 21 -- Batch 529/ 842, training loss 0.3229462206363678\n",
      "Epoch 21 -- Batch 530/ 842, training loss 0.3296509385108948\n",
      "Epoch 21 -- Batch 531/ 842, training loss 0.31983116269111633\n",
      "Epoch 21 -- Batch 532/ 842, training loss 0.34141114354133606\n",
      "Epoch 21 -- Batch 533/ 842, training loss 0.32506513595581055\n",
      "Epoch 21 -- Batch 534/ 842, training loss 0.31666865944862366\n",
      "Epoch 21 -- Batch 535/ 842, training loss 0.33632251620292664\n",
      "Epoch 21 -- Batch 536/ 842, training loss 0.3270983099937439\n",
      "Epoch 21 -- Batch 537/ 842, training loss 0.3247550129890442\n",
      "Epoch 21 -- Batch 538/ 842, training loss 0.32781079411506653\n",
      "Epoch 21 -- Batch 539/ 842, training loss 0.3347996473312378\n",
      "Epoch 21 -- Batch 540/ 842, training loss 0.3150404691696167\n",
      "Epoch 21 -- Batch 541/ 842, training loss 0.32476016879081726\n",
      "Epoch 21 -- Batch 542/ 842, training loss 0.3174569606781006\n",
      "Epoch 21 -- Batch 543/ 842, training loss 0.334778755903244\n",
      "Epoch 21 -- Batch 544/ 842, training loss 0.314010888338089\n",
      "Epoch 21 -- Batch 545/ 842, training loss 0.32150039076805115\n",
      "Epoch 21 -- Batch 546/ 842, training loss 0.314944863319397\n",
      "Epoch 21 -- Batch 547/ 842, training loss 0.33211347460746765\n",
      "Epoch 21 -- Batch 548/ 842, training loss 0.3325677216053009\n",
      "Epoch 21 -- Batch 549/ 842, training loss 0.3268718421459198\n",
      "Epoch 21 -- Batch 550/ 842, training loss 0.3229996860027313\n",
      "Epoch 21 -- Batch 551/ 842, training loss 0.3155519366264343\n",
      "Epoch 21 -- Batch 552/ 842, training loss 0.32380685210227966\n",
      "Epoch 21 -- Batch 553/ 842, training loss 0.3222862184047699\n",
      "Epoch 21 -- Batch 554/ 842, training loss 0.3229624629020691\n",
      "Epoch 21 -- Batch 555/ 842, training loss 0.3156927824020386\n",
      "Epoch 21 -- Batch 556/ 842, training loss 0.3234299421310425\n",
      "Epoch 21 -- Batch 557/ 842, training loss 0.31964054703712463\n",
      "Epoch 21 -- Batch 558/ 842, training loss 0.31026583909988403\n",
      "Epoch 21 -- Batch 559/ 842, training loss 0.33085253834724426\n",
      "Epoch 21 -- Batch 560/ 842, training loss 0.3299044966697693\n",
      "Epoch 21 -- Batch 561/ 842, training loss 0.3254111707210541\n",
      "Epoch 21 -- Batch 562/ 842, training loss 0.33638709783554077\n",
      "Epoch 21 -- Batch 563/ 842, training loss 0.32768943905830383\n",
      "Epoch 21 -- Batch 564/ 842, training loss 0.3192630708217621\n",
      "Epoch 21 -- Batch 565/ 842, training loss 0.331551194190979\n",
      "Epoch 21 -- Batch 566/ 842, training loss 0.3189433515071869\n",
      "Epoch 21 -- Batch 567/ 842, training loss 0.3256620168685913\n",
      "Epoch 21 -- Batch 568/ 842, training loss 0.3329903185367584\n",
      "Epoch 21 -- Batch 569/ 842, training loss 0.3274541199207306\n",
      "Epoch 21 -- Batch 570/ 842, training loss 0.31942299008369446\n",
      "Epoch 21 -- Batch 571/ 842, training loss 0.3280177116394043\n",
      "Epoch 21 -- Batch 572/ 842, training loss 0.3325871527194977\n",
      "Epoch 21 -- Batch 573/ 842, training loss 0.3269268572330475\n",
      "Epoch 21 -- Batch 574/ 842, training loss 0.332522451877594\n",
      "Epoch 21 -- Batch 575/ 842, training loss 0.32648980617523193\n",
      "Epoch 21 -- Batch 576/ 842, training loss 0.3351975083351135\n",
      "Epoch 21 -- Batch 577/ 842, training loss 0.31987613439559937\n",
      "Epoch 21 -- Batch 578/ 842, training loss 0.3167741298675537\n",
      "Epoch 21 -- Batch 579/ 842, training loss 0.3213396966457367\n",
      "Epoch 21 -- Batch 580/ 842, training loss 0.32603713870048523\n",
      "Epoch 21 -- Batch 581/ 842, training loss 0.3322843015193939\n",
      "Epoch 21 -- Batch 582/ 842, training loss 0.32804104685783386\n",
      "Epoch 21 -- Batch 583/ 842, training loss 0.33227095007896423\n",
      "Epoch 21 -- Batch 584/ 842, training loss 0.3289051949977875\n",
      "Epoch 21 -- Batch 585/ 842, training loss 0.3224483132362366\n",
      "Epoch 21 -- Batch 586/ 842, training loss 0.32397907972335815\n",
      "Epoch 21 -- Batch 587/ 842, training loss 0.32423150539398193\n",
      "Epoch 21 -- Batch 588/ 842, training loss 0.3205563426017761\n",
      "Epoch 21 -- Batch 589/ 842, training loss 0.3157292902469635\n",
      "Epoch 21 -- Batch 590/ 842, training loss 0.3281255066394806\n",
      "Epoch 21 -- Batch 591/ 842, training loss 0.3362842798233032\n",
      "Epoch 21 -- Batch 592/ 842, training loss 0.3153165578842163\n",
      "Epoch 21 -- Batch 593/ 842, training loss 0.33312538266181946\n",
      "Epoch 21 -- Batch 594/ 842, training loss 0.33195188641548157\n",
      "Epoch 21 -- Batch 595/ 842, training loss 0.3350987136363983\n",
      "Epoch 21 -- Batch 596/ 842, training loss 0.3321264088153839\n",
      "Epoch 21 -- Batch 597/ 842, training loss 0.3260439932346344\n",
      "Epoch 21 -- Batch 598/ 842, training loss 0.3404463231563568\n",
      "Epoch 21 -- Batch 599/ 842, training loss 0.3471551537513733\n",
      "Epoch 21 -- Batch 600/ 842, training loss 0.33136865496635437\n",
      "Epoch 21 -- Batch 601/ 842, training loss 0.3286130130290985\n",
      "Epoch 21 -- Batch 602/ 842, training loss 0.32792818546295166\n",
      "Epoch 21 -- Batch 603/ 842, training loss 0.3164568841457367\n",
      "Epoch 21 -- Batch 604/ 842, training loss 0.3145979642868042\n",
      "Epoch 21 -- Batch 605/ 842, training loss 0.3254062831401825\n",
      "Epoch 21 -- Batch 606/ 842, training loss 0.3338450789451599\n",
      "Epoch 21 -- Batch 607/ 842, training loss 0.33913108706474304\n",
      "Epoch 21 -- Batch 608/ 842, training loss 0.3262997269630432\n",
      "Epoch 21 -- Batch 609/ 842, training loss 0.32386699318885803\n",
      "Epoch 21 -- Batch 610/ 842, training loss 0.32580456137657166\n",
      "Epoch 21 -- Batch 611/ 842, training loss 0.33507871627807617\n",
      "Epoch 21 -- Batch 612/ 842, training loss 0.32671263813972473\n",
      "Epoch 21 -- Batch 613/ 842, training loss 0.34090936183929443\n",
      "Epoch 21 -- Batch 614/ 842, training loss 0.3369258642196655\n",
      "Epoch 21 -- Batch 615/ 842, training loss 0.33965572714805603\n",
      "Epoch 21 -- Batch 616/ 842, training loss 0.3227722942829132\n",
      "Epoch 21 -- Batch 617/ 842, training loss 0.3262479901313782\n",
      "Epoch 21 -- Batch 618/ 842, training loss 0.31918278336524963\n",
      "Epoch 21 -- Batch 619/ 842, training loss 0.3285016119480133\n",
      "Epoch 21 -- Batch 620/ 842, training loss 0.3246946632862091\n",
      "Epoch 21 -- Batch 621/ 842, training loss 0.33024370670318604\n",
      "Epoch 21 -- Batch 622/ 842, training loss 0.31557270884513855\n",
      "Epoch 21 -- Batch 623/ 842, training loss 0.32545408606529236\n",
      "Epoch 21 -- Batch 624/ 842, training loss 0.33295029401779175\n",
      "Epoch 21 -- Batch 625/ 842, training loss 0.3199167251586914\n",
      "Epoch 21 -- Batch 626/ 842, training loss 0.337103009223938\n",
      "Epoch 21 -- Batch 627/ 842, training loss 0.32494211196899414\n",
      "Epoch 21 -- Batch 628/ 842, training loss 0.32012006640434265\n",
      "Epoch 21 -- Batch 629/ 842, training loss 0.32216528058052063\n",
      "Epoch 21 -- Batch 630/ 842, training loss 0.3245261013507843\n",
      "Epoch 21 -- Batch 631/ 842, training loss 0.3284122049808502\n",
      "Epoch 21 -- Batch 632/ 842, training loss 0.3257471024990082\n",
      "Epoch 21 -- Batch 633/ 842, training loss 0.3180190920829773\n",
      "Epoch 21 -- Batch 634/ 842, training loss 0.3351612985134125\n",
      "Epoch 21 -- Batch 635/ 842, training loss 0.32176637649536133\n",
      "Epoch 21 -- Batch 636/ 842, training loss 0.3240721821784973\n",
      "Epoch 21 -- Batch 637/ 842, training loss 0.33088260889053345\n",
      "Epoch 21 -- Batch 638/ 842, training loss 0.3399706184864044\n",
      "Epoch 21 -- Batch 639/ 842, training loss 0.33435019850730896\n",
      "Epoch 21 -- Batch 640/ 842, training loss 0.3305225372314453\n",
      "Epoch 21 -- Batch 641/ 842, training loss 0.32947179675102234\n",
      "Epoch 21 -- Batch 642/ 842, training loss 0.32916513085365295\n",
      "Epoch 21 -- Batch 643/ 842, training loss 0.31499916315078735\n",
      "Epoch 21 -- Batch 644/ 842, training loss 0.33393093943595886\n",
      "Epoch 21 -- Batch 645/ 842, training loss 0.32480278611183167\n",
      "Epoch 21 -- Batch 646/ 842, training loss 0.3355696201324463\n",
      "Epoch 21 -- Batch 647/ 842, training loss 0.32352375984191895\n",
      "Epoch 21 -- Batch 648/ 842, training loss 0.3307252526283264\n",
      "Epoch 21 -- Batch 649/ 842, training loss 0.32942694425582886\n",
      "Epoch 21 -- Batch 650/ 842, training loss 0.3331793546676636\n",
      "Epoch 21 -- Batch 651/ 842, training loss 0.32744958996772766\n",
      "Epoch 21 -- Batch 652/ 842, training loss 0.31609949469566345\n",
      "Epoch 21 -- Batch 653/ 842, training loss 0.33244258165359497\n",
      "Epoch 21 -- Batch 654/ 842, training loss 0.32868409156799316\n",
      "Epoch 21 -- Batch 655/ 842, training loss 0.34058138728141785\n",
      "Epoch 21 -- Batch 656/ 842, training loss 0.32334837317466736\n",
      "Epoch 21 -- Batch 657/ 842, training loss 0.32265183329582214\n",
      "Epoch 21 -- Batch 658/ 842, training loss 0.33343249559402466\n",
      "Epoch 21 -- Batch 659/ 842, training loss 0.33916160464286804\n",
      "Epoch 21 -- Batch 660/ 842, training loss 0.32799026370048523\n",
      "Epoch 21 -- Batch 661/ 842, training loss 0.32954174280166626\n",
      "Epoch 21 -- Batch 662/ 842, training loss 0.32618817687034607\n",
      "Epoch 21 -- Batch 663/ 842, training loss 0.3157893419265747\n",
      "Epoch 21 -- Batch 664/ 842, training loss 0.3216625452041626\n",
      "Epoch 21 -- Batch 665/ 842, training loss 0.31647738814353943\n",
      "Epoch 21 -- Batch 666/ 842, training loss 0.3272973597049713\n",
      "Epoch 21 -- Batch 667/ 842, training loss 0.32209888100624084\n",
      "Epoch 21 -- Batch 668/ 842, training loss 0.326588898897171\n",
      "Epoch 21 -- Batch 669/ 842, training loss 0.32264748215675354\n",
      "Epoch 21 -- Batch 670/ 842, training loss 0.3319247364997864\n",
      "Epoch 21 -- Batch 671/ 842, training loss 0.3318215608596802\n",
      "Epoch 21 -- Batch 672/ 842, training loss 0.31777891516685486\n",
      "Epoch 21 -- Batch 673/ 842, training loss 0.3287339508533478\n",
      "Epoch 21 -- Batch 674/ 842, training loss 0.32321110367774963\n",
      "Epoch 21 -- Batch 675/ 842, training loss 0.3400753438472748\n",
      "Epoch 21 -- Batch 676/ 842, training loss 0.31744876503944397\n",
      "Epoch 21 -- Batch 677/ 842, training loss 0.3151314854621887\n",
      "Epoch 21 -- Batch 678/ 842, training loss 0.32904210686683655\n",
      "Epoch 21 -- Batch 679/ 842, training loss 0.3327837288379669\n",
      "Epoch 21 -- Batch 680/ 842, training loss 0.3429470956325531\n",
      "Epoch 21 -- Batch 681/ 842, training loss 0.3269031047821045\n",
      "Epoch 21 -- Batch 682/ 842, training loss 0.333773136138916\n",
      "Epoch 21 -- Batch 683/ 842, training loss 0.3263353407382965\n",
      "Epoch 21 -- Batch 684/ 842, training loss 0.33428362011909485\n",
      "Epoch 21 -- Batch 685/ 842, training loss 0.32801586389541626\n",
      "Epoch 21 -- Batch 686/ 842, training loss 0.33844077587127686\n",
      "Epoch 21 -- Batch 687/ 842, training loss 0.3334013521671295\n",
      "Epoch 21 -- Batch 688/ 842, training loss 0.3242526650428772\n",
      "Epoch 21 -- Batch 689/ 842, training loss 0.3243102431297302\n",
      "Epoch 21 -- Batch 690/ 842, training loss 0.3287392556667328\n",
      "Epoch 21 -- Batch 691/ 842, training loss 0.3185320794582367\n",
      "Epoch 21 -- Batch 692/ 842, training loss 0.3226681351661682\n",
      "Epoch 21 -- Batch 693/ 842, training loss 0.3198930323123932\n",
      "Epoch 21 -- Batch 694/ 842, training loss 0.3455541729927063\n",
      "Epoch 21 -- Batch 695/ 842, training loss 0.3270171880722046\n",
      "Epoch 21 -- Batch 696/ 842, training loss 0.31602707505226135\n",
      "Epoch 21 -- Batch 697/ 842, training loss 0.33687958121299744\n",
      "Epoch 21 -- Batch 698/ 842, training loss 0.3255468010902405\n",
      "Epoch 21 -- Batch 699/ 842, training loss 0.3226306438446045\n",
      "Epoch 21 -- Batch 700/ 842, training loss 0.3222103714942932\n",
      "Epoch 21 -- Batch 701/ 842, training loss 0.32722827792167664\n",
      "Epoch 21 -- Batch 702/ 842, training loss 0.32192304730415344\n",
      "Epoch 21 -- Batch 703/ 842, training loss 0.3356691002845764\n",
      "Epoch 21 -- Batch 704/ 842, training loss 0.31977689266204834\n",
      "Epoch 21 -- Batch 705/ 842, training loss 0.34199267625808716\n",
      "Epoch 21 -- Batch 706/ 842, training loss 0.3367941081523895\n",
      "Epoch 21 -- Batch 707/ 842, training loss 0.32447513937950134\n",
      "Epoch 21 -- Batch 708/ 842, training loss 0.3260565400123596\n",
      "Epoch 21 -- Batch 709/ 842, training loss 0.326284795999527\n",
      "Epoch 21 -- Batch 710/ 842, training loss 0.3212811350822449\n",
      "Epoch 21 -- Batch 711/ 842, training loss 0.3331820070743561\n",
      "Epoch 21 -- Batch 712/ 842, training loss 0.32335764169692993\n",
      "Epoch 21 -- Batch 713/ 842, training loss 0.3257874548435211\n",
      "Epoch 21 -- Batch 714/ 842, training loss 0.3234460949897766\n",
      "Epoch 21 -- Batch 715/ 842, training loss 0.32934480905532837\n",
      "Epoch 21 -- Batch 716/ 842, training loss 0.3225520849227905\n",
      "Epoch 21 -- Batch 717/ 842, training loss 0.31512030959129333\n",
      "Epoch 21 -- Batch 718/ 842, training loss 0.3338492214679718\n",
      "Epoch 21 -- Batch 719/ 842, training loss 0.32654881477355957\n",
      "Epoch 21 -- Batch 720/ 842, training loss 0.330013632774353\n",
      "Epoch 21 -- Batch 721/ 842, training loss 0.3199637532234192\n",
      "Epoch 21 -- Batch 722/ 842, training loss 0.3240055441856384\n",
      "Epoch 21 -- Batch 723/ 842, training loss 0.33382749557495117\n",
      "Epoch 21 -- Batch 724/ 842, training loss 0.3452817499637604\n",
      "Epoch 21 -- Batch 725/ 842, training loss 0.3218348026275635\n",
      "Epoch 21 -- Batch 726/ 842, training loss 0.32147493958473206\n",
      "Epoch 21 -- Batch 727/ 842, training loss 0.3283475339412689\n",
      "Epoch 21 -- Batch 728/ 842, training loss 0.32955771684646606\n",
      "Epoch 21 -- Batch 729/ 842, training loss 0.3198065459728241\n",
      "Epoch 21 -- Batch 730/ 842, training loss 0.3321009576320648\n",
      "Epoch 21 -- Batch 731/ 842, training loss 0.3202103078365326\n",
      "Epoch 21 -- Batch 732/ 842, training loss 0.33446004986763\n",
      "Epoch 21 -- Batch 733/ 842, training loss 0.3220318555831909\n",
      "Epoch 21 -- Batch 734/ 842, training loss 0.3211156129837036\n",
      "Epoch 21 -- Batch 735/ 842, training loss 0.32843202352523804\n",
      "Epoch 21 -- Batch 736/ 842, training loss 0.3228878080844879\n",
      "Epoch 21 -- Batch 737/ 842, training loss 0.3226211965084076\n",
      "Epoch 21 -- Batch 738/ 842, training loss 0.32938075065612793\n",
      "Epoch 21 -- Batch 739/ 842, training loss 0.32626256346702576\n",
      "Epoch 21 -- Batch 740/ 842, training loss 0.3231293261051178\n",
      "Epoch 21 -- Batch 741/ 842, training loss 0.3049091696739197\n",
      "Epoch 21 -- Batch 742/ 842, training loss 0.31890788674354553\n",
      "Epoch 21 -- Batch 743/ 842, training loss 0.3309229016304016\n",
      "Epoch 21 -- Batch 744/ 842, training loss 0.3240129053592682\n",
      "Epoch 21 -- Batch 745/ 842, training loss 0.31481876969337463\n",
      "Epoch 21 -- Batch 746/ 842, training loss 0.3264559209346771\n",
      "Epoch 21 -- Batch 747/ 842, training loss 0.33049827814102173\n",
      "Epoch 21 -- Batch 748/ 842, training loss 0.3142375648021698\n",
      "Epoch 21 -- Batch 749/ 842, training loss 0.3297725319862366\n",
      "Epoch 21 -- Batch 750/ 842, training loss 0.3306058347225189\n",
      "Epoch 21 -- Batch 751/ 842, training loss 0.3177529275417328\n",
      "Epoch 21 -- Batch 752/ 842, training loss 0.32064297795295715\n",
      "Epoch 21 -- Batch 753/ 842, training loss 0.3236749768257141\n",
      "Epoch 21 -- Batch 754/ 842, training loss 0.3172193467617035\n",
      "Epoch 21 -- Batch 755/ 842, training loss 0.3240601122379303\n",
      "Epoch 21 -- Batch 756/ 842, training loss 0.3145160973072052\n",
      "Epoch 21 -- Batch 757/ 842, training loss 0.3283078968524933\n",
      "Epoch 21 -- Batch 758/ 842, training loss 0.32376813888549805\n",
      "Epoch 21 -- Batch 759/ 842, training loss 0.33537185192108154\n",
      "Epoch 21 -- Batch 760/ 842, training loss 0.32375776767730713\n",
      "Epoch 21 -- Batch 761/ 842, training loss 0.33621272444725037\n",
      "Epoch 21 -- Batch 762/ 842, training loss 0.32468387484550476\n",
      "Epoch 21 -- Batch 763/ 842, training loss 0.3139454126358032\n",
      "Epoch 21 -- Batch 764/ 842, training loss 0.3203628361225128\n",
      "Epoch 21 -- Batch 765/ 842, training loss 0.32523518800735474\n",
      "Epoch 21 -- Batch 766/ 842, training loss 0.3203193247318268\n",
      "Epoch 21 -- Batch 767/ 842, training loss 0.32832443714141846\n",
      "Epoch 21 -- Batch 768/ 842, training loss 0.3283914625644684\n",
      "Epoch 21 -- Batch 769/ 842, training loss 0.33158379793167114\n",
      "Epoch 21 -- Batch 770/ 842, training loss 0.3341582417488098\n",
      "Epoch 21 -- Batch 771/ 842, training loss 0.32795968651771545\n",
      "Epoch 21 -- Batch 772/ 842, training loss 0.33355680108070374\n",
      "Epoch 21 -- Batch 773/ 842, training loss 0.3372896909713745\n",
      "Epoch 21 -- Batch 774/ 842, training loss 0.32829737663269043\n",
      "Epoch 21 -- Batch 775/ 842, training loss 0.3279203176498413\n",
      "Epoch 21 -- Batch 776/ 842, training loss 0.332450807094574\n",
      "Epoch 21 -- Batch 777/ 842, training loss 0.325700581073761\n",
      "Epoch 21 -- Batch 778/ 842, training loss 0.3254794180393219\n",
      "Epoch 21 -- Batch 779/ 842, training loss 0.32178324460983276\n",
      "Epoch 21 -- Batch 780/ 842, training loss 0.3233490288257599\n",
      "Epoch 21 -- Batch 781/ 842, training loss 0.3310769200325012\n",
      "Epoch 21 -- Batch 782/ 842, training loss 0.330577552318573\n",
      "Epoch 21 -- Batch 783/ 842, training loss 0.33102160692214966\n",
      "Epoch 21 -- Batch 784/ 842, training loss 0.3231409192085266\n",
      "Epoch 21 -- Batch 785/ 842, training loss 0.32745587825775146\n",
      "Epoch 21 -- Batch 786/ 842, training loss 0.32077446579933167\n",
      "Epoch 21 -- Batch 787/ 842, training loss 0.3336816728115082\n",
      "Epoch 21 -- Batch 788/ 842, training loss 0.3318769931793213\n",
      "Epoch 21 -- Batch 789/ 842, training loss 0.3318345844745636\n",
      "Epoch 21 -- Batch 790/ 842, training loss 0.3219566345214844\n",
      "Epoch 21 -- Batch 791/ 842, training loss 0.32793986797332764\n",
      "Epoch 21 -- Batch 792/ 842, training loss 0.32714971899986267\n",
      "Epoch 21 -- Batch 793/ 842, training loss 0.3219408392906189\n",
      "Epoch 21 -- Batch 794/ 842, training loss 0.3274890184402466\n",
      "Epoch 21 -- Batch 795/ 842, training loss 0.3327769935131073\n",
      "Epoch 21 -- Batch 796/ 842, training loss 0.33257266879081726\n",
      "Epoch 21 -- Batch 797/ 842, training loss 0.33430156111717224\n",
      "Epoch 21 -- Batch 798/ 842, training loss 0.3342542052268982\n",
      "Epoch 21 -- Batch 799/ 842, training loss 0.3327213227748871\n",
      "Epoch 21 -- Batch 800/ 842, training loss 0.33779969811439514\n",
      "Epoch 21 -- Batch 801/ 842, training loss 0.3208317160606384\n",
      "Epoch 21 -- Batch 802/ 842, training loss 0.3309825360774994\n",
      "Epoch 21 -- Batch 803/ 842, training loss 0.3156919479370117\n",
      "Epoch 21 -- Batch 804/ 842, training loss 0.33524197340011597\n",
      "Epoch 21 -- Batch 805/ 842, training loss 0.3299851417541504\n",
      "Epoch 21 -- Batch 806/ 842, training loss 0.33588510751724243\n",
      "Epoch 21 -- Batch 807/ 842, training loss 0.3367045521736145\n",
      "Epoch 21 -- Batch 808/ 842, training loss 0.3234036862850189\n",
      "Epoch 21 -- Batch 809/ 842, training loss 0.317107617855072\n",
      "Epoch 21 -- Batch 810/ 842, training loss 0.3237695097923279\n",
      "Epoch 21 -- Batch 811/ 842, training loss 0.3182891607284546\n",
      "Epoch 21 -- Batch 812/ 842, training loss 0.3104950487613678\n",
      "Epoch 21 -- Batch 813/ 842, training loss 0.32772019505500793\n",
      "Epoch 21 -- Batch 814/ 842, training loss 0.3319946825504303\n",
      "Epoch 21 -- Batch 815/ 842, training loss 0.3270861804485321\n",
      "Epoch 21 -- Batch 816/ 842, training loss 0.32124459743499756\n",
      "Epoch 21 -- Batch 817/ 842, training loss 0.33454591035842896\n",
      "Epoch 21 -- Batch 818/ 842, training loss 0.315495103597641\n",
      "Epoch 21 -- Batch 819/ 842, training loss 0.32571229338645935\n",
      "Epoch 21 -- Batch 820/ 842, training loss 0.332300066947937\n",
      "Epoch 21 -- Batch 821/ 842, training loss 0.3356020748615265\n",
      "Epoch 21 -- Batch 822/ 842, training loss 0.31969714164733887\n",
      "Epoch 21 -- Batch 823/ 842, training loss 0.33060967922210693\n",
      "Epoch 21 -- Batch 824/ 842, training loss 0.3217315673828125\n",
      "Epoch 21 -- Batch 825/ 842, training loss 0.3229357600212097\n",
      "Epoch 21 -- Batch 826/ 842, training loss 0.3383654057979584\n",
      "Epoch 21 -- Batch 827/ 842, training loss 0.33767443895339966\n",
      "Epoch 21 -- Batch 828/ 842, training loss 0.33007311820983887\n",
      "Epoch 21 -- Batch 829/ 842, training loss 0.32429346442222595\n",
      "Epoch 21 -- Batch 830/ 842, training loss 0.31978654861450195\n",
      "Epoch 21 -- Batch 831/ 842, training loss 0.33773359656333923\n",
      "Epoch 21 -- Batch 832/ 842, training loss 0.320781409740448\n",
      "Epoch 21 -- Batch 833/ 842, training loss 0.33036258816719055\n",
      "Epoch 21 -- Batch 834/ 842, training loss 0.325151652097702\n",
      "Epoch 21 -- Batch 835/ 842, training loss 0.31723853945732117\n",
      "Epoch 21 -- Batch 836/ 842, training loss 0.3236931264400482\n",
      "Epoch 21 -- Batch 837/ 842, training loss 0.33996230363845825\n",
      "Epoch 21 -- Batch 838/ 842, training loss 0.32631397247314453\n",
      "Epoch 21 -- Batch 839/ 842, training loss 0.3377792537212372\n",
      "Epoch 21 -- Batch 840/ 842, training loss 0.33316943049430847\n",
      "Epoch 21 -- Batch 841/ 842, training loss 0.31978294253349304\n",
      "Epoch 21 -- Batch 842/ 842, training loss 0.3351963758468628\n",
      "----------------------------------------------------------------------\n",
      "Epoch 21 -- Batch 1/ 94, validation loss 0.3222445249557495\n",
      "Epoch 21 -- Batch 2/ 94, validation loss 0.3209984600543976\n",
      "Epoch 21 -- Batch 3/ 94, validation loss 0.3180169463157654\n",
      "Epoch 21 -- Batch 4/ 94, validation loss 0.3102821111679077\n",
      "Epoch 21 -- Batch 5/ 94, validation loss 0.31807374954223633\n",
      "Epoch 21 -- Batch 6/ 94, validation loss 0.31931746006011963\n",
      "Epoch 21 -- Batch 7/ 94, validation loss 0.32163718342781067\n",
      "Epoch 21 -- Batch 8/ 94, validation loss 0.3199045658111572\n",
      "Epoch 21 -- Batch 9/ 94, validation loss 0.31584081053733826\n",
      "Epoch 21 -- Batch 10/ 94, validation loss 0.30869629979133606\n",
      "Epoch 21 -- Batch 11/ 94, validation loss 0.3435228765010834\n",
      "Epoch 21 -- Batch 12/ 94, validation loss 0.30866992473602295\n",
      "Epoch 21 -- Batch 13/ 94, validation loss 0.3192320168018341\n",
      "Epoch 21 -- Batch 14/ 94, validation loss 0.3109129071235657\n",
      "Epoch 21 -- Batch 15/ 94, validation loss 0.3133196532726288\n",
      "Epoch 21 -- Batch 16/ 94, validation loss 0.32343894243240356\n",
      "Epoch 21 -- Batch 17/ 94, validation loss 0.31054505705833435\n",
      "Epoch 21 -- Batch 18/ 94, validation loss 0.3068818151950836\n",
      "Epoch 21 -- Batch 19/ 94, validation loss 0.31363195180892944\n",
      "Epoch 21 -- Batch 20/ 94, validation loss 0.30291831493377686\n",
      "Epoch 21 -- Batch 21/ 94, validation loss 0.3143704831600189\n",
      "Epoch 21 -- Batch 22/ 94, validation loss 0.3186056911945343\n",
      "Epoch 21 -- Batch 23/ 94, validation loss 0.31303706765174866\n",
      "Epoch 21 -- Batch 24/ 94, validation loss 0.3191904127597809\n",
      "Epoch 21 -- Batch 25/ 94, validation loss 0.3144139349460602\n",
      "Epoch 21 -- Batch 26/ 94, validation loss 0.3178080916404724\n",
      "Epoch 21 -- Batch 27/ 94, validation loss 0.3245800733566284\n",
      "Epoch 21 -- Batch 28/ 94, validation loss 0.3158125579357147\n",
      "Epoch 21 -- Batch 29/ 94, validation loss 0.32572323083877563\n",
      "Epoch 21 -- Batch 30/ 94, validation loss 0.32482269406318665\n",
      "Epoch 21 -- Batch 31/ 94, validation loss 0.3089499771595001\n",
      "Epoch 21 -- Batch 32/ 94, validation loss 0.3023282587528229\n",
      "Epoch 21 -- Batch 33/ 94, validation loss 0.31248295307159424\n",
      "Epoch 21 -- Batch 34/ 94, validation loss 0.3200910687446594\n",
      "Epoch 21 -- Batch 35/ 94, validation loss 0.31616470217704773\n",
      "Epoch 21 -- Batch 36/ 94, validation loss 0.3133269250392914\n",
      "Epoch 21 -- Batch 37/ 94, validation loss 0.3247736394405365\n",
      "Epoch 21 -- Batch 38/ 94, validation loss 0.31524062156677246\n",
      "Epoch 21 -- Batch 39/ 94, validation loss 0.31218817830085754\n",
      "Epoch 21 -- Batch 40/ 94, validation loss 0.3306295573711395\n",
      "Epoch 21 -- Batch 41/ 94, validation loss 0.3313049077987671\n",
      "Epoch 21 -- Batch 42/ 94, validation loss 0.31999847292900085\n",
      "Epoch 21 -- Batch 43/ 94, validation loss 0.30068737268447876\n",
      "Epoch 21 -- Batch 44/ 94, validation loss 0.3179631233215332\n",
      "Epoch 21 -- Batch 45/ 94, validation loss 0.3170595169067383\n",
      "Epoch 21 -- Batch 46/ 94, validation loss 0.3102527856826782\n",
      "Epoch 21 -- Batch 47/ 94, validation loss 0.34539344906806946\n",
      "Epoch 21 -- Batch 48/ 94, validation loss 0.31543198227882385\n",
      "Epoch 21 -- Batch 49/ 94, validation loss 0.3102956712245941\n",
      "Epoch 21 -- Batch 50/ 94, validation loss 0.3088274896144867\n",
      "Epoch 21 -- Batch 51/ 94, validation loss 0.31990674138069153\n",
      "Epoch 21 -- Batch 52/ 94, validation loss 0.36514517664909363\n",
      "Epoch 21 -- Batch 53/ 94, validation loss 0.3121444582939148\n",
      "Epoch 21 -- Batch 54/ 94, validation loss 0.3113190531730652\n",
      "Epoch 21 -- Batch 55/ 94, validation loss 0.3142775893211365\n",
      "Epoch 21 -- Batch 56/ 94, validation loss 0.3147090673446655\n",
      "Epoch 21 -- Batch 57/ 94, validation loss 0.32037198543548584\n",
      "Epoch 21 -- Batch 58/ 94, validation loss 0.3186291754245758\n",
      "Epoch 21 -- Batch 59/ 94, validation loss 0.31869059801101685\n",
      "Epoch 21 -- Batch 60/ 94, validation loss 0.3096713125705719\n",
      "Epoch 21 -- Batch 61/ 94, validation loss 0.29658716917037964\n",
      "Epoch 21 -- Batch 62/ 94, validation loss 0.3101765215396881\n",
      "Epoch 21 -- Batch 63/ 94, validation loss 0.3260447382926941\n",
      "Epoch 21 -- Batch 64/ 94, validation loss 0.3087806701660156\n",
      "Epoch 21 -- Batch 65/ 94, validation loss 0.3096378743648529\n",
      "Epoch 21 -- Batch 66/ 94, validation loss 0.32776162028312683\n",
      "Epoch 21 -- Batch 67/ 94, validation loss 0.31864631175994873\n",
      "Epoch 21 -- Batch 68/ 94, validation loss 0.3140539526939392\n",
      "Epoch 21 -- Batch 69/ 94, validation loss 0.31926408410072327\n",
      "Epoch 21 -- Batch 70/ 94, validation loss 0.3031623363494873\n",
      "Epoch 21 -- Batch 71/ 94, validation loss 0.3134043216705322\n",
      "Epoch 21 -- Batch 72/ 94, validation loss 0.3133441209793091\n",
      "Epoch 21 -- Batch 73/ 94, validation loss 0.3342585861682892\n",
      "Epoch 21 -- Batch 74/ 94, validation loss 0.3191288411617279\n",
      "Epoch 21 -- Batch 75/ 94, validation loss 0.31295984983444214\n",
      "Epoch 21 -- Batch 76/ 94, validation loss 0.30742087960243225\n",
      "Epoch 21 -- Batch 77/ 94, validation loss 0.31872743368148804\n",
      "Epoch 21 -- Batch 78/ 94, validation loss 0.32159242033958435\n",
      "Epoch 21 -- Batch 79/ 94, validation loss 0.3047161400318146\n",
      "Epoch 21 -- Batch 80/ 94, validation loss 0.3074919283390045\n",
      "Epoch 21 -- Batch 81/ 94, validation loss 0.31584152579307556\n",
      "Epoch 21 -- Batch 82/ 94, validation loss 0.32559332251548767\n",
      "Epoch 21 -- Batch 83/ 94, validation loss 0.30555272102355957\n",
      "Epoch 21 -- Batch 84/ 94, validation loss 0.2983573079109192\n",
      "Epoch 21 -- Batch 85/ 94, validation loss 0.29823774099349976\n",
      "Epoch 21 -- Batch 86/ 94, validation loss 0.31264498829841614\n",
      "Epoch 21 -- Batch 87/ 94, validation loss 0.30590733885765076\n",
      "Epoch 21 -- Batch 88/ 94, validation loss 0.3121940791606903\n",
      "Epoch 21 -- Batch 89/ 94, validation loss 0.3111000955104828\n",
      "Epoch 21 -- Batch 90/ 94, validation loss 0.31411731243133545\n",
      "Epoch 21 -- Batch 91/ 94, validation loss 0.3231562376022339\n",
      "Epoch 21 -- Batch 92/ 94, validation loss 0.32340893149375916\n",
      "Epoch 21 -- Batch 93/ 94, validation loss 0.3124450743198395\n",
      "Epoch 21 -- Batch 94/ 94, validation loss 0.32505232095718384\n",
      "----------------------------------------------------------------------\n",
      "Epoch 21 loss: Training 0.3248373866081238, Validation 0.32505232095718384\n",
      "----------------------------------------------------------------------\n",
      "Epoch 22/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 4\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC2([N+](=O)[O-])NN=C(c2ccccc2O)S1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 16 17 18\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 23 24 25 26\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 21\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)[C@H]2[C@@H](CN3CCc3cnn(C)c3C2)N1Cc1cccc(Cl)c1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CN1CCC(Cc2noc(C3CN(CCC(C)CC(C)C4)n3)CC2)cc1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1C=C(C)C2C=[N+](=O)/C1=C\\c1ccc(OCc2ccccc2F)cc1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC1CCC2C3CCC4=NC(=NCc5ccccc5)N3N(CC)C(=O)C2C1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 14 15 25\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 9 10 12 13 14 15 16 18 19\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(CN2C[C@@H](Sc3ccccn3)[C@H]3C[C@@H]2CO)c1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 3\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CCCN1CCc2c(sc3nc(SCCc4ccccc4)nc3c23)C1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 11 12 13 15 16 18 19\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 18 19 20 22 23 24 25 26 27 28 29 30\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 27\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'O=C(CN1C(=O)CC12CCC(Cc2ccccc2)CC1)NCCc1ccc(F)cc1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 2 3 10 11 12 14 15 16 17\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'O=C(NN1CSC1=NC(=O)c2ccccc2N1)c1cccc(Cl)c1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(Cn2c(N3CCN(Cc4ccccc4)CC3)nc3c2c(=O)[nH]c(=O)n2C)c1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 19 22 23\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'O=C1OC2(CCCC2)Oc2ccccc2C1C2C=CCC1C4'\n",
      "[19:07:21] SMILES Parse Error: syntax error while parsing: C=CCN(C==)c1c(C)nn(Cc2ccccc2Cl)c1C(=O)O\n",
      "[19:07:21] SMILES Parse Error: Failed parsing SMILES 'C=CCN(C==)c1c(C)nn(Cc2ccccc2Cl)c1C(=O)O' for input: 'C=CCN(C==)c1c(C)nn(Cc2ccccc2Cl)c1C(=O)O'\n",
      "[19:07:21] Explicit valence for atom # 4 C, 5, is greater than permitted\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 12 14 15 16 17 27 28\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'COc1cc(/C=C2\\C#N)C(=O)N(c3ccccc3C)C2=C(C#N)C(C)(C)C3ccccc12'\n",
      "[19:07:21] SMILES Parse Error: ring closure 2 duplicates bond between atom 24 and atom 25 for input: 'FC(F)(F)c1ccc(N2N=C(C3CCCCC3)SC2CS(=O)(=O)CC2c2ccccc2)cc1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 15 16 17 18 26 27 29\n",
      "[19:07:21] non-ring atom 16 marked aromatic\n",
      "[19:07:21] Explicit valence for atom # 8 C, 5, is greater than permitted\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1NCc1nc2ccccc2n1Cc1ccc(ccc1)cc1'\n",
      "[19:07:21] SMILES Parse Error: extra open parentheses for input: 'CCc1ccc(OCC2C=CC(C2C(=O)OC2(C)CCCCC2)cc1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'COCCNC(=O)c1ccc(-c2ccccc2CO)n2c1NCc1ccc(OC)cc1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC(=O)NC1(c2cccc(F)c2)CCN(CC2COc3ccccc4O2)CC1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 7 8 21\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC[C@@H]1[C@@H]2CN(C(=O)C3CCOCC3)C[C@H]2[C@H](OC)[C@H]1O2'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 4 5 12 13 14 15 16 30 31\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)C(c1ccc3c(c1)nnn2C)C(=O)N2Cc1ccc2c(c1)OCO2'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 1 2 3 9 10 15 16\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 24\n",
      "[19:07:21] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)Nc2ccc3c(c2)[C@H]2C[C@@H](CC(=O)N4CCN(C)CC4)O[C@H](CO[C@@H](C)[C@H]3O3)[C@H]2CC2)cc1'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC(C)C#Cc1cnc2c(c1)C(=O)N([C@@H](C)CO)C[C@H](C)[C@H](CN(C)Cc1ccccc1O(F)(=O)O2)O2'\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'CC1=NCC(C)(CCCC2)C1=O'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 15 16 17 19 20\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 9 11 20 21\n",
      "[19:07:21] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCNc2ncnc3c2nc(N2CCCN(Cc4ccccc4)CC2)c3ccccc23)cc1'\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 3 12 17\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 15 16 17 21 22 23 24 25 26\n",
      "[19:07:21] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 8 9 11 12 13 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 -- Batch 1/ 842, training loss 0.30925559997558594\n",
      "Epoch 22 -- Batch 2/ 842, training loss 0.31402188539505005\n",
      "Epoch 22 -- Batch 3/ 842, training loss 0.31802913546562195\n",
      "Epoch 22 -- Batch 4/ 842, training loss 0.32954928278923035\n",
      "Epoch 22 -- Batch 5/ 842, training loss 0.3185601532459259\n",
      "Epoch 22 -- Batch 6/ 842, training loss 0.31877049803733826\n",
      "Epoch 22 -- Batch 7/ 842, training loss 0.3206152021884918\n",
      "Epoch 22 -- Batch 8/ 842, training loss 0.3252555727958679\n",
      "Epoch 22 -- Batch 9/ 842, training loss 0.3230220079421997\n",
      "Epoch 22 -- Batch 10/ 842, training loss 0.3275728225708008\n",
      "Epoch 22 -- Batch 11/ 842, training loss 0.323304146528244\n",
      "Epoch 22 -- Batch 12/ 842, training loss 0.31753620505332947\n",
      "Epoch 22 -- Batch 13/ 842, training loss 0.32849815487861633\n",
      "Epoch 22 -- Batch 14/ 842, training loss 0.32148101925849915\n",
      "Epoch 22 -- Batch 15/ 842, training loss 0.3152467608451843\n",
      "Epoch 22 -- Batch 16/ 842, training loss 0.3187935948371887\n",
      "Epoch 22 -- Batch 17/ 842, training loss 0.31885990500450134\n",
      "Epoch 22 -- Batch 18/ 842, training loss 0.31960877776145935\n",
      "Epoch 22 -- Batch 19/ 842, training loss 0.3079754710197449\n",
      "Epoch 22 -- Batch 20/ 842, training loss 0.3184816241264343\n",
      "Epoch 22 -- Batch 21/ 842, training loss 0.31118252873420715\n",
      "Epoch 22 -- Batch 22/ 842, training loss 0.321460098028183\n",
      "Epoch 22 -- Batch 23/ 842, training loss 0.30777108669281006\n",
      "Epoch 22 -- Batch 24/ 842, training loss 0.3222084045410156\n",
      "Epoch 22 -- Batch 25/ 842, training loss 0.3251332640647888\n",
      "Epoch 22 -- Batch 26/ 842, training loss 0.3175998330116272\n",
      "Epoch 22 -- Batch 27/ 842, training loss 0.3209272027015686\n",
      "Epoch 22 -- Batch 28/ 842, training loss 0.31140589714050293\n",
      "Epoch 22 -- Batch 29/ 842, training loss 0.31294524669647217\n",
      "Epoch 22 -- Batch 30/ 842, training loss 0.30910393595695496\n",
      "Epoch 22 -- Batch 31/ 842, training loss 0.3184465765953064\n",
      "Epoch 22 -- Batch 32/ 842, training loss 0.3152084946632385\n",
      "Epoch 22 -- Batch 33/ 842, training loss 0.31618383526802063\n",
      "Epoch 22 -- Batch 34/ 842, training loss 0.3170163333415985\n",
      "Epoch 22 -- Batch 35/ 842, training loss 0.3251485228538513\n",
      "Epoch 22 -- Batch 36/ 842, training loss 0.33272868394851685\n",
      "Epoch 22 -- Batch 37/ 842, training loss 0.3133368194103241\n",
      "Epoch 22 -- Batch 38/ 842, training loss 0.32178738713264465\n",
      "Epoch 22 -- Batch 39/ 842, training loss 0.322511225938797\n",
      "Epoch 22 -- Batch 40/ 842, training loss 0.3158264458179474\n",
      "Epoch 22 -- Batch 41/ 842, training loss 0.3260854482650757\n",
      "Epoch 22 -- Batch 42/ 842, training loss 0.3194692134857178\n",
      "Epoch 22 -- Batch 43/ 842, training loss 0.32251840829849243\n",
      "Epoch 22 -- Batch 44/ 842, training loss 0.30630820989608765\n",
      "Epoch 22 -- Batch 45/ 842, training loss 0.304232656955719\n",
      "Epoch 22 -- Batch 46/ 842, training loss 0.31824997067451477\n",
      "Epoch 22 -- Batch 47/ 842, training loss 0.3127495348453522\n",
      "Epoch 22 -- Batch 48/ 842, training loss 0.31146255135536194\n",
      "Epoch 22 -- Batch 49/ 842, training loss 0.32742196321487427\n",
      "Epoch 22 -- Batch 50/ 842, training loss 0.3181006908416748\n",
      "Epoch 22 -- Batch 51/ 842, training loss 0.31622663140296936\n",
      "Epoch 22 -- Batch 52/ 842, training loss 0.3188572824001312\n",
      "Epoch 22 -- Batch 53/ 842, training loss 0.30636194348335266\n",
      "Epoch 22 -- Batch 54/ 842, training loss 0.31754130125045776\n",
      "Epoch 22 -- Batch 55/ 842, training loss 0.32059264183044434\n",
      "Epoch 22 -- Batch 56/ 842, training loss 0.30541524291038513\n",
      "Epoch 22 -- Batch 57/ 842, training loss 0.3299850821495056\n",
      "Epoch 22 -- Batch 58/ 842, training loss 0.3205241858959198\n",
      "Epoch 22 -- Batch 59/ 842, training loss 0.3233031630516052\n",
      "Epoch 22 -- Batch 60/ 842, training loss 0.3166262209415436\n",
      "Epoch 22 -- Batch 61/ 842, training loss 0.3159556984901428\n",
      "Epoch 22 -- Batch 62/ 842, training loss 0.31062471866607666\n",
      "Epoch 22 -- Batch 63/ 842, training loss 0.3066626489162445\n",
      "Epoch 22 -- Batch 64/ 842, training loss 0.3165607750415802\n",
      "Epoch 22 -- Batch 65/ 842, training loss 0.3276112675666809\n",
      "Epoch 22 -- Batch 66/ 842, training loss 0.31642889976501465\n",
      "Epoch 22 -- Batch 67/ 842, training loss 0.3235895335674286\n",
      "Epoch 22 -- Batch 68/ 842, training loss 0.32333308458328247\n",
      "Epoch 22 -- Batch 69/ 842, training loss 0.3124588131904602\n",
      "Epoch 22 -- Batch 70/ 842, training loss 0.31449660658836365\n",
      "Epoch 22 -- Batch 71/ 842, training loss 0.3048706352710724\n",
      "Epoch 22 -- Batch 72/ 842, training loss 0.3222127854824066\n",
      "Epoch 22 -- Batch 73/ 842, training loss 0.31740695238113403\n",
      "Epoch 22 -- Batch 74/ 842, training loss 0.32425522804260254\n",
      "Epoch 22 -- Batch 75/ 842, training loss 0.32117894291877747\n",
      "Epoch 22 -- Batch 76/ 842, training loss 0.31659629940986633\n",
      "Epoch 22 -- Batch 77/ 842, training loss 0.31578391790390015\n",
      "Epoch 22 -- Batch 78/ 842, training loss 0.3113192915916443\n",
      "Epoch 22 -- Batch 79/ 842, training loss 0.3225625455379486\n",
      "Epoch 22 -- Batch 80/ 842, training loss 0.30629226565361023\n",
      "Epoch 22 -- Batch 81/ 842, training loss 0.3121100068092346\n",
      "Epoch 22 -- Batch 82/ 842, training loss 0.3329813778400421\n",
      "Epoch 22 -- Batch 83/ 842, training loss 0.3295327126979828\n",
      "Epoch 22 -- Batch 84/ 842, training loss 0.3097015917301178\n",
      "Epoch 22 -- Batch 85/ 842, training loss 0.31900814175605774\n",
      "Epoch 22 -- Batch 86/ 842, training loss 0.331722617149353\n",
      "Epoch 22 -- Batch 87/ 842, training loss 0.31500908732414246\n",
      "Epoch 22 -- Batch 88/ 842, training loss 0.307974249124527\n",
      "Epoch 22 -- Batch 89/ 842, training loss 0.32671576738357544\n",
      "Epoch 22 -- Batch 90/ 842, training loss 0.31783750653266907\n",
      "Epoch 22 -- Batch 91/ 842, training loss 0.3071593642234802\n",
      "Epoch 22 -- Batch 92/ 842, training loss 0.3112960159778595\n",
      "Epoch 22 -- Batch 93/ 842, training loss 0.3121335804462433\n",
      "Epoch 22 -- Batch 94/ 842, training loss 0.3061561584472656\n",
      "Epoch 22 -- Batch 95/ 842, training loss 0.31421205401420593\n",
      "Epoch 22 -- Batch 96/ 842, training loss 0.3155142664909363\n",
      "Epoch 22 -- Batch 97/ 842, training loss 0.3228619396686554\n",
      "Epoch 22 -- Batch 98/ 842, training loss 0.3183881640434265\n",
      "Epoch 22 -- Batch 99/ 842, training loss 0.3231188654899597\n",
      "Epoch 22 -- Batch 100/ 842, training loss 0.31438037753105164\n",
      "Epoch 22 -- Batch 101/ 842, training loss 0.3137986958026886\n",
      "Epoch 22 -- Batch 102/ 842, training loss 0.32235512137413025\n",
      "Epoch 22 -- Batch 103/ 842, training loss 0.3051929175853729\n",
      "Epoch 22 -- Batch 104/ 842, training loss 0.32216498255729675\n",
      "Epoch 22 -- Batch 105/ 842, training loss 0.3164643943309784\n",
      "Epoch 22 -- Batch 106/ 842, training loss 0.30887484550476074\n",
      "Epoch 22 -- Batch 107/ 842, training loss 0.3214777708053589\n",
      "Epoch 22 -- Batch 108/ 842, training loss 0.30559709668159485\n",
      "Epoch 22 -- Batch 109/ 842, training loss 0.320140540599823\n",
      "Epoch 22 -- Batch 110/ 842, training loss 0.3232311010360718\n",
      "Epoch 22 -- Batch 111/ 842, training loss 0.3110639452934265\n",
      "Epoch 22 -- Batch 112/ 842, training loss 0.3188672661781311\n",
      "Epoch 22 -- Batch 113/ 842, training loss 0.316982239484787\n",
      "Epoch 22 -- Batch 114/ 842, training loss 0.3129999339580536\n",
      "Epoch 22 -- Batch 115/ 842, training loss 0.32894250750541687\n",
      "Epoch 22 -- Batch 116/ 842, training loss 0.30782151222229004\n",
      "Epoch 22 -- Batch 117/ 842, training loss 0.31751295924186707\n",
      "Epoch 22 -- Batch 118/ 842, training loss 0.3158981502056122\n",
      "Epoch 22 -- Batch 119/ 842, training loss 0.31990665197372437\n",
      "Epoch 22 -- Batch 120/ 842, training loss 0.3213498890399933\n",
      "Epoch 22 -- Batch 121/ 842, training loss 0.31648561358451843\n",
      "Epoch 22 -- Batch 122/ 842, training loss 0.3164946138858795\n",
      "Epoch 22 -- Batch 123/ 842, training loss 0.31235936284065247\n",
      "Epoch 22 -- Batch 124/ 842, training loss 0.3198621869087219\n",
      "Epoch 22 -- Batch 125/ 842, training loss 0.3196554183959961\n",
      "Epoch 22 -- Batch 126/ 842, training loss 0.32096871733665466\n",
      "Epoch 22 -- Batch 127/ 842, training loss 0.31240031123161316\n",
      "Epoch 22 -- Batch 128/ 842, training loss 0.3114788830280304\n",
      "Epoch 22 -- Batch 129/ 842, training loss 0.3139246702194214\n",
      "Epoch 22 -- Batch 130/ 842, training loss 0.3247475028038025\n",
      "Epoch 22 -- Batch 131/ 842, training loss 0.32410958409309387\n",
      "Epoch 22 -- Batch 132/ 842, training loss 0.31967973709106445\n",
      "Epoch 22 -- Batch 133/ 842, training loss 0.31207194924354553\n",
      "Epoch 22 -- Batch 134/ 842, training loss 0.3142060339450836\n",
      "Epoch 22 -- Batch 135/ 842, training loss 0.32371553778648376\n",
      "Epoch 22 -- Batch 136/ 842, training loss 0.32311511039733887\n",
      "Epoch 22 -- Batch 137/ 842, training loss 0.30191272497177124\n",
      "Epoch 22 -- Batch 138/ 842, training loss 0.32052692770957947\n",
      "Epoch 22 -- Batch 139/ 842, training loss 0.31306296586990356\n",
      "Epoch 22 -- Batch 140/ 842, training loss 0.32662326097488403\n",
      "Epoch 22 -- Batch 141/ 842, training loss 0.311056911945343\n",
      "Epoch 22 -- Batch 142/ 842, training loss 0.31005600094795227\n",
      "Epoch 22 -- Batch 143/ 842, training loss 0.34011441469192505\n",
      "Epoch 22 -- Batch 144/ 842, training loss 0.3179103136062622\n",
      "Epoch 22 -- Batch 145/ 842, training loss 0.31002482771873474\n",
      "Epoch 22 -- Batch 146/ 842, training loss 0.3206113576889038\n",
      "Epoch 22 -- Batch 147/ 842, training loss 0.32197433710098267\n",
      "Epoch 22 -- Batch 148/ 842, training loss 0.31800970435142517\n",
      "Epoch 22 -- Batch 149/ 842, training loss 0.32110294699668884\n",
      "Epoch 22 -- Batch 150/ 842, training loss 0.30596494674682617\n",
      "Epoch 22 -- Batch 151/ 842, training loss 0.3268664479255676\n",
      "Epoch 22 -- Batch 152/ 842, training loss 0.32642602920532227\n",
      "Epoch 22 -- Batch 153/ 842, training loss 0.31968259811401367\n",
      "Epoch 22 -- Batch 154/ 842, training loss 0.3158944547176361\n",
      "Epoch 22 -- Batch 155/ 842, training loss 0.3109818398952484\n",
      "Epoch 22 -- Batch 156/ 842, training loss 0.32131463289260864\n",
      "Epoch 22 -- Batch 157/ 842, training loss 0.3237440586090088\n",
      "Epoch 22 -- Batch 158/ 842, training loss 0.3204776644706726\n",
      "Epoch 22 -- Batch 159/ 842, training loss 0.32523590326309204\n",
      "Epoch 22 -- Batch 160/ 842, training loss 0.32821840047836304\n",
      "Epoch 22 -- Batch 161/ 842, training loss 0.31515049934387207\n",
      "Epoch 22 -- Batch 162/ 842, training loss 0.3260217010974884\n",
      "Epoch 22 -- Batch 163/ 842, training loss 0.3180120587348938\n",
      "Epoch 22 -- Batch 164/ 842, training loss 0.3202866017818451\n",
      "Epoch 22 -- Batch 165/ 842, training loss 0.3258764445781708\n",
      "Epoch 22 -- Batch 166/ 842, training loss 0.31786906719207764\n",
      "Epoch 22 -- Batch 167/ 842, training loss 0.31729069352149963\n",
      "Epoch 22 -- Batch 168/ 842, training loss 0.3297739028930664\n",
      "Epoch 22 -- Batch 169/ 842, training loss 0.32407405972480774\n",
      "Epoch 22 -- Batch 170/ 842, training loss 0.30647602677345276\n",
      "Epoch 22 -- Batch 171/ 842, training loss 0.31644368171691895\n",
      "Epoch 22 -- Batch 172/ 842, training loss 0.3126424551010132\n",
      "Epoch 22 -- Batch 173/ 842, training loss 0.3054153621196747\n",
      "Epoch 22 -- Batch 174/ 842, training loss 0.31727489829063416\n",
      "Epoch 22 -- Batch 175/ 842, training loss 0.3180822432041168\n",
      "Epoch 22 -- Batch 176/ 842, training loss 0.3113742172718048\n",
      "Epoch 22 -- Batch 177/ 842, training loss 0.3170398771762848\n",
      "Epoch 22 -- Batch 178/ 842, training loss 0.32582804560661316\n",
      "Epoch 22 -- Batch 179/ 842, training loss 0.31965115666389465\n",
      "Epoch 22 -- Batch 180/ 842, training loss 0.3165796101093292\n",
      "Epoch 22 -- Batch 181/ 842, training loss 0.31895601749420166\n",
      "Epoch 22 -- Batch 182/ 842, training loss 0.316287636756897\n",
      "Epoch 22 -- Batch 183/ 842, training loss 0.3123369812965393\n",
      "Epoch 22 -- Batch 184/ 842, training loss 0.32584401965141296\n",
      "Epoch 22 -- Batch 185/ 842, training loss 0.32616594433784485\n",
      "Epoch 22 -- Batch 186/ 842, training loss 0.3114924132823944\n",
      "Epoch 22 -- Batch 187/ 842, training loss 0.3249881863594055\n",
      "Epoch 22 -- Batch 188/ 842, training loss 0.32431760430336\n",
      "Epoch 22 -- Batch 189/ 842, training loss 0.3348897099494934\n",
      "Epoch 22 -- Batch 190/ 842, training loss 0.31547629833221436\n",
      "Epoch 22 -- Batch 191/ 842, training loss 0.31547439098358154\n",
      "Epoch 22 -- Batch 192/ 842, training loss 0.32215943932533264\n",
      "Epoch 22 -- Batch 193/ 842, training loss 0.3145935833454132\n",
      "Epoch 22 -- Batch 194/ 842, training loss 0.31479987502098083\n",
      "Epoch 22 -- Batch 195/ 842, training loss 0.32013803720474243\n",
      "Epoch 22 -- Batch 196/ 842, training loss 0.32013189792633057\n",
      "Epoch 22 -- Batch 197/ 842, training loss 0.32095393538475037\n",
      "Epoch 22 -- Batch 198/ 842, training loss 0.3185272216796875\n",
      "Epoch 22 -- Batch 199/ 842, training loss 0.3242383897304535\n",
      "Epoch 22 -- Batch 200/ 842, training loss 0.322632759809494\n",
      "Epoch 22 -- Batch 201/ 842, training loss 0.32535791397094727\n",
      "Epoch 22 -- Batch 202/ 842, training loss 0.3156244456768036\n",
      "Epoch 22 -- Batch 203/ 842, training loss 0.31122612953186035\n",
      "Epoch 22 -- Batch 204/ 842, training loss 0.31706658005714417\n",
      "Epoch 22 -- Batch 205/ 842, training loss 0.314423143863678\n",
      "Epoch 22 -- Batch 206/ 842, training loss 0.3251052498817444\n",
      "Epoch 22 -- Batch 207/ 842, training loss 0.3132920265197754\n",
      "Epoch 22 -- Batch 208/ 842, training loss 0.3123506009578705\n",
      "Epoch 22 -- Batch 209/ 842, training loss 0.32661083340644836\n",
      "Epoch 22 -- Batch 210/ 842, training loss 0.3116774260997772\n",
      "Epoch 22 -- Batch 211/ 842, training loss 0.315342515707016\n",
      "Epoch 22 -- Batch 212/ 842, training loss 0.313476026058197\n",
      "Epoch 22 -- Batch 213/ 842, training loss 0.32131925225257874\n",
      "Epoch 22 -- Batch 214/ 842, training loss 0.31276389956474304\n",
      "Epoch 22 -- Batch 215/ 842, training loss 0.3145001232624054\n",
      "Epoch 22 -- Batch 216/ 842, training loss 0.3121725618839264\n",
      "Epoch 22 -- Batch 217/ 842, training loss 0.3204409182071686\n",
      "Epoch 22 -- Batch 218/ 842, training loss 0.3160593509674072\n",
      "Epoch 22 -- Batch 219/ 842, training loss 0.32880574464797974\n",
      "Epoch 22 -- Batch 220/ 842, training loss 0.30923992395401\n",
      "Epoch 22 -- Batch 221/ 842, training loss 0.3176765739917755\n",
      "Epoch 22 -- Batch 222/ 842, training loss 0.32212430238723755\n",
      "Epoch 22 -- Batch 223/ 842, training loss 0.3224835693836212\n",
      "Epoch 22 -- Batch 224/ 842, training loss 0.3276844322681427\n",
      "Epoch 22 -- Batch 225/ 842, training loss 0.32372573018074036\n",
      "Epoch 22 -- Batch 226/ 842, training loss 0.3237645626068115\n",
      "Epoch 22 -- Batch 227/ 842, training loss 0.32458335161209106\n",
      "Epoch 22 -- Batch 228/ 842, training loss 0.30870285630226135\n",
      "Epoch 22 -- Batch 229/ 842, training loss 0.31773582100868225\n",
      "Epoch 22 -- Batch 230/ 842, training loss 0.3092646300792694\n",
      "Epoch 22 -- Batch 231/ 842, training loss 0.3150949478149414\n",
      "Epoch 22 -- Batch 232/ 842, training loss 0.3148251175880432\n",
      "Epoch 22 -- Batch 233/ 842, training loss 0.31401944160461426\n",
      "Epoch 22 -- Batch 234/ 842, training loss 0.3180201053619385\n",
      "Epoch 22 -- Batch 235/ 842, training loss 0.3144527077674866\n",
      "Epoch 22 -- Batch 236/ 842, training loss 0.3075713813304901\n",
      "Epoch 22 -- Batch 237/ 842, training loss 0.316962331533432\n",
      "Epoch 22 -- Batch 238/ 842, training loss 0.31231364607810974\n",
      "Epoch 22 -- Batch 239/ 842, training loss 0.31963926553726196\n",
      "Epoch 22 -- Batch 240/ 842, training loss 0.3176876902580261\n",
      "Epoch 22 -- Batch 241/ 842, training loss 0.31602510809898376\n",
      "Epoch 22 -- Batch 242/ 842, training loss 0.31781673431396484\n",
      "Epoch 22 -- Batch 243/ 842, training loss 0.3255555033683777\n",
      "Epoch 22 -- Batch 244/ 842, training loss 0.3164677023887634\n",
      "Epoch 22 -- Batch 245/ 842, training loss 0.32099512219429016\n",
      "Epoch 22 -- Batch 246/ 842, training loss 0.31164777278900146\n",
      "Epoch 22 -- Batch 247/ 842, training loss 0.30753642320632935\n",
      "Epoch 22 -- Batch 248/ 842, training loss 0.32513341307640076\n",
      "Epoch 22 -- Batch 249/ 842, training loss 0.31835606694221497\n",
      "Epoch 22 -- Batch 250/ 842, training loss 0.3226034939289093\n",
      "Epoch 22 -- Batch 251/ 842, training loss 0.31360453367233276\n",
      "Epoch 22 -- Batch 252/ 842, training loss 0.3334835469722748\n",
      "Epoch 22 -- Batch 253/ 842, training loss 0.3086804747581482\n",
      "Epoch 22 -- Batch 254/ 842, training loss 0.3168456554412842\n",
      "Epoch 22 -- Batch 255/ 842, training loss 0.33167222142219543\n",
      "Epoch 22 -- Batch 256/ 842, training loss 0.3069496154785156\n",
      "Epoch 22 -- Batch 257/ 842, training loss 0.31098613142967224\n",
      "Epoch 22 -- Batch 258/ 842, training loss 0.321865439414978\n",
      "Epoch 22 -- Batch 259/ 842, training loss 0.3108559548854828\n",
      "Epoch 22 -- Batch 260/ 842, training loss 0.3129359483718872\n",
      "Epoch 22 -- Batch 261/ 842, training loss 0.332197904586792\n",
      "Epoch 22 -- Batch 262/ 842, training loss 0.31406986713409424\n",
      "Epoch 22 -- Batch 263/ 842, training loss 0.3342871367931366\n",
      "Epoch 22 -- Batch 264/ 842, training loss 0.3071451187133789\n",
      "Epoch 22 -- Batch 265/ 842, training loss 0.32811829447746277\n",
      "Epoch 22 -- Batch 266/ 842, training loss 0.3255300223827362\n",
      "Epoch 22 -- Batch 267/ 842, training loss 0.31409552693367004\n",
      "Epoch 22 -- Batch 268/ 842, training loss 0.32431909441947937\n",
      "Epoch 22 -- Batch 269/ 842, training loss 0.3219682276248932\n",
      "Epoch 22 -- Batch 270/ 842, training loss 0.31955933570861816\n",
      "Epoch 22 -- Batch 271/ 842, training loss 0.3190189301967621\n",
      "Epoch 22 -- Batch 272/ 842, training loss 0.31001418828964233\n",
      "Epoch 22 -- Batch 273/ 842, training loss 0.31969502568244934\n",
      "Epoch 22 -- Batch 274/ 842, training loss 0.31534525752067566\n",
      "Epoch 22 -- Batch 275/ 842, training loss 0.3282272517681122\n",
      "Epoch 22 -- Batch 276/ 842, training loss 0.327089786529541\n",
      "Epoch 22 -- Batch 277/ 842, training loss 0.34124624729156494\n",
      "Epoch 22 -- Batch 278/ 842, training loss 0.31414008140563965\n",
      "Epoch 22 -- Batch 279/ 842, training loss 0.3243052363395691\n",
      "Epoch 22 -- Batch 280/ 842, training loss 0.3180443346500397\n",
      "Epoch 22 -- Batch 281/ 842, training loss 0.33072054386138916\n",
      "Epoch 22 -- Batch 282/ 842, training loss 0.32036057114601135\n",
      "Epoch 22 -- Batch 283/ 842, training loss 0.31468716263771057\n",
      "Epoch 22 -- Batch 284/ 842, training loss 0.3396194577217102\n",
      "Epoch 22 -- Batch 285/ 842, training loss 0.32720687985420227\n",
      "Epoch 22 -- Batch 286/ 842, training loss 0.324174165725708\n",
      "Epoch 22 -- Batch 287/ 842, training loss 0.3267490267753601\n",
      "Epoch 22 -- Batch 288/ 842, training loss 0.3147689700126648\n",
      "Epoch 22 -- Batch 289/ 842, training loss 0.31286272406578064\n",
      "Epoch 22 -- Batch 290/ 842, training loss 0.3281251788139343\n",
      "Epoch 22 -- Batch 291/ 842, training loss 0.3237655460834503\n",
      "Epoch 22 -- Batch 292/ 842, training loss 0.3123682737350464\n",
      "Epoch 22 -- Batch 293/ 842, training loss 0.3221195638179779\n",
      "Epoch 22 -- Batch 294/ 842, training loss 0.3272644281387329\n",
      "Epoch 22 -- Batch 295/ 842, training loss 0.31976038217544556\n",
      "Epoch 22 -- Batch 296/ 842, training loss 0.31907233595848083\n",
      "Epoch 22 -- Batch 297/ 842, training loss 0.33657509088516235\n",
      "Epoch 22 -- Batch 298/ 842, training loss 0.33009281754493713\n",
      "Epoch 22 -- Batch 299/ 842, training loss 0.3191415071487427\n",
      "Epoch 22 -- Batch 300/ 842, training loss 0.31328627467155457\n",
      "Epoch 22 -- Batch 301/ 842, training loss 0.3221181035041809\n",
      "Epoch 22 -- Batch 302/ 842, training loss 0.3116394877433777\n",
      "Epoch 22 -- Batch 303/ 842, training loss 0.3172576427459717\n",
      "Epoch 22 -- Batch 304/ 842, training loss 0.3191746175289154\n",
      "Epoch 22 -- Batch 305/ 842, training loss 0.3216201364994049\n",
      "Epoch 22 -- Batch 306/ 842, training loss 0.32459548115730286\n",
      "Epoch 22 -- Batch 307/ 842, training loss 0.32400670647621155\n",
      "Epoch 22 -- Batch 308/ 842, training loss 0.3212772011756897\n",
      "Epoch 22 -- Batch 309/ 842, training loss 0.32322177290916443\n",
      "Epoch 22 -- Batch 310/ 842, training loss 0.32414552569389343\n",
      "Epoch 22 -- Batch 311/ 842, training loss 0.3191346526145935\n",
      "Epoch 22 -- Batch 312/ 842, training loss 0.3327736556529999\n",
      "Epoch 22 -- Batch 313/ 842, training loss 0.3265151083469391\n",
      "Epoch 22 -- Batch 314/ 842, training loss 0.3089413642883301\n",
      "Epoch 22 -- Batch 315/ 842, training loss 0.30989328026771545\n",
      "Epoch 22 -- Batch 316/ 842, training loss 0.32426533102989197\n",
      "Epoch 22 -- Batch 317/ 842, training loss 0.3168017864227295\n",
      "Epoch 22 -- Batch 318/ 842, training loss 0.323835164308548\n",
      "Epoch 22 -- Batch 319/ 842, training loss 0.31722256541252136\n",
      "Epoch 22 -- Batch 320/ 842, training loss 0.32370713353157043\n",
      "Epoch 22 -- Batch 321/ 842, training loss 0.3272882103919983\n",
      "Epoch 22 -- Batch 322/ 842, training loss 0.3203670382499695\n",
      "Epoch 22 -- Batch 323/ 842, training loss 0.3130149841308594\n",
      "Epoch 22 -- Batch 324/ 842, training loss 0.32075172662734985\n",
      "Epoch 22 -- Batch 325/ 842, training loss 0.3140799105167389\n",
      "Epoch 22 -- Batch 326/ 842, training loss 0.3264000415802002\n",
      "Epoch 22 -- Batch 327/ 842, training loss 0.32733678817749023\n",
      "Epoch 22 -- Batch 328/ 842, training loss 0.3199724853038788\n",
      "Epoch 22 -- Batch 329/ 842, training loss 0.32319340109825134\n",
      "Epoch 22 -- Batch 330/ 842, training loss 0.3125957250595093\n",
      "Epoch 22 -- Batch 331/ 842, training loss 0.3175278604030609\n",
      "Epoch 22 -- Batch 332/ 842, training loss 0.31629011034965515\n",
      "Epoch 22 -- Batch 333/ 842, training loss 0.3198133707046509\n",
      "Epoch 22 -- Batch 334/ 842, training loss 0.3248313367366791\n",
      "Epoch 22 -- Batch 335/ 842, training loss 0.32490718364715576\n",
      "Epoch 22 -- Batch 336/ 842, training loss 0.31618598103523254\n",
      "Epoch 22 -- Batch 337/ 842, training loss 0.32689934968948364\n",
      "Epoch 22 -- Batch 338/ 842, training loss 0.3353588283061981\n",
      "Epoch 22 -- Batch 339/ 842, training loss 0.3258325457572937\n",
      "Epoch 22 -- Batch 340/ 842, training loss 0.32362839579582214\n",
      "Epoch 22 -- Batch 341/ 842, training loss 0.31589871644973755\n",
      "Epoch 22 -- Batch 342/ 842, training loss 0.3134593069553375\n",
      "Epoch 22 -- Batch 343/ 842, training loss 0.31971776485443115\n",
      "Epoch 22 -- Batch 344/ 842, training loss 0.33672821521759033\n",
      "Epoch 22 -- Batch 345/ 842, training loss 0.3194877803325653\n",
      "Epoch 22 -- Batch 346/ 842, training loss 0.3229650855064392\n",
      "Epoch 22 -- Batch 347/ 842, training loss 0.3181575834751129\n",
      "Epoch 22 -- Batch 348/ 842, training loss 0.32233482599258423\n",
      "Epoch 22 -- Batch 349/ 842, training loss 0.3193720579147339\n",
      "Epoch 22 -- Batch 350/ 842, training loss 0.31678667664527893\n",
      "Epoch 22 -- Batch 351/ 842, training loss 0.32398298382759094\n",
      "Epoch 22 -- Batch 352/ 842, training loss 0.31835678219795227\n",
      "Epoch 22 -- Batch 353/ 842, training loss 0.32204893231391907\n",
      "Epoch 22 -- Batch 354/ 842, training loss 0.32686519622802734\n",
      "Epoch 22 -- Batch 355/ 842, training loss 0.3216123878955841\n",
      "Epoch 22 -- Batch 356/ 842, training loss 0.3142877221107483\n",
      "Epoch 22 -- Batch 357/ 842, training loss 0.3184051513671875\n",
      "Epoch 22 -- Batch 358/ 842, training loss 0.33002662658691406\n",
      "Epoch 22 -- Batch 359/ 842, training loss 0.31477653980255127\n",
      "Epoch 22 -- Batch 360/ 842, training loss 0.33206886053085327\n",
      "Epoch 22 -- Batch 361/ 842, training loss 0.3110012710094452\n",
      "Epoch 22 -- Batch 362/ 842, training loss 0.33523857593536377\n",
      "Epoch 22 -- Batch 363/ 842, training loss 0.3308132588863373\n",
      "Epoch 22 -- Batch 364/ 842, training loss 0.3242141902446747\n",
      "Epoch 22 -- Batch 365/ 842, training loss 0.3137213885784149\n",
      "Epoch 22 -- Batch 366/ 842, training loss 0.31786417961120605\n",
      "Epoch 22 -- Batch 367/ 842, training loss 0.3301966190338135\n",
      "Epoch 22 -- Batch 368/ 842, training loss 0.31796029210090637\n",
      "Epoch 22 -- Batch 369/ 842, training loss 0.327945351600647\n",
      "Epoch 22 -- Batch 370/ 842, training loss 0.3224329948425293\n",
      "Epoch 22 -- Batch 371/ 842, training loss 0.31554722785949707\n",
      "Epoch 22 -- Batch 372/ 842, training loss 0.331292986869812\n",
      "Epoch 22 -- Batch 373/ 842, training loss 0.3317418098449707\n",
      "Epoch 22 -- Batch 374/ 842, training loss 0.3301509618759155\n",
      "Epoch 22 -- Batch 375/ 842, training loss 0.32092732191085815\n",
      "Epoch 22 -- Batch 376/ 842, training loss 0.3172784149646759\n",
      "Epoch 22 -- Batch 377/ 842, training loss 0.31385767459869385\n",
      "Epoch 22 -- Batch 378/ 842, training loss 0.3318270444869995\n",
      "Epoch 22 -- Batch 379/ 842, training loss 0.32893064618110657\n",
      "Epoch 22 -- Batch 380/ 842, training loss 0.3372488021850586\n",
      "Epoch 22 -- Batch 381/ 842, training loss 0.3210582435131073\n",
      "Epoch 22 -- Batch 382/ 842, training loss 0.32248422503471375\n",
      "Epoch 22 -- Batch 383/ 842, training loss 0.3358464241027832\n",
      "Epoch 22 -- Batch 384/ 842, training loss 0.32690706849098206\n",
      "Epoch 22 -- Batch 385/ 842, training loss 0.3139641582965851\n",
      "Epoch 22 -- Batch 386/ 842, training loss 0.3191494643688202\n",
      "Epoch 22 -- Batch 387/ 842, training loss 0.3212487995624542\n",
      "Epoch 22 -- Batch 388/ 842, training loss 0.32605865597724915\n",
      "Epoch 22 -- Batch 389/ 842, training loss 0.3192879557609558\n",
      "Epoch 22 -- Batch 390/ 842, training loss 0.32772180438041687\n",
      "Epoch 22 -- Batch 391/ 842, training loss 0.32845640182495117\n",
      "Epoch 22 -- Batch 392/ 842, training loss 0.3268238306045532\n",
      "Epoch 22 -- Batch 393/ 842, training loss 0.33789706230163574\n",
      "Epoch 22 -- Batch 394/ 842, training loss 0.3247283101081848\n",
      "Epoch 22 -- Batch 395/ 842, training loss 0.3158709704875946\n",
      "Epoch 22 -- Batch 396/ 842, training loss 0.3303436040878296\n",
      "Epoch 22 -- Batch 397/ 842, training loss 0.3182641863822937\n",
      "Epoch 22 -- Batch 398/ 842, training loss 0.3139508068561554\n",
      "Epoch 22 -- Batch 399/ 842, training loss 0.3138211965560913\n",
      "Epoch 22 -- Batch 400/ 842, training loss 0.3231903314590454\n",
      "Epoch 22 -- Batch 401/ 842, training loss 0.3244403004646301\n",
      "Epoch 22 -- Batch 402/ 842, training loss 0.33754783868789673\n",
      "Epoch 22 -- Batch 403/ 842, training loss 0.32544243335723877\n",
      "Epoch 22 -- Batch 404/ 842, training loss 0.3288835883140564\n",
      "Epoch 22 -- Batch 405/ 842, training loss 0.32442378997802734\n",
      "Epoch 22 -- Batch 406/ 842, training loss 0.32073041796684265\n",
      "Epoch 22 -- Batch 407/ 842, training loss 0.316325843334198\n",
      "Epoch 22 -- Batch 408/ 842, training loss 0.3074190616607666\n",
      "Epoch 22 -- Batch 409/ 842, training loss 0.3296976089477539\n",
      "Epoch 22 -- Batch 410/ 842, training loss 0.31111228466033936\n",
      "Epoch 22 -- Batch 411/ 842, training loss 0.3311159610748291\n",
      "Epoch 22 -- Batch 412/ 842, training loss 0.31434255838394165\n",
      "Epoch 22 -- Batch 413/ 842, training loss 0.3269760310649872\n",
      "Epoch 22 -- Batch 414/ 842, training loss 0.3087998628616333\n",
      "Epoch 22 -- Batch 415/ 842, training loss 0.3326564133167267\n",
      "Epoch 22 -- Batch 416/ 842, training loss 0.332494854927063\n",
      "Epoch 22 -- Batch 417/ 842, training loss 0.31440839171409607\n",
      "Epoch 22 -- Batch 418/ 842, training loss 0.32143375277519226\n",
      "Epoch 22 -- Batch 419/ 842, training loss 0.31863465905189514\n",
      "Epoch 22 -- Batch 420/ 842, training loss 0.3219670355319977\n",
      "Epoch 22 -- Batch 421/ 842, training loss 0.3147617280483246\n",
      "Epoch 22 -- Batch 422/ 842, training loss 0.328193336725235\n",
      "Epoch 22 -- Batch 423/ 842, training loss 0.3253527581691742\n",
      "Epoch 22 -- Batch 424/ 842, training loss 0.3227299451828003\n",
      "Epoch 22 -- Batch 425/ 842, training loss 0.324371337890625\n",
      "Epoch 22 -- Batch 426/ 842, training loss 0.3256950080394745\n",
      "Epoch 22 -- Batch 427/ 842, training loss 0.328146368265152\n",
      "Epoch 22 -- Batch 428/ 842, training loss 0.32017430663108826\n",
      "Epoch 22 -- Batch 429/ 842, training loss 0.3227306008338928\n",
      "Epoch 22 -- Batch 430/ 842, training loss 0.3180043399333954\n",
      "Epoch 22 -- Batch 431/ 842, training loss 0.3119843304157257\n",
      "Epoch 22 -- Batch 432/ 842, training loss 0.32032835483551025\n",
      "Epoch 22 -- Batch 433/ 842, training loss 0.3272828459739685\n",
      "Epoch 22 -- Batch 434/ 842, training loss 0.3237593173980713\n",
      "Epoch 22 -- Batch 435/ 842, training loss 0.31929105520248413\n",
      "Epoch 22 -- Batch 436/ 842, training loss 0.3336425721645355\n",
      "Epoch 22 -- Batch 437/ 842, training loss 0.3201873302459717\n",
      "Epoch 22 -- Batch 438/ 842, training loss 0.31813037395477295\n",
      "Epoch 22 -- Batch 439/ 842, training loss 0.31455862522125244\n",
      "Epoch 22 -- Batch 440/ 842, training loss 0.33171316981315613\n",
      "Epoch 22 -- Batch 441/ 842, training loss 0.327344685792923\n",
      "Epoch 22 -- Batch 442/ 842, training loss 0.328762412071228\n",
      "Epoch 22 -- Batch 443/ 842, training loss 0.33220335841178894\n",
      "Epoch 22 -- Batch 444/ 842, training loss 0.3195315897464752\n",
      "Epoch 22 -- Batch 445/ 842, training loss 0.321688175201416\n",
      "Epoch 22 -- Batch 446/ 842, training loss 0.3249971568584442\n",
      "Epoch 22 -- Batch 447/ 842, training loss 0.3233717978000641\n",
      "Epoch 22 -- Batch 448/ 842, training loss 0.31837499141693115\n",
      "Epoch 22 -- Batch 449/ 842, training loss 0.314497709274292\n",
      "Epoch 22 -- Batch 450/ 842, training loss 0.3268001675605774\n",
      "Epoch 22 -- Batch 451/ 842, training loss 0.33404913544654846\n",
      "Epoch 22 -- Batch 452/ 842, training loss 0.3303658366203308\n",
      "Epoch 22 -- Batch 453/ 842, training loss 0.3132077157497406\n",
      "Epoch 22 -- Batch 454/ 842, training loss 0.32317546010017395\n",
      "Epoch 22 -- Batch 455/ 842, training loss 0.3204828202724457\n",
      "Epoch 22 -- Batch 456/ 842, training loss 0.3279949724674225\n",
      "Epoch 22 -- Batch 457/ 842, training loss 0.3253951966762543\n",
      "Epoch 22 -- Batch 458/ 842, training loss 0.32325172424316406\n",
      "Epoch 22 -- Batch 459/ 842, training loss 0.3169748783111572\n",
      "Epoch 22 -- Batch 460/ 842, training loss 0.3179701864719391\n",
      "Epoch 22 -- Batch 461/ 842, training loss 0.308516263961792\n",
      "Epoch 22 -- Batch 462/ 842, training loss 0.31684610247612\n",
      "Epoch 22 -- Batch 463/ 842, training loss 0.31375646591186523\n",
      "Epoch 22 -- Batch 464/ 842, training loss 0.3219679296016693\n",
      "Epoch 22 -- Batch 465/ 842, training loss 0.3239760398864746\n",
      "Epoch 22 -- Batch 466/ 842, training loss 0.32571762800216675\n",
      "Epoch 22 -- Batch 467/ 842, training loss 0.3203924894332886\n",
      "Epoch 22 -- Batch 468/ 842, training loss 0.3261535167694092\n",
      "Epoch 22 -- Batch 469/ 842, training loss 0.31736230850219727\n",
      "Epoch 22 -- Batch 470/ 842, training loss 0.3145042657852173\n",
      "Epoch 22 -- Batch 471/ 842, training loss 0.3224179446697235\n",
      "Epoch 22 -- Batch 472/ 842, training loss 0.3176354169845581\n",
      "Epoch 22 -- Batch 473/ 842, training loss 0.3192601799964905\n",
      "Epoch 22 -- Batch 474/ 842, training loss 0.3131217360496521\n",
      "Epoch 22 -- Batch 475/ 842, training loss 0.32623058557510376\n",
      "Epoch 22 -- Batch 476/ 842, training loss 0.3189575970172882\n",
      "Epoch 22 -- Batch 477/ 842, training loss 0.31078869104385376\n",
      "Epoch 22 -- Batch 478/ 842, training loss 0.3119429051876068\n",
      "Epoch 22 -- Batch 479/ 842, training loss 0.30454352498054504\n",
      "Epoch 22 -- Batch 480/ 842, training loss 0.32916000485420227\n",
      "Epoch 22 -- Batch 481/ 842, training loss 0.32504722476005554\n",
      "Epoch 22 -- Batch 482/ 842, training loss 0.33480769395828247\n",
      "Epoch 22 -- Batch 483/ 842, training loss 0.31943583488464355\n",
      "Epoch 22 -- Batch 484/ 842, training loss 0.31953510642051697\n",
      "Epoch 22 -- Batch 485/ 842, training loss 0.3233776092529297\n",
      "Epoch 22 -- Batch 486/ 842, training loss 0.325141578912735\n",
      "Epoch 22 -- Batch 487/ 842, training loss 0.32659754157066345\n",
      "Epoch 22 -- Batch 488/ 842, training loss 0.3207316994667053\n",
      "Epoch 22 -- Batch 489/ 842, training loss 0.31670400500297546\n",
      "Epoch 22 -- Batch 490/ 842, training loss 0.3224552571773529\n",
      "Epoch 22 -- Batch 491/ 842, training loss 0.3231213092803955\n",
      "Epoch 22 -- Batch 492/ 842, training loss 0.33246085047721863\n",
      "Epoch 22 -- Batch 493/ 842, training loss 0.3337671756744385\n",
      "Epoch 22 -- Batch 494/ 842, training loss 0.32078322768211365\n",
      "Epoch 22 -- Batch 495/ 842, training loss 0.3165971338748932\n",
      "Epoch 22 -- Batch 496/ 842, training loss 0.3304475247859955\n",
      "Epoch 22 -- Batch 497/ 842, training loss 0.3330127000808716\n",
      "Epoch 22 -- Batch 498/ 842, training loss 0.3266158401966095\n",
      "Epoch 22 -- Batch 499/ 842, training loss 0.3076029121875763\n",
      "Epoch 22 -- Batch 500/ 842, training loss 0.32782888412475586\n",
      "Epoch 22 -- Batch 501/ 842, training loss 0.3166952431201935\n",
      "Epoch 22 -- Batch 502/ 842, training loss 0.3128988742828369\n",
      "Epoch 22 -- Batch 503/ 842, training loss 0.3234136700630188\n",
      "Epoch 22 -- Batch 504/ 842, training loss 0.31429919600486755\n",
      "Epoch 22 -- Batch 505/ 842, training loss 0.32680588960647583\n",
      "Epoch 22 -- Batch 506/ 842, training loss 0.32639697194099426\n",
      "Epoch 22 -- Batch 507/ 842, training loss 0.3315673768520355\n",
      "Epoch 22 -- Batch 508/ 842, training loss 0.32309725880622864\n",
      "Epoch 22 -- Batch 509/ 842, training loss 0.3232277035713196\n",
      "Epoch 22 -- Batch 510/ 842, training loss 0.3216271996498108\n",
      "Epoch 22 -- Batch 511/ 842, training loss 0.32676947116851807\n",
      "Epoch 22 -- Batch 512/ 842, training loss 0.3170413374900818\n",
      "Epoch 22 -- Batch 513/ 842, training loss 0.3191668689250946\n",
      "Epoch 22 -- Batch 514/ 842, training loss 0.3234984874725342\n",
      "Epoch 22 -- Batch 515/ 842, training loss 0.30605244636535645\n",
      "Epoch 22 -- Batch 516/ 842, training loss 0.3255438208580017\n",
      "Epoch 22 -- Batch 517/ 842, training loss 0.3084569573402405\n",
      "Epoch 22 -- Batch 518/ 842, training loss 0.3286697268486023\n",
      "Epoch 22 -- Batch 519/ 842, training loss 0.32326000928878784\n",
      "Epoch 22 -- Batch 520/ 842, training loss 0.3153032660484314\n",
      "Epoch 22 -- Batch 521/ 842, training loss 0.3323131203651428\n",
      "Epoch 22 -- Batch 522/ 842, training loss 0.32666414976119995\n",
      "Epoch 22 -- Batch 523/ 842, training loss 0.32491573691368103\n",
      "Epoch 22 -- Batch 524/ 842, training loss 0.32400551438331604\n",
      "Epoch 22 -- Batch 525/ 842, training loss 0.30714765191078186\n",
      "Epoch 22 -- Batch 526/ 842, training loss 0.31890690326690674\n",
      "Epoch 22 -- Batch 527/ 842, training loss 0.32691970467567444\n",
      "Epoch 22 -- Batch 528/ 842, training loss 0.30363455414772034\n",
      "Epoch 22 -- Batch 529/ 842, training loss 0.3297824263572693\n",
      "Epoch 22 -- Batch 530/ 842, training loss 0.3219473659992218\n",
      "Epoch 22 -- Batch 531/ 842, training loss 0.31726527214050293\n",
      "Epoch 22 -- Batch 532/ 842, training loss 0.31572699546813965\n",
      "Epoch 22 -- Batch 533/ 842, training loss 0.31590738892555237\n",
      "Epoch 22 -- Batch 534/ 842, training loss 0.31980904936790466\n",
      "Epoch 22 -- Batch 535/ 842, training loss 0.31481847167015076\n",
      "Epoch 22 -- Batch 536/ 842, training loss 0.32956844568252563\n",
      "Epoch 22 -- Batch 537/ 842, training loss 0.3134647309780121\n",
      "Epoch 22 -- Batch 538/ 842, training loss 0.3272806406021118\n",
      "Epoch 22 -- Batch 539/ 842, training loss 0.3146524131298065\n",
      "Epoch 22 -- Batch 540/ 842, training loss 0.3063136339187622\n",
      "Epoch 22 -- Batch 541/ 842, training loss 0.32106807827949524\n",
      "Epoch 22 -- Batch 542/ 842, training loss 0.3318318724632263\n",
      "Epoch 22 -- Batch 543/ 842, training loss 0.32638752460479736\n",
      "Epoch 22 -- Batch 544/ 842, training loss 0.3066495954990387\n",
      "Epoch 22 -- Batch 545/ 842, training loss 0.327280193567276\n",
      "Epoch 22 -- Batch 546/ 842, training loss 0.32555294036865234\n",
      "Epoch 22 -- Batch 547/ 842, training loss 0.32898104190826416\n",
      "Epoch 22 -- Batch 548/ 842, training loss 0.3250686824321747\n",
      "Epoch 22 -- Batch 549/ 842, training loss 0.3182664215564728\n",
      "Epoch 22 -- Batch 550/ 842, training loss 0.3210461139678955\n",
      "Epoch 22 -- Batch 551/ 842, training loss 0.30453428626060486\n",
      "Epoch 22 -- Batch 552/ 842, training loss 0.31761887669563293\n",
      "Epoch 22 -- Batch 553/ 842, training loss 0.32257145643234253\n",
      "Epoch 22 -- Batch 554/ 842, training loss 0.32445693016052246\n",
      "Epoch 22 -- Batch 555/ 842, training loss 0.33434656262397766\n",
      "Epoch 22 -- Batch 556/ 842, training loss 0.3217068910598755\n",
      "Epoch 22 -- Batch 557/ 842, training loss 0.338062584400177\n",
      "Epoch 22 -- Batch 558/ 842, training loss 0.32113051414489746\n",
      "Epoch 22 -- Batch 559/ 842, training loss 0.31789496541023254\n",
      "Epoch 22 -- Batch 560/ 842, training loss 0.32018306851387024\n",
      "Epoch 22 -- Batch 561/ 842, training loss 0.3226337432861328\n",
      "Epoch 22 -- Batch 562/ 842, training loss 0.3144644796848297\n",
      "Epoch 22 -- Batch 563/ 842, training loss 0.3139432370662689\n",
      "Epoch 22 -- Batch 564/ 842, training loss 0.326427161693573\n",
      "Epoch 22 -- Batch 565/ 842, training loss 0.32904964685440063\n",
      "Epoch 22 -- Batch 566/ 842, training loss 0.33723047375679016\n",
      "Epoch 22 -- Batch 567/ 842, training loss 0.3234243094921112\n",
      "Epoch 22 -- Batch 568/ 842, training loss 0.31989815831184387\n",
      "Epoch 22 -- Batch 569/ 842, training loss 0.31172382831573486\n",
      "Epoch 22 -- Batch 570/ 842, training loss 0.3085166811943054\n",
      "Epoch 22 -- Batch 571/ 842, training loss 0.30831825733184814\n",
      "Epoch 22 -- Batch 572/ 842, training loss 0.33858227729797363\n",
      "Epoch 22 -- Batch 573/ 842, training loss 0.328277051448822\n",
      "Epoch 22 -- Batch 574/ 842, training loss 0.32676148414611816\n",
      "Epoch 22 -- Batch 575/ 842, training loss 0.3305858373641968\n",
      "Epoch 22 -- Batch 576/ 842, training loss 0.3163529634475708\n",
      "Epoch 22 -- Batch 577/ 842, training loss 0.33757779002189636\n",
      "Epoch 22 -- Batch 578/ 842, training loss 0.31958505511283875\n",
      "Epoch 22 -- Batch 579/ 842, training loss 0.3253944516181946\n",
      "Epoch 22 -- Batch 580/ 842, training loss 0.3188093900680542\n",
      "Epoch 22 -- Batch 581/ 842, training loss 0.31949371099472046\n",
      "Epoch 22 -- Batch 582/ 842, training loss 0.32233157753944397\n",
      "Epoch 22 -- Batch 583/ 842, training loss 0.3319922685623169\n",
      "Epoch 22 -- Batch 584/ 842, training loss 0.3309547007083893\n",
      "Epoch 22 -- Batch 585/ 842, training loss 0.32390525937080383\n",
      "Epoch 22 -- Batch 586/ 842, training loss 0.31844526529312134\n",
      "Epoch 22 -- Batch 587/ 842, training loss 0.32578253746032715\n",
      "Epoch 22 -- Batch 588/ 842, training loss 0.32383593916893005\n",
      "Epoch 22 -- Batch 589/ 842, training loss 0.3272969424724579\n",
      "Epoch 22 -- Batch 590/ 842, training loss 0.33007946610450745\n",
      "Epoch 22 -- Batch 591/ 842, training loss 0.3183763921260834\n",
      "Epoch 22 -- Batch 592/ 842, training loss 0.328306645154953\n",
      "Epoch 22 -- Batch 593/ 842, training loss 0.3091110587120056\n",
      "Epoch 22 -- Batch 594/ 842, training loss 0.3275255858898163\n",
      "Epoch 22 -- Batch 595/ 842, training loss 0.32825541496276855\n",
      "Epoch 22 -- Batch 596/ 842, training loss 0.30765300989151\n",
      "Epoch 22 -- Batch 597/ 842, training loss 0.3321845233440399\n",
      "Epoch 22 -- Batch 598/ 842, training loss 0.31790095567703247\n",
      "Epoch 22 -- Batch 599/ 842, training loss 0.32332664728164673\n",
      "Epoch 22 -- Batch 600/ 842, training loss 0.32439032196998596\n",
      "Epoch 22 -- Batch 601/ 842, training loss 0.31535741686820984\n",
      "Epoch 22 -- Batch 602/ 842, training loss 0.33590927720069885\n",
      "Epoch 22 -- Batch 603/ 842, training loss 0.31008264422416687\n",
      "Epoch 22 -- Batch 604/ 842, training loss 0.3158635199069977\n",
      "Epoch 22 -- Batch 605/ 842, training loss 0.32501742243766785\n",
      "Epoch 22 -- Batch 606/ 842, training loss 0.32775118947029114\n",
      "Epoch 22 -- Batch 607/ 842, training loss 0.3146805465221405\n",
      "Epoch 22 -- Batch 608/ 842, training loss 0.33608320355415344\n",
      "Epoch 22 -- Batch 609/ 842, training loss 0.3300611078739166\n",
      "Epoch 22 -- Batch 610/ 842, training loss 0.32591545581817627\n",
      "Epoch 22 -- Batch 611/ 842, training loss 0.31341591477394104\n",
      "Epoch 22 -- Batch 612/ 842, training loss 0.32556626200675964\n",
      "Epoch 22 -- Batch 613/ 842, training loss 0.3171957731246948\n",
      "Epoch 22 -- Batch 614/ 842, training loss 0.3313677906990051\n",
      "Epoch 22 -- Batch 615/ 842, training loss 0.3263235092163086\n",
      "Epoch 22 -- Batch 616/ 842, training loss 0.32359394431114197\n",
      "Epoch 22 -- Batch 617/ 842, training loss 0.32522818446159363\n",
      "Epoch 22 -- Batch 618/ 842, training loss 0.32613760232925415\n",
      "Epoch 22 -- Batch 619/ 842, training loss 0.31609439849853516\n",
      "Epoch 22 -- Batch 620/ 842, training loss 0.32879313826560974\n",
      "Epoch 22 -- Batch 621/ 842, training loss 0.32353365421295166\n",
      "Epoch 22 -- Batch 622/ 842, training loss 0.3143588602542877\n",
      "Epoch 22 -- Batch 623/ 842, training loss 0.32467198371887207\n",
      "Epoch 22 -- Batch 624/ 842, training loss 0.3222940266132355\n",
      "Epoch 22 -- Batch 625/ 842, training loss 0.3400380313396454\n",
      "Epoch 22 -- Batch 626/ 842, training loss 0.3213335871696472\n",
      "Epoch 22 -- Batch 627/ 842, training loss 0.3264349400997162\n",
      "Epoch 22 -- Batch 628/ 842, training loss 0.3195355236530304\n",
      "Epoch 22 -- Batch 629/ 842, training loss 0.3333336114883423\n",
      "Epoch 22 -- Batch 630/ 842, training loss 0.3362676501274109\n",
      "Epoch 22 -- Batch 631/ 842, training loss 0.3266315162181854\n",
      "Epoch 22 -- Batch 632/ 842, training loss 0.32604095339775085\n",
      "Epoch 22 -- Batch 633/ 842, training loss 0.3190639317035675\n",
      "Epoch 22 -- Batch 634/ 842, training loss 0.3308279812335968\n",
      "Epoch 22 -- Batch 635/ 842, training loss 0.3295738101005554\n",
      "Epoch 22 -- Batch 636/ 842, training loss 0.31229427456855774\n",
      "Epoch 22 -- Batch 637/ 842, training loss 0.3217933773994446\n",
      "Epoch 22 -- Batch 638/ 842, training loss 0.32671189308166504\n",
      "Epoch 22 -- Batch 639/ 842, training loss 0.3403806686401367\n",
      "Epoch 22 -- Batch 640/ 842, training loss 0.3165619373321533\n",
      "Epoch 22 -- Batch 641/ 842, training loss 0.3195357024669647\n",
      "Epoch 22 -- Batch 642/ 842, training loss 0.3246718645095825\n",
      "Epoch 22 -- Batch 643/ 842, training loss 0.32386383414268494\n",
      "Epoch 22 -- Batch 644/ 842, training loss 0.34174081683158875\n",
      "Epoch 22 -- Batch 645/ 842, training loss 0.3143528997898102\n",
      "Epoch 22 -- Batch 646/ 842, training loss 0.3199380934238434\n",
      "Epoch 22 -- Batch 647/ 842, training loss 0.329556941986084\n",
      "Epoch 22 -- Batch 648/ 842, training loss 0.325139582157135\n",
      "Epoch 22 -- Batch 649/ 842, training loss 0.3156713545322418\n",
      "Epoch 22 -- Batch 650/ 842, training loss 0.31375283002853394\n",
      "Epoch 22 -- Batch 651/ 842, training loss 0.32651185989379883\n",
      "Epoch 22 -- Batch 652/ 842, training loss 0.3134961426258087\n",
      "Epoch 22 -- Batch 653/ 842, training loss 0.32252517342567444\n",
      "Epoch 22 -- Batch 654/ 842, training loss 0.3154338598251343\n",
      "Epoch 22 -- Batch 655/ 842, training loss 0.31877002120018005\n",
      "Epoch 22 -- Batch 656/ 842, training loss 0.31892114877700806\n",
      "Epoch 22 -- Batch 657/ 842, training loss 0.32541462779045105\n",
      "Epoch 22 -- Batch 658/ 842, training loss 0.320453017950058\n",
      "Epoch 22 -- Batch 659/ 842, training loss 0.32334819436073303\n",
      "Epoch 22 -- Batch 660/ 842, training loss 0.3204783797264099\n",
      "Epoch 22 -- Batch 661/ 842, training loss 0.32008230686187744\n",
      "Epoch 22 -- Batch 662/ 842, training loss 0.3174937069416046\n",
      "Epoch 22 -- Batch 663/ 842, training loss 0.32042810320854187\n",
      "Epoch 22 -- Batch 664/ 842, training loss 0.3168160319328308\n",
      "Epoch 22 -- Batch 665/ 842, training loss 0.3191337287425995\n",
      "Epoch 22 -- Batch 666/ 842, training loss 0.3264605402946472\n",
      "Epoch 22 -- Batch 667/ 842, training loss 0.32314252853393555\n",
      "Epoch 22 -- Batch 668/ 842, training loss 0.3255194127559662\n",
      "Epoch 22 -- Batch 669/ 842, training loss 0.31558310985565186\n",
      "Epoch 22 -- Batch 670/ 842, training loss 0.33313414454460144\n",
      "Epoch 22 -- Batch 671/ 842, training loss 0.32106173038482666\n",
      "Epoch 22 -- Batch 672/ 842, training loss 0.3141234815120697\n",
      "Epoch 22 -- Batch 673/ 842, training loss 0.3233029842376709\n",
      "Epoch 22 -- Batch 674/ 842, training loss 0.32300469279289246\n",
      "Epoch 22 -- Batch 675/ 842, training loss 0.31813332438468933\n",
      "Epoch 22 -- Batch 676/ 842, training loss 0.3201989233493805\n",
      "Epoch 22 -- Batch 677/ 842, training loss 0.3240945041179657\n",
      "Epoch 22 -- Batch 678/ 842, training loss 0.3322906196117401\n",
      "Epoch 22 -- Batch 679/ 842, training loss 0.3234803080558777\n",
      "Epoch 22 -- Batch 680/ 842, training loss 0.3227616846561432\n",
      "Epoch 22 -- Batch 681/ 842, training loss 0.3258730471134186\n",
      "Epoch 22 -- Batch 682/ 842, training loss 0.3269411027431488\n",
      "Epoch 22 -- Batch 683/ 842, training loss 0.33152806758880615\n",
      "Epoch 22 -- Batch 684/ 842, training loss 0.31812188029289246\n",
      "Epoch 22 -- Batch 685/ 842, training loss 0.31512629985809326\n",
      "Epoch 22 -- Batch 686/ 842, training loss 0.31777477264404297\n",
      "Epoch 22 -- Batch 687/ 842, training loss 0.3110463321208954\n",
      "Epoch 22 -- Batch 688/ 842, training loss 0.31394317746162415\n",
      "Epoch 22 -- Batch 689/ 842, training loss 0.32737284898757935\n",
      "Epoch 22 -- Batch 690/ 842, training loss 0.31335684657096863\n",
      "Epoch 22 -- Batch 691/ 842, training loss 0.32105109095573425\n",
      "Epoch 22 -- Batch 692/ 842, training loss 0.3183639645576477\n",
      "Epoch 22 -- Batch 693/ 842, training loss 0.3234858810901642\n",
      "Epoch 22 -- Batch 694/ 842, training loss 0.33666834235191345\n",
      "Epoch 22 -- Batch 695/ 842, training loss 0.3307473957538605\n",
      "Epoch 22 -- Batch 696/ 842, training loss 0.32974886894226074\n",
      "Epoch 22 -- Batch 697/ 842, training loss 0.32078006863594055\n",
      "Epoch 22 -- Batch 698/ 842, training loss 0.3158968687057495\n",
      "Epoch 22 -- Batch 699/ 842, training loss 0.33401820063591003\n",
      "Epoch 22 -- Batch 700/ 842, training loss 0.33178937435150146\n",
      "Epoch 22 -- Batch 701/ 842, training loss 0.3196064829826355\n",
      "Epoch 22 -- Batch 702/ 842, training loss 0.31585150957107544\n",
      "Epoch 22 -- Batch 703/ 842, training loss 0.3119668662548065\n",
      "Epoch 22 -- Batch 704/ 842, training loss 0.3185168206691742\n",
      "Epoch 22 -- Batch 705/ 842, training loss 0.3197146952152252\n",
      "Epoch 22 -- Batch 706/ 842, training loss 0.30631470680236816\n",
      "Epoch 22 -- Batch 707/ 842, training loss 0.3162294030189514\n",
      "Epoch 22 -- Batch 708/ 842, training loss 0.32232677936553955\n",
      "Epoch 22 -- Batch 709/ 842, training loss 0.3314819037914276\n",
      "Epoch 22 -- Batch 710/ 842, training loss 0.3239195644855499\n",
      "Epoch 22 -- Batch 711/ 842, training loss 0.331084281206131\n",
      "Epoch 22 -- Batch 712/ 842, training loss 0.3189435303211212\n",
      "Epoch 22 -- Batch 713/ 842, training loss 0.325645387172699\n",
      "Epoch 22 -- Batch 714/ 842, training loss 0.32112324237823486\n",
      "Epoch 22 -- Batch 715/ 842, training loss 0.31552737951278687\n",
      "Epoch 22 -- Batch 716/ 842, training loss 0.3193202018737793\n",
      "Epoch 22 -- Batch 717/ 842, training loss 0.3246851861476898\n",
      "Epoch 22 -- Batch 718/ 842, training loss 0.3186468780040741\n",
      "Epoch 22 -- Batch 719/ 842, training loss 0.33326685428619385\n",
      "Epoch 22 -- Batch 720/ 842, training loss 0.323527067899704\n",
      "Epoch 22 -- Batch 721/ 842, training loss 0.3281034827232361\n",
      "Epoch 22 -- Batch 722/ 842, training loss 0.3166466951370239\n",
      "Epoch 22 -- Batch 723/ 842, training loss 0.3197901248931885\n",
      "Epoch 22 -- Batch 724/ 842, training loss 0.32912883162498474\n",
      "Epoch 22 -- Batch 725/ 842, training loss 0.32392600178718567\n",
      "Epoch 22 -- Batch 726/ 842, training loss 0.3288421630859375\n",
      "Epoch 22 -- Batch 727/ 842, training loss 0.32836776971817017\n",
      "Epoch 22 -- Batch 728/ 842, training loss 0.3202875554561615\n",
      "Epoch 22 -- Batch 729/ 842, training loss 0.32498738169670105\n",
      "Epoch 22 -- Batch 730/ 842, training loss 0.3306763768196106\n",
      "Epoch 22 -- Batch 731/ 842, training loss 0.3325149714946747\n",
      "Epoch 22 -- Batch 732/ 842, training loss 0.3202653229236603\n",
      "Epoch 22 -- Batch 733/ 842, training loss 0.32691556215286255\n",
      "Epoch 22 -- Batch 734/ 842, training loss 0.3241601288318634\n",
      "Epoch 22 -- Batch 735/ 842, training loss 0.31556031107902527\n",
      "Epoch 22 -- Batch 736/ 842, training loss 0.3251083195209503\n",
      "Epoch 22 -- Batch 737/ 842, training loss 0.3188341557979584\n",
      "Epoch 22 -- Batch 738/ 842, training loss 0.3240686058998108\n",
      "Epoch 22 -- Batch 739/ 842, training loss 0.33532649278640747\n",
      "Epoch 22 -- Batch 740/ 842, training loss 0.3203294277191162\n",
      "Epoch 22 -- Batch 741/ 842, training loss 0.3346270024776459\n",
      "Epoch 22 -- Batch 742/ 842, training loss 0.3304813802242279\n",
      "Epoch 22 -- Batch 743/ 842, training loss 0.3184671401977539\n",
      "Epoch 22 -- Batch 744/ 842, training loss 0.3297714293003082\n",
      "Epoch 22 -- Batch 745/ 842, training loss 0.3118697702884674\n",
      "Epoch 22 -- Batch 746/ 842, training loss 0.324895441532135\n",
      "Epoch 22 -- Batch 747/ 842, training loss 0.33113494515419006\n",
      "Epoch 22 -- Batch 748/ 842, training loss 0.33069562911987305\n",
      "Epoch 22 -- Batch 749/ 842, training loss 0.3289805054664612\n",
      "Epoch 22 -- Batch 750/ 842, training loss 0.32573893666267395\n",
      "Epoch 22 -- Batch 751/ 842, training loss 0.32256779074668884\n",
      "Epoch 22 -- Batch 752/ 842, training loss 0.32933393120765686\n",
      "Epoch 22 -- Batch 753/ 842, training loss 0.3237918019294739\n",
      "Epoch 22 -- Batch 754/ 842, training loss 0.3114452064037323\n",
      "Epoch 22 -- Batch 755/ 842, training loss 0.31362485885620117\n",
      "Epoch 22 -- Batch 756/ 842, training loss 0.3278656601905823\n",
      "Epoch 22 -- Batch 757/ 842, training loss 0.31527385115623474\n",
      "Epoch 22 -- Batch 758/ 842, training loss 0.329489529132843\n",
      "Epoch 22 -- Batch 759/ 842, training loss 0.3337254524230957\n",
      "Epoch 22 -- Batch 760/ 842, training loss 0.3176669478416443\n",
      "Epoch 22 -- Batch 761/ 842, training loss 0.3231313228607178\n",
      "Epoch 22 -- Batch 762/ 842, training loss 0.31669357419013977\n",
      "Epoch 22 -- Batch 763/ 842, training loss 0.33348548412323\n",
      "Epoch 22 -- Batch 764/ 842, training loss 0.3152042329311371\n",
      "Epoch 22 -- Batch 765/ 842, training loss 0.33658960461616516\n",
      "Epoch 22 -- Batch 766/ 842, training loss 0.3217761814594269\n",
      "Epoch 22 -- Batch 767/ 842, training loss 0.3196052312850952\n",
      "Epoch 22 -- Batch 768/ 842, training loss 0.33046698570251465\n",
      "Epoch 22 -- Batch 769/ 842, training loss 0.3171911835670471\n",
      "Epoch 22 -- Batch 770/ 842, training loss 0.3186022937297821\n",
      "Epoch 22 -- Batch 771/ 842, training loss 0.3264007866382599\n",
      "Epoch 22 -- Batch 772/ 842, training loss 0.32554030418395996\n",
      "Epoch 22 -- Batch 773/ 842, training loss 0.3210577368736267\n",
      "Epoch 22 -- Batch 774/ 842, training loss 0.31994664669036865\n",
      "Epoch 22 -- Batch 775/ 842, training loss 0.3325064480304718\n",
      "Epoch 22 -- Batch 776/ 842, training loss 0.31838852167129517\n",
      "Epoch 22 -- Batch 777/ 842, training loss 0.31696853041648865\n",
      "Epoch 22 -- Batch 778/ 842, training loss 0.3196772634983063\n",
      "Epoch 22 -- Batch 779/ 842, training loss 0.3127482235431671\n",
      "Epoch 22 -- Batch 780/ 842, training loss 0.3240240514278412\n",
      "Epoch 22 -- Batch 781/ 842, training loss 0.3208611011505127\n",
      "Epoch 22 -- Batch 782/ 842, training loss 0.32213494181632996\n",
      "Epoch 22 -- Batch 783/ 842, training loss 0.3169418275356293\n",
      "Epoch 22 -- Batch 784/ 842, training loss 0.33496055006980896\n",
      "Epoch 22 -- Batch 785/ 842, training loss 0.3099071681499481\n",
      "Epoch 22 -- Batch 786/ 842, training loss 0.3239414691925049\n",
      "Epoch 22 -- Batch 787/ 842, training loss 0.328795850276947\n",
      "Epoch 22 -- Batch 788/ 842, training loss 0.33117255568504333\n",
      "Epoch 22 -- Batch 789/ 842, training loss 0.33186525106430054\n",
      "Epoch 22 -- Batch 790/ 842, training loss 0.3312048017978668\n",
      "Epoch 22 -- Batch 791/ 842, training loss 0.3214050531387329\n",
      "Epoch 22 -- Batch 792/ 842, training loss 0.3202041983604431\n",
      "Epoch 22 -- Batch 793/ 842, training loss 0.3262307047843933\n",
      "Epoch 22 -- Batch 794/ 842, training loss 0.3278713822364807\n",
      "Epoch 22 -- Batch 795/ 842, training loss 0.32116466760635376\n",
      "Epoch 22 -- Batch 796/ 842, training loss 0.3260909914970398\n",
      "Epoch 22 -- Batch 797/ 842, training loss 0.3164967894554138\n",
      "Epoch 22 -- Batch 798/ 842, training loss 0.31901246309280396\n",
      "Epoch 22 -- Batch 799/ 842, training loss 0.3274666666984558\n",
      "Epoch 22 -- Batch 800/ 842, training loss 0.3221408426761627\n",
      "Epoch 22 -- Batch 801/ 842, training loss 0.3391859829425812\n",
      "Epoch 22 -- Batch 802/ 842, training loss 0.32708901166915894\n",
      "Epoch 22 -- Batch 803/ 842, training loss 0.32342860102653503\n",
      "Epoch 22 -- Batch 804/ 842, training loss 0.3189294636249542\n",
      "Epoch 22 -- Batch 805/ 842, training loss 0.3230447471141815\n",
      "Epoch 22 -- Batch 806/ 842, training loss 0.3213467001914978\n",
      "Epoch 22 -- Batch 807/ 842, training loss 0.3355425000190735\n",
      "Epoch 22 -- Batch 808/ 842, training loss 0.32984182238578796\n",
      "Epoch 22 -- Batch 809/ 842, training loss 0.32372114062309265\n",
      "Epoch 22 -- Batch 810/ 842, training loss 0.3248087465763092\n",
      "Epoch 22 -- Batch 811/ 842, training loss 0.3227163553237915\n",
      "Epoch 22 -- Batch 812/ 842, training loss 0.30939409136772156\n",
      "Epoch 22 -- Batch 813/ 842, training loss 0.3177791237831116\n",
      "Epoch 22 -- Batch 814/ 842, training loss 0.3298835754394531\n",
      "Epoch 22 -- Batch 815/ 842, training loss 0.3263057470321655\n",
      "Epoch 22 -- Batch 816/ 842, training loss 0.3203182816505432\n",
      "Epoch 22 -- Batch 817/ 842, training loss 0.3367137610912323\n",
      "Epoch 22 -- Batch 818/ 842, training loss 0.32705995440483093\n",
      "Epoch 22 -- Batch 819/ 842, training loss 0.3272045850753784\n",
      "Epoch 22 -- Batch 820/ 842, training loss 0.33896180987358093\n",
      "Epoch 22 -- Batch 821/ 842, training loss 0.3107970356941223\n",
      "Epoch 22 -- Batch 822/ 842, training loss 0.3198540508747101\n",
      "Epoch 22 -- Batch 823/ 842, training loss 0.32982057332992554\n",
      "Epoch 22 -- Batch 824/ 842, training loss 0.3249577283859253\n",
      "Epoch 22 -- Batch 825/ 842, training loss 0.32066503167152405\n",
      "Epoch 22 -- Batch 826/ 842, training loss 0.3163115680217743\n",
      "Epoch 22 -- Batch 827/ 842, training loss 0.3169716000556946\n",
      "Epoch 22 -- Batch 828/ 842, training loss 0.3298474848270416\n",
      "Epoch 22 -- Batch 829/ 842, training loss 0.3222436308860779\n",
      "Epoch 22 -- Batch 830/ 842, training loss 0.3195647597312927\n",
      "Epoch 22 -- Batch 831/ 842, training loss 0.3110560178756714\n",
      "Epoch 22 -- Batch 832/ 842, training loss 0.31795674562454224\n",
      "Epoch 22 -- Batch 833/ 842, training loss 0.31247052550315857\n",
      "Epoch 22 -- Batch 834/ 842, training loss 0.32439637184143066\n",
      "Epoch 22 -- Batch 835/ 842, training loss 0.3275192975997925\n",
      "Epoch 22 -- Batch 836/ 842, training loss 0.32579919695854187\n",
      "Epoch 22 -- Batch 837/ 842, training loss 0.31872060894966125\n",
      "Epoch 22 -- Batch 838/ 842, training loss 0.3264658749103546\n",
      "Epoch 22 -- Batch 839/ 842, training loss 0.30518871545791626\n",
      "Epoch 22 -- Batch 840/ 842, training loss 0.30560189485549927\n",
      "Epoch 22 -- Batch 841/ 842, training loss 0.31972384452819824\n",
      "Epoch 22 -- Batch 842/ 842, training loss 0.37021511793136597\n",
      "----------------------------------------------------------------------\n",
      "Epoch 22 -- Batch 1/ 94, validation loss 0.3155995309352875\n",
      "Epoch 22 -- Batch 2/ 94, validation loss 0.31216681003570557\n",
      "Epoch 22 -- Batch 3/ 94, validation loss 0.3077719211578369\n",
      "Epoch 22 -- Batch 4/ 94, validation loss 0.3102603256702423\n",
      "Epoch 22 -- Batch 5/ 94, validation loss 0.31837427616119385\n",
      "Epoch 22 -- Batch 6/ 94, validation loss 0.31814903020858765\n",
      "Epoch 22 -- Batch 7/ 94, validation loss 0.3111756443977356\n",
      "Epoch 22 -- Batch 8/ 94, validation loss 0.31774961948394775\n",
      "Epoch 22 -- Batch 9/ 94, validation loss 0.3004927635192871\n",
      "Epoch 22 -- Batch 10/ 94, validation loss 0.29948890209198\n",
      "Epoch 22 -- Batch 11/ 94, validation loss 0.30723837018013\n",
      "Epoch 22 -- Batch 12/ 94, validation loss 0.3103415071964264\n",
      "Epoch 22 -- Batch 13/ 94, validation loss 0.3414880037307739\n",
      "Epoch 22 -- Batch 14/ 94, validation loss 0.3148624300956726\n",
      "Epoch 22 -- Batch 15/ 94, validation loss 0.3118588924407959\n",
      "Epoch 22 -- Batch 16/ 94, validation loss 0.31598803400993347\n",
      "Epoch 22 -- Batch 17/ 94, validation loss 0.3153989315032959\n",
      "Epoch 22 -- Batch 18/ 94, validation loss 0.3051414489746094\n",
      "Epoch 22 -- Batch 19/ 94, validation loss 0.3049009144306183\n",
      "Epoch 22 -- Batch 20/ 94, validation loss 0.3178856074810028\n",
      "Epoch 22 -- Batch 21/ 94, validation loss 0.32488924264907837\n",
      "Epoch 22 -- Batch 22/ 94, validation loss 0.30893269181251526\n",
      "Epoch 22 -- Batch 23/ 94, validation loss 0.3165969252586365\n",
      "Epoch 22 -- Batch 24/ 94, validation loss 0.32885485887527466\n",
      "Epoch 22 -- Batch 25/ 94, validation loss 0.30541113018989563\n",
      "Epoch 22 -- Batch 26/ 94, validation loss 0.3213634788990021\n",
      "Epoch 22 -- Batch 27/ 94, validation loss 0.30891096591949463\n",
      "Epoch 22 -- Batch 28/ 94, validation loss 0.35039812326431274\n",
      "Epoch 22 -- Batch 29/ 94, validation loss 0.3075689673423767\n",
      "Epoch 22 -- Batch 30/ 94, validation loss 0.3146350681781769\n",
      "Epoch 22 -- Batch 31/ 94, validation loss 0.32139360904693604\n",
      "Epoch 22 -- Batch 32/ 94, validation loss 0.3105981647968292\n",
      "Epoch 22 -- Batch 33/ 94, validation loss 0.3168138861656189\n",
      "Epoch 22 -- Batch 34/ 94, validation loss 0.3061690032482147\n",
      "Epoch 22 -- Batch 35/ 94, validation loss 0.32245951890945435\n",
      "Epoch 22 -- Batch 36/ 94, validation loss 0.30595454573631287\n",
      "Epoch 22 -- Batch 37/ 94, validation loss 0.32937735319137573\n",
      "Epoch 22 -- Batch 38/ 94, validation loss 0.32714784145355225\n",
      "Epoch 22 -- Batch 39/ 94, validation loss 0.3180643320083618\n",
      "Epoch 22 -- Batch 40/ 94, validation loss 0.30351829528808594\n",
      "Epoch 22 -- Batch 41/ 94, validation loss 0.306647926568985\n",
      "Epoch 22 -- Batch 42/ 94, validation loss 0.31186643242836\n",
      "Epoch 22 -- Batch 43/ 94, validation loss 0.31435588002204895\n",
      "Epoch 22 -- Batch 44/ 94, validation loss 0.3093253970146179\n",
      "Epoch 22 -- Batch 45/ 94, validation loss 0.3005248010158539\n",
      "Epoch 22 -- Batch 46/ 94, validation loss 0.31771838665008545\n",
      "Epoch 22 -- Batch 47/ 94, validation loss 0.3080557584762573\n",
      "Epoch 22 -- Batch 48/ 94, validation loss 0.3037705421447754\n",
      "Epoch 22 -- Batch 49/ 94, validation loss 0.31277453899383545\n",
      "Epoch 22 -- Batch 50/ 94, validation loss 0.31598547101020813\n",
      "Epoch 22 -- Batch 51/ 94, validation loss 0.3148430585861206\n",
      "Epoch 22 -- Batch 52/ 94, validation loss 0.30622854828834534\n",
      "Epoch 22 -- Batch 53/ 94, validation loss 0.31060516834259033\n",
      "Epoch 22 -- Batch 54/ 94, validation loss 0.3093613088130951\n",
      "Epoch 22 -- Batch 55/ 94, validation loss 0.3049600422382355\n",
      "Epoch 22 -- Batch 56/ 94, validation loss 0.31858813762664795\n",
      "Epoch 22 -- Batch 57/ 94, validation loss 0.3207748234272003\n",
      "Epoch 22 -- Batch 58/ 94, validation loss 0.352679580450058\n",
      "Epoch 22 -- Batch 59/ 94, validation loss 0.30842167139053345\n",
      "Epoch 22 -- Batch 60/ 94, validation loss 0.3086280822753906\n",
      "Epoch 22 -- Batch 61/ 94, validation loss 0.3136138916015625\n",
      "Epoch 22 -- Batch 62/ 94, validation loss 0.3053942322731018\n",
      "Epoch 22 -- Batch 63/ 94, validation loss 0.3072764277458191\n",
      "Epoch 22 -- Batch 64/ 94, validation loss 0.3252504765987396\n",
      "Epoch 22 -- Batch 65/ 94, validation loss 0.3116479218006134\n",
      "Epoch 22 -- Batch 66/ 94, validation loss 0.3114398121833801\n",
      "Epoch 22 -- Batch 67/ 94, validation loss 0.3144110441207886\n",
      "Epoch 22 -- Batch 68/ 94, validation loss 0.2943773865699768\n",
      "Epoch 22 -- Batch 69/ 94, validation loss 0.3224608600139618\n",
      "Epoch 22 -- Batch 70/ 94, validation loss 0.31883466243743896\n",
      "Epoch 22 -- Batch 71/ 94, validation loss 0.31212589144706726\n",
      "Epoch 22 -- Batch 72/ 94, validation loss 0.30722928047180176\n",
      "Epoch 22 -- Batch 73/ 94, validation loss 0.3104221224784851\n",
      "Epoch 22 -- Batch 74/ 94, validation loss 0.3034330904483795\n",
      "Epoch 22 -- Batch 75/ 94, validation loss 0.3067769706249237\n",
      "Epoch 22 -- Batch 76/ 94, validation loss 0.31454506516456604\n",
      "Epoch 22 -- Batch 77/ 94, validation loss 0.29332461953163147\n",
      "Epoch 22 -- Batch 78/ 94, validation loss 0.312374472618103\n",
      "Epoch 22 -- Batch 79/ 94, validation loss 0.32111281156539917\n",
      "Epoch 22 -- Batch 80/ 94, validation loss 0.3077669143676758\n",
      "Epoch 22 -- Batch 81/ 94, validation loss 0.3033891022205353\n",
      "Epoch 22 -- Batch 82/ 94, validation loss 0.3141821026802063\n",
      "Epoch 22 -- Batch 83/ 94, validation loss 0.31687405705451965\n",
      "Epoch 22 -- Batch 84/ 94, validation loss 0.3172024190425873\n",
      "Epoch 22 -- Batch 85/ 94, validation loss 0.31185290217399597\n",
      "Epoch 22 -- Batch 86/ 94, validation loss 0.31560829281806946\n",
      "Epoch 22 -- Batch 87/ 94, validation loss 0.32129159569740295\n",
      "Epoch 22 -- Batch 88/ 94, validation loss 0.3236862123012543\n",
      "Epoch 22 -- Batch 89/ 94, validation loss 0.30192118883132935\n",
      "Epoch 22 -- Batch 90/ 94, validation loss 0.3126724064350128\n",
      "Epoch 22 -- Batch 91/ 94, validation loss 0.3140559494495392\n",
      "Epoch 22 -- Batch 92/ 94, validation loss 0.304759681224823\n",
      "Epoch 22 -- Batch 93/ 94, validation loss 0.3086716830730438\n",
      "Epoch 22 -- Batch 94/ 94, validation loss 0.30646154284477234\n",
      "----------------------------------------------------------------------\n",
      "Epoch 22 loss: Training 0.3211357295513153, Validation 0.30646154284477234\n",
      "----------------------------------------------------------------------\n",
      "Epoch 23/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'CS(=O)(=O)c1ccc(CN2CCCC3(CC3)OCc2ccccc2)cc1'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 14 15 16 19 21\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C2=C(Sc3nc(-c4ccccc4)no3)N3CCC[C@@]32C2=O)c1'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 3 4 6\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 1 2 11 13 14 15 16 17 18\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 10 12 13 14 15 17 18\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(C2c3ccnc3c(SCC(=O)Nc4ccc(C)cc4)ncnc32)cc1'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'O=C1CCc2cc(C(=O)c3ccc4c(Br)cccc3c3)ccc2N1'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'CN(C)S(=O)(=O)n1cc(/C=C(\\NC(=O)c2ccccc2F)C(=O)NCCO)C[C@@H]2C=Cc3ccccc31'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'Cc1nc(N2CCOCC2)c2c(C)c(C(=O)Nc3ccc(N5CCOCC4)cc3)sc2n1'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1nnc2n3cccc3c(-c4cccnc4)nn2c12)NCCCN1'\n",
      "[19:07:31] SMILES Parse Error: syntax error while parsing: CCCCCCCCCCCCCC(=O)NCCc1c(-)c2cc(C)cc(C)c2[nH]c1=O\n",
      "[19:07:31] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCCCC(=O)NCCc1c(-)c2cc(C)cc(C)c2[nH]c1=O' for input: 'CCCCCCCCCCCCCC(=O)NCCc1c(-)c2cc(C)cc(C)c2[nH]c1=O'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'COc1cc(NC(=O)CSC2=C(C#N)C(c3cccs3)CC3=O)c(=O)n(C)c2cc1OC'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'CC1(C(=O)c2ccc(Cl)cc2)C2COC(O3)C1C(=O)C2'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)C(=O)CC1C2CCN(C(=O)c3ccc(F)cc3)CC1C2'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2[nH]c(=O)c(C3CC(c4ccccc4)=NN3C(=O)C2CC2)cc1'\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'O=C1CCCN1CCPNC(=O)[C@H]1CC=C[C@@H]2COC(=O)[C@@H]3C[C@@H]2N1Cc1ccccc1'\n",
      "[19:07:31] Explicit valence for atom # 8 F, 2, is greater than permitted\n",
      "[19:07:31] Explicit valence for atom # 8 Cl, 2, is greater than permitted\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'CC(=O)N(Cc1cc2c(C1CC3)on1)CCOCc1cccnc1C'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 5 7 8 9 16 17 19 20 21\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'Cc1cc(-c2nnc(NC(=O)Cc3cccc3ccccc34)o2)c(C)o1'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 20\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 5 7 8 9 11 13 22 24 25 26\n",
      "[19:07:31] SMILES Parse Error: unclosed ring for input: 'C=CCc1ccc(Oc2ncnc3c2cnn3-c2nccc(C)c23)cc1'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 13 14 15 22 23 25 26\n",
      "[19:07:31] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(Cl)c2nc(-c3ccccc3)c(-c3nc(C)c(C)o3)o2)c1\n",
      "[19:07:31] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(Cl)c2nc(-c3ccccc3)c(-c3nc(C)c(C)o3)o2)c1' for input: 'Cc1cc(Cl)c2nc(-c3ccccc3)c(-c3nc(C)c(C)o3)o2)c1'\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 23\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 2 3 20\n",
      "[19:07:31] Can't kekulize mol.  Unkekulized atoms: 7 9 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 -- Batch 1/ 842, training loss 0.31698521971702576\n",
      "Epoch 23 -- Batch 2/ 842, training loss 0.31302446126937866\n",
      "Epoch 23 -- Batch 3/ 842, training loss 0.30325308442115784\n",
      "Epoch 23 -- Batch 4/ 842, training loss 0.3065296709537506\n",
      "Epoch 23 -- Batch 5/ 842, training loss 0.31090816855430603\n",
      "Epoch 23 -- Batch 6/ 842, training loss 0.30656513571739197\n",
      "Epoch 23 -- Batch 7/ 842, training loss 0.324036568403244\n",
      "Epoch 23 -- Batch 8/ 842, training loss 0.31825658679008484\n",
      "Epoch 23 -- Batch 9/ 842, training loss 0.3187321424484253\n",
      "Epoch 23 -- Batch 10/ 842, training loss 0.3160231113433838\n",
      "Epoch 23 -- Batch 11/ 842, training loss 0.32248708605766296\n",
      "Epoch 23 -- Batch 12/ 842, training loss 0.3184175491333008\n",
      "Epoch 23 -- Batch 13/ 842, training loss 0.3104223608970642\n",
      "Epoch 23 -- Batch 14/ 842, training loss 0.3281659185886383\n",
      "Epoch 23 -- Batch 15/ 842, training loss 0.3225456774234772\n",
      "Epoch 23 -- Batch 16/ 842, training loss 0.31720343232154846\n",
      "Epoch 23 -- Batch 17/ 842, training loss 0.31086215376853943\n",
      "Epoch 23 -- Batch 18/ 842, training loss 0.310922235250473\n",
      "Epoch 23 -- Batch 19/ 842, training loss 0.317963570356369\n",
      "Epoch 23 -- Batch 20/ 842, training loss 0.31935638189315796\n",
      "Epoch 23 -- Batch 21/ 842, training loss 0.30538973212242126\n",
      "Epoch 23 -- Batch 22/ 842, training loss 0.31155630946159363\n",
      "Epoch 23 -- Batch 23/ 842, training loss 0.31449460983276367\n",
      "Epoch 23 -- Batch 24/ 842, training loss 0.3102930188179016\n",
      "Epoch 23 -- Batch 25/ 842, training loss 0.31113722920417786\n",
      "Epoch 23 -- Batch 26/ 842, training loss 0.3108591139316559\n",
      "Epoch 23 -- Batch 27/ 842, training loss 0.32101166248321533\n",
      "Epoch 23 -- Batch 28/ 842, training loss 0.3156624138355255\n",
      "Epoch 23 -- Batch 29/ 842, training loss 0.31292733550071716\n",
      "Epoch 23 -- Batch 30/ 842, training loss 0.31820133328437805\n",
      "Epoch 23 -- Batch 31/ 842, training loss 0.32183438539505005\n",
      "Epoch 23 -- Batch 32/ 842, training loss 0.30993616580963135\n",
      "Epoch 23 -- Batch 33/ 842, training loss 0.31825190782546997\n",
      "Epoch 23 -- Batch 34/ 842, training loss 0.3223404884338379\n",
      "Epoch 23 -- Batch 35/ 842, training loss 0.3105102479457855\n",
      "Epoch 23 -- Batch 36/ 842, training loss 0.3284429609775543\n",
      "Epoch 23 -- Batch 37/ 842, training loss 0.3241029381752014\n",
      "Epoch 23 -- Batch 38/ 842, training loss 0.31967583298683167\n",
      "Epoch 23 -- Batch 39/ 842, training loss 0.32805925607681274\n",
      "Epoch 23 -- Batch 40/ 842, training loss 0.31397679448127747\n",
      "Epoch 23 -- Batch 41/ 842, training loss 0.3194115161895752\n",
      "Epoch 23 -- Batch 42/ 842, training loss 0.31799453496932983\n",
      "Epoch 23 -- Batch 43/ 842, training loss 0.3106239140033722\n",
      "Epoch 23 -- Batch 44/ 842, training loss 0.32905125617980957\n",
      "Epoch 23 -- Batch 45/ 842, training loss 0.314372181892395\n",
      "Epoch 23 -- Batch 46/ 842, training loss 0.30828413367271423\n",
      "Epoch 23 -- Batch 47/ 842, training loss 0.32164356112480164\n",
      "Epoch 23 -- Batch 48/ 842, training loss 0.31770059466362\n",
      "Epoch 23 -- Batch 49/ 842, training loss 0.31719985604286194\n",
      "Epoch 23 -- Batch 50/ 842, training loss 0.3236260414123535\n",
      "Epoch 23 -- Batch 51/ 842, training loss 0.31670844554901123\n",
      "Epoch 23 -- Batch 52/ 842, training loss 0.3190663456916809\n",
      "Epoch 23 -- Batch 53/ 842, training loss 0.3212079107761383\n",
      "Epoch 23 -- Batch 54/ 842, training loss 0.30652758479118347\n",
      "Epoch 23 -- Batch 55/ 842, training loss 0.3220408260822296\n",
      "Epoch 23 -- Batch 56/ 842, training loss 0.3205201029777527\n",
      "Epoch 23 -- Batch 57/ 842, training loss 0.32442644238471985\n",
      "Epoch 23 -- Batch 58/ 842, training loss 0.3011586368083954\n",
      "Epoch 23 -- Batch 59/ 842, training loss 0.31103643774986267\n",
      "Epoch 23 -- Batch 60/ 842, training loss 0.3046141266822815\n",
      "Epoch 23 -- Batch 61/ 842, training loss 0.3294967710971832\n",
      "Epoch 23 -- Batch 62/ 842, training loss 0.33501261472702026\n",
      "Epoch 23 -- Batch 63/ 842, training loss 0.3122657537460327\n",
      "Epoch 23 -- Batch 64/ 842, training loss 0.30691251158714294\n",
      "Epoch 23 -- Batch 65/ 842, training loss 0.32739555835723877\n",
      "Epoch 23 -- Batch 66/ 842, training loss 0.3181300759315491\n",
      "Epoch 23 -- Batch 67/ 842, training loss 0.3043593168258667\n",
      "Epoch 23 -- Batch 68/ 842, training loss 0.2986448109149933\n",
      "Epoch 23 -- Batch 69/ 842, training loss 0.3151802122592926\n",
      "Epoch 23 -- Batch 70/ 842, training loss 0.3224758207798004\n",
      "Epoch 23 -- Batch 71/ 842, training loss 0.3139456510543823\n",
      "Epoch 23 -- Batch 72/ 842, training loss 0.3098991811275482\n",
      "Epoch 23 -- Batch 73/ 842, training loss 0.3082149624824524\n",
      "Epoch 23 -- Batch 74/ 842, training loss 0.3176572322845459\n",
      "Epoch 23 -- Batch 75/ 842, training loss 0.3253013789653778\n",
      "Epoch 23 -- Batch 76/ 842, training loss 0.3088912069797516\n",
      "Epoch 23 -- Batch 77/ 842, training loss 0.3105293810367584\n",
      "Epoch 23 -- Batch 78/ 842, training loss 0.304492712020874\n",
      "Epoch 23 -- Batch 79/ 842, training loss 0.306924045085907\n",
      "Epoch 23 -- Batch 80/ 842, training loss 0.3106236457824707\n",
      "Epoch 23 -- Batch 81/ 842, training loss 0.31972864270210266\n",
      "Epoch 23 -- Batch 82/ 842, training loss 0.3122127056121826\n",
      "Epoch 23 -- Batch 83/ 842, training loss 0.3049405515193939\n",
      "Epoch 23 -- Batch 84/ 842, training loss 0.30994752049446106\n",
      "Epoch 23 -- Batch 85/ 842, training loss 0.32085758447647095\n",
      "Epoch 23 -- Batch 86/ 842, training loss 0.31382545828819275\n",
      "Epoch 23 -- Batch 87/ 842, training loss 0.3097321689128876\n",
      "Epoch 23 -- Batch 88/ 842, training loss 0.3226860463619232\n",
      "Epoch 23 -- Batch 89/ 842, training loss 0.30811363458633423\n",
      "Epoch 23 -- Batch 90/ 842, training loss 0.3131674528121948\n",
      "Epoch 23 -- Batch 91/ 842, training loss 0.32979297637939453\n",
      "Epoch 23 -- Batch 92/ 842, training loss 0.31272804737091064\n",
      "Epoch 23 -- Batch 93/ 842, training loss 0.30774450302124023\n",
      "Epoch 23 -- Batch 94/ 842, training loss 0.31534913182258606\n",
      "Epoch 23 -- Batch 95/ 842, training loss 0.31152528524398804\n",
      "Epoch 23 -- Batch 96/ 842, training loss 0.3077387809753418\n",
      "Epoch 23 -- Batch 97/ 842, training loss 0.30712267756462097\n",
      "Epoch 23 -- Batch 98/ 842, training loss 0.31644463539123535\n",
      "Epoch 23 -- Batch 99/ 842, training loss 0.3102761507034302\n",
      "Epoch 23 -- Batch 100/ 842, training loss 0.31872618198394775\n",
      "Epoch 23 -- Batch 101/ 842, training loss 0.31734147667884827\n",
      "Epoch 23 -- Batch 102/ 842, training loss 0.3033430278301239\n",
      "Epoch 23 -- Batch 103/ 842, training loss 0.31350475549697876\n",
      "Epoch 23 -- Batch 104/ 842, training loss 0.31275808811187744\n",
      "Epoch 23 -- Batch 105/ 842, training loss 0.30501097440719604\n",
      "Epoch 23 -- Batch 106/ 842, training loss 0.3214469850063324\n",
      "Epoch 23 -- Batch 107/ 842, training loss 0.31113746762275696\n",
      "Epoch 23 -- Batch 108/ 842, training loss 0.3171772360801697\n",
      "Epoch 23 -- Batch 109/ 842, training loss 0.31974682211875916\n",
      "Epoch 23 -- Batch 110/ 842, training loss 0.314531534910202\n",
      "Epoch 23 -- Batch 111/ 842, training loss 0.3045886754989624\n",
      "Epoch 23 -- Batch 112/ 842, training loss 0.3129201829433441\n",
      "Epoch 23 -- Batch 113/ 842, training loss 0.31457799673080444\n",
      "Epoch 23 -- Batch 114/ 842, training loss 0.31493285298347473\n",
      "Epoch 23 -- Batch 115/ 842, training loss 0.30833256244659424\n",
      "Epoch 23 -- Batch 116/ 842, training loss 0.3077479898929596\n",
      "Epoch 23 -- Batch 117/ 842, training loss 0.31432849168777466\n",
      "Epoch 23 -- Batch 118/ 842, training loss 0.3231312334537506\n",
      "Epoch 23 -- Batch 119/ 842, training loss 0.3153277337551117\n",
      "Epoch 23 -- Batch 120/ 842, training loss 0.3101568818092346\n",
      "Epoch 23 -- Batch 121/ 842, training loss 0.3165213167667389\n",
      "Epoch 23 -- Batch 122/ 842, training loss 0.312751442193985\n",
      "Epoch 23 -- Batch 123/ 842, training loss 0.3144132196903229\n",
      "Epoch 23 -- Batch 124/ 842, training loss 0.3064820468425751\n",
      "Epoch 23 -- Batch 125/ 842, training loss 0.3209618628025055\n",
      "Epoch 23 -- Batch 126/ 842, training loss 0.31568387150764465\n",
      "Epoch 23 -- Batch 127/ 842, training loss 0.3025956153869629\n",
      "Epoch 23 -- Batch 128/ 842, training loss 0.3155645728111267\n",
      "Epoch 23 -- Batch 129/ 842, training loss 0.3175959289073944\n",
      "Epoch 23 -- Batch 130/ 842, training loss 0.3120959997177124\n",
      "Epoch 23 -- Batch 131/ 842, training loss 0.31311506032943726\n",
      "Epoch 23 -- Batch 132/ 842, training loss 0.3116229176521301\n",
      "Epoch 23 -- Batch 133/ 842, training loss 0.32004114985466003\n",
      "Epoch 23 -- Batch 134/ 842, training loss 0.3247341215610504\n",
      "Epoch 23 -- Batch 135/ 842, training loss 0.32838329672813416\n",
      "Epoch 23 -- Batch 136/ 842, training loss 0.31183239817619324\n",
      "Epoch 23 -- Batch 137/ 842, training loss 0.33119824528694153\n",
      "Epoch 23 -- Batch 138/ 842, training loss 0.3158877193927765\n",
      "Epoch 23 -- Batch 139/ 842, training loss 0.3083133399486542\n",
      "Epoch 23 -- Batch 140/ 842, training loss 0.31259676814079285\n",
      "Epoch 23 -- Batch 141/ 842, training loss 0.31597939133644104\n",
      "Epoch 23 -- Batch 142/ 842, training loss 0.32371604442596436\n",
      "Epoch 23 -- Batch 143/ 842, training loss 0.31668558716773987\n",
      "Epoch 23 -- Batch 144/ 842, training loss 0.31460633873939514\n",
      "Epoch 23 -- Batch 145/ 842, training loss 0.31583380699157715\n",
      "Epoch 23 -- Batch 146/ 842, training loss 0.3111432194709778\n",
      "Epoch 23 -- Batch 147/ 842, training loss 0.30960267782211304\n",
      "Epoch 23 -- Batch 148/ 842, training loss 0.3235376179218292\n",
      "Epoch 23 -- Batch 149/ 842, training loss 0.3202015161514282\n",
      "Epoch 23 -- Batch 150/ 842, training loss 0.31071874499320984\n",
      "Epoch 23 -- Batch 151/ 842, training loss 0.3039895296096802\n",
      "Epoch 23 -- Batch 152/ 842, training loss 0.3141360580921173\n",
      "Epoch 23 -- Batch 153/ 842, training loss 0.3344214856624603\n",
      "Epoch 23 -- Batch 154/ 842, training loss 0.3208398222923279\n",
      "Epoch 23 -- Batch 155/ 842, training loss 0.3078393340110779\n",
      "Epoch 23 -- Batch 156/ 842, training loss 0.31934884190559387\n",
      "Epoch 23 -- Batch 157/ 842, training loss 0.31943562626838684\n",
      "Epoch 23 -- Batch 158/ 842, training loss 0.32262319326400757\n",
      "Epoch 23 -- Batch 159/ 842, training loss 0.30924680829048157\n",
      "Epoch 23 -- Batch 160/ 842, training loss 0.31151747703552246\n",
      "Epoch 23 -- Batch 161/ 842, training loss 0.3075447976589203\n",
      "Epoch 23 -- Batch 162/ 842, training loss 0.3224506974220276\n",
      "Epoch 23 -- Batch 163/ 842, training loss 0.3206632733345032\n",
      "Epoch 23 -- Batch 164/ 842, training loss 0.3318807780742645\n",
      "Epoch 23 -- Batch 165/ 842, training loss 0.31567269563674927\n",
      "Epoch 23 -- Batch 166/ 842, training loss 0.3170422911643982\n",
      "Epoch 23 -- Batch 167/ 842, training loss 0.3102772831916809\n",
      "Epoch 23 -- Batch 168/ 842, training loss 0.31192877888679504\n",
      "Epoch 23 -- Batch 169/ 842, training loss 0.31256166100502014\n",
      "Epoch 23 -- Batch 170/ 842, training loss 0.3053132891654968\n",
      "Epoch 23 -- Batch 171/ 842, training loss 0.3244040906429291\n",
      "Epoch 23 -- Batch 172/ 842, training loss 0.3298012316226959\n",
      "Epoch 23 -- Batch 173/ 842, training loss 0.3226641118526459\n",
      "Epoch 23 -- Batch 174/ 842, training loss 0.3150586187839508\n",
      "Epoch 23 -- Batch 175/ 842, training loss 0.31414639949798584\n",
      "Epoch 23 -- Batch 176/ 842, training loss 0.3077290952205658\n",
      "Epoch 23 -- Batch 177/ 842, training loss 0.3178597390651703\n",
      "Epoch 23 -- Batch 178/ 842, training loss 0.31596940755844116\n",
      "Epoch 23 -- Batch 179/ 842, training loss 0.31832265853881836\n",
      "Epoch 23 -- Batch 180/ 842, training loss 0.32692426443099976\n",
      "Epoch 23 -- Batch 181/ 842, training loss 0.3178037106990814\n",
      "Epoch 23 -- Batch 182/ 842, training loss 0.3161470293998718\n",
      "Epoch 23 -- Batch 183/ 842, training loss 0.31157389283180237\n",
      "Epoch 23 -- Batch 184/ 842, training loss 0.31439831852912903\n",
      "Epoch 23 -- Batch 185/ 842, training loss 0.3185332715511322\n",
      "Epoch 23 -- Batch 186/ 842, training loss 0.30971604585647583\n",
      "Epoch 23 -- Batch 187/ 842, training loss 0.30610403418540955\n",
      "Epoch 23 -- Batch 188/ 842, training loss 0.3214615285396576\n",
      "Epoch 23 -- Batch 189/ 842, training loss 0.30570095777511597\n",
      "Epoch 23 -- Batch 190/ 842, training loss 0.325021356344223\n",
      "Epoch 23 -- Batch 191/ 842, training loss 0.31793567538261414\n",
      "Epoch 23 -- Batch 192/ 842, training loss 0.3267371356487274\n",
      "Epoch 23 -- Batch 193/ 842, training loss 0.3209898769855499\n",
      "Epoch 23 -- Batch 194/ 842, training loss 0.3115015923976898\n",
      "Epoch 23 -- Batch 195/ 842, training loss 0.31945398449897766\n",
      "Epoch 23 -- Batch 196/ 842, training loss 0.31777873635292053\n",
      "Epoch 23 -- Batch 197/ 842, training loss 0.3086162507534027\n",
      "Epoch 23 -- Batch 198/ 842, training loss 0.3079824149608612\n",
      "Epoch 23 -- Batch 199/ 842, training loss 0.31400126218795776\n",
      "Epoch 23 -- Batch 200/ 842, training loss 0.32512378692626953\n",
      "Epoch 23 -- Batch 201/ 842, training loss 0.3115767240524292\n",
      "Epoch 23 -- Batch 202/ 842, training loss 0.3201618790626526\n",
      "Epoch 23 -- Batch 203/ 842, training loss 0.30775192379951477\n",
      "Epoch 23 -- Batch 204/ 842, training loss 0.31244710087776184\n",
      "Epoch 23 -- Batch 205/ 842, training loss 0.3208974599838257\n",
      "Epoch 23 -- Batch 206/ 842, training loss 0.3137499988079071\n",
      "Epoch 23 -- Batch 207/ 842, training loss 0.314521849155426\n",
      "Epoch 23 -- Batch 208/ 842, training loss 0.31841784715652466\n",
      "Epoch 23 -- Batch 209/ 842, training loss 0.3295811116695404\n",
      "Epoch 23 -- Batch 210/ 842, training loss 0.31894543766975403\n",
      "Epoch 23 -- Batch 211/ 842, training loss 0.3219342529773712\n",
      "Epoch 23 -- Batch 212/ 842, training loss 0.31922394037246704\n",
      "Epoch 23 -- Batch 213/ 842, training loss 0.315946102142334\n",
      "Epoch 23 -- Batch 214/ 842, training loss 0.3208543360233307\n",
      "Epoch 23 -- Batch 215/ 842, training loss 0.3190198242664337\n",
      "Epoch 23 -- Batch 216/ 842, training loss 0.32393670082092285\n",
      "Epoch 23 -- Batch 217/ 842, training loss 0.3192831575870514\n",
      "Epoch 23 -- Batch 218/ 842, training loss 0.31504392623901367\n",
      "Epoch 23 -- Batch 219/ 842, training loss 0.3171529769897461\n",
      "Epoch 23 -- Batch 220/ 842, training loss 0.308965265750885\n",
      "Epoch 23 -- Batch 221/ 842, training loss 0.3232489824295044\n",
      "Epoch 23 -- Batch 222/ 842, training loss 0.3125620484352112\n",
      "Epoch 23 -- Batch 223/ 842, training loss 0.3131832480430603\n",
      "Epoch 23 -- Batch 224/ 842, training loss 0.3189034163951874\n",
      "Epoch 23 -- Batch 225/ 842, training loss 0.3255968987941742\n",
      "Epoch 23 -- Batch 226/ 842, training loss 0.31250160932540894\n",
      "Epoch 23 -- Batch 227/ 842, training loss 0.3215327858924866\n",
      "Epoch 23 -- Batch 228/ 842, training loss 0.3143351972103119\n",
      "Epoch 23 -- Batch 229/ 842, training loss 0.3128277063369751\n",
      "Epoch 23 -- Batch 230/ 842, training loss 0.31678569316864014\n",
      "Epoch 23 -- Batch 231/ 842, training loss 0.32240748405456543\n",
      "Epoch 23 -- Batch 232/ 842, training loss 0.3155248165130615\n",
      "Epoch 23 -- Batch 233/ 842, training loss 0.3238142132759094\n",
      "Epoch 23 -- Batch 234/ 842, training loss 0.3183146119117737\n",
      "Epoch 23 -- Batch 235/ 842, training loss 0.32015469670295715\n",
      "Epoch 23 -- Batch 236/ 842, training loss 0.3253020644187927\n",
      "Epoch 23 -- Batch 237/ 842, training loss 0.3185218572616577\n",
      "Epoch 23 -- Batch 238/ 842, training loss 0.3270834982395172\n",
      "Epoch 23 -- Batch 239/ 842, training loss 0.3180035650730133\n",
      "Epoch 23 -- Batch 240/ 842, training loss 0.32779234647750854\n",
      "Epoch 23 -- Batch 241/ 842, training loss 0.32555466890335083\n",
      "Epoch 23 -- Batch 242/ 842, training loss 0.3243615925312042\n",
      "Epoch 23 -- Batch 243/ 842, training loss 0.30586349964141846\n",
      "Epoch 23 -- Batch 244/ 842, training loss 0.31269997358322144\n",
      "Epoch 23 -- Batch 245/ 842, training loss 0.3232285678386688\n",
      "Epoch 23 -- Batch 246/ 842, training loss 0.3210713863372803\n",
      "Epoch 23 -- Batch 247/ 842, training loss 0.323214054107666\n",
      "Epoch 23 -- Batch 248/ 842, training loss 0.3118112087249756\n",
      "Epoch 23 -- Batch 249/ 842, training loss 0.32411491870880127\n",
      "Epoch 23 -- Batch 250/ 842, training loss 0.31500229239463806\n",
      "Epoch 23 -- Batch 251/ 842, training loss 0.31734904646873474\n",
      "Epoch 23 -- Batch 252/ 842, training loss 0.3134026527404785\n",
      "Epoch 23 -- Batch 253/ 842, training loss 0.3216823935508728\n",
      "Epoch 23 -- Batch 254/ 842, training loss 0.3113865554332733\n",
      "Epoch 23 -- Batch 255/ 842, training loss 0.32004424929618835\n",
      "Epoch 23 -- Batch 256/ 842, training loss 0.3109901547431946\n",
      "Epoch 23 -- Batch 257/ 842, training loss 0.3243274390697479\n",
      "Epoch 23 -- Batch 258/ 842, training loss 0.3200189471244812\n",
      "Epoch 23 -- Batch 259/ 842, training loss 0.32274070382118225\n",
      "Epoch 23 -- Batch 260/ 842, training loss 0.3220249116420746\n",
      "Epoch 23 -- Batch 261/ 842, training loss 0.3344883322715759\n",
      "Epoch 23 -- Batch 262/ 842, training loss 0.32541707158088684\n",
      "Epoch 23 -- Batch 263/ 842, training loss 0.31313684582710266\n",
      "Epoch 23 -- Batch 264/ 842, training loss 0.32233139872550964\n",
      "Epoch 23 -- Batch 265/ 842, training loss 0.31577420234680176\n",
      "Epoch 23 -- Batch 266/ 842, training loss 0.31657785177230835\n",
      "Epoch 23 -- Batch 267/ 842, training loss 0.3021235764026642\n",
      "Epoch 23 -- Batch 268/ 842, training loss 0.33473944664001465\n",
      "Epoch 23 -- Batch 269/ 842, training loss 0.31441816687583923\n",
      "Epoch 23 -- Batch 270/ 842, training loss 0.32393524050712585\n",
      "Epoch 23 -- Batch 271/ 842, training loss 0.3129030466079712\n",
      "Epoch 23 -- Batch 272/ 842, training loss 0.32115960121154785\n",
      "Epoch 23 -- Batch 273/ 842, training loss 0.31043216586112976\n",
      "Epoch 23 -- Batch 274/ 842, training loss 0.30116474628448486\n",
      "Epoch 23 -- Batch 275/ 842, training loss 0.31957948207855225\n",
      "Epoch 23 -- Batch 276/ 842, training loss 0.3178028464317322\n",
      "Epoch 23 -- Batch 277/ 842, training loss 0.3170083463191986\n",
      "Epoch 23 -- Batch 278/ 842, training loss 0.31130656599998474\n",
      "Epoch 23 -- Batch 279/ 842, training loss 0.3194931745529175\n",
      "Epoch 23 -- Batch 280/ 842, training loss 0.32140135765075684\n",
      "Epoch 23 -- Batch 281/ 842, training loss 0.327710896730423\n",
      "Epoch 23 -- Batch 282/ 842, training loss 0.3272794783115387\n",
      "Epoch 23 -- Batch 283/ 842, training loss 0.31967461109161377\n",
      "Epoch 23 -- Batch 284/ 842, training loss 0.3176637887954712\n",
      "Epoch 23 -- Batch 285/ 842, training loss 0.31820163130760193\n",
      "Epoch 23 -- Batch 286/ 842, training loss 0.3183726668357849\n",
      "Epoch 23 -- Batch 287/ 842, training loss 0.3186739385128021\n",
      "Epoch 23 -- Batch 288/ 842, training loss 0.32195791602134705\n",
      "Epoch 23 -- Batch 289/ 842, training loss 0.3216519057750702\n",
      "Epoch 23 -- Batch 290/ 842, training loss 0.31006088852882385\n",
      "Epoch 23 -- Batch 291/ 842, training loss 0.3190505802631378\n",
      "Epoch 23 -- Batch 292/ 842, training loss 0.31892186403274536\n",
      "Epoch 23 -- Batch 293/ 842, training loss 0.32244962453842163\n",
      "Epoch 23 -- Batch 294/ 842, training loss 0.3243432641029358\n",
      "Epoch 23 -- Batch 295/ 842, training loss 0.3050665855407715\n",
      "Epoch 23 -- Batch 296/ 842, training loss 0.32588720321655273\n",
      "Epoch 23 -- Batch 297/ 842, training loss 0.30120015144348145\n",
      "Epoch 23 -- Batch 298/ 842, training loss 0.31580984592437744\n",
      "Epoch 23 -- Batch 299/ 842, training loss 0.30998679995536804\n",
      "Epoch 23 -- Batch 300/ 842, training loss 0.31204330921173096\n",
      "Epoch 23 -- Batch 301/ 842, training loss 0.32373863458633423\n",
      "Epoch 23 -- Batch 302/ 842, training loss 0.3280986249446869\n",
      "Epoch 23 -- Batch 303/ 842, training loss 0.31888750195503235\n",
      "Epoch 23 -- Batch 304/ 842, training loss 0.32263633608818054\n",
      "Epoch 23 -- Batch 305/ 842, training loss 0.30816495418548584\n",
      "Epoch 23 -- Batch 306/ 842, training loss 0.3209683299064636\n",
      "Epoch 23 -- Batch 307/ 842, training loss 0.3199160397052765\n",
      "Epoch 23 -- Batch 308/ 842, training loss 0.34436190128326416\n",
      "Epoch 23 -- Batch 309/ 842, training loss 0.3256573975086212\n",
      "Epoch 23 -- Batch 310/ 842, training loss 0.3066701889038086\n",
      "Epoch 23 -- Batch 311/ 842, training loss 0.3204251229763031\n",
      "Epoch 23 -- Batch 312/ 842, training loss 0.32626840472221375\n",
      "Epoch 23 -- Batch 313/ 842, training loss 0.3330620527267456\n",
      "Epoch 23 -- Batch 314/ 842, training loss 0.32117384672164917\n",
      "Epoch 23 -- Batch 315/ 842, training loss 0.3132636845111847\n",
      "Epoch 23 -- Batch 316/ 842, training loss 0.3096928596496582\n",
      "Epoch 23 -- Batch 317/ 842, training loss 0.3082790672779083\n",
      "Epoch 23 -- Batch 318/ 842, training loss 0.3208121955394745\n",
      "Epoch 23 -- Batch 319/ 842, training loss 0.31738829612731934\n",
      "Epoch 23 -- Batch 320/ 842, training loss 0.3183228373527527\n",
      "Epoch 23 -- Batch 321/ 842, training loss 0.3225291073322296\n",
      "Epoch 23 -- Batch 322/ 842, training loss 0.3219408094882965\n",
      "Epoch 23 -- Batch 323/ 842, training loss 0.3098513185977936\n",
      "Epoch 23 -- Batch 324/ 842, training loss 0.3224974274635315\n",
      "Epoch 23 -- Batch 325/ 842, training loss 0.3272487223148346\n",
      "Epoch 23 -- Batch 326/ 842, training loss 0.3207333981990814\n",
      "Epoch 23 -- Batch 327/ 842, training loss 0.31561410427093506\n",
      "Epoch 23 -- Batch 328/ 842, training loss 0.3188071548938751\n",
      "Epoch 23 -- Batch 329/ 842, training loss 0.32307279109954834\n",
      "Epoch 23 -- Batch 330/ 842, training loss 0.3101801872253418\n",
      "Epoch 23 -- Batch 331/ 842, training loss 0.32077986001968384\n",
      "Epoch 23 -- Batch 332/ 842, training loss 0.3160158097743988\n",
      "Epoch 23 -- Batch 333/ 842, training loss 0.33190760016441345\n",
      "Epoch 23 -- Batch 334/ 842, training loss 0.32293057441711426\n",
      "Epoch 23 -- Batch 335/ 842, training loss 0.3213442862033844\n",
      "Epoch 23 -- Batch 336/ 842, training loss 0.32433879375457764\n",
      "Epoch 23 -- Batch 337/ 842, training loss 0.32645002007484436\n",
      "Epoch 23 -- Batch 338/ 842, training loss 0.3175019919872284\n",
      "Epoch 23 -- Batch 339/ 842, training loss 0.3084123432636261\n",
      "Epoch 23 -- Batch 340/ 842, training loss 0.31549984216690063\n",
      "Epoch 23 -- Batch 341/ 842, training loss 0.32447823882102966\n",
      "Epoch 23 -- Batch 342/ 842, training loss 0.31701311469078064\n",
      "Epoch 23 -- Batch 343/ 842, training loss 0.3207087218761444\n",
      "Epoch 23 -- Batch 344/ 842, training loss 0.3168053925037384\n",
      "Epoch 23 -- Batch 345/ 842, training loss 0.3231407701969147\n",
      "Epoch 23 -- Batch 346/ 842, training loss 0.3213545083999634\n",
      "Epoch 23 -- Batch 347/ 842, training loss 0.3274637758731842\n",
      "Epoch 23 -- Batch 348/ 842, training loss 0.3182096779346466\n",
      "Epoch 23 -- Batch 349/ 842, training loss 0.33300575613975525\n",
      "Epoch 23 -- Batch 350/ 842, training loss 0.3216326832771301\n",
      "Epoch 23 -- Batch 351/ 842, training loss 0.3134523630142212\n",
      "Epoch 23 -- Batch 352/ 842, training loss 0.29900380969047546\n",
      "Epoch 23 -- Batch 353/ 842, training loss 0.3193284571170807\n",
      "Epoch 23 -- Batch 354/ 842, training loss 0.3168686628341675\n",
      "Epoch 23 -- Batch 355/ 842, training loss 0.3202935755252838\n",
      "Epoch 23 -- Batch 356/ 842, training loss 0.30955541133880615\n",
      "Epoch 23 -- Batch 357/ 842, training loss 0.3275855481624603\n",
      "Epoch 23 -- Batch 358/ 842, training loss 0.3203052878379822\n",
      "Epoch 23 -- Batch 359/ 842, training loss 0.32162415981292725\n",
      "Epoch 23 -- Batch 360/ 842, training loss 0.31774213910102844\n",
      "Epoch 23 -- Batch 361/ 842, training loss 0.3199700117111206\n",
      "Epoch 23 -- Batch 362/ 842, training loss 0.3103479743003845\n",
      "Epoch 23 -- Batch 363/ 842, training loss 0.3283158242702484\n",
      "Epoch 23 -- Batch 364/ 842, training loss 0.308857798576355\n",
      "Epoch 23 -- Batch 365/ 842, training loss 0.3179028332233429\n",
      "Epoch 23 -- Batch 366/ 842, training loss 0.31923460960388184\n",
      "Epoch 23 -- Batch 367/ 842, training loss 0.32338520884513855\n",
      "Epoch 23 -- Batch 368/ 842, training loss 0.32048699259757996\n",
      "Epoch 23 -- Batch 369/ 842, training loss 0.31404539942741394\n",
      "Epoch 23 -- Batch 370/ 842, training loss 0.3329930603504181\n",
      "Epoch 23 -- Batch 371/ 842, training loss 0.3172385096549988\n",
      "Epoch 23 -- Batch 372/ 842, training loss 0.31291818618774414\n",
      "Epoch 23 -- Batch 373/ 842, training loss 0.32031533122062683\n",
      "Epoch 23 -- Batch 374/ 842, training loss 0.331221342086792\n",
      "Epoch 23 -- Batch 375/ 842, training loss 0.31448689103126526\n",
      "Epoch 23 -- Batch 376/ 842, training loss 0.3230201005935669\n",
      "Epoch 23 -- Batch 377/ 842, training loss 0.3179480731487274\n",
      "Epoch 23 -- Batch 378/ 842, training loss 0.316362589597702\n",
      "Epoch 23 -- Batch 379/ 842, training loss 0.3241395354270935\n",
      "Epoch 23 -- Batch 380/ 842, training loss 0.32505762577056885\n",
      "Epoch 23 -- Batch 381/ 842, training loss 0.31605491042137146\n",
      "Epoch 23 -- Batch 382/ 842, training loss 0.3201441764831543\n",
      "Epoch 23 -- Batch 383/ 842, training loss 0.31834638118743896\n",
      "Epoch 23 -- Batch 384/ 842, training loss 0.32166147232055664\n",
      "Epoch 23 -- Batch 385/ 842, training loss 0.3203757703304291\n",
      "Epoch 23 -- Batch 386/ 842, training loss 0.327992707490921\n",
      "Epoch 23 -- Batch 387/ 842, training loss 0.31616687774658203\n",
      "Epoch 23 -- Batch 388/ 842, training loss 0.32339179515838623\n",
      "Epoch 23 -- Batch 389/ 842, training loss 0.31886038184165955\n",
      "Epoch 23 -- Batch 390/ 842, training loss 0.32997792959213257\n",
      "Epoch 23 -- Batch 391/ 842, training loss 0.32033824920654297\n",
      "Epoch 23 -- Batch 392/ 842, training loss 0.31818848848342896\n",
      "Epoch 23 -- Batch 393/ 842, training loss 0.3294154703617096\n",
      "Epoch 23 -- Batch 394/ 842, training loss 0.3228481411933899\n",
      "Epoch 23 -- Batch 395/ 842, training loss 0.32775864005088806\n",
      "Epoch 23 -- Batch 396/ 842, training loss 0.31678691506385803\n",
      "Epoch 23 -- Batch 397/ 842, training loss 0.31173667311668396\n",
      "Epoch 23 -- Batch 398/ 842, training loss 0.31029796600341797\n",
      "Epoch 23 -- Batch 399/ 842, training loss 0.3322192430496216\n",
      "Epoch 23 -- Batch 400/ 842, training loss 0.33132022619247437\n",
      "Epoch 23 -- Batch 401/ 842, training loss 0.33238962292671204\n",
      "Epoch 23 -- Batch 402/ 842, training loss 0.32264822721481323\n",
      "Epoch 23 -- Batch 403/ 842, training loss 0.3113733232021332\n",
      "Epoch 23 -- Batch 404/ 842, training loss 0.32495611906051636\n",
      "Epoch 23 -- Batch 405/ 842, training loss 0.3173213601112366\n",
      "Epoch 23 -- Batch 406/ 842, training loss 0.3099249601364136\n",
      "Epoch 23 -- Batch 407/ 842, training loss 0.32321012020111084\n",
      "Epoch 23 -- Batch 408/ 842, training loss 0.3216854929924011\n",
      "Epoch 23 -- Batch 409/ 842, training loss 0.3227114677429199\n",
      "Epoch 23 -- Batch 410/ 842, training loss 0.3206748366355896\n",
      "Epoch 23 -- Batch 411/ 842, training loss 0.323974609375\n",
      "Epoch 23 -- Batch 412/ 842, training loss 0.3146713078022003\n",
      "Epoch 23 -- Batch 413/ 842, training loss 0.3186289668083191\n",
      "Epoch 23 -- Batch 414/ 842, training loss 0.3218581974506378\n",
      "Epoch 23 -- Batch 415/ 842, training loss 0.324051171541214\n",
      "Epoch 23 -- Batch 416/ 842, training loss 0.30486294627189636\n",
      "Epoch 23 -- Batch 417/ 842, training loss 0.3238598704338074\n",
      "Epoch 23 -- Batch 418/ 842, training loss 0.32857561111450195\n",
      "Epoch 23 -- Batch 419/ 842, training loss 0.3367985486984253\n",
      "Epoch 23 -- Batch 420/ 842, training loss 0.3117354214191437\n",
      "Epoch 23 -- Batch 421/ 842, training loss 0.32136401534080505\n",
      "Epoch 23 -- Batch 422/ 842, training loss 0.328695684671402\n",
      "Epoch 23 -- Batch 423/ 842, training loss 0.3208956718444824\n",
      "Epoch 23 -- Batch 424/ 842, training loss 0.3271394371986389\n",
      "Epoch 23 -- Batch 425/ 842, training loss 0.32490986585617065\n",
      "Epoch 23 -- Batch 426/ 842, training loss 0.31688591837882996\n",
      "Epoch 23 -- Batch 427/ 842, training loss 0.3301236629486084\n",
      "Epoch 23 -- Batch 428/ 842, training loss 0.32750973105430603\n",
      "Epoch 23 -- Batch 429/ 842, training loss 0.32097405195236206\n",
      "Epoch 23 -- Batch 430/ 842, training loss 0.3096373379230499\n",
      "Epoch 23 -- Batch 431/ 842, training loss 0.32288673520088196\n",
      "Epoch 23 -- Batch 432/ 842, training loss 0.32275187969207764\n",
      "Epoch 23 -- Batch 433/ 842, training loss 0.32219743728637695\n",
      "Epoch 23 -- Batch 434/ 842, training loss 0.32465314865112305\n",
      "Epoch 23 -- Batch 435/ 842, training loss 0.33170968294143677\n",
      "Epoch 23 -- Batch 436/ 842, training loss 0.3300539255142212\n",
      "Epoch 23 -- Batch 437/ 842, training loss 0.3230241537094116\n",
      "Epoch 23 -- Batch 438/ 842, training loss 0.3257739841938019\n",
      "Epoch 23 -- Batch 439/ 842, training loss 0.30522608757019043\n",
      "Epoch 23 -- Batch 440/ 842, training loss 0.31994616985321045\n",
      "Epoch 23 -- Batch 441/ 842, training loss 0.3160676956176758\n",
      "Epoch 23 -- Batch 442/ 842, training loss 0.32188573479652405\n",
      "Epoch 23 -- Batch 443/ 842, training loss 0.32960426807403564\n",
      "Epoch 23 -- Batch 444/ 842, training loss 0.3214903175830841\n",
      "Epoch 23 -- Batch 445/ 842, training loss 0.31378138065338135\n",
      "Epoch 23 -- Batch 446/ 842, training loss 0.3200876712799072\n",
      "Epoch 23 -- Batch 447/ 842, training loss 0.32397690415382385\n",
      "Epoch 23 -- Batch 448/ 842, training loss 0.326928049325943\n",
      "Epoch 23 -- Batch 449/ 842, training loss 0.31508395075798035\n",
      "Epoch 23 -- Batch 450/ 842, training loss 0.3090556263923645\n",
      "Epoch 23 -- Batch 451/ 842, training loss 0.31412866711616516\n",
      "Epoch 23 -- Batch 452/ 842, training loss 0.3178938031196594\n",
      "Epoch 23 -- Batch 453/ 842, training loss 0.31858834624290466\n",
      "Epoch 23 -- Batch 454/ 842, training loss 0.3217628598213196\n",
      "Epoch 23 -- Batch 455/ 842, training loss 0.3244512677192688\n",
      "Epoch 23 -- Batch 456/ 842, training loss 0.3226568102836609\n",
      "Epoch 23 -- Batch 457/ 842, training loss 0.32687926292419434\n",
      "Epoch 23 -- Batch 458/ 842, training loss 0.3109918236732483\n",
      "Epoch 23 -- Batch 459/ 842, training loss 0.3174748718738556\n",
      "Epoch 23 -- Batch 460/ 842, training loss 0.3015214800834656\n",
      "Epoch 23 -- Batch 461/ 842, training loss 0.32115739583969116\n",
      "Epoch 23 -- Batch 462/ 842, training loss 0.33465129137039185\n",
      "Epoch 23 -- Batch 463/ 842, training loss 0.3291063904762268\n",
      "Epoch 23 -- Batch 464/ 842, training loss 0.3121718764305115\n",
      "Epoch 23 -- Batch 465/ 842, training loss 0.3179282546043396\n",
      "Epoch 23 -- Batch 466/ 842, training loss 0.32412630319595337\n",
      "Epoch 23 -- Batch 467/ 842, training loss 0.3173319697380066\n",
      "Epoch 23 -- Batch 468/ 842, training loss 0.30494141578674316\n",
      "Epoch 23 -- Batch 469/ 842, training loss 0.3247351050376892\n",
      "Epoch 23 -- Batch 470/ 842, training loss 0.31474220752716064\n",
      "Epoch 23 -- Batch 471/ 842, training loss 0.3123783469200134\n",
      "Epoch 23 -- Batch 472/ 842, training loss 0.3075803220272064\n",
      "Epoch 23 -- Batch 473/ 842, training loss 0.3129650950431824\n",
      "Epoch 23 -- Batch 474/ 842, training loss 0.31175196170806885\n",
      "Epoch 23 -- Batch 475/ 842, training loss 0.31679150462150574\n",
      "Epoch 23 -- Batch 476/ 842, training loss 0.32211169600486755\n",
      "Epoch 23 -- Batch 477/ 842, training loss 0.31561708450317383\n",
      "Epoch 23 -- Batch 478/ 842, training loss 0.319400429725647\n",
      "Epoch 23 -- Batch 479/ 842, training loss 0.3196865916252136\n",
      "Epoch 23 -- Batch 480/ 842, training loss 0.319913387298584\n",
      "Epoch 23 -- Batch 481/ 842, training loss 0.32320836186408997\n",
      "Epoch 23 -- Batch 482/ 842, training loss 0.3183194398880005\n",
      "Epoch 23 -- Batch 483/ 842, training loss 0.31199073791503906\n",
      "Epoch 23 -- Batch 484/ 842, training loss 0.331808865070343\n",
      "Epoch 23 -- Batch 485/ 842, training loss 0.3222048580646515\n",
      "Epoch 23 -- Batch 486/ 842, training loss 0.32071277499198914\n",
      "Epoch 23 -- Batch 487/ 842, training loss 0.32349875569343567\n",
      "Epoch 23 -- Batch 488/ 842, training loss 0.3101968467235565\n",
      "Epoch 23 -- Batch 489/ 842, training loss 0.3111932873725891\n",
      "Epoch 23 -- Batch 490/ 842, training loss 0.3259233236312866\n",
      "Epoch 23 -- Batch 491/ 842, training loss 0.3232323229312897\n",
      "Epoch 23 -- Batch 492/ 842, training loss 0.31948816776275635\n",
      "Epoch 23 -- Batch 493/ 842, training loss 0.3181074857711792\n",
      "Epoch 23 -- Batch 494/ 842, training loss 0.29789450764656067\n",
      "Epoch 23 -- Batch 495/ 842, training loss 0.31685417890548706\n",
      "Epoch 23 -- Batch 496/ 842, training loss 0.31761229038238525\n",
      "Epoch 23 -- Batch 497/ 842, training loss 0.3244043290615082\n",
      "Epoch 23 -- Batch 498/ 842, training loss 0.32159873843193054\n",
      "Epoch 23 -- Batch 499/ 842, training loss 0.32086181640625\n",
      "Epoch 23 -- Batch 500/ 842, training loss 0.30996307730674744\n",
      "Epoch 23 -- Batch 501/ 842, training loss 0.32997244596481323\n",
      "Epoch 23 -- Batch 502/ 842, training loss 0.31610798835754395\n",
      "Epoch 23 -- Batch 503/ 842, training loss 0.3394942283630371\n",
      "Epoch 23 -- Batch 504/ 842, training loss 0.317030668258667\n",
      "Epoch 23 -- Batch 505/ 842, training loss 0.31818658113479614\n",
      "Epoch 23 -- Batch 506/ 842, training loss 0.32021471858024597\n",
      "Epoch 23 -- Batch 507/ 842, training loss 0.3163962960243225\n",
      "Epoch 23 -- Batch 508/ 842, training loss 0.32266974449157715\n",
      "Epoch 23 -- Batch 509/ 842, training loss 0.3170851171016693\n",
      "Epoch 23 -- Batch 510/ 842, training loss 0.31771910190582275\n",
      "Epoch 23 -- Batch 511/ 842, training loss 0.322064608335495\n",
      "Epoch 23 -- Batch 512/ 842, training loss 0.3148033320903778\n",
      "Epoch 23 -- Batch 513/ 842, training loss 0.33216896653175354\n",
      "Epoch 23 -- Batch 514/ 842, training loss 0.3256741464138031\n",
      "Epoch 23 -- Batch 515/ 842, training loss 0.33210960030555725\n",
      "Epoch 23 -- Batch 516/ 842, training loss 0.31759878993034363\n",
      "Epoch 23 -- Batch 517/ 842, training loss 0.310261994600296\n",
      "Epoch 23 -- Batch 518/ 842, training loss 0.3266286849975586\n",
      "Epoch 23 -- Batch 519/ 842, training loss 0.3186109662055969\n",
      "Epoch 23 -- Batch 520/ 842, training loss 0.3106560707092285\n",
      "Epoch 23 -- Batch 521/ 842, training loss 0.32587698101997375\n",
      "Epoch 23 -- Batch 522/ 842, training loss 0.31656402349472046\n",
      "Epoch 23 -- Batch 523/ 842, training loss 0.3294309079647064\n",
      "Epoch 23 -- Batch 524/ 842, training loss 0.3180595338344574\n",
      "Epoch 23 -- Batch 525/ 842, training loss 0.331490695476532\n",
      "Epoch 23 -- Batch 526/ 842, training loss 0.3291807472705841\n",
      "Epoch 23 -- Batch 527/ 842, training loss 0.32035863399505615\n",
      "Epoch 23 -- Batch 528/ 842, training loss 0.3215404450893402\n",
      "Epoch 23 -- Batch 529/ 842, training loss 0.31292328238487244\n",
      "Epoch 23 -- Batch 530/ 842, training loss 0.3014592230319977\n",
      "Epoch 23 -- Batch 531/ 842, training loss 0.31566712260246277\n",
      "Epoch 23 -- Batch 532/ 842, training loss 0.31573498249053955\n",
      "Epoch 23 -- Batch 533/ 842, training loss 0.3295474946498871\n",
      "Epoch 23 -- Batch 534/ 842, training loss 0.308915913105011\n",
      "Epoch 23 -- Batch 535/ 842, training loss 0.32760393619537354\n",
      "Epoch 23 -- Batch 536/ 842, training loss 0.32515978813171387\n",
      "Epoch 23 -- Batch 537/ 842, training loss 0.3270275592803955\n",
      "Epoch 23 -- Batch 538/ 842, training loss 0.3093246519565582\n",
      "Epoch 23 -- Batch 539/ 842, training loss 0.3193385899066925\n",
      "Epoch 23 -- Batch 540/ 842, training loss 0.32312965393066406\n",
      "Epoch 23 -- Batch 541/ 842, training loss 0.3175545334815979\n",
      "Epoch 23 -- Batch 542/ 842, training loss 0.329732745885849\n",
      "Epoch 23 -- Batch 543/ 842, training loss 0.32006222009658813\n",
      "Epoch 23 -- Batch 544/ 842, training loss 0.3239685893058777\n",
      "Epoch 23 -- Batch 545/ 842, training loss 0.31842896342277527\n",
      "Epoch 23 -- Batch 546/ 842, training loss 0.31876760721206665\n",
      "Epoch 23 -- Batch 547/ 842, training loss 0.3286704123020172\n",
      "Epoch 23 -- Batch 548/ 842, training loss 0.31101953983306885\n",
      "Epoch 23 -- Batch 549/ 842, training loss 0.318745493888855\n",
      "Epoch 23 -- Batch 550/ 842, training loss 0.3223890960216522\n",
      "Epoch 23 -- Batch 551/ 842, training loss 0.3349435031414032\n",
      "Epoch 23 -- Batch 552/ 842, training loss 0.31897905468940735\n",
      "Epoch 23 -- Batch 553/ 842, training loss 0.31194666028022766\n",
      "Epoch 23 -- Batch 554/ 842, training loss 0.31900086998939514\n",
      "Epoch 23 -- Batch 555/ 842, training loss 0.31416720151901245\n",
      "Epoch 23 -- Batch 556/ 842, training loss 0.3195936679840088\n",
      "Epoch 23 -- Batch 557/ 842, training loss 0.3217862844467163\n",
      "Epoch 23 -- Batch 558/ 842, training loss 0.317115843296051\n",
      "Epoch 23 -- Batch 559/ 842, training loss 0.3204217255115509\n",
      "Epoch 23 -- Batch 560/ 842, training loss 0.32843270897865295\n",
      "Epoch 23 -- Batch 561/ 842, training loss 0.3192640542984009\n",
      "Epoch 23 -- Batch 562/ 842, training loss 0.31684717535972595\n",
      "Epoch 23 -- Batch 563/ 842, training loss 0.30664774775505066\n",
      "Epoch 23 -- Batch 564/ 842, training loss 0.32336077094078064\n",
      "Epoch 23 -- Batch 565/ 842, training loss 0.30891841650009155\n",
      "Epoch 23 -- Batch 566/ 842, training loss 0.3209652304649353\n",
      "Epoch 23 -- Batch 567/ 842, training loss 0.3073044717311859\n",
      "Epoch 23 -- Batch 568/ 842, training loss 0.32042059302330017\n",
      "Epoch 23 -- Batch 569/ 842, training loss 0.33449944853782654\n",
      "Epoch 23 -- Batch 570/ 842, training loss 0.31436944007873535\n",
      "Epoch 23 -- Batch 571/ 842, training loss 0.32011014223098755\n",
      "Epoch 23 -- Batch 572/ 842, training loss 0.31780704855918884\n",
      "Epoch 23 -- Batch 573/ 842, training loss 0.32633882761001587\n",
      "Epoch 23 -- Batch 574/ 842, training loss 0.31911700963974\n",
      "Epoch 23 -- Batch 575/ 842, training loss 0.317080020904541\n",
      "Epoch 23 -- Batch 576/ 842, training loss 0.3348206579685211\n",
      "Epoch 23 -- Batch 577/ 842, training loss 0.3141714334487915\n",
      "Epoch 23 -- Batch 578/ 842, training loss 0.31271034479141235\n",
      "Epoch 23 -- Batch 579/ 842, training loss 0.31923040747642517\n",
      "Epoch 23 -- Batch 580/ 842, training loss 0.3208279311656952\n",
      "Epoch 23 -- Batch 581/ 842, training loss 0.3174501359462738\n",
      "Epoch 23 -- Batch 582/ 842, training loss 0.30612805485725403\n",
      "Epoch 23 -- Batch 583/ 842, training loss 0.3237014710903168\n",
      "Epoch 23 -- Batch 584/ 842, training loss 0.30552661418914795\n",
      "Epoch 23 -- Batch 585/ 842, training loss 0.3208565413951874\n",
      "Epoch 23 -- Batch 586/ 842, training loss 0.3199891746044159\n",
      "Epoch 23 -- Batch 587/ 842, training loss 0.3214646875858307\n",
      "Epoch 23 -- Batch 588/ 842, training loss 0.32394322752952576\n",
      "Epoch 23 -- Batch 589/ 842, training loss 0.31859374046325684\n",
      "Epoch 23 -- Batch 590/ 842, training loss 0.30206358432769775\n",
      "Epoch 23 -- Batch 591/ 842, training loss 0.32082924246788025\n",
      "Epoch 23 -- Batch 592/ 842, training loss 0.32195940613746643\n",
      "Epoch 23 -- Batch 593/ 842, training loss 0.3173304796218872\n",
      "Epoch 23 -- Batch 594/ 842, training loss 0.32247766852378845\n",
      "Epoch 23 -- Batch 595/ 842, training loss 0.32647138833999634\n",
      "Epoch 23 -- Batch 596/ 842, training loss 0.31488335132598877\n",
      "Epoch 23 -- Batch 597/ 842, training loss 0.3028644323348999\n",
      "Epoch 23 -- Batch 598/ 842, training loss 0.3255976438522339\n",
      "Epoch 23 -- Batch 599/ 842, training loss 0.3147743344306946\n",
      "Epoch 23 -- Batch 600/ 842, training loss 0.3406882584095001\n",
      "Epoch 23 -- Batch 601/ 842, training loss 0.31336888670921326\n",
      "Epoch 23 -- Batch 602/ 842, training loss 0.32326987385749817\n",
      "Epoch 23 -- Batch 603/ 842, training loss 0.32261696457862854\n",
      "Epoch 23 -- Batch 604/ 842, training loss 0.32184329628944397\n",
      "Epoch 23 -- Batch 605/ 842, training loss 0.322831928730011\n",
      "Epoch 23 -- Batch 606/ 842, training loss 0.29945629835128784\n",
      "Epoch 23 -- Batch 607/ 842, training loss 0.329241544008255\n",
      "Epoch 23 -- Batch 608/ 842, training loss 0.3120681643486023\n",
      "Epoch 23 -- Batch 609/ 842, training loss 0.32146647572517395\n",
      "Epoch 23 -- Batch 610/ 842, training loss 0.31187310814857483\n",
      "Epoch 23 -- Batch 611/ 842, training loss 0.3073509931564331\n",
      "Epoch 23 -- Batch 612/ 842, training loss 0.3236073851585388\n",
      "Epoch 23 -- Batch 613/ 842, training loss 0.3208199143409729\n",
      "Epoch 23 -- Batch 614/ 842, training loss 0.3141959011554718\n",
      "Epoch 23 -- Batch 615/ 842, training loss 0.3048214018344879\n",
      "Epoch 23 -- Batch 616/ 842, training loss 0.33070486783981323\n",
      "Epoch 23 -- Batch 617/ 842, training loss 0.3157292902469635\n",
      "Epoch 23 -- Batch 618/ 842, training loss 0.32672634720802307\n",
      "Epoch 23 -- Batch 619/ 842, training loss 0.3199973702430725\n",
      "Epoch 23 -- Batch 620/ 842, training loss 0.33122017979621887\n",
      "Epoch 23 -- Batch 621/ 842, training loss 0.3269199728965759\n",
      "Epoch 23 -- Batch 622/ 842, training loss 0.3220423758029938\n",
      "Epoch 23 -- Batch 623/ 842, training loss 0.33648329973220825\n",
      "Epoch 23 -- Batch 624/ 842, training loss 0.329532653093338\n",
      "Epoch 23 -- Batch 625/ 842, training loss 0.3106231093406677\n",
      "Epoch 23 -- Batch 626/ 842, training loss 0.32946905493736267\n",
      "Epoch 23 -- Batch 627/ 842, training loss 0.32032516598701477\n",
      "Epoch 23 -- Batch 628/ 842, training loss 0.3273490369319916\n",
      "Epoch 23 -- Batch 629/ 842, training loss 0.31366991996765137\n",
      "Epoch 23 -- Batch 630/ 842, training loss 0.32197725772857666\n",
      "Epoch 23 -- Batch 631/ 842, training loss 0.3240527808666229\n",
      "Epoch 23 -- Batch 632/ 842, training loss 0.31880292296409607\n",
      "Epoch 23 -- Batch 633/ 842, training loss 0.3146148920059204\n",
      "Epoch 23 -- Batch 634/ 842, training loss 0.32767152786254883\n",
      "Epoch 23 -- Batch 635/ 842, training loss 0.3290948271751404\n",
      "Epoch 23 -- Batch 636/ 842, training loss 0.3237132132053375\n",
      "Epoch 23 -- Batch 637/ 842, training loss 0.31755998730659485\n",
      "Epoch 23 -- Batch 638/ 842, training loss 0.30993035435676575\n",
      "Epoch 23 -- Batch 639/ 842, training loss 0.3306780755519867\n",
      "Epoch 23 -- Batch 640/ 842, training loss 0.31695130467414856\n",
      "Epoch 23 -- Batch 641/ 842, training loss 0.3210689425468445\n",
      "Epoch 23 -- Batch 642/ 842, training loss 0.3071396052837372\n",
      "Epoch 23 -- Batch 643/ 842, training loss 0.31388309597969055\n",
      "Epoch 23 -- Batch 644/ 842, training loss 0.3217906355857849\n",
      "Epoch 23 -- Batch 645/ 842, training loss 0.333570659160614\n",
      "Epoch 23 -- Batch 646/ 842, training loss 0.33215317130088806\n",
      "Epoch 23 -- Batch 647/ 842, training loss 0.322907418012619\n",
      "Epoch 23 -- Batch 648/ 842, training loss 0.3152385950088501\n",
      "Epoch 23 -- Batch 649/ 842, training loss 0.3168351352214813\n",
      "Epoch 23 -- Batch 650/ 842, training loss 0.3233465850353241\n",
      "Epoch 23 -- Batch 651/ 842, training loss 0.30813655257225037\n",
      "Epoch 23 -- Batch 652/ 842, training loss 0.314639151096344\n",
      "Epoch 23 -- Batch 653/ 842, training loss 0.32280322909355164\n",
      "Epoch 23 -- Batch 654/ 842, training loss 0.3258678615093231\n",
      "Epoch 23 -- Batch 655/ 842, training loss 0.31674107909202576\n",
      "Epoch 23 -- Batch 656/ 842, training loss 0.324928343296051\n",
      "Epoch 23 -- Batch 657/ 842, training loss 0.3104647696018219\n",
      "Epoch 23 -- Batch 658/ 842, training loss 0.3357347249984741\n",
      "Epoch 23 -- Batch 659/ 842, training loss 0.3176043629646301\n",
      "Epoch 23 -- Batch 660/ 842, training loss 0.3216908872127533\n",
      "Epoch 23 -- Batch 661/ 842, training loss 0.32312923669815063\n",
      "Epoch 23 -- Batch 662/ 842, training loss 0.3068486750125885\n",
      "Epoch 23 -- Batch 663/ 842, training loss 0.32090073823928833\n",
      "Epoch 23 -- Batch 664/ 842, training loss 0.33127331733703613\n",
      "Epoch 23 -- Batch 665/ 842, training loss 0.3399662375450134\n",
      "Epoch 23 -- Batch 666/ 842, training loss 0.31769683957099915\n",
      "Epoch 23 -- Batch 667/ 842, training loss 0.3148496448993683\n",
      "Epoch 23 -- Batch 668/ 842, training loss 0.3179481029510498\n",
      "Epoch 23 -- Batch 669/ 842, training loss 0.32201850414276123\n",
      "Epoch 23 -- Batch 670/ 842, training loss 0.32918837666511536\n",
      "Epoch 23 -- Batch 671/ 842, training loss 0.30162185430526733\n",
      "Epoch 23 -- Batch 672/ 842, training loss 0.3359231650829315\n",
      "Epoch 23 -- Batch 673/ 842, training loss 0.31778261065483093\n",
      "Epoch 23 -- Batch 674/ 842, training loss 0.32513493299484253\n",
      "Epoch 23 -- Batch 675/ 842, training loss 0.31658709049224854\n",
      "Epoch 23 -- Batch 676/ 842, training loss 0.32695794105529785\n",
      "Epoch 23 -- Batch 677/ 842, training loss 0.31898337602615356\n",
      "Epoch 23 -- Batch 678/ 842, training loss 0.31174448132514954\n",
      "Epoch 23 -- Batch 679/ 842, training loss 0.30721810460090637\n",
      "Epoch 23 -- Batch 680/ 842, training loss 0.3296796679496765\n",
      "Epoch 23 -- Batch 681/ 842, training loss 0.3211930990219116\n",
      "Epoch 23 -- Batch 682/ 842, training loss 0.32653507590293884\n",
      "Epoch 23 -- Batch 683/ 842, training loss 0.3227129280567169\n",
      "Epoch 23 -- Batch 684/ 842, training loss 0.3082659840583801\n",
      "Epoch 23 -- Batch 685/ 842, training loss 0.31276562809944153\n",
      "Epoch 23 -- Batch 686/ 842, training loss 0.31297314167022705\n",
      "Epoch 23 -- Batch 687/ 842, training loss 0.3214414715766907\n",
      "Epoch 23 -- Batch 688/ 842, training loss 0.3210859000682831\n",
      "Epoch 23 -- Batch 689/ 842, training loss 0.3180948495864868\n",
      "Epoch 23 -- Batch 690/ 842, training loss 0.31577661633491516\n",
      "Epoch 23 -- Batch 691/ 842, training loss 0.3169920742511749\n",
      "Epoch 23 -- Batch 692/ 842, training loss 0.3292982876300812\n",
      "Epoch 23 -- Batch 693/ 842, training loss 0.3194372355937958\n",
      "Epoch 23 -- Batch 694/ 842, training loss 0.31462180614471436\n",
      "Epoch 23 -- Batch 695/ 842, training loss 0.3273017406463623\n",
      "Epoch 23 -- Batch 696/ 842, training loss 0.32210829854011536\n",
      "Epoch 23 -- Batch 697/ 842, training loss 0.3189561069011688\n",
      "Epoch 23 -- Batch 698/ 842, training loss 0.3181522786617279\n",
      "Epoch 23 -- Batch 699/ 842, training loss 0.3161432445049286\n",
      "Epoch 23 -- Batch 700/ 842, training loss 0.32491400837898254\n",
      "Epoch 23 -- Batch 701/ 842, training loss 0.3131243884563446\n",
      "Epoch 23 -- Batch 702/ 842, training loss 0.31064358353614807\n",
      "Epoch 23 -- Batch 703/ 842, training loss 0.31879037618637085\n",
      "Epoch 23 -- Batch 704/ 842, training loss 0.3221519887447357\n",
      "Epoch 23 -- Batch 705/ 842, training loss 0.3281558156013489\n",
      "Epoch 23 -- Batch 706/ 842, training loss 0.3157734274864197\n",
      "Epoch 23 -- Batch 707/ 842, training loss 0.3225833773612976\n",
      "Epoch 23 -- Batch 708/ 842, training loss 0.33403003215789795\n",
      "Epoch 23 -- Batch 709/ 842, training loss 0.32607194781303406\n",
      "Epoch 23 -- Batch 710/ 842, training loss 0.32156065106391907\n",
      "Epoch 23 -- Batch 711/ 842, training loss 0.3169771730899811\n",
      "Epoch 23 -- Batch 712/ 842, training loss 0.31891191005706787\n",
      "Epoch 23 -- Batch 713/ 842, training loss 0.31185153126716614\n",
      "Epoch 23 -- Batch 714/ 842, training loss 0.31929296255111694\n",
      "Epoch 23 -- Batch 715/ 842, training loss 0.3188057541847229\n",
      "Epoch 23 -- Batch 716/ 842, training loss 0.3145933151245117\n",
      "Epoch 23 -- Batch 717/ 842, training loss 0.3217230439186096\n",
      "Epoch 23 -- Batch 718/ 842, training loss 0.3201199769973755\n",
      "Epoch 23 -- Batch 719/ 842, training loss 0.31576064229011536\n",
      "Epoch 23 -- Batch 720/ 842, training loss 0.32861727476119995\n",
      "Epoch 23 -- Batch 721/ 842, training loss 0.3188445270061493\n",
      "Epoch 23 -- Batch 722/ 842, training loss 0.3242518901824951\n",
      "Epoch 23 -- Batch 723/ 842, training loss 0.3081430494785309\n",
      "Epoch 23 -- Batch 724/ 842, training loss 0.3251875936985016\n",
      "Epoch 23 -- Batch 725/ 842, training loss 0.3262779712677002\n",
      "Epoch 23 -- Batch 726/ 842, training loss 0.3150753974914551\n",
      "Epoch 23 -- Batch 727/ 842, training loss 0.3388936221599579\n",
      "Epoch 23 -- Batch 728/ 842, training loss 0.3282730281352997\n",
      "Epoch 23 -- Batch 729/ 842, training loss 0.32954683899879456\n",
      "Epoch 23 -- Batch 730/ 842, training loss 0.32520273327827454\n",
      "Epoch 23 -- Batch 731/ 842, training loss 0.3143727779388428\n",
      "Epoch 23 -- Batch 732/ 842, training loss 0.3177451491355896\n",
      "Epoch 23 -- Batch 733/ 842, training loss 0.319751113653183\n",
      "Epoch 23 -- Batch 734/ 842, training loss 0.3342699706554413\n",
      "Epoch 23 -- Batch 735/ 842, training loss 0.32344678044319153\n",
      "Epoch 23 -- Batch 736/ 842, training loss 0.32215213775634766\n",
      "Epoch 23 -- Batch 737/ 842, training loss 0.3155357241630554\n",
      "Epoch 23 -- Batch 738/ 842, training loss 0.31913188099861145\n",
      "Epoch 23 -- Batch 739/ 842, training loss 0.31416910886764526\n",
      "Epoch 23 -- Batch 740/ 842, training loss 0.31900113821029663\n",
      "Epoch 23 -- Batch 741/ 842, training loss 0.32192474603652954\n",
      "Epoch 23 -- Batch 742/ 842, training loss 0.31604263186454773\n",
      "Epoch 23 -- Batch 743/ 842, training loss 0.3281871974468231\n",
      "Epoch 23 -- Batch 744/ 842, training loss 0.31306561827659607\n",
      "Epoch 23 -- Batch 745/ 842, training loss 0.3181336224079132\n",
      "Epoch 23 -- Batch 746/ 842, training loss 0.3190164566040039\n",
      "Epoch 23 -- Batch 747/ 842, training loss 0.3170657157897949\n",
      "Epoch 23 -- Batch 748/ 842, training loss 0.3261038362979889\n",
      "Epoch 23 -- Batch 749/ 842, training loss 0.31567689776420593\n",
      "Epoch 23 -- Batch 750/ 842, training loss 0.32332244515419006\n",
      "Epoch 23 -- Batch 751/ 842, training loss 0.31158649921417236\n",
      "Epoch 23 -- Batch 752/ 842, training loss 0.31649863719940186\n",
      "Epoch 23 -- Batch 753/ 842, training loss 0.3165248930454254\n",
      "Epoch 23 -- Batch 754/ 842, training loss 0.3147594928741455\n",
      "Epoch 23 -- Batch 755/ 842, training loss 0.31706151366233826\n",
      "Epoch 23 -- Batch 756/ 842, training loss 0.318510502576828\n",
      "Epoch 23 -- Batch 757/ 842, training loss 0.334301620721817\n",
      "Epoch 23 -- Batch 758/ 842, training loss 0.3250301480293274\n",
      "Epoch 23 -- Batch 759/ 842, training loss 0.3149198591709137\n",
      "Epoch 23 -- Batch 760/ 842, training loss 0.3230559825897217\n",
      "Epoch 23 -- Batch 761/ 842, training loss 0.31992894411087036\n",
      "Epoch 23 -- Batch 762/ 842, training loss 0.3202093541622162\n",
      "Epoch 23 -- Batch 763/ 842, training loss 0.32495012879371643\n",
      "Epoch 23 -- Batch 764/ 842, training loss 0.32461124658584595\n",
      "Epoch 23 -- Batch 765/ 842, training loss 0.3333602547645569\n",
      "Epoch 23 -- Batch 766/ 842, training loss 0.31988781690597534\n",
      "Epoch 23 -- Batch 767/ 842, training loss 0.3207978904247284\n",
      "Epoch 23 -- Batch 768/ 842, training loss 0.30513080954551697\n",
      "Epoch 23 -- Batch 769/ 842, training loss 0.3263198733329773\n",
      "Epoch 23 -- Batch 770/ 842, training loss 0.3189832270145416\n",
      "Epoch 23 -- Batch 771/ 842, training loss 0.3215232193470001\n",
      "Epoch 23 -- Batch 772/ 842, training loss 0.3016209304332733\n",
      "Epoch 23 -- Batch 773/ 842, training loss 0.31954196095466614\n",
      "Epoch 23 -- Batch 774/ 842, training loss 0.3264026343822479\n",
      "Epoch 23 -- Batch 775/ 842, training loss 0.3174949586391449\n",
      "Epoch 23 -- Batch 776/ 842, training loss 0.32234084606170654\n",
      "Epoch 23 -- Batch 777/ 842, training loss 0.31498733162879944\n",
      "Epoch 23 -- Batch 778/ 842, training loss 0.31135833263397217\n",
      "Epoch 23 -- Batch 779/ 842, training loss 0.32286542654037476\n",
      "Epoch 23 -- Batch 780/ 842, training loss 0.3189827799797058\n",
      "Epoch 23 -- Batch 781/ 842, training loss 0.31653186678886414\n",
      "Epoch 23 -- Batch 782/ 842, training loss 0.3251682221889496\n",
      "Epoch 23 -- Batch 783/ 842, training loss 0.3143112361431122\n",
      "Epoch 23 -- Batch 784/ 842, training loss 0.32429951429367065\n",
      "Epoch 23 -- Batch 785/ 842, training loss 0.3290066719055176\n",
      "Epoch 23 -- Batch 786/ 842, training loss 0.3081563413143158\n",
      "Epoch 23 -- Batch 787/ 842, training loss 0.3080132305622101\n",
      "Epoch 23 -- Batch 788/ 842, training loss 0.3073054254055023\n",
      "Epoch 23 -- Batch 789/ 842, training loss 0.32959720492362976\n",
      "Epoch 23 -- Batch 790/ 842, training loss 0.31810200214385986\n",
      "Epoch 23 -- Batch 791/ 842, training loss 0.3168444335460663\n",
      "Epoch 23 -- Batch 792/ 842, training loss 0.3162287175655365\n",
      "Epoch 23 -- Batch 793/ 842, training loss 0.32032421231269836\n",
      "Epoch 23 -- Batch 794/ 842, training loss 0.3217780590057373\n",
      "Epoch 23 -- Batch 795/ 842, training loss 0.31970593333244324\n",
      "Epoch 23 -- Batch 796/ 842, training loss 0.32481837272644043\n",
      "Epoch 23 -- Batch 797/ 842, training loss 0.31422024965286255\n",
      "Epoch 23 -- Batch 798/ 842, training loss 0.31063541769981384\n",
      "Epoch 23 -- Batch 799/ 842, training loss 0.333362877368927\n",
      "Epoch 23 -- Batch 800/ 842, training loss 0.3289531469345093\n",
      "Epoch 23 -- Batch 801/ 842, training loss 0.33324339985847473\n",
      "Epoch 23 -- Batch 802/ 842, training loss 0.31637901067733765\n",
      "Epoch 23 -- Batch 803/ 842, training loss 0.3263189196586609\n",
      "Epoch 23 -- Batch 804/ 842, training loss 0.3111152648925781\n",
      "Epoch 23 -- Batch 805/ 842, training loss 0.32507386803627014\n",
      "Epoch 23 -- Batch 806/ 842, training loss 0.3156755864620209\n",
      "Epoch 23 -- Batch 807/ 842, training loss 0.3214341700077057\n",
      "Epoch 23 -- Batch 808/ 842, training loss 0.32929256558418274\n",
      "Epoch 23 -- Batch 809/ 842, training loss 0.324258953332901\n",
      "Epoch 23 -- Batch 810/ 842, training loss 0.3259153962135315\n",
      "Epoch 23 -- Batch 811/ 842, training loss 0.3263440430164337\n",
      "Epoch 23 -- Batch 812/ 842, training loss 0.3199103772640228\n",
      "Epoch 23 -- Batch 813/ 842, training loss 0.3262777328491211\n",
      "Epoch 23 -- Batch 814/ 842, training loss 0.3152759373188019\n",
      "Epoch 23 -- Batch 815/ 842, training loss 0.3205391466617584\n",
      "Epoch 23 -- Batch 816/ 842, training loss 0.3259029984474182\n",
      "Epoch 23 -- Batch 817/ 842, training loss 0.3177688717842102\n",
      "Epoch 23 -- Batch 818/ 842, training loss 0.31529176235198975\n",
      "Epoch 23 -- Batch 819/ 842, training loss 0.3216276466846466\n",
      "Epoch 23 -- Batch 820/ 842, training loss 0.3296999931335449\n",
      "Epoch 23 -- Batch 821/ 842, training loss 0.3202836513519287\n",
      "Epoch 23 -- Batch 822/ 842, training loss 0.3202817440032959\n",
      "Epoch 23 -- Batch 823/ 842, training loss 0.32417091727256775\n",
      "Epoch 23 -- Batch 824/ 842, training loss 0.32557010650634766\n",
      "Epoch 23 -- Batch 825/ 842, training loss 0.3142048120498657\n",
      "Epoch 23 -- Batch 826/ 842, training loss 0.31703096628189087\n",
      "Epoch 23 -- Batch 827/ 842, training loss 0.32371026277542114\n",
      "Epoch 23 -- Batch 828/ 842, training loss 0.3195664584636688\n",
      "Epoch 23 -- Batch 829/ 842, training loss 0.31428831815719604\n",
      "Epoch 23 -- Batch 830/ 842, training loss 0.3385833501815796\n",
      "Epoch 23 -- Batch 831/ 842, training loss 0.31361037492752075\n",
      "Epoch 23 -- Batch 832/ 842, training loss 0.3100034296512604\n",
      "Epoch 23 -- Batch 833/ 842, training loss 0.3146821856498718\n",
      "Epoch 23 -- Batch 834/ 842, training loss 0.31590867042541504\n",
      "Epoch 23 -- Batch 835/ 842, training loss 0.32380107045173645\n",
      "Epoch 23 -- Batch 836/ 842, training loss 0.3217427730560303\n",
      "Epoch 23 -- Batch 837/ 842, training loss 0.32147693634033203\n",
      "Epoch 23 -- Batch 838/ 842, training loss 0.31801971793174744\n",
      "Epoch 23 -- Batch 839/ 842, training loss 0.3176403343677521\n",
      "Epoch 23 -- Batch 840/ 842, training loss 0.3034482002258301\n",
      "Epoch 23 -- Batch 841/ 842, training loss 0.3164163827896118\n",
      "Epoch 23 -- Batch 842/ 842, training loss 0.3279092311859131\n",
      "----------------------------------------------------------------------\n",
      "Epoch 23 -- Batch 1/ 94, validation loss 0.30232617259025574\n",
      "Epoch 23 -- Batch 2/ 94, validation loss 0.30756843090057373\n",
      "Epoch 23 -- Batch 3/ 94, validation loss 0.3140183091163635\n",
      "Epoch 23 -- Batch 4/ 94, validation loss 0.32424256205558777\n",
      "Epoch 23 -- Batch 5/ 94, validation loss 0.3096226751804352\n",
      "Epoch 23 -- Batch 6/ 94, validation loss 0.3108857274055481\n",
      "Epoch 23 -- Batch 7/ 94, validation loss 0.3072265684604645\n",
      "Epoch 23 -- Batch 8/ 94, validation loss 0.30932459235191345\n",
      "Epoch 23 -- Batch 9/ 94, validation loss 0.32534492015838623\n",
      "Epoch 23 -- Batch 10/ 94, validation loss 0.31241920590400696\n",
      "Epoch 23 -- Batch 11/ 94, validation loss 0.3021678924560547\n",
      "Epoch 23 -- Batch 12/ 94, validation loss 0.3078247606754303\n",
      "Epoch 23 -- Batch 13/ 94, validation loss 0.31188443303108215\n",
      "Epoch 23 -- Batch 14/ 94, validation loss 0.30235326290130615\n",
      "Epoch 23 -- Batch 15/ 94, validation loss 0.30992868542671204\n",
      "Epoch 23 -- Batch 16/ 94, validation loss 0.3151673972606659\n",
      "Epoch 23 -- Batch 17/ 94, validation loss 0.3069695234298706\n",
      "Epoch 23 -- Batch 18/ 94, validation loss 0.3134024143218994\n",
      "Epoch 23 -- Batch 19/ 94, validation loss 0.31552767753601074\n",
      "Epoch 23 -- Batch 20/ 94, validation loss 0.3149827718734741\n",
      "Epoch 23 -- Batch 21/ 94, validation loss 0.3132147192955017\n",
      "Epoch 23 -- Batch 22/ 94, validation loss 0.3044103682041168\n",
      "Epoch 23 -- Batch 23/ 94, validation loss 0.3080027103424072\n",
      "Epoch 23 -- Batch 24/ 94, validation loss 0.30740755796432495\n",
      "Epoch 23 -- Batch 25/ 94, validation loss 0.312735915184021\n",
      "Epoch 23 -- Batch 26/ 94, validation loss 0.30925509333610535\n",
      "Epoch 23 -- Batch 27/ 94, validation loss 0.30601146817207336\n",
      "Epoch 23 -- Batch 28/ 94, validation loss 0.29686489701271057\n",
      "Epoch 23 -- Batch 29/ 94, validation loss 0.31846746802330017\n",
      "Epoch 23 -- Batch 30/ 94, validation loss 0.31325799226760864\n",
      "Epoch 23 -- Batch 31/ 94, validation loss 0.3121386170387268\n",
      "Epoch 23 -- Batch 32/ 94, validation loss 0.30703750252723694\n",
      "Epoch 23 -- Batch 33/ 94, validation loss 0.31584396958351135\n",
      "Epoch 23 -- Batch 34/ 94, validation loss 0.31226640939712524\n",
      "Epoch 23 -- Batch 35/ 94, validation loss 0.29134270548820496\n",
      "Epoch 23 -- Batch 36/ 94, validation loss 0.3015212118625641\n",
      "Epoch 23 -- Batch 37/ 94, validation loss 0.30376046895980835\n",
      "Epoch 23 -- Batch 38/ 94, validation loss 0.30778437852859497\n",
      "Epoch 23 -- Batch 39/ 94, validation loss 0.3079953193664551\n",
      "Epoch 23 -- Batch 40/ 94, validation loss 0.3098297119140625\n",
      "Epoch 23 -- Batch 41/ 94, validation loss 0.29516011476516724\n",
      "Epoch 23 -- Batch 42/ 94, validation loss 0.3214265704154968\n",
      "Epoch 23 -- Batch 43/ 94, validation loss 0.3047495484352112\n",
      "Epoch 23 -- Batch 44/ 94, validation loss 0.3093501627445221\n",
      "Epoch 23 -- Batch 45/ 94, validation loss 0.30841773748397827\n",
      "Epoch 23 -- Batch 46/ 94, validation loss 0.3055689334869385\n",
      "Epoch 23 -- Batch 47/ 94, validation loss 0.30986642837524414\n",
      "Epoch 23 -- Batch 48/ 94, validation loss 0.298257976770401\n",
      "Epoch 23 -- Batch 49/ 94, validation loss 0.3033845126628876\n",
      "Epoch 23 -- Batch 50/ 94, validation loss 0.31317418813705444\n",
      "Epoch 23 -- Batch 51/ 94, validation loss 0.3059596121311188\n",
      "Epoch 23 -- Batch 52/ 94, validation loss 0.3028649687767029\n",
      "Epoch 23 -- Batch 53/ 94, validation loss 0.30251485109329224\n",
      "Epoch 23 -- Batch 54/ 94, validation loss 0.3064802587032318\n",
      "Epoch 23 -- Batch 55/ 94, validation loss 0.31887900829315186\n",
      "Epoch 23 -- Batch 56/ 94, validation loss 0.31104591488838196\n",
      "Epoch 23 -- Batch 57/ 94, validation loss 0.30746036767959595\n",
      "Epoch 23 -- Batch 58/ 94, validation loss 0.3147016763687134\n",
      "Epoch 23 -- Batch 59/ 94, validation loss 0.3113224506378174\n",
      "Epoch 23 -- Batch 60/ 94, validation loss 0.30893293023109436\n",
      "Epoch 23 -- Batch 61/ 94, validation loss 0.3074760138988495\n",
      "Epoch 23 -- Batch 62/ 94, validation loss 0.32009074091911316\n",
      "Epoch 23 -- Batch 63/ 94, validation loss 0.3324904441833496\n",
      "Epoch 23 -- Batch 64/ 94, validation loss 0.3000536262989044\n",
      "Epoch 23 -- Batch 65/ 94, validation loss 0.3120458126068115\n",
      "Epoch 23 -- Batch 66/ 94, validation loss 0.31412726640701294\n",
      "Epoch 23 -- Batch 67/ 94, validation loss 0.2959309220314026\n",
      "Epoch 23 -- Batch 68/ 94, validation loss 0.30586495995521545\n",
      "Epoch 23 -- Batch 69/ 94, validation loss 0.3083409070968628\n",
      "Epoch 23 -- Batch 70/ 94, validation loss 0.3158940374851227\n",
      "Epoch 23 -- Batch 71/ 94, validation loss 0.3190721273422241\n",
      "Epoch 23 -- Batch 72/ 94, validation loss 0.2988673448562622\n",
      "Epoch 23 -- Batch 73/ 94, validation loss 0.3103443682193756\n",
      "Epoch 23 -- Batch 74/ 94, validation loss 0.30613529682159424\n",
      "Epoch 23 -- Batch 75/ 94, validation loss 0.3096481263637543\n",
      "Epoch 23 -- Batch 76/ 94, validation loss 0.31164538860321045\n",
      "Epoch 23 -- Batch 77/ 94, validation loss 0.3083628714084625\n",
      "Epoch 23 -- Batch 78/ 94, validation loss 0.3012465536594391\n",
      "Epoch 23 -- Batch 79/ 94, validation loss 0.32510316371917725\n",
      "Epoch 23 -- Batch 80/ 94, validation loss 0.3132001757621765\n",
      "Epoch 23 -- Batch 81/ 94, validation loss 0.3296808898448944\n",
      "Epoch 23 -- Batch 82/ 94, validation loss 0.3114840090274811\n",
      "Epoch 23 -- Batch 83/ 94, validation loss 0.3100661337375641\n",
      "Epoch 23 -- Batch 84/ 94, validation loss 0.3018476068973541\n",
      "Epoch 23 -- Batch 85/ 94, validation loss 0.3109833002090454\n",
      "Epoch 23 -- Batch 86/ 94, validation loss 0.30788755416870117\n",
      "Epoch 23 -- Batch 87/ 94, validation loss 0.31199929118156433\n",
      "Epoch 23 -- Batch 88/ 94, validation loss 0.30555617809295654\n",
      "Epoch 23 -- Batch 89/ 94, validation loss 0.2955871820449829\n",
      "Epoch 23 -- Batch 90/ 94, validation loss 0.3154789209365845\n",
      "Epoch 23 -- Batch 91/ 94, validation loss 0.2984321713447571\n",
      "Epoch 23 -- Batch 92/ 94, validation loss 0.28787514567375183\n",
      "Epoch 23 -- Batch 93/ 94, validation loss 0.3049035966396332\n",
      "Epoch 23 -- Batch 94/ 94, validation loss 0.3142695724964142\n",
      "----------------------------------------------------------------------\n",
      "Epoch 23 loss: Training 0.3187779486179352, Validation 0.3142695724964142\n",
      "----------------------------------------------------------------------\n",
      "Epoch 24/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 11 22 23 24 25\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'Cc1noc(C)c1CN(C)C(Cc1ccccc1)c1nccc2n1Cc1ccccc1'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2ccc3c(c2)[C@@H]2C[C@H](N(C)C(=O)CN(C)C)[C@H](CO)O2)cc1'\n",
      "[19:07:41] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:07:41] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(-c2cc3ccccc3o2)c2ccco2)cc1\n",
      "[19:07:41] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(-c2cc3ccccc3o2)c2ccco2)cc1' for input: 'Cc1cc(-c2cc3ccccc3o2)c2ccco2)cc1'\n",
      "[19:07:41] SMILES Parse Error: duplicated ring closure 4 bonds atom 21 to itself for input: 'Cc1cc(C)n2nc(C(=O)N3CCC4(CCc4ccccc44)C3)nc2n1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 8 9 22 23 24 25 30\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'C/C(Cc1ccc([C@@H]2C(=O)NCCCN2CCCC2=O)cc1)NC(=O)c1ccco1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 6 7 8 33 34\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 2 12 13 15 23\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'N#Cc1cccc(NC(=O)NCc2nnnn3C2CCCCC2)c1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1cccnc1)c1cc2c(=O)n3ccccc4nc2s1'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CC(C)c1cn(C(=O)NCc2ccccc2C)C1CCN(C)CC1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 28 29 30\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'c1ccc2c(-c3cnc4c(NCCO)ncnc4cn3ccnc23)cc1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:07:41] SMILES Parse Error: extra open parentheses for input: 'Nc1nnnn1CC(=O)N(c1cccc(Cl)c1'\n",
      "[19:07:41] Explicit valence for atom # 28 O, 3, is greater than permitted\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])c1ccc2c(c1NC1=CC3CCC2C3)c1ccc(-c3ccccc3)cc1'\n",
      "[19:07:41] non-ring atom 10 marked aromatic\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1nc2cc3n(c(C)c-2c1C)C(O)C1C3c3ccccc3C(=O)N12'\n",
      "[19:07:41] non-ring atom 16 marked aromatic\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 1 2 3 19 21\n",
      "[19:07:41] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(COc2cccc(N3C(=O)C4C5c5ccccc5C4C(c5ccc(C)cc54)C3C2=O)cc1'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CC1(C)OO[C@@H]2CC(=O)OC[C@]23C=C[C@H](OCc4ccc(F)cc4)C(=O)C21C'\n",
      "[19:07:41] Explicit valence for atom # 16 Cl, 2, is greater than permitted\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'O=C(c1c2c(=O)n3ccccc3nc2n1CCCN2Cc1cccs1)N1CCC2(CC1)OCCO2'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(N2C(=O)CC(N3CCC(C(N)=O)(N3CCCCC4)CC2)CC1)c1ccccc1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CSc2nc3ncnn3c(NCc4cccnc4)c3c2n2CCCC3)cc1'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CN(C)CCN(Cc1c(-c2ccccc2)noc1N1CCc1ccccc1)C(=O)c1cccc(F)c1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 13 15 16\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'Cc1nc(NC(=O)CCCNc2ccc(C(F)(F)F)cc2)nc2c1c(-c3ccccc3)nn1-2'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 5 6 20 22 23\n",
      "[19:07:41] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:07:41] Explicit valence for atom # 29 C, 5, is greater than permitted\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2cc3onc(CCC(=O)NCCc4c[nH]c5ccccc44)c3n2)cc1'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'COc1cc(-c2cccc3c2OC(CNC(=O)[C@H]2C[C@@H]4C=C[C@H]2C4)c2ccccn2)cc(OC)c1OC'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CC1N(C)C(=N)S2C(=O)N(C(C)C)C(=O)C12CCN(Cc1cccc3ccccc13)CC2'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CN1C2NC(=O)C=C(c2ccccc2[N+](=O)[O-])C1=O'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CCc1nnc(NC(=O)C2C3CC4CC(C2)CC(C4)C2)c1C'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)nc(S(=O)(=O)N2CCCSc3nc(C)cc(=O)[nH]3)n1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 8 9 19 20 21\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'C#CCn1c(=NC(=O)CCS(=O)(=O)c2ccc3c(c2)OCC(=O)O)sc2cc(Br)ccc21'\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)N1CCN(C(=O)Cc2cc(-c3cc4ccc5ccccc4c4sc3=O)CC2)cc1'\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 26 27\n",
      "[19:07:41] SMILES Parse Error: unclosed ring for input: 'CCCc1cc2c(=S)c(-c3cc4c(-c4cccc(OC)c4)nnc3sc3=O)cc(C)c2oc1=O'\n",
      "[19:07:41] Explicit valence for atom # 25 O, 5, is greater than permitted\n",
      "[19:07:41] Can't kekulize mol.  Unkekulized atoms: 2 3 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 -- Batch 1/ 842, training loss 0.31341686844825745\n",
      "Epoch 24 -- Batch 2/ 842, training loss 0.30258673429489136\n",
      "Epoch 24 -- Batch 3/ 842, training loss 0.3117695152759552\n",
      "Epoch 24 -- Batch 4/ 842, training loss 0.3212900757789612\n",
      "Epoch 24 -- Batch 5/ 842, training loss 0.30760207772254944\n",
      "Epoch 24 -- Batch 6/ 842, training loss 0.30804431438446045\n",
      "Epoch 24 -- Batch 7/ 842, training loss 0.3204834461212158\n",
      "Epoch 24 -- Batch 8/ 842, training loss 0.3108193874359131\n",
      "Epoch 24 -- Batch 9/ 842, training loss 0.3129231333732605\n",
      "Epoch 24 -- Batch 10/ 842, training loss 0.3188689649105072\n",
      "Epoch 24 -- Batch 11/ 842, training loss 0.31039971113204956\n",
      "Epoch 24 -- Batch 12/ 842, training loss 0.31858697533607483\n",
      "Epoch 24 -- Batch 13/ 842, training loss 0.31347814202308655\n",
      "Epoch 24 -- Batch 14/ 842, training loss 0.3278818726539612\n",
      "Epoch 24 -- Batch 15/ 842, training loss 0.3084886074066162\n",
      "Epoch 24 -- Batch 16/ 842, training loss 0.3264237940311432\n",
      "Epoch 24 -- Batch 17/ 842, training loss 0.31592196226119995\n",
      "Epoch 24 -- Batch 18/ 842, training loss 0.3231744170188904\n",
      "Epoch 24 -- Batch 19/ 842, training loss 0.31268131732940674\n",
      "Epoch 24 -- Batch 20/ 842, training loss 0.3168937861919403\n",
      "Epoch 24 -- Batch 21/ 842, training loss 0.3180044889450073\n",
      "Epoch 24 -- Batch 22/ 842, training loss 0.3207099735736847\n",
      "Epoch 24 -- Batch 23/ 842, training loss 0.31622013449668884\n",
      "Epoch 24 -- Batch 24/ 842, training loss 0.3225370943546295\n",
      "Epoch 24 -- Batch 25/ 842, training loss 0.31536853313446045\n",
      "Epoch 24 -- Batch 26/ 842, training loss 0.3211912512779236\n",
      "Epoch 24 -- Batch 27/ 842, training loss 0.312004029750824\n",
      "Epoch 24 -- Batch 28/ 842, training loss 0.3040226399898529\n",
      "Epoch 24 -- Batch 29/ 842, training loss 0.3112472593784332\n",
      "Epoch 24 -- Batch 30/ 842, training loss 0.3075218200683594\n",
      "Epoch 24 -- Batch 31/ 842, training loss 0.307027131319046\n",
      "Epoch 24 -- Batch 32/ 842, training loss 0.31459635496139526\n",
      "Epoch 24 -- Batch 33/ 842, training loss 0.3198353946208954\n",
      "Epoch 24 -- Batch 34/ 842, training loss 0.3092076778411865\n",
      "Epoch 24 -- Batch 35/ 842, training loss 0.3124307692050934\n",
      "Epoch 24 -- Batch 36/ 842, training loss 0.31008362770080566\n",
      "Epoch 24 -- Batch 37/ 842, training loss 0.32327502965927124\n",
      "Epoch 24 -- Batch 38/ 842, training loss 0.3099842369556427\n",
      "Epoch 24 -- Batch 39/ 842, training loss 0.3095613121986389\n",
      "Epoch 24 -- Batch 40/ 842, training loss 0.3042679727077484\n",
      "Epoch 24 -- Batch 41/ 842, training loss 0.3116401433944702\n",
      "Epoch 24 -- Batch 42/ 842, training loss 0.3254566788673401\n",
      "Epoch 24 -- Batch 43/ 842, training loss 0.31802627444267273\n",
      "Epoch 24 -- Batch 44/ 842, training loss 0.30797719955444336\n",
      "Epoch 24 -- Batch 45/ 842, training loss 0.3119170069694519\n",
      "Epoch 24 -- Batch 46/ 842, training loss 0.3113095164299011\n",
      "Epoch 24 -- Batch 47/ 842, training loss 0.3108449876308441\n",
      "Epoch 24 -- Batch 48/ 842, training loss 0.3169531226158142\n",
      "Epoch 24 -- Batch 49/ 842, training loss 0.3069782853126526\n",
      "Epoch 24 -- Batch 50/ 842, training loss 0.3241501450538635\n",
      "Epoch 24 -- Batch 51/ 842, training loss 0.3241315484046936\n",
      "Epoch 24 -- Batch 52/ 842, training loss 0.316826730966568\n",
      "Epoch 24 -- Batch 53/ 842, training loss 0.3169872462749481\n",
      "Epoch 24 -- Batch 54/ 842, training loss 0.3052121102809906\n",
      "Epoch 24 -- Batch 55/ 842, training loss 0.3144364058971405\n",
      "Epoch 24 -- Batch 56/ 842, training loss 0.31035086512565613\n",
      "Epoch 24 -- Batch 57/ 842, training loss 0.30576106905937195\n",
      "Epoch 24 -- Batch 58/ 842, training loss 0.30846577882766724\n",
      "Epoch 24 -- Batch 59/ 842, training loss 0.32143762707710266\n",
      "Epoch 24 -- Batch 60/ 842, training loss 0.3078203499317169\n",
      "Epoch 24 -- Batch 61/ 842, training loss 0.316989541053772\n",
      "Epoch 24 -- Batch 62/ 842, training loss 0.3158278465270996\n",
      "Epoch 24 -- Batch 63/ 842, training loss 0.3190320134162903\n",
      "Epoch 24 -- Batch 64/ 842, training loss 0.30545300245285034\n",
      "Epoch 24 -- Batch 65/ 842, training loss 0.31612077355384827\n",
      "Epoch 24 -- Batch 66/ 842, training loss 0.3207484483718872\n",
      "Epoch 24 -- Batch 67/ 842, training loss 0.30938273668289185\n",
      "Epoch 24 -- Batch 68/ 842, training loss 0.31341052055358887\n",
      "Epoch 24 -- Batch 69/ 842, training loss 0.30518901348114014\n",
      "Epoch 24 -- Batch 70/ 842, training loss 0.32038697600364685\n",
      "Epoch 24 -- Batch 71/ 842, training loss 0.30799612402915955\n",
      "Epoch 24 -- Batch 72/ 842, training loss 0.3035227358341217\n",
      "Epoch 24 -- Batch 73/ 842, training loss 0.3134828507900238\n",
      "Epoch 24 -- Batch 74/ 842, training loss 0.30109891295433044\n",
      "Epoch 24 -- Batch 75/ 842, training loss 0.3224896192550659\n",
      "Epoch 24 -- Batch 76/ 842, training loss 0.3136376142501831\n",
      "Epoch 24 -- Batch 77/ 842, training loss 0.3163524270057678\n",
      "Epoch 24 -- Batch 78/ 842, training loss 0.31542426347732544\n",
      "Epoch 24 -- Batch 79/ 842, training loss 0.31260332465171814\n",
      "Epoch 24 -- Batch 80/ 842, training loss 0.317111998796463\n",
      "Epoch 24 -- Batch 81/ 842, training loss 0.311501681804657\n",
      "Epoch 24 -- Batch 82/ 842, training loss 0.29763591289520264\n",
      "Epoch 24 -- Batch 83/ 842, training loss 0.30921077728271484\n",
      "Epoch 24 -- Batch 84/ 842, training loss 0.3100597560405731\n",
      "Epoch 24 -- Batch 85/ 842, training loss 0.31719323992729187\n",
      "Epoch 24 -- Batch 86/ 842, training loss 0.31845560669898987\n",
      "Epoch 24 -- Batch 87/ 842, training loss 0.3073069155216217\n",
      "Epoch 24 -- Batch 88/ 842, training loss 0.31883302330970764\n",
      "Epoch 24 -- Batch 89/ 842, training loss 0.3270118236541748\n",
      "Epoch 24 -- Batch 90/ 842, training loss 0.29498744010925293\n",
      "Epoch 24 -- Batch 91/ 842, training loss 0.3093090355396271\n",
      "Epoch 24 -- Batch 92/ 842, training loss 0.31491929292678833\n",
      "Epoch 24 -- Batch 93/ 842, training loss 0.31427228450775146\n",
      "Epoch 24 -- Batch 94/ 842, training loss 0.313257098197937\n",
      "Epoch 24 -- Batch 95/ 842, training loss 0.31716910004615784\n",
      "Epoch 24 -- Batch 96/ 842, training loss 0.313122421503067\n",
      "Epoch 24 -- Batch 97/ 842, training loss 0.324118047952652\n",
      "Epoch 24 -- Batch 98/ 842, training loss 0.31215032935142517\n",
      "Epoch 24 -- Batch 99/ 842, training loss 0.312049925327301\n",
      "Epoch 24 -- Batch 100/ 842, training loss 0.3077336847782135\n",
      "Epoch 24 -- Batch 101/ 842, training loss 0.3031025826931\n",
      "Epoch 24 -- Batch 102/ 842, training loss 0.3200373947620392\n",
      "Epoch 24 -- Batch 103/ 842, training loss 0.31144747138023376\n",
      "Epoch 24 -- Batch 104/ 842, training loss 0.3285030126571655\n",
      "Epoch 24 -- Batch 105/ 842, training loss 0.32665860652923584\n",
      "Epoch 24 -- Batch 106/ 842, training loss 0.30988267064094543\n",
      "Epoch 24 -- Batch 107/ 842, training loss 0.31227824091911316\n",
      "Epoch 24 -- Batch 108/ 842, training loss 0.30339518189430237\n",
      "Epoch 24 -- Batch 109/ 842, training loss 0.32031986117362976\n",
      "Epoch 24 -- Batch 110/ 842, training loss 0.323371022939682\n",
      "Epoch 24 -- Batch 111/ 842, training loss 0.3123907446861267\n",
      "Epoch 24 -- Batch 112/ 842, training loss 0.3167465031147003\n",
      "Epoch 24 -- Batch 113/ 842, training loss 0.3043472170829773\n",
      "Epoch 24 -- Batch 114/ 842, training loss 0.31360751390457153\n",
      "Epoch 24 -- Batch 115/ 842, training loss 0.30233854055404663\n",
      "Epoch 24 -- Batch 116/ 842, training loss 0.3154565095901489\n",
      "Epoch 24 -- Batch 117/ 842, training loss 0.315713495016098\n",
      "Epoch 24 -- Batch 118/ 842, training loss 0.3162554204463959\n",
      "Epoch 24 -- Batch 119/ 842, training loss 0.31297728419303894\n",
      "Epoch 24 -- Batch 120/ 842, training loss 0.3151964843273163\n",
      "Epoch 24 -- Batch 121/ 842, training loss 0.2984011769294739\n",
      "Epoch 24 -- Batch 122/ 842, training loss 0.31114092469215393\n",
      "Epoch 24 -- Batch 123/ 842, training loss 0.3049255907535553\n",
      "Epoch 24 -- Batch 124/ 842, training loss 0.3056495487689972\n",
      "Epoch 24 -- Batch 125/ 842, training loss 0.30252185463905334\n",
      "Epoch 24 -- Batch 126/ 842, training loss 0.2953309118747711\n",
      "Epoch 24 -- Batch 127/ 842, training loss 0.3132065236568451\n",
      "Epoch 24 -- Batch 128/ 842, training loss 0.3160610795021057\n",
      "Epoch 24 -- Batch 129/ 842, training loss 0.3074108362197876\n",
      "Epoch 24 -- Batch 130/ 842, training loss 0.31310662627220154\n",
      "Epoch 24 -- Batch 131/ 842, training loss 0.3181384205818176\n",
      "Epoch 24 -- Batch 132/ 842, training loss 0.3201286196708679\n",
      "Epoch 24 -- Batch 133/ 842, training loss 0.31367579102516174\n",
      "Epoch 24 -- Batch 134/ 842, training loss 0.3094058930873871\n",
      "Epoch 24 -- Batch 135/ 842, training loss 0.3074329197406769\n",
      "Epoch 24 -- Batch 136/ 842, training loss 0.3215668201446533\n",
      "Epoch 24 -- Batch 137/ 842, training loss 0.3091679811477661\n",
      "Epoch 24 -- Batch 138/ 842, training loss 0.30991169810295105\n",
      "Epoch 24 -- Batch 139/ 842, training loss 0.3024761974811554\n",
      "Epoch 24 -- Batch 140/ 842, training loss 0.3121859133243561\n",
      "Epoch 24 -- Batch 141/ 842, training loss 0.3065852224826813\n",
      "Epoch 24 -- Batch 142/ 842, training loss 0.2992165684700012\n",
      "Epoch 24 -- Batch 143/ 842, training loss 0.30836203694343567\n",
      "Epoch 24 -- Batch 144/ 842, training loss 0.31531569361686707\n",
      "Epoch 24 -- Batch 145/ 842, training loss 0.3101406395435333\n",
      "Epoch 24 -- Batch 146/ 842, training loss 0.3275548815727234\n",
      "Epoch 24 -- Batch 147/ 842, training loss 0.32026374340057373\n",
      "Epoch 24 -- Batch 148/ 842, training loss 0.3268634080886841\n",
      "Epoch 24 -- Batch 149/ 842, training loss 0.32371214032173157\n",
      "Epoch 24 -- Batch 150/ 842, training loss 0.3120002746582031\n",
      "Epoch 24 -- Batch 151/ 842, training loss 0.30896395444869995\n",
      "Epoch 24 -- Batch 152/ 842, training loss 0.31174829602241516\n",
      "Epoch 24 -- Batch 153/ 842, training loss 0.31750887632369995\n",
      "Epoch 24 -- Batch 154/ 842, training loss 0.3192843496799469\n",
      "Epoch 24 -- Batch 155/ 842, training loss 0.3059313893318176\n",
      "Epoch 24 -- Batch 156/ 842, training loss 0.3146536648273468\n",
      "Epoch 24 -- Batch 157/ 842, training loss 0.3044697642326355\n",
      "Epoch 24 -- Batch 158/ 842, training loss 0.31399446725845337\n",
      "Epoch 24 -- Batch 159/ 842, training loss 0.31845971941947937\n",
      "Epoch 24 -- Batch 160/ 842, training loss 0.3155558705329895\n",
      "Epoch 24 -- Batch 161/ 842, training loss 0.31302446126937866\n",
      "Epoch 24 -- Batch 162/ 842, training loss 0.31729617714881897\n",
      "Epoch 24 -- Batch 163/ 842, training loss 0.3202526271343231\n",
      "Epoch 24 -- Batch 164/ 842, training loss 0.31893953680992126\n",
      "Epoch 24 -- Batch 165/ 842, training loss 0.3148374855518341\n",
      "Epoch 24 -- Batch 166/ 842, training loss 0.3191523551940918\n",
      "Epoch 24 -- Batch 167/ 842, training loss 0.3264729678630829\n",
      "Epoch 24 -- Batch 168/ 842, training loss 0.33594733476638794\n",
      "Epoch 24 -- Batch 169/ 842, training loss 0.30498048663139343\n",
      "Epoch 24 -- Batch 170/ 842, training loss 0.3266647756099701\n",
      "Epoch 24 -- Batch 171/ 842, training loss 0.3257544934749603\n",
      "Epoch 24 -- Batch 172/ 842, training loss 0.30557781457901\n",
      "Epoch 24 -- Batch 173/ 842, training loss 0.3064236342906952\n",
      "Epoch 24 -- Batch 174/ 842, training loss 0.31785109639167786\n",
      "Epoch 24 -- Batch 175/ 842, training loss 0.3057941496372223\n",
      "Epoch 24 -- Batch 176/ 842, training loss 0.3071964979171753\n",
      "Epoch 24 -- Batch 177/ 842, training loss 0.3115447461605072\n",
      "Epoch 24 -- Batch 178/ 842, training loss 0.3136139214038849\n",
      "Epoch 24 -- Batch 179/ 842, training loss 0.3186163604259491\n",
      "Epoch 24 -- Batch 180/ 842, training loss 0.31674566864967346\n",
      "Epoch 24 -- Batch 181/ 842, training loss 0.30472734570503235\n",
      "Epoch 24 -- Batch 182/ 842, training loss 0.3054012656211853\n",
      "Epoch 24 -- Batch 183/ 842, training loss 0.3155062794685364\n",
      "Epoch 24 -- Batch 184/ 842, training loss 0.3047420382499695\n",
      "Epoch 24 -- Batch 185/ 842, training loss 0.3199768364429474\n",
      "Epoch 24 -- Batch 186/ 842, training loss 0.3190706670284271\n",
      "Epoch 24 -- Batch 187/ 842, training loss 0.3182872235774994\n",
      "Epoch 24 -- Batch 188/ 842, training loss 0.3185780346393585\n",
      "Epoch 24 -- Batch 189/ 842, training loss 0.3151361644268036\n",
      "Epoch 24 -- Batch 190/ 842, training loss 0.31573939323425293\n",
      "Epoch 24 -- Batch 191/ 842, training loss 0.3153802454471588\n",
      "Epoch 24 -- Batch 192/ 842, training loss 0.32556676864624023\n",
      "Epoch 24 -- Batch 193/ 842, training loss 0.3189809024333954\n",
      "Epoch 24 -- Batch 194/ 842, training loss 0.3323782980442047\n",
      "Epoch 24 -- Batch 195/ 842, training loss 0.3129262626171112\n",
      "Epoch 24 -- Batch 196/ 842, training loss 0.31960970163345337\n",
      "Epoch 24 -- Batch 197/ 842, training loss 0.3123704791069031\n",
      "Epoch 24 -- Batch 198/ 842, training loss 0.3107337951660156\n",
      "Epoch 24 -- Batch 199/ 842, training loss 0.3198912441730499\n",
      "Epoch 24 -- Batch 200/ 842, training loss 0.30935847759246826\n",
      "Epoch 24 -- Batch 201/ 842, training loss 0.30910173058509827\n",
      "Epoch 24 -- Batch 202/ 842, training loss 0.30974411964416504\n",
      "Epoch 24 -- Batch 203/ 842, training loss 0.3133912682533264\n",
      "Epoch 24 -- Batch 204/ 842, training loss 0.32116207480430603\n",
      "Epoch 24 -- Batch 205/ 842, training loss 0.3149242103099823\n",
      "Epoch 24 -- Batch 206/ 842, training loss 0.30288419127464294\n",
      "Epoch 24 -- Batch 207/ 842, training loss 0.30902180075645447\n",
      "Epoch 24 -- Batch 208/ 842, training loss 0.3307782709598541\n",
      "Epoch 24 -- Batch 209/ 842, training loss 0.31476110219955444\n",
      "Epoch 24 -- Batch 210/ 842, training loss 0.31275081634521484\n",
      "Epoch 24 -- Batch 211/ 842, training loss 0.3093239665031433\n",
      "Epoch 24 -- Batch 212/ 842, training loss 0.3194105327129364\n",
      "Epoch 24 -- Batch 213/ 842, training loss 0.3208843767642975\n",
      "Epoch 24 -- Batch 214/ 842, training loss 0.31219151616096497\n",
      "Epoch 24 -- Batch 215/ 842, training loss 0.30834436416625977\n",
      "Epoch 24 -- Batch 216/ 842, training loss 0.3226708769798279\n",
      "Epoch 24 -- Batch 217/ 842, training loss 0.30609357357025146\n",
      "Epoch 24 -- Batch 218/ 842, training loss 0.3168998062610626\n",
      "Epoch 24 -- Batch 219/ 842, training loss 0.31306153535842896\n",
      "Epoch 24 -- Batch 220/ 842, training loss 0.3168509006500244\n",
      "Epoch 24 -- Batch 221/ 842, training loss 0.3098766803741455\n",
      "Epoch 24 -- Batch 222/ 842, training loss 0.307332307100296\n",
      "Epoch 24 -- Batch 223/ 842, training loss 0.3120509684085846\n",
      "Epoch 24 -- Batch 224/ 842, training loss 0.31282201409339905\n",
      "Epoch 24 -- Batch 225/ 842, training loss 0.3037567734718323\n",
      "Epoch 24 -- Batch 226/ 842, training loss 0.31928035616874695\n",
      "Epoch 24 -- Batch 227/ 842, training loss 0.3145131766796112\n",
      "Epoch 24 -- Batch 228/ 842, training loss 0.315056174993515\n",
      "Epoch 24 -- Batch 229/ 842, training loss 0.30796870589256287\n",
      "Epoch 24 -- Batch 230/ 842, training loss 0.30880439281463623\n",
      "Epoch 24 -- Batch 231/ 842, training loss 0.31058812141418457\n",
      "Epoch 24 -- Batch 232/ 842, training loss 0.3127197325229645\n",
      "Epoch 24 -- Batch 233/ 842, training loss 0.311173677444458\n",
      "Epoch 24 -- Batch 234/ 842, training loss 0.31381550431251526\n",
      "Epoch 24 -- Batch 235/ 842, training loss 0.31338128447532654\n",
      "Epoch 24 -- Batch 236/ 842, training loss 0.31655701994895935\n",
      "Epoch 24 -- Batch 237/ 842, training loss 0.324333131313324\n",
      "Epoch 24 -- Batch 238/ 842, training loss 0.3264685273170471\n",
      "Epoch 24 -- Batch 239/ 842, training loss 0.306938111782074\n",
      "Epoch 24 -- Batch 240/ 842, training loss 0.3145275115966797\n",
      "Epoch 24 -- Batch 241/ 842, training loss 0.30153268575668335\n",
      "Epoch 24 -- Batch 242/ 842, training loss 0.3114047348499298\n",
      "Epoch 24 -- Batch 243/ 842, training loss 0.3079988360404968\n",
      "Epoch 24 -- Batch 244/ 842, training loss 0.3024376332759857\n",
      "Epoch 24 -- Batch 245/ 842, training loss 0.3231951892375946\n",
      "Epoch 24 -- Batch 246/ 842, training loss 0.3160611391067505\n",
      "Epoch 24 -- Batch 247/ 842, training loss 0.3113318681716919\n",
      "Epoch 24 -- Batch 248/ 842, training loss 0.31650522351264954\n",
      "Epoch 24 -- Batch 249/ 842, training loss 0.3098180294036865\n",
      "Epoch 24 -- Batch 250/ 842, training loss 0.31250569224357605\n",
      "Epoch 24 -- Batch 251/ 842, training loss 0.310733824968338\n",
      "Epoch 24 -- Batch 252/ 842, training loss 0.3074950575828552\n",
      "Epoch 24 -- Batch 253/ 842, training loss 0.31464526057243347\n",
      "Epoch 24 -- Batch 254/ 842, training loss 0.299939900636673\n",
      "Epoch 24 -- Batch 255/ 842, training loss 0.3106955587863922\n",
      "Epoch 24 -- Batch 256/ 842, training loss 0.31400761008262634\n",
      "Epoch 24 -- Batch 257/ 842, training loss 0.3187350630760193\n",
      "Epoch 24 -- Batch 258/ 842, training loss 0.3270442485809326\n",
      "Epoch 24 -- Batch 259/ 842, training loss 0.30313241481781006\n",
      "Epoch 24 -- Batch 260/ 842, training loss 0.3157268166542053\n",
      "Epoch 24 -- Batch 261/ 842, training loss 0.327191561460495\n",
      "Epoch 24 -- Batch 262/ 842, training loss 0.320967435836792\n",
      "Epoch 24 -- Batch 263/ 842, training loss 0.31710678339004517\n",
      "Epoch 24 -- Batch 264/ 842, training loss 0.31173330545425415\n",
      "Epoch 24 -- Batch 265/ 842, training loss 0.3076266050338745\n",
      "Epoch 24 -- Batch 266/ 842, training loss 0.2984115779399872\n",
      "Epoch 24 -- Batch 267/ 842, training loss 0.32368531823158264\n",
      "Epoch 24 -- Batch 268/ 842, training loss 0.31509387493133545\n",
      "Epoch 24 -- Batch 269/ 842, training loss 0.31873008608818054\n",
      "Epoch 24 -- Batch 270/ 842, training loss 0.3107888698577881\n",
      "Epoch 24 -- Batch 271/ 842, training loss 0.313873291015625\n",
      "Epoch 24 -- Batch 272/ 842, training loss 0.32720479369163513\n",
      "Epoch 24 -- Batch 273/ 842, training loss 0.3122437298297882\n",
      "Epoch 24 -- Batch 274/ 842, training loss 0.314917653799057\n",
      "Epoch 24 -- Batch 275/ 842, training loss 0.3190401792526245\n",
      "Epoch 24 -- Batch 276/ 842, training loss 0.30927735567092896\n",
      "Epoch 24 -- Batch 277/ 842, training loss 0.32219114899635315\n",
      "Epoch 24 -- Batch 278/ 842, training loss 0.31989941000938416\n",
      "Epoch 24 -- Batch 279/ 842, training loss 0.3174586892127991\n",
      "Epoch 24 -- Batch 280/ 842, training loss 0.3207310438156128\n",
      "Epoch 24 -- Batch 281/ 842, training loss 0.32032716274261475\n",
      "Epoch 24 -- Batch 282/ 842, training loss 0.30839547514915466\n",
      "Epoch 24 -- Batch 283/ 842, training loss 0.30103009939193726\n",
      "Epoch 24 -- Batch 284/ 842, training loss 0.32825565338134766\n",
      "Epoch 24 -- Batch 285/ 842, training loss 0.30050501227378845\n",
      "Epoch 24 -- Batch 286/ 842, training loss 0.31702879071235657\n",
      "Epoch 24 -- Batch 287/ 842, training loss 0.3167012631893158\n",
      "Epoch 24 -- Batch 288/ 842, training loss 0.32074567675590515\n",
      "Epoch 24 -- Batch 289/ 842, training loss 0.3202812969684601\n",
      "Epoch 24 -- Batch 290/ 842, training loss 0.32567790150642395\n",
      "Epoch 24 -- Batch 291/ 842, training loss 0.3251478970050812\n",
      "Epoch 24 -- Batch 292/ 842, training loss 0.32371383905410767\n",
      "Epoch 24 -- Batch 293/ 842, training loss 0.319476455450058\n",
      "Epoch 24 -- Batch 294/ 842, training loss 0.3176639974117279\n",
      "Epoch 24 -- Batch 295/ 842, training loss 0.30815428495407104\n",
      "Epoch 24 -- Batch 296/ 842, training loss 0.3202441334724426\n",
      "Epoch 24 -- Batch 297/ 842, training loss 0.31923434138298035\n",
      "Epoch 24 -- Batch 298/ 842, training loss 0.32632240653038025\n",
      "Epoch 24 -- Batch 299/ 842, training loss 0.3122319281101227\n",
      "Epoch 24 -- Batch 300/ 842, training loss 0.3141535222530365\n",
      "Epoch 24 -- Batch 301/ 842, training loss 0.30899733304977417\n",
      "Epoch 24 -- Batch 302/ 842, training loss 0.3140207529067993\n",
      "Epoch 24 -- Batch 303/ 842, training loss 0.3153018653392792\n",
      "Epoch 24 -- Batch 304/ 842, training loss 0.3207509219646454\n",
      "Epoch 24 -- Batch 305/ 842, training loss 0.30597764253616333\n",
      "Epoch 24 -- Batch 306/ 842, training loss 0.3343085050582886\n",
      "Epoch 24 -- Batch 307/ 842, training loss 0.2984939217567444\n",
      "Epoch 24 -- Batch 308/ 842, training loss 0.3234257698059082\n",
      "Epoch 24 -- Batch 309/ 842, training loss 0.317147821187973\n",
      "Epoch 24 -- Batch 310/ 842, training loss 0.31085270643234253\n",
      "Epoch 24 -- Batch 311/ 842, training loss 0.3055677115917206\n",
      "Epoch 24 -- Batch 312/ 842, training loss 0.31330612301826477\n",
      "Epoch 24 -- Batch 313/ 842, training loss 0.3168431222438812\n",
      "Epoch 24 -- Batch 314/ 842, training loss 0.31999003887176514\n",
      "Epoch 24 -- Batch 315/ 842, training loss 0.30242159962654114\n",
      "Epoch 24 -- Batch 316/ 842, training loss 0.3125492036342621\n",
      "Epoch 24 -- Batch 317/ 842, training loss 0.3296039402484894\n",
      "Epoch 24 -- Batch 318/ 842, training loss 0.30716225504875183\n",
      "Epoch 24 -- Batch 319/ 842, training loss 0.31084969639778137\n",
      "Epoch 24 -- Batch 320/ 842, training loss 0.3144172132015228\n",
      "Epoch 24 -- Batch 321/ 842, training loss 0.3141372799873352\n",
      "Epoch 24 -- Batch 322/ 842, training loss 0.31071051955223083\n",
      "Epoch 24 -- Batch 323/ 842, training loss 0.32344603538513184\n",
      "Epoch 24 -- Batch 324/ 842, training loss 0.3117343485355377\n",
      "Epoch 24 -- Batch 325/ 842, training loss 0.3130461871623993\n",
      "Epoch 24 -- Batch 326/ 842, training loss 0.3154200315475464\n",
      "Epoch 24 -- Batch 327/ 842, training loss 0.3191526532173157\n",
      "Epoch 24 -- Batch 328/ 842, training loss 0.32914063334465027\n",
      "Epoch 24 -- Batch 329/ 842, training loss 0.301907479763031\n",
      "Epoch 24 -- Batch 330/ 842, training loss 0.3021394908428192\n",
      "Epoch 24 -- Batch 331/ 842, training loss 0.3086790144443512\n",
      "Epoch 24 -- Batch 332/ 842, training loss 0.3172397315502167\n",
      "Epoch 24 -- Batch 333/ 842, training loss 0.3214869201183319\n",
      "Epoch 24 -- Batch 334/ 842, training loss 0.3166205585002899\n",
      "Epoch 24 -- Batch 335/ 842, training loss 0.3119296729564667\n",
      "Epoch 24 -- Batch 336/ 842, training loss 0.32516002655029297\n",
      "Epoch 24 -- Batch 337/ 842, training loss 0.31284651160240173\n",
      "Epoch 24 -- Batch 338/ 842, training loss 0.3254551291465759\n",
      "Epoch 24 -- Batch 339/ 842, training loss 0.31372910737991333\n",
      "Epoch 24 -- Batch 340/ 842, training loss 0.3140169382095337\n",
      "Epoch 24 -- Batch 341/ 842, training loss 0.32031622529029846\n",
      "Epoch 24 -- Batch 342/ 842, training loss 0.3241795003414154\n",
      "Epoch 24 -- Batch 343/ 842, training loss 0.31163984537124634\n",
      "Epoch 24 -- Batch 344/ 842, training loss 0.3105297386646271\n",
      "Epoch 24 -- Batch 345/ 842, training loss 0.31923407316207886\n",
      "Epoch 24 -- Batch 346/ 842, training loss 0.3310164511203766\n",
      "Epoch 24 -- Batch 347/ 842, training loss 0.31044137477874756\n",
      "Epoch 24 -- Batch 348/ 842, training loss 0.31494930386543274\n",
      "Epoch 24 -- Batch 349/ 842, training loss 0.30522865056991577\n",
      "Epoch 24 -- Batch 350/ 842, training loss 0.32177382707595825\n",
      "Epoch 24 -- Batch 351/ 842, training loss 0.29994285106658936\n",
      "Epoch 24 -- Batch 352/ 842, training loss 0.32148486375808716\n",
      "Epoch 24 -- Batch 353/ 842, training loss 0.3041461408138275\n",
      "Epoch 24 -- Batch 354/ 842, training loss 0.3134832978248596\n",
      "Epoch 24 -- Batch 355/ 842, training loss 0.31447264552116394\n",
      "Epoch 24 -- Batch 356/ 842, training loss 0.32603099942207336\n",
      "Epoch 24 -- Batch 357/ 842, training loss 0.3108524680137634\n",
      "Epoch 24 -- Batch 358/ 842, training loss 0.31806203722953796\n",
      "Epoch 24 -- Batch 359/ 842, training loss 0.3343713879585266\n",
      "Epoch 24 -- Batch 360/ 842, training loss 0.30962684750556946\n",
      "Epoch 24 -- Batch 361/ 842, training loss 0.3110244870185852\n",
      "Epoch 24 -- Batch 362/ 842, training loss 0.304268479347229\n",
      "Epoch 24 -- Batch 363/ 842, training loss 0.31771233677864075\n",
      "Epoch 24 -- Batch 364/ 842, training loss 0.32459577918052673\n",
      "Epoch 24 -- Batch 365/ 842, training loss 0.3110997676849365\n",
      "Epoch 24 -- Batch 366/ 842, training loss 0.3316555619239807\n",
      "Epoch 24 -- Batch 367/ 842, training loss 0.32502177357673645\n",
      "Epoch 24 -- Batch 368/ 842, training loss 0.30638206005096436\n",
      "Epoch 24 -- Batch 369/ 842, training loss 0.3151022493839264\n",
      "Epoch 24 -- Batch 370/ 842, training loss 0.3232729732990265\n",
      "Epoch 24 -- Batch 371/ 842, training loss 0.31797507405281067\n",
      "Epoch 24 -- Batch 372/ 842, training loss 0.31197455525398254\n",
      "Epoch 24 -- Batch 373/ 842, training loss 0.3269464671611786\n",
      "Epoch 24 -- Batch 374/ 842, training loss 0.31003305315971375\n",
      "Epoch 24 -- Batch 375/ 842, training loss 0.3212955594062805\n",
      "Epoch 24 -- Batch 376/ 842, training loss 0.3102622628211975\n",
      "Epoch 24 -- Batch 377/ 842, training loss 0.3210507333278656\n",
      "Epoch 24 -- Batch 378/ 842, training loss 0.30644577741622925\n",
      "Epoch 24 -- Batch 379/ 842, training loss 0.309002548456192\n",
      "Epoch 24 -- Batch 380/ 842, training loss 0.3177580237388611\n",
      "Epoch 24 -- Batch 381/ 842, training loss 0.3225192129611969\n",
      "Epoch 24 -- Batch 382/ 842, training loss 0.3178298771381378\n",
      "Epoch 24 -- Batch 383/ 842, training loss 0.31972312927246094\n",
      "Epoch 24 -- Batch 384/ 842, training loss 0.3130604922771454\n",
      "Epoch 24 -- Batch 385/ 842, training loss 0.3137900233268738\n",
      "Epoch 24 -- Batch 386/ 842, training loss 0.3132483661174774\n",
      "Epoch 24 -- Batch 387/ 842, training loss 0.31154903769493103\n",
      "Epoch 24 -- Batch 388/ 842, training loss 0.3113757371902466\n",
      "Epoch 24 -- Batch 389/ 842, training loss 0.3188340961933136\n",
      "Epoch 24 -- Batch 390/ 842, training loss 0.31440046429634094\n",
      "Epoch 24 -- Batch 391/ 842, training loss 0.31930026412010193\n",
      "Epoch 24 -- Batch 392/ 842, training loss 0.30878427624702454\n",
      "Epoch 24 -- Batch 393/ 842, training loss 0.3247239291667938\n",
      "Epoch 24 -- Batch 394/ 842, training loss 0.31709176301956177\n",
      "Epoch 24 -- Batch 395/ 842, training loss 0.3251505494117737\n",
      "Epoch 24 -- Batch 396/ 842, training loss 0.32008737325668335\n",
      "Epoch 24 -- Batch 397/ 842, training loss 0.3127192556858063\n",
      "Epoch 24 -- Batch 398/ 842, training loss 0.31004711985588074\n",
      "Epoch 24 -- Batch 399/ 842, training loss 0.31401771306991577\n",
      "Epoch 24 -- Batch 400/ 842, training loss 0.3069096505641937\n",
      "Epoch 24 -- Batch 401/ 842, training loss 0.31828001141548157\n",
      "Epoch 24 -- Batch 402/ 842, training loss 0.31436777114868164\n",
      "Epoch 24 -- Batch 403/ 842, training loss 0.325361967086792\n",
      "Epoch 24 -- Batch 404/ 842, training loss 0.3237655460834503\n",
      "Epoch 24 -- Batch 405/ 842, training loss 0.3282102942466736\n",
      "Epoch 24 -- Batch 406/ 842, training loss 0.3202206492424011\n",
      "Epoch 24 -- Batch 407/ 842, training loss 0.3215906023979187\n",
      "Epoch 24 -- Batch 408/ 842, training loss 0.31319859623908997\n",
      "Epoch 24 -- Batch 409/ 842, training loss 0.3208041787147522\n",
      "Epoch 24 -- Batch 410/ 842, training loss 0.3179265856742859\n",
      "Epoch 24 -- Batch 411/ 842, training loss 0.33141928911209106\n",
      "Epoch 24 -- Batch 412/ 842, training loss 0.31280219554901123\n",
      "Epoch 24 -- Batch 413/ 842, training loss 0.3191068172454834\n",
      "Epoch 24 -- Batch 414/ 842, training loss 0.31428706645965576\n",
      "Epoch 24 -- Batch 415/ 842, training loss 0.3137851059436798\n",
      "Epoch 24 -- Batch 416/ 842, training loss 0.32516786456108093\n",
      "Epoch 24 -- Batch 417/ 842, training loss 0.31354326009750366\n",
      "Epoch 24 -- Batch 418/ 842, training loss 0.33631274104118347\n",
      "Epoch 24 -- Batch 419/ 842, training loss 0.3158131539821625\n",
      "Epoch 24 -- Batch 420/ 842, training loss 0.3121649920940399\n",
      "Epoch 24 -- Batch 421/ 842, training loss 0.3076416850090027\n",
      "Epoch 24 -- Batch 422/ 842, training loss 0.31524354219436646\n",
      "Epoch 24 -- Batch 423/ 842, training loss 0.316744327545166\n",
      "Epoch 24 -- Batch 424/ 842, training loss 0.31390368938446045\n",
      "Epoch 24 -- Batch 425/ 842, training loss 0.31731805205345154\n",
      "Epoch 24 -- Batch 426/ 842, training loss 0.31703680753707886\n",
      "Epoch 24 -- Batch 427/ 842, training loss 0.32355019450187683\n",
      "Epoch 24 -- Batch 428/ 842, training loss 0.31004324555397034\n",
      "Epoch 24 -- Batch 429/ 842, training loss 0.32321643829345703\n",
      "Epoch 24 -- Batch 430/ 842, training loss 0.3217773735523224\n",
      "Epoch 24 -- Batch 431/ 842, training loss 0.3195142149925232\n",
      "Epoch 24 -- Batch 432/ 842, training loss 0.31873470544815063\n",
      "Epoch 24 -- Batch 433/ 842, training loss 0.32406389713287354\n",
      "Epoch 24 -- Batch 434/ 842, training loss 0.3194669783115387\n",
      "Epoch 24 -- Batch 435/ 842, training loss 0.3017406761646271\n",
      "Epoch 24 -- Batch 436/ 842, training loss 0.32811975479125977\n",
      "Epoch 24 -- Batch 437/ 842, training loss 0.30951443314552307\n",
      "Epoch 24 -- Batch 438/ 842, training loss 0.3150773048400879\n",
      "Epoch 24 -- Batch 439/ 842, training loss 0.3084813058376312\n",
      "Epoch 24 -- Batch 440/ 842, training loss 0.31925249099731445\n",
      "Epoch 24 -- Batch 441/ 842, training loss 0.30180299282073975\n",
      "Epoch 24 -- Batch 442/ 842, training loss 0.3087576627731323\n",
      "Epoch 24 -- Batch 443/ 842, training loss 0.31676045060157776\n",
      "Epoch 24 -- Batch 444/ 842, training loss 0.3187851905822754\n",
      "Epoch 24 -- Batch 445/ 842, training loss 0.31304246187210083\n",
      "Epoch 24 -- Batch 446/ 842, training loss 0.29903581738471985\n",
      "Epoch 24 -- Batch 447/ 842, training loss 0.31633320450782776\n",
      "Epoch 24 -- Batch 448/ 842, training loss 0.3126327395439148\n",
      "Epoch 24 -- Batch 449/ 842, training loss 0.30577796697616577\n",
      "Epoch 24 -- Batch 450/ 842, training loss 0.3087274730205536\n",
      "Epoch 24 -- Batch 451/ 842, training loss 0.31965386867523193\n",
      "Epoch 24 -- Batch 452/ 842, training loss 0.3091852366924286\n",
      "Epoch 24 -- Batch 453/ 842, training loss 0.3211592137813568\n",
      "Epoch 24 -- Batch 454/ 842, training loss 0.30862149596214294\n",
      "Epoch 24 -- Batch 455/ 842, training loss 0.30866408348083496\n",
      "Epoch 24 -- Batch 456/ 842, training loss 0.30727896094322205\n",
      "Epoch 24 -- Batch 457/ 842, training loss 0.32094913721084595\n",
      "Epoch 24 -- Batch 458/ 842, training loss 0.3282566964626312\n",
      "Epoch 24 -- Batch 459/ 842, training loss 0.32960668206214905\n",
      "Epoch 24 -- Batch 460/ 842, training loss 0.3272132873535156\n",
      "Epoch 24 -- Batch 461/ 842, training loss 0.3153461813926697\n",
      "Epoch 24 -- Batch 462/ 842, training loss 0.3199879825115204\n",
      "Epoch 24 -- Batch 463/ 842, training loss 0.30662932991981506\n",
      "Epoch 24 -- Batch 464/ 842, training loss 0.3208044767379761\n",
      "Epoch 24 -- Batch 465/ 842, training loss 0.30757686495780945\n",
      "Epoch 24 -- Batch 466/ 842, training loss 0.32244786620140076\n",
      "Epoch 24 -- Batch 467/ 842, training loss 0.3094824254512787\n",
      "Epoch 24 -- Batch 468/ 842, training loss 0.3188556134700775\n",
      "Epoch 24 -- Batch 469/ 842, training loss 0.30682268738746643\n",
      "Epoch 24 -- Batch 470/ 842, training loss 0.3213118314743042\n",
      "Epoch 24 -- Batch 471/ 842, training loss 0.3330892324447632\n",
      "Epoch 24 -- Batch 472/ 842, training loss 0.31567826867103577\n",
      "Epoch 24 -- Batch 473/ 842, training loss 0.31698495149612427\n",
      "Epoch 24 -- Batch 474/ 842, training loss 0.32274341583251953\n",
      "Epoch 24 -- Batch 475/ 842, training loss 0.320534348487854\n",
      "Epoch 24 -- Batch 476/ 842, training loss 0.31905603408813477\n",
      "Epoch 24 -- Batch 477/ 842, training loss 0.3231826722621918\n",
      "Epoch 24 -- Batch 478/ 842, training loss 0.32527825236320496\n",
      "Epoch 24 -- Batch 479/ 842, training loss 0.3147038221359253\n",
      "Epoch 24 -- Batch 480/ 842, training loss 0.31635117530822754\n",
      "Epoch 24 -- Batch 481/ 842, training loss 0.32546553015708923\n",
      "Epoch 24 -- Batch 482/ 842, training loss 0.31762897968292236\n",
      "Epoch 24 -- Batch 483/ 842, training loss 0.33171769976615906\n",
      "Epoch 24 -- Batch 484/ 842, training loss 0.30476924777030945\n",
      "Epoch 24 -- Batch 485/ 842, training loss 0.3141757845878601\n",
      "Epoch 24 -- Batch 486/ 842, training loss 0.3204810321331024\n",
      "Epoch 24 -- Batch 487/ 842, training loss 0.3267694115638733\n",
      "Epoch 24 -- Batch 488/ 842, training loss 0.3167324960231781\n",
      "Epoch 24 -- Batch 489/ 842, training loss 0.319681853055954\n",
      "Epoch 24 -- Batch 490/ 842, training loss 0.3236687481403351\n",
      "Epoch 24 -- Batch 491/ 842, training loss 0.3160776197910309\n",
      "Epoch 24 -- Batch 492/ 842, training loss 0.3189253807067871\n",
      "Epoch 24 -- Batch 493/ 842, training loss 0.3121909201145172\n",
      "Epoch 24 -- Batch 494/ 842, training loss 0.3168451488018036\n",
      "Epoch 24 -- Batch 495/ 842, training loss 0.32200345396995544\n",
      "Epoch 24 -- Batch 496/ 842, training loss 0.31745603680610657\n",
      "Epoch 24 -- Batch 497/ 842, training loss 0.31409066915512085\n",
      "Epoch 24 -- Batch 498/ 842, training loss 0.3186867833137512\n",
      "Epoch 24 -- Batch 499/ 842, training loss 0.32141777873039246\n",
      "Epoch 24 -- Batch 500/ 842, training loss 0.3128391206264496\n",
      "Epoch 24 -- Batch 501/ 842, training loss 0.3007904291152954\n",
      "Epoch 24 -- Batch 502/ 842, training loss 0.30784714221954346\n",
      "Epoch 24 -- Batch 503/ 842, training loss 0.3193676173686981\n",
      "Epoch 24 -- Batch 504/ 842, training loss 0.3195935785770416\n",
      "Epoch 24 -- Batch 505/ 842, training loss 0.3119143545627594\n",
      "Epoch 24 -- Batch 506/ 842, training loss 0.32663875818252563\n",
      "Epoch 24 -- Batch 507/ 842, training loss 0.3371097147464752\n",
      "Epoch 24 -- Batch 508/ 842, training loss 0.31552109122276306\n",
      "Epoch 24 -- Batch 509/ 842, training loss 0.3192124366760254\n",
      "Epoch 24 -- Batch 510/ 842, training loss 0.3126341700553894\n",
      "Epoch 24 -- Batch 511/ 842, training loss 0.3066210448741913\n",
      "Epoch 24 -- Batch 512/ 842, training loss 0.30593141913414\n",
      "Epoch 24 -- Batch 513/ 842, training loss 0.32409265637397766\n",
      "Epoch 24 -- Batch 514/ 842, training loss 0.3165401518344879\n",
      "Epoch 24 -- Batch 515/ 842, training loss 0.32469141483306885\n",
      "Epoch 24 -- Batch 516/ 842, training loss 0.31532156467437744\n",
      "Epoch 24 -- Batch 517/ 842, training loss 0.3053811192512512\n",
      "Epoch 24 -- Batch 518/ 842, training loss 0.32532864809036255\n",
      "Epoch 24 -- Batch 519/ 842, training loss 0.32079097628593445\n",
      "Epoch 24 -- Batch 520/ 842, training loss 0.33750131726264954\n",
      "Epoch 24 -- Batch 521/ 842, training loss 0.3129376769065857\n",
      "Epoch 24 -- Batch 522/ 842, training loss 0.32350456714630127\n",
      "Epoch 24 -- Batch 523/ 842, training loss 0.33033573627471924\n",
      "Epoch 24 -- Batch 524/ 842, training loss 0.33007577061653137\n",
      "Epoch 24 -- Batch 525/ 842, training loss 0.31669294834136963\n",
      "Epoch 24 -- Batch 526/ 842, training loss 0.3152889907360077\n",
      "Epoch 24 -- Batch 527/ 842, training loss 0.3158206045627594\n",
      "Epoch 24 -- Batch 528/ 842, training loss 0.3230242431163788\n",
      "Epoch 24 -- Batch 529/ 842, training loss 0.32228758931159973\n",
      "Epoch 24 -- Batch 530/ 842, training loss 0.31229665875434875\n",
      "Epoch 24 -- Batch 531/ 842, training loss 0.3076978325843811\n",
      "Epoch 24 -- Batch 532/ 842, training loss 0.3233351409435272\n",
      "Epoch 24 -- Batch 533/ 842, training loss 0.31243422627449036\n",
      "Epoch 24 -- Batch 534/ 842, training loss 0.31152161955833435\n",
      "Epoch 24 -- Batch 535/ 842, training loss 0.3190758526325226\n",
      "Epoch 24 -- Batch 536/ 842, training loss 0.3130274713039398\n",
      "Epoch 24 -- Batch 537/ 842, training loss 0.3226349949836731\n",
      "Epoch 24 -- Batch 538/ 842, training loss 0.3065839111804962\n",
      "Epoch 24 -- Batch 539/ 842, training loss 0.3187105655670166\n",
      "Epoch 24 -- Batch 540/ 842, training loss 0.31760117411613464\n",
      "Epoch 24 -- Batch 541/ 842, training loss 0.32694804668426514\n",
      "Epoch 24 -- Batch 542/ 842, training loss 0.3141077160835266\n",
      "Epoch 24 -- Batch 543/ 842, training loss 0.3198056221008301\n",
      "Epoch 24 -- Batch 544/ 842, training loss 0.32316768169403076\n",
      "Epoch 24 -- Batch 545/ 842, training loss 0.3238445520401001\n",
      "Epoch 24 -- Batch 546/ 842, training loss 0.3171660006046295\n",
      "Epoch 24 -- Batch 547/ 842, training loss 0.31356602907180786\n",
      "Epoch 24 -- Batch 548/ 842, training loss 0.30806344747543335\n",
      "Epoch 24 -- Batch 549/ 842, training loss 0.3196008801460266\n",
      "Epoch 24 -- Batch 550/ 842, training loss 0.32104232907295227\n",
      "Epoch 24 -- Batch 551/ 842, training loss 0.33561262488365173\n",
      "Epoch 24 -- Batch 552/ 842, training loss 0.31829607486724854\n",
      "Epoch 24 -- Batch 553/ 842, training loss 0.3190614581108093\n",
      "Epoch 24 -- Batch 554/ 842, training loss 0.3224402368068695\n",
      "Epoch 24 -- Batch 555/ 842, training loss 0.31499671936035156\n",
      "Epoch 24 -- Batch 556/ 842, training loss 0.3121461272239685\n",
      "Epoch 24 -- Batch 557/ 842, training loss 0.31153634190559387\n",
      "Epoch 24 -- Batch 558/ 842, training loss 0.3165726065635681\n",
      "Epoch 24 -- Batch 559/ 842, training loss 0.32995128631591797\n",
      "Epoch 24 -- Batch 560/ 842, training loss 0.3120865821838379\n",
      "Epoch 24 -- Batch 561/ 842, training loss 0.3085356056690216\n",
      "Epoch 24 -- Batch 562/ 842, training loss 0.3308068513870239\n",
      "Epoch 24 -- Batch 563/ 842, training loss 0.329997181892395\n",
      "Epoch 24 -- Batch 564/ 842, training loss 0.31660032272338867\n",
      "Epoch 24 -- Batch 565/ 842, training loss 0.32770925760269165\n",
      "Epoch 24 -- Batch 566/ 842, training loss 0.3300081491470337\n",
      "Epoch 24 -- Batch 567/ 842, training loss 0.31693997979164124\n",
      "Epoch 24 -- Batch 568/ 842, training loss 0.30426469445228577\n",
      "Epoch 24 -- Batch 569/ 842, training loss 0.31949687004089355\n",
      "Epoch 24 -- Batch 570/ 842, training loss 0.3137962520122528\n",
      "Epoch 24 -- Batch 571/ 842, training loss 0.32296687364578247\n",
      "Epoch 24 -- Batch 572/ 842, training loss 0.3182373344898224\n",
      "Epoch 24 -- Batch 573/ 842, training loss 0.3152707517147064\n",
      "Epoch 24 -- Batch 574/ 842, training loss 0.30958253145217896\n",
      "Epoch 24 -- Batch 575/ 842, training loss 0.31608960032463074\n",
      "Epoch 24 -- Batch 576/ 842, training loss 0.31930819153785706\n",
      "Epoch 24 -- Batch 577/ 842, training loss 0.31938761472702026\n",
      "Epoch 24 -- Batch 578/ 842, training loss 0.29950520396232605\n",
      "Epoch 24 -- Batch 579/ 842, training loss 0.3319706320762634\n",
      "Epoch 24 -- Batch 580/ 842, training loss 0.33012285828590393\n",
      "Epoch 24 -- Batch 581/ 842, training loss 0.3164564073085785\n",
      "Epoch 24 -- Batch 582/ 842, training loss 0.3075042963027954\n",
      "Epoch 24 -- Batch 583/ 842, training loss 0.29832276701927185\n",
      "Epoch 24 -- Batch 584/ 842, training loss 0.2986987233161926\n",
      "Epoch 24 -- Batch 585/ 842, training loss 0.31902894377708435\n",
      "Epoch 24 -- Batch 586/ 842, training loss 0.30737966299057007\n",
      "Epoch 24 -- Batch 587/ 842, training loss 0.3336508572101593\n",
      "Epoch 24 -- Batch 588/ 842, training loss 0.32579681277275085\n",
      "Epoch 24 -- Batch 589/ 842, training loss 0.3132954239845276\n",
      "Epoch 24 -- Batch 590/ 842, training loss 0.3317250609397888\n",
      "Epoch 24 -- Batch 591/ 842, training loss 0.31040334701538086\n",
      "Epoch 24 -- Batch 592/ 842, training loss 0.31693923473358154\n",
      "Epoch 24 -- Batch 593/ 842, training loss 0.3141222894191742\n",
      "Epoch 24 -- Batch 594/ 842, training loss 0.3124120533466339\n",
      "Epoch 24 -- Batch 595/ 842, training loss 0.33125680685043335\n",
      "Epoch 24 -- Batch 596/ 842, training loss 0.321782648563385\n",
      "Epoch 24 -- Batch 597/ 842, training loss 0.3215676248073578\n",
      "Epoch 24 -- Batch 598/ 842, training loss 0.3262517750263214\n",
      "Epoch 24 -- Batch 599/ 842, training loss 0.3171561360359192\n",
      "Epoch 24 -- Batch 600/ 842, training loss 0.3197072148323059\n",
      "Epoch 24 -- Batch 601/ 842, training loss 0.3251606822013855\n",
      "Epoch 24 -- Batch 602/ 842, training loss 0.3199271857738495\n",
      "Epoch 24 -- Batch 603/ 842, training loss 0.30573853850364685\n",
      "Epoch 24 -- Batch 604/ 842, training loss 0.3247908651828766\n",
      "Epoch 24 -- Batch 605/ 842, training loss 0.3077894151210785\n",
      "Epoch 24 -- Batch 606/ 842, training loss 0.30748340487480164\n",
      "Epoch 24 -- Batch 607/ 842, training loss 0.30768245458602905\n",
      "Epoch 24 -- Batch 608/ 842, training loss 0.30295106768608093\n",
      "Epoch 24 -- Batch 609/ 842, training loss 0.31873390078544617\n",
      "Epoch 24 -- Batch 610/ 842, training loss 0.31646478176116943\n",
      "Epoch 24 -- Batch 611/ 842, training loss 0.3134682774543762\n",
      "Epoch 24 -- Batch 612/ 842, training loss 0.31094610691070557\n",
      "Epoch 24 -- Batch 613/ 842, training loss 0.31890854239463806\n",
      "Epoch 24 -- Batch 614/ 842, training loss 0.32611748576164246\n",
      "Epoch 24 -- Batch 615/ 842, training loss 0.31801551580429077\n",
      "Epoch 24 -- Batch 616/ 842, training loss 0.308753103017807\n",
      "Epoch 24 -- Batch 617/ 842, training loss 0.3216864764690399\n",
      "Epoch 24 -- Batch 618/ 842, training loss 0.31655749678611755\n",
      "Epoch 24 -- Batch 619/ 842, training loss 0.31588423252105713\n",
      "Epoch 24 -- Batch 620/ 842, training loss 0.326342910528183\n",
      "Epoch 24 -- Batch 621/ 842, training loss 0.3154716491699219\n",
      "Epoch 24 -- Batch 622/ 842, training loss 0.32065829634666443\n",
      "Epoch 24 -- Batch 623/ 842, training loss 0.3172140419483185\n",
      "Epoch 24 -- Batch 624/ 842, training loss 0.3122502863407135\n",
      "Epoch 24 -- Batch 625/ 842, training loss 0.33350831270217896\n",
      "Epoch 24 -- Batch 626/ 842, training loss 0.32098156213760376\n",
      "Epoch 24 -- Batch 627/ 842, training loss 0.31923791766166687\n",
      "Epoch 24 -- Batch 628/ 842, training loss 0.3054503798484802\n",
      "Epoch 24 -- Batch 629/ 842, training loss 0.3099800646305084\n",
      "Epoch 24 -- Batch 630/ 842, training loss 0.3143496513366699\n",
      "Epoch 24 -- Batch 631/ 842, training loss 0.3235282003879547\n",
      "Epoch 24 -- Batch 632/ 842, training loss 0.3179071545600891\n",
      "Epoch 24 -- Batch 633/ 842, training loss 0.3313548266887665\n",
      "Epoch 24 -- Batch 634/ 842, training loss 0.31567367911338806\n",
      "Epoch 24 -- Batch 635/ 842, training loss 0.31854701042175293\n",
      "Epoch 24 -- Batch 636/ 842, training loss 0.31204935908317566\n",
      "Epoch 24 -- Batch 637/ 842, training loss 0.31368115544319153\n",
      "Epoch 24 -- Batch 638/ 842, training loss 0.32186251878738403\n",
      "Epoch 24 -- Batch 639/ 842, training loss 0.31118977069854736\n",
      "Epoch 24 -- Batch 640/ 842, training loss 0.30095580220222473\n",
      "Epoch 24 -- Batch 641/ 842, training loss 0.3140789866447449\n",
      "Epoch 24 -- Batch 642/ 842, training loss 0.3234725594520569\n",
      "Epoch 24 -- Batch 643/ 842, training loss 0.2999083697795868\n",
      "Epoch 24 -- Batch 644/ 842, training loss 0.32384568452835083\n",
      "Epoch 24 -- Batch 645/ 842, training loss 0.32464760541915894\n",
      "Epoch 24 -- Batch 646/ 842, training loss 0.31882354617118835\n",
      "Epoch 24 -- Batch 647/ 842, training loss 0.3260261118412018\n",
      "Epoch 24 -- Batch 648/ 842, training loss 0.3229004442691803\n",
      "Epoch 24 -- Batch 649/ 842, training loss 0.32028844952583313\n",
      "Epoch 24 -- Batch 650/ 842, training loss 0.3116310238838196\n",
      "Epoch 24 -- Batch 651/ 842, training loss 0.3117217719554901\n",
      "Epoch 24 -- Batch 652/ 842, training loss 0.29782527685165405\n",
      "Epoch 24 -- Batch 653/ 842, training loss 0.31554701924324036\n",
      "Epoch 24 -- Batch 654/ 842, training loss 0.31636109948158264\n",
      "Epoch 24 -- Batch 655/ 842, training loss 0.31279775500297546\n",
      "Epoch 24 -- Batch 656/ 842, training loss 0.3162122070789337\n",
      "Epoch 24 -- Batch 657/ 842, training loss 0.3185632824897766\n",
      "Epoch 24 -- Batch 658/ 842, training loss 0.31727856397628784\n",
      "Epoch 24 -- Batch 659/ 842, training loss 0.3178538978099823\n",
      "Epoch 24 -- Batch 660/ 842, training loss 0.3165513873100281\n",
      "Epoch 24 -- Batch 661/ 842, training loss 0.32661402225494385\n",
      "Epoch 24 -- Batch 662/ 842, training loss 0.3234419822692871\n",
      "Epoch 24 -- Batch 663/ 842, training loss 0.31736990809440613\n",
      "Epoch 24 -- Batch 664/ 842, training loss 0.32560238242149353\n",
      "Epoch 24 -- Batch 665/ 842, training loss 0.3179813027381897\n",
      "Epoch 24 -- Batch 666/ 842, training loss 0.3243938088417053\n",
      "Epoch 24 -- Batch 667/ 842, training loss 0.31000909209251404\n",
      "Epoch 24 -- Batch 668/ 842, training loss 0.30686816573143005\n",
      "Epoch 24 -- Batch 669/ 842, training loss 0.32598885893821716\n",
      "Epoch 24 -- Batch 670/ 842, training loss 0.3108235001564026\n",
      "Epoch 24 -- Batch 671/ 842, training loss 0.30798661708831787\n",
      "Epoch 24 -- Batch 672/ 842, training loss 0.32396066188812256\n",
      "Epoch 24 -- Batch 673/ 842, training loss 0.3089335262775421\n",
      "Epoch 24 -- Batch 674/ 842, training loss 0.33235493302345276\n",
      "Epoch 24 -- Batch 675/ 842, training loss 0.32003694772720337\n",
      "Epoch 24 -- Batch 676/ 842, training loss 0.3178023099899292\n",
      "Epoch 24 -- Batch 677/ 842, training loss 0.3149925172328949\n",
      "Epoch 24 -- Batch 678/ 842, training loss 0.32241711020469666\n",
      "Epoch 24 -- Batch 679/ 842, training loss 0.31035667657852173\n",
      "Epoch 24 -- Batch 680/ 842, training loss 0.32106804847717285\n",
      "Epoch 24 -- Batch 681/ 842, training loss 0.32701030373573303\n",
      "Epoch 24 -- Batch 682/ 842, training loss 0.3106638193130493\n",
      "Epoch 24 -- Batch 683/ 842, training loss 0.31956222653388977\n",
      "Epoch 24 -- Batch 684/ 842, training loss 0.32627564668655396\n",
      "Epoch 24 -- Batch 685/ 842, training loss 0.32514089345932007\n",
      "Epoch 24 -- Batch 686/ 842, training loss 0.31870904564857483\n",
      "Epoch 24 -- Batch 687/ 842, training loss 0.3223216235637665\n",
      "Epoch 24 -- Batch 688/ 842, training loss 0.3198600709438324\n",
      "Epoch 24 -- Batch 689/ 842, training loss 0.32056108117103577\n",
      "Epoch 24 -- Batch 690/ 842, training loss 0.31885308027267456\n",
      "Epoch 24 -- Batch 691/ 842, training loss 0.3144832253456116\n",
      "Epoch 24 -- Batch 692/ 842, training loss 0.3230547308921814\n",
      "Epoch 24 -- Batch 693/ 842, training loss 0.31314191222190857\n",
      "Epoch 24 -- Batch 694/ 842, training loss 0.3125370740890503\n",
      "Epoch 24 -- Batch 695/ 842, training loss 0.31842297315597534\n",
      "Epoch 24 -- Batch 696/ 842, training loss 0.31267356872558594\n",
      "Epoch 24 -- Batch 697/ 842, training loss 0.30563774704933167\n",
      "Epoch 24 -- Batch 698/ 842, training loss 0.3157460689544678\n",
      "Epoch 24 -- Batch 699/ 842, training loss 0.30964913964271545\n",
      "Epoch 24 -- Batch 700/ 842, training loss 0.3128547966480255\n",
      "Epoch 24 -- Batch 701/ 842, training loss 0.31965839862823486\n",
      "Epoch 24 -- Batch 702/ 842, training loss 0.32977360486984253\n",
      "Epoch 24 -- Batch 703/ 842, training loss 0.320471853017807\n",
      "Epoch 24 -- Batch 704/ 842, training loss 0.31433922052383423\n",
      "Epoch 24 -- Batch 705/ 842, training loss 0.32000768184661865\n",
      "Epoch 24 -- Batch 706/ 842, training loss 0.33184704184532166\n",
      "Epoch 24 -- Batch 707/ 842, training loss 0.32049110531806946\n",
      "Epoch 24 -- Batch 708/ 842, training loss 0.31905925273895264\n",
      "Epoch 24 -- Batch 709/ 842, training loss 0.3124845027923584\n",
      "Epoch 24 -- Batch 710/ 842, training loss 0.3184618055820465\n",
      "Epoch 24 -- Batch 711/ 842, training loss 0.32342323660850525\n",
      "Epoch 24 -- Batch 712/ 842, training loss 0.3209674060344696\n",
      "Epoch 24 -- Batch 713/ 842, training loss 0.3313371539115906\n",
      "Epoch 24 -- Batch 714/ 842, training loss 0.3135114908218384\n",
      "Epoch 24 -- Batch 715/ 842, training loss 0.3242608606815338\n",
      "Epoch 24 -- Batch 716/ 842, training loss 0.32203662395477295\n",
      "Epoch 24 -- Batch 717/ 842, training loss 0.3170056641101837\n",
      "Epoch 24 -- Batch 718/ 842, training loss 0.3166167736053467\n",
      "Epoch 24 -- Batch 719/ 842, training loss 0.32008200883865356\n",
      "Epoch 24 -- Batch 720/ 842, training loss 0.3228219747543335\n",
      "Epoch 24 -- Batch 721/ 842, training loss 0.32978761196136475\n",
      "Epoch 24 -- Batch 722/ 842, training loss 0.30732473731040955\n",
      "Epoch 24 -- Batch 723/ 842, training loss 0.30642348527908325\n",
      "Epoch 24 -- Batch 724/ 842, training loss 0.32238689064979553\n",
      "Epoch 24 -- Batch 725/ 842, training loss 0.3214971721172333\n",
      "Epoch 24 -- Batch 726/ 842, training loss 0.31142204999923706\n",
      "Epoch 24 -- Batch 727/ 842, training loss 0.32002589106559753\n",
      "Epoch 24 -- Batch 728/ 842, training loss 0.3043823540210724\n",
      "Epoch 24 -- Batch 729/ 842, training loss 0.30763185024261475\n",
      "Epoch 24 -- Batch 730/ 842, training loss 0.32069849967956543\n",
      "Epoch 24 -- Batch 731/ 842, training loss 0.3129458427429199\n",
      "Epoch 24 -- Batch 732/ 842, training loss 0.3130374550819397\n",
      "Epoch 24 -- Batch 733/ 842, training loss 0.3163906931877136\n",
      "Epoch 24 -- Batch 734/ 842, training loss 0.31820449233055115\n",
      "Epoch 24 -- Batch 735/ 842, training loss 0.3199973404407501\n",
      "Epoch 24 -- Batch 736/ 842, training loss 0.31666386127471924\n",
      "Epoch 24 -- Batch 737/ 842, training loss 0.3265209496021271\n",
      "Epoch 24 -- Batch 738/ 842, training loss 0.3105304539203644\n",
      "Epoch 24 -- Batch 739/ 842, training loss 0.3028089106082916\n",
      "Epoch 24 -- Batch 740/ 842, training loss 0.32050666213035583\n",
      "Epoch 24 -- Batch 741/ 842, training loss 0.2990255355834961\n",
      "Epoch 24 -- Batch 742/ 842, training loss 0.3218488097190857\n",
      "Epoch 24 -- Batch 743/ 842, training loss 0.3179584741592407\n",
      "Epoch 24 -- Batch 744/ 842, training loss 0.3086342215538025\n",
      "Epoch 24 -- Batch 745/ 842, training loss 0.3078097403049469\n",
      "Epoch 24 -- Batch 746/ 842, training loss 0.3185068666934967\n",
      "Epoch 24 -- Batch 747/ 842, training loss 0.31476736068725586\n",
      "Epoch 24 -- Batch 748/ 842, training loss 0.31775540113449097\n",
      "Epoch 24 -- Batch 749/ 842, training loss 0.312905490398407\n",
      "Epoch 24 -- Batch 750/ 842, training loss 0.322054922580719\n",
      "Epoch 24 -- Batch 751/ 842, training loss 0.31817397475242615\n",
      "Epoch 24 -- Batch 752/ 842, training loss 0.3208393454551697\n",
      "Epoch 24 -- Batch 753/ 842, training loss 0.32435545325279236\n",
      "Epoch 24 -- Batch 754/ 842, training loss 0.3203486502170563\n",
      "Epoch 24 -- Batch 755/ 842, training loss 0.32046425342559814\n",
      "Epoch 24 -- Batch 756/ 842, training loss 0.32321760058403015\n",
      "Epoch 24 -- Batch 757/ 842, training loss 0.3183790445327759\n",
      "Epoch 24 -- Batch 758/ 842, training loss 0.316010445356369\n",
      "Epoch 24 -- Batch 759/ 842, training loss 0.31920164823532104\n",
      "Epoch 24 -- Batch 760/ 842, training loss 0.3198036849498749\n",
      "Epoch 24 -- Batch 761/ 842, training loss 0.32505306601524353\n",
      "Epoch 24 -- Batch 762/ 842, training loss 0.3196603059768677\n",
      "Epoch 24 -- Batch 763/ 842, training loss 0.32601919770240784\n",
      "Epoch 24 -- Batch 764/ 842, training loss 0.3219919204711914\n",
      "Epoch 24 -- Batch 765/ 842, training loss 0.3232126533985138\n",
      "Epoch 24 -- Batch 766/ 842, training loss 0.3145565688610077\n",
      "Epoch 24 -- Batch 767/ 842, training loss 0.30908823013305664\n",
      "Epoch 24 -- Batch 768/ 842, training loss 0.307801753282547\n",
      "Epoch 24 -- Batch 769/ 842, training loss 0.3100506067276001\n",
      "Epoch 24 -- Batch 770/ 842, training loss 0.3182520270347595\n",
      "Epoch 24 -- Batch 771/ 842, training loss 0.31710678339004517\n",
      "Epoch 24 -- Batch 772/ 842, training loss 0.32434922456741333\n",
      "Epoch 24 -- Batch 773/ 842, training loss 0.3155980706214905\n",
      "Epoch 24 -- Batch 774/ 842, training loss 0.32484832406044006\n",
      "Epoch 24 -- Batch 775/ 842, training loss 0.3100825250148773\n",
      "Epoch 24 -- Batch 776/ 842, training loss 0.3285771608352661\n",
      "Epoch 24 -- Batch 777/ 842, training loss 0.3182235062122345\n",
      "Epoch 24 -- Batch 778/ 842, training loss 0.3134835362434387\n",
      "Epoch 24 -- Batch 779/ 842, training loss 0.319476455450058\n",
      "Epoch 24 -- Batch 780/ 842, training loss 0.3218502700328827\n",
      "Epoch 24 -- Batch 781/ 842, training loss 0.31567034125328064\n",
      "Epoch 24 -- Batch 782/ 842, training loss 0.31622132658958435\n",
      "Epoch 24 -- Batch 783/ 842, training loss 0.3196905851364136\n",
      "Epoch 24 -- Batch 784/ 842, training loss 0.3145897388458252\n",
      "Epoch 24 -- Batch 785/ 842, training loss 0.31771641969680786\n",
      "Epoch 24 -- Batch 786/ 842, training loss 0.31880930066108704\n",
      "Epoch 24 -- Batch 787/ 842, training loss 0.3134070634841919\n",
      "Epoch 24 -- Batch 788/ 842, training loss 0.31614410877227783\n",
      "Epoch 24 -- Batch 789/ 842, training loss 0.3216092586517334\n",
      "Epoch 24 -- Batch 790/ 842, training loss 0.31631800532341003\n",
      "Epoch 24 -- Batch 791/ 842, training loss 0.3149931728839874\n",
      "Epoch 24 -- Batch 792/ 842, training loss 0.3130086064338684\n",
      "Epoch 24 -- Batch 793/ 842, training loss 0.3085190951824188\n",
      "Epoch 24 -- Batch 794/ 842, training loss 0.3154498040676117\n",
      "Epoch 24 -- Batch 795/ 842, training loss 0.31321096420288086\n",
      "Epoch 24 -- Batch 796/ 842, training loss 0.30711066722869873\n",
      "Epoch 24 -- Batch 797/ 842, training loss 0.3014197051525116\n",
      "Epoch 24 -- Batch 798/ 842, training loss 0.31976521015167236\n",
      "Epoch 24 -- Batch 799/ 842, training loss 0.31475624442100525\n",
      "Epoch 24 -- Batch 800/ 842, training loss 0.3272295892238617\n",
      "Epoch 24 -- Batch 801/ 842, training loss 0.3251158595085144\n",
      "Epoch 24 -- Batch 802/ 842, training loss 0.3216640055179596\n",
      "Epoch 24 -- Batch 803/ 842, training loss 0.32513874769210815\n",
      "Epoch 24 -- Batch 804/ 842, training loss 0.3183175027370453\n",
      "Epoch 24 -- Batch 805/ 842, training loss 0.31850486993789673\n",
      "Epoch 24 -- Batch 806/ 842, training loss 0.31327560544013977\n",
      "Epoch 24 -- Batch 807/ 842, training loss 0.3222123384475708\n",
      "Epoch 24 -- Batch 808/ 842, training loss 0.32372426986694336\n",
      "Epoch 24 -- Batch 809/ 842, training loss 0.3210565745830536\n",
      "Epoch 24 -- Batch 810/ 842, training loss 0.3135296106338501\n",
      "Epoch 24 -- Batch 811/ 842, training loss 0.327878475189209\n",
      "Epoch 24 -- Batch 812/ 842, training loss 0.3126116991043091\n",
      "Epoch 24 -- Batch 813/ 842, training loss 0.3211423456668854\n",
      "Epoch 24 -- Batch 814/ 842, training loss 0.32912713289260864\n",
      "Epoch 24 -- Batch 815/ 842, training loss 0.3198259472846985\n",
      "Epoch 24 -- Batch 816/ 842, training loss 0.3097879886627197\n",
      "Epoch 24 -- Batch 817/ 842, training loss 0.3010702431201935\n",
      "Epoch 24 -- Batch 818/ 842, training loss 0.31858840584754944\n",
      "Epoch 24 -- Batch 819/ 842, training loss 0.32455673813819885\n",
      "Epoch 24 -- Batch 820/ 842, training loss 0.3055492043495178\n",
      "Epoch 24 -- Batch 821/ 842, training loss 0.3023693859577179\n",
      "Epoch 24 -- Batch 822/ 842, training loss 0.3311404883861542\n",
      "Epoch 24 -- Batch 823/ 842, training loss 0.3315671682357788\n",
      "Epoch 24 -- Batch 824/ 842, training loss 0.3190467357635498\n",
      "Epoch 24 -- Batch 825/ 842, training loss 0.3179280459880829\n",
      "Epoch 24 -- Batch 826/ 842, training loss 0.32560819387435913\n",
      "Epoch 24 -- Batch 827/ 842, training loss 0.319943904876709\n",
      "Epoch 24 -- Batch 828/ 842, training loss 0.3177604079246521\n",
      "Epoch 24 -- Batch 829/ 842, training loss 0.32025036215782166\n",
      "Epoch 24 -- Batch 830/ 842, training loss 0.3158363103866577\n",
      "Epoch 24 -- Batch 831/ 842, training loss 0.32163000106811523\n",
      "Epoch 24 -- Batch 832/ 842, training loss 0.32327350974082947\n",
      "Epoch 24 -- Batch 833/ 842, training loss 0.3136509656906128\n",
      "Epoch 24 -- Batch 834/ 842, training loss 0.32506296038627625\n",
      "Epoch 24 -- Batch 835/ 842, training loss 0.3252657949924469\n",
      "Epoch 24 -- Batch 836/ 842, training loss 0.32604026794433594\n",
      "Epoch 24 -- Batch 837/ 842, training loss 0.3277558386325836\n",
      "Epoch 24 -- Batch 838/ 842, training loss 0.3123795688152313\n",
      "Epoch 24 -- Batch 839/ 842, training loss 0.31215932965278625\n",
      "Epoch 24 -- Batch 840/ 842, training loss 0.3203927278518677\n",
      "Epoch 24 -- Batch 841/ 842, training loss 0.3179158866405487\n",
      "Epoch 24 -- Batch 842/ 842, training loss 0.300078421831131\n",
      "----------------------------------------------------------------------\n",
      "Epoch 24 -- Batch 1/ 94, validation loss 0.3039182424545288\n",
      "Epoch 24 -- Batch 2/ 94, validation loss 0.30933451652526855\n",
      "Epoch 24 -- Batch 3/ 94, validation loss 0.30294156074523926\n",
      "Epoch 24 -- Batch 4/ 94, validation loss 0.317277729511261\n",
      "Epoch 24 -- Batch 5/ 94, validation loss 0.3039918541908264\n",
      "Epoch 24 -- Batch 6/ 94, validation loss 0.3026205003261566\n",
      "Epoch 24 -- Batch 7/ 94, validation loss 0.30235040187835693\n",
      "Epoch 24 -- Batch 8/ 94, validation loss 0.31051525473594666\n",
      "Epoch 24 -- Batch 9/ 94, validation loss 0.3161109983921051\n",
      "Epoch 24 -- Batch 10/ 94, validation loss 0.2964242994785309\n",
      "Epoch 24 -- Batch 11/ 94, validation loss 0.31544366478919983\n",
      "Epoch 24 -- Batch 12/ 94, validation loss 0.29700788855552673\n",
      "Epoch 24 -- Batch 13/ 94, validation loss 0.2998425364494324\n",
      "Epoch 24 -- Batch 14/ 94, validation loss 0.30084314942359924\n",
      "Epoch 24 -- Batch 15/ 94, validation loss 0.2925913333892822\n",
      "Epoch 24 -- Batch 16/ 94, validation loss 0.3027001917362213\n",
      "Epoch 24 -- Batch 17/ 94, validation loss 0.3074345290660858\n",
      "Epoch 24 -- Batch 18/ 94, validation loss 0.29970574378967285\n",
      "Epoch 24 -- Batch 19/ 94, validation loss 0.3068515360355377\n",
      "Epoch 24 -- Batch 20/ 94, validation loss 0.305180162191391\n",
      "Epoch 24 -- Batch 21/ 94, validation loss 0.33703649044036865\n",
      "Epoch 24 -- Batch 22/ 94, validation loss 0.31856295466423035\n",
      "Epoch 24 -- Batch 23/ 94, validation loss 0.29428941011428833\n",
      "Epoch 24 -- Batch 24/ 94, validation loss 0.341515451669693\n",
      "Epoch 24 -- Batch 25/ 94, validation loss 0.31166043877601624\n",
      "Epoch 24 -- Batch 26/ 94, validation loss 0.30195915699005127\n",
      "Epoch 24 -- Batch 27/ 94, validation loss 0.2984156608581543\n",
      "Epoch 24 -- Batch 28/ 94, validation loss 0.3194710910320282\n",
      "Epoch 24 -- Batch 29/ 94, validation loss 0.31297767162323\n",
      "Epoch 24 -- Batch 30/ 94, validation loss 0.306337833404541\n",
      "Epoch 24 -- Batch 31/ 94, validation loss 0.2965027391910553\n",
      "Epoch 24 -- Batch 32/ 94, validation loss 0.3076666593551636\n",
      "Epoch 24 -- Batch 33/ 94, validation loss 0.31929489970207214\n",
      "Epoch 24 -- Batch 34/ 94, validation loss 0.30259013175964355\n",
      "Epoch 24 -- Batch 35/ 94, validation loss 0.3003223240375519\n",
      "Epoch 24 -- Batch 36/ 94, validation loss 0.3077413737773895\n",
      "Epoch 24 -- Batch 37/ 94, validation loss 0.31025442481040955\n",
      "Epoch 24 -- Batch 38/ 94, validation loss 0.30217525362968445\n",
      "Epoch 24 -- Batch 39/ 94, validation loss 0.2974971532821655\n",
      "Epoch 24 -- Batch 40/ 94, validation loss 0.30310532450675964\n",
      "Epoch 24 -- Batch 41/ 94, validation loss 0.29776015877723694\n",
      "Epoch 24 -- Batch 42/ 94, validation loss 0.311698317527771\n",
      "Epoch 24 -- Batch 43/ 94, validation loss 0.3065645396709442\n",
      "Epoch 24 -- Batch 44/ 94, validation loss 0.3025866448879242\n",
      "Epoch 24 -- Batch 45/ 94, validation loss 0.3018573224544525\n",
      "Epoch 24 -- Batch 46/ 94, validation loss 0.3024643063545227\n",
      "Epoch 24 -- Batch 47/ 94, validation loss 0.3160860538482666\n",
      "Epoch 24 -- Batch 48/ 94, validation loss 0.2993713319301605\n",
      "Epoch 24 -- Batch 49/ 94, validation loss 0.3173958361148834\n",
      "Epoch 24 -- Batch 50/ 94, validation loss 0.3013386130332947\n",
      "Epoch 24 -- Batch 51/ 94, validation loss 0.31339389085769653\n",
      "Epoch 24 -- Batch 52/ 94, validation loss 0.3024485409259796\n",
      "Epoch 24 -- Batch 53/ 94, validation loss 0.3156050443649292\n",
      "Epoch 24 -- Batch 54/ 94, validation loss 0.3044055104255676\n",
      "Epoch 24 -- Batch 55/ 94, validation loss 0.3013645112514496\n",
      "Epoch 24 -- Batch 56/ 94, validation loss 0.3050246238708496\n",
      "Epoch 24 -- Batch 57/ 94, validation loss 0.30070605874061584\n",
      "Epoch 24 -- Batch 58/ 94, validation loss 0.3102840185165405\n",
      "Epoch 24 -- Batch 59/ 94, validation loss 0.33625712990760803\n",
      "Epoch 24 -- Batch 60/ 94, validation loss 0.30992448329925537\n",
      "Epoch 24 -- Batch 61/ 94, validation loss 0.3122403919696808\n",
      "Epoch 24 -- Batch 62/ 94, validation loss 0.3113120496273041\n",
      "Epoch 24 -- Batch 63/ 94, validation loss 0.31155404448509216\n",
      "Epoch 24 -- Batch 64/ 94, validation loss 0.3071725070476532\n",
      "Epoch 24 -- Batch 65/ 94, validation loss 0.29966679215431213\n",
      "Epoch 24 -- Batch 66/ 94, validation loss 0.31885504722595215\n",
      "Epoch 24 -- Batch 67/ 94, validation loss 0.30945512652397156\n",
      "Epoch 24 -- Batch 68/ 94, validation loss 0.31498828530311584\n",
      "Epoch 24 -- Batch 69/ 94, validation loss 0.2960475981235504\n",
      "Epoch 24 -- Batch 70/ 94, validation loss 0.299452006816864\n",
      "Epoch 24 -- Batch 71/ 94, validation loss 0.2985646426677704\n",
      "Epoch 24 -- Batch 72/ 94, validation loss 0.2933656573295593\n",
      "Epoch 24 -- Batch 73/ 94, validation loss 0.3103049397468567\n",
      "Epoch 24 -- Batch 74/ 94, validation loss 0.2976803183555603\n",
      "Epoch 24 -- Batch 75/ 94, validation loss 0.3016645312309265\n",
      "Epoch 24 -- Batch 76/ 94, validation loss 0.31450724601745605\n",
      "Epoch 24 -- Batch 77/ 94, validation loss 0.3070375621318817\n",
      "Epoch 24 -- Batch 78/ 94, validation loss 0.30636903643608093\n",
      "Epoch 24 -- Batch 79/ 94, validation loss 0.31465253233909607\n",
      "Epoch 24 -- Batch 80/ 94, validation loss 0.3227146863937378\n",
      "Epoch 24 -- Batch 81/ 94, validation loss 0.310416042804718\n",
      "Epoch 24 -- Batch 82/ 94, validation loss 0.3070865571498871\n",
      "Epoch 24 -- Batch 83/ 94, validation loss 0.3048630356788635\n",
      "Epoch 24 -- Batch 84/ 94, validation loss 0.30711278319358826\n",
      "Epoch 24 -- Batch 85/ 94, validation loss 0.3154994249343872\n",
      "Epoch 24 -- Batch 86/ 94, validation loss 0.3009324073791504\n",
      "Epoch 24 -- Batch 87/ 94, validation loss 0.31113147735595703\n",
      "Epoch 24 -- Batch 88/ 94, validation loss 0.31476056575775146\n",
      "Epoch 24 -- Batch 89/ 94, validation loss 0.3096146881580353\n",
      "Epoch 24 -- Batch 90/ 94, validation loss 0.3079330027103424\n",
      "Epoch 24 -- Batch 91/ 94, validation loss 0.3003605008125305\n",
      "Epoch 24 -- Batch 92/ 94, validation loss 0.30845287442207336\n",
      "Epoch 24 -- Batch 93/ 94, validation loss 0.3057454526424408\n",
      "Epoch 24 -- Batch 94/ 94, validation loss 0.3107368052005768\n",
      "----------------------------------------------------------------------\n",
      "Epoch 24 loss: Training 0.3161139488220215, Validation 0.3107368052005768\n",
      "----------------------------------------------------------------------\n",
      "Epoch 25/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:07:52] SMILES Parse Error: syntax error while parsing: Cc1ccc(NC(=O)Cc2csc(-c3cccc()c3)n2)cc1NS(C)(=O)=O\n",
      "[19:07:52] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(NC(=O)Cc2csc(-c3cccc()c3)n2)cc1NS(C)(=O)=O' for input: 'Cc1ccc(NC(=O)Cc2csc(-c3cccc()c3)n2)cc1NS(C)(=O)=O'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 18 20 21\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 18 19 23\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C2CCC3(CCC(=O)N3CCSC3=O)CC2)cc1'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 1 2 5 6 28\n",
      "[19:07:52] SMILES Parse Error: ring closure 2 duplicates bond between atom 5 and atom 10 for input: 'CCOC(=O)C12C(=O)OCC12C(=O)N(C(C)C)c1ccccc12'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 27 28 29 30 31\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'COCC(=O)Nc1ccc2c(c1)[C@@H]1C[C@@H](CC(=O)O)O[C@@H]2C[C@@H](O)[C@@H]1O2'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'COc1cc(-c2nnc(CCC(=O)N3CCCC(O)4)C2)on1'\n",
      "[19:07:52] SMILES Parse Error: syntax error while parsing: CN1/=C(C2SC2(n4ccccc4)CC(=O)N2)Cc2ccccc21\n",
      "[19:07:52] SMILES Parse Error: Failed parsing SMILES 'CN1/=C(C2SC2(n4ccccc4)CC(=O)N2)Cc2ccccc21' for input: 'CN1/=C(C2SC2(n4ccccc4)CC(=O)N2)Cc2ccccc21'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CC(C)C[C@@H]1c2ccccc2-c2ccccc2[C@H]1[C@@H](C(=O)NCc2ccc(F)cc2)[C@H](CO)O1'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 2 3 7 16 18\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CCC(C)C1/C(NN=C/c2ccccc2)C(=O)N=C2N(Cc2cccc(Cl)c2)C3=NC12'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC(=O)C=C(NC2CC3)C(=O)C1C'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 12 13 20 21 24\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 8\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 3 4 5 16 17\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CC(C)[C@H]1C2CCC2C3CC[C@H]4CC[C@H]4O[C@H](O)C[C@]5(C)[C@@]4CC[C@]3(C)[C@@H]2CC[C@@]11C'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@@H]1CC[C@@H]2SC[C@@H](C)N1Cc1cccc(F)c1)c1cccc(Cl)c1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CN1C(=O)[C@H]2Cc3c([nH]c4ccccc34)C(C)(C)N2C(=O)c1ccc(F)cc1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1-c1nn(CN2CCCC3)c(=S)n2C1CC1'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 13 15 16 17 18 19 20 21 23\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'O=C(c1cccc(OC2CCN(Cc3cccn3ncnc3)c2)C1)N1CCN(c2ccccc2)CC1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CCCCO[C@@H](=N)C(=O)COC(C)(C)C'\n",
      "[19:07:52] SMILES Parse Error: ring closure 2 duplicates bond between atom 17 and atom 18 for input: 'O=CC12C=CCC1C(=O)OC1(CO)C2(C)CCC2C12'\n",
      "[19:07:52] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CC(C)Oc1ccc2cc(C(=O)NC(=O)CCN3C(=O)C4C=CC(C5)C3)sc2c1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2nnc(-c3ccccc3[C@@H]oCN3C(C)=O)n2)cc1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(OCC(=O)N2CCN(S(=O)(=O)c3cc4c5c(c3)CCN3C(=O)CC4)CC2)cc1'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 2 4 19 21 22 23 24 25 26\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Fc1ccc2nc(NC3=NCCN3CCCC3)cc2c1'\n",
      "[19:07:52] SMILES Parse Error: extra close parentheses while parsing: Cc1nn(-c2ccccc2)c(N2CCN(C(=O)c3cc(F)ccc3F)CC2)n1)c1ccccc1\n",
      "[19:07:52] SMILES Parse Error: Failed parsing SMILES 'Cc1nn(-c2ccccc2)c(N2CCN(C(=O)c3cc(F)ccc3F)CC2)n1)c1ccccc1' for input: 'Cc1nn(-c2ccccc2)c(N2CCN(C(=O)c3cc(F)ccc3F)CC2)n1)c1ccccc1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'C/C=C/c1ccc2n(c1=O)C[C@H]1[C@H](CO)[C@@H](C(=O)NCCc3cnn4ccccc44)[C@H]2N1CCC(F)(F)F'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 23\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 10 12 13\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Cc1nnc2ncnn1N1CCN(C(=O)c2ccccc2)CC1'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CCCCN(CCCC)Cc1c(O)cc(C)c2c(O)c(C)ccc2n12'\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 1 2 6 7 25 26 27\n",
      "[19:07:52] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 13\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'NC(=S)N/N=C1/Cc2ccccc2C2=O'\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(-c2c(NCCC3=O)C(=O)c3ccccc3S2)c1'\n",
      "[19:07:52] Explicit valence for atom # 16 O, 3, is greater than permitted\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'COC12COC3(C1)C(=O)OCC2C3C=CC2(O3)C(=O)C1CC=CCC1'\n",
      "[19:07:52] non-ring atom 20 marked aromatic\n",
      "[19:07:52] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC(=O)C2=C(Nc3ccccc3)C(=O)C1c1ccccc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 -- Batch 1/ 842, training loss 0.29613691568374634\n",
      "Epoch 25 -- Batch 2/ 842, training loss 0.3096993565559387\n",
      "Epoch 25 -- Batch 3/ 842, training loss 0.30765050649642944\n",
      "Epoch 25 -- Batch 4/ 842, training loss 0.3119145929813385\n",
      "Epoch 25 -- Batch 5/ 842, training loss 0.324607253074646\n",
      "Epoch 25 -- Batch 6/ 842, training loss 0.31439635157585144\n",
      "Epoch 25 -- Batch 7/ 842, training loss 0.30113211274147034\n",
      "Epoch 25 -- Batch 8/ 842, training loss 0.29917970299720764\n",
      "Epoch 25 -- Batch 9/ 842, training loss 0.3189898133277893\n",
      "Epoch 25 -- Batch 10/ 842, training loss 0.31025129556655884\n",
      "Epoch 25 -- Batch 11/ 842, training loss 0.31116724014282227\n",
      "Epoch 25 -- Batch 12/ 842, training loss 0.302013099193573\n",
      "Epoch 25 -- Batch 13/ 842, training loss 0.3145207166671753\n",
      "Epoch 25 -- Batch 14/ 842, training loss 0.3141750395298004\n",
      "Epoch 25 -- Batch 15/ 842, training loss 0.31307703256607056\n",
      "Epoch 25 -- Batch 16/ 842, training loss 0.30915096402168274\n",
      "Epoch 25 -- Batch 17/ 842, training loss 0.2983981668949127\n",
      "Epoch 25 -- Batch 18/ 842, training loss 0.31230467557907104\n",
      "Epoch 25 -- Batch 19/ 842, training loss 0.31358227133750916\n",
      "Epoch 25 -- Batch 20/ 842, training loss 0.31581300497055054\n",
      "Epoch 25 -- Batch 21/ 842, training loss 0.3159811496734619\n",
      "Epoch 25 -- Batch 22/ 842, training loss 0.3105021119117737\n",
      "Epoch 25 -- Batch 23/ 842, training loss 0.30039867758750916\n",
      "Epoch 25 -- Batch 24/ 842, training loss 0.30565914511680603\n",
      "Epoch 25 -- Batch 25/ 842, training loss 0.30707505345344543\n",
      "Epoch 25 -- Batch 26/ 842, training loss 0.30711570382118225\n",
      "Epoch 25 -- Batch 27/ 842, training loss 0.31641659140586853\n",
      "Epoch 25 -- Batch 28/ 842, training loss 0.3076658844947815\n",
      "Epoch 25 -- Batch 29/ 842, training loss 0.31011858582496643\n",
      "Epoch 25 -- Batch 30/ 842, training loss 0.3119509816169739\n",
      "Epoch 25 -- Batch 31/ 842, training loss 0.30460023880004883\n",
      "Epoch 25 -- Batch 32/ 842, training loss 0.31721261143684387\n",
      "Epoch 25 -- Batch 33/ 842, training loss 0.308232843875885\n",
      "Epoch 25 -- Batch 34/ 842, training loss 0.30739840865135193\n",
      "Epoch 25 -- Batch 35/ 842, training loss 0.31575068831443787\n",
      "Epoch 25 -- Batch 36/ 842, training loss 0.30862849950790405\n",
      "Epoch 25 -- Batch 37/ 842, training loss 0.2975226640701294\n",
      "Epoch 25 -- Batch 38/ 842, training loss 0.31862664222717285\n",
      "Epoch 25 -- Batch 39/ 842, training loss 0.3088389039039612\n",
      "Epoch 25 -- Batch 40/ 842, training loss 0.307565838098526\n",
      "Epoch 25 -- Batch 41/ 842, training loss 0.3114236891269684\n",
      "Epoch 25 -- Batch 42/ 842, training loss 0.3054192066192627\n",
      "Epoch 25 -- Batch 43/ 842, training loss 0.31399261951446533\n",
      "Epoch 25 -- Batch 44/ 842, training loss 0.3110032081604004\n",
      "Epoch 25 -- Batch 45/ 842, training loss 0.30802881717681885\n",
      "Epoch 25 -- Batch 46/ 842, training loss 0.32460519671440125\n",
      "Epoch 25 -- Batch 47/ 842, training loss 0.30944204330444336\n",
      "Epoch 25 -- Batch 48/ 842, training loss 0.31070971488952637\n",
      "Epoch 25 -- Batch 49/ 842, training loss 0.3246217668056488\n",
      "Epoch 25 -- Batch 50/ 842, training loss 0.3173624873161316\n",
      "Epoch 25 -- Batch 51/ 842, training loss 0.30553558468818665\n",
      "Epoch 25 -- Batch 52/ 842, training loss 0.3109641373157501\n",
      "Epoch 25 -- Batch 53/ 842, training loss 0.30954381823539734\n",
      "Epoch 25 -- Batch 54/ 842, training loss 0.29890793561935425\n",
      "Epoch 25 -- Batch 55/ 842, training loss 0.3122163712978363\n",
      "Epoch 25 -- Batch 56/ 842, training loss 0.3078489303588867\n",
      "Epoch 25 -- Batch 57/ 842, training loss 0.3031606674194336\n",
      "Epoch 25 -- Batch 58/ 842, training loss 0.31005778908729553\n",
      "Epoch 25 -- Batch 59/ 842, training loss 0.3055818974971771\n",
      "Epoch 25 -- Batch 60/ 842, training loss 0.29992160201072693\n",
      "Epoch 25 -- Batch 61/ 842, training loss 0.30650997161865234\n",
      "Epoch 25 -- Batch 62/ 842, training loss 0.30362457036972046\n",
      "Epoch 25 -- Batch 63/ 842, training loss 0.30392107367515564\n",
      "Epoch 25 -- Batch 64/ 842, training loss 0.30379703640937805\n",
      "Epoch 25 -- Batch 65/ 842, training loss 0.30764535069465637\n",
      "Epoch 25 -- Batch 66/ 842, training loss 0.3040330111980438\n",
      "Epoch 25 -- Batch 67/ 842, training loss 0.31074997782707214\n",
      "Epoch 25 -- Batch 68/ 842, training loss 0.3096628189086914\n",
      "Epoch 25 -- Batch 69/ 842, training loss 0.3124293088912964\n",
      "Epoch 25 -- Batch 70/ 842, training loss 0.31123870611190796\n",
      "Epoch 25 -- Batch 71/ 842, training loss 0.3141739070415497\n",
      "Epoch 25 -- Batch 72/ 842, training loss 0.30916374921798706\n",
      "Epoch 25 -- Batch 73/ 842, training loss 0.30696919560432434\n",
      "Epoch 25 -- Batch 74/ 842, training loss 0.308404803276062\n",
      "Epoch 25 -- Batch 75/ 842, training loss 0.32441723346710205\n",
      "Epoch 25 -- Batch 76/ 842, training loss 0.31333351135253906\n",
      "Epoch 25 -- Batch 77/ 842, training loss 0.30582451820373535\n",
      "Epoch 25 -- Batch 78/ 842, training loss 0.30250218510627747\n",
      "Epoch 25 -- Batch 79/ 842, training loss 0.2999390661716461\n",
      "Epoch 25 -- Batch 80/ 842, training loss 0.3090127110481262\n",
      "Epoch 25 -- Batch 81/ 842, training loss 0.3014906048774719\n",
      "Epoch 25 -- Batch 82/ 842, training loss 0.3251127600669861\n",
      "Epoch 25 -- Batch 83/ 842, training loss 0.3177338242530823\n",
      "Epoch 25 -- Batch 84/ 842, training loss 0.30315276980400085\n",
      "Epoch 25 -- Batch 85/ 842, training loss 0.30051159858703613\n",
      "Epoch 25 -- Batch 86/ 842, training loss 0.32011622190475464\n",
      "Epoch 25 -- Batch 87/ 842, training loss 0.29919642210006714\n",
      "Epoch 25 -- Batch 88/ 842, training loss 0.3008357584476471\n",
      "Epoch 25 -- Batch 89/ 842, training loss 0.30776965618133545\n",
      "Epoch 25 -- Batch 90/ 842, training loss 0.31780433654785156\n",
      "Epoch 25 -- Batch 91/ 842, training loss 0.317491352558136\n",
      "Epoch 25 -- Batch 92/ 842, training loss 0.2983402609825134\n",
      "Epoch 25 -- Batch 93/ 842, training loss 0.2993815839290619\n",
      "Epoch 25 -- Batch 94/ 842, training loss 0.3151773512363434\n",
      "Epoch 25 -- Batch 95/ 842, training loss 0.3116803765296936\n",
      "Epoch 25 -- Batch 96/ 842, training loss 0.3073936700820923\n",
      "Epoch 25 -- Batch 97/ 842, training loss 0.32166215777397156\n",
      "Epoch 25 -- Batch 98/ 842, training loss 0.3127364218235016\n",
      "Epoch 25 -- Batch 99/ 842, training loss 0.307420551776886\n",
      "Epoch 25 -- Batch 100/ 842, training loss 0.3120805621147156\n",
      "Epoch 25 -- Batch 101/ 842, training loss 0.3065895438194275\n",
      "Epoch 25 -- Batch 102/ 842, training loss 0.30862799286842346\n",
      "Epoch 25 -- Batch 103/ 842, training loss 0.31596216559410095\n",
      "Epoch 25 -- Batch 104/ 842, training loss 0.30944979190826416\n",
      "Epoch 25 -- Batch 105/ 842, training loss 0.3082180321216583\n",
      "Epoch 25 -- Batch 106/ 842, training loss 0.3057308495044708\n",
      "Epoch 25 -- Batch 107/ 842, training loss 0.3036807179450989\n",
      "Epoch 25 -- Batch 108/ 842, training loss 0.3115401268005371\n",
      "Epoch 25 -- Batch 109/ 842, training loss 0.31310930848121643\n",
      "Epoch 25 -- Batch 110/ 842, training loss 0.3230477571487427\n",
      "Epoch 25 -- Batch 111/ 842, training loss 0.3166950047016144\n",
      "Epoch 25 -- Batch 112/ 842, training loss 0.3014654815196991\n",
      "Epoch 25 -- Batch 113/ 842, training loss 0.3150814473628998\n",
      "Epoch 25 -- Batch 114/ 842, training loss 0.29906049370765686\n",
      "Epoch 25 -- Batch 115/ 842, training loss 0.3116077482700348\n",
      "Epoch 25 -- Batch 116/ 842, training loss 0.3008793890476227\n",
      "Epoch 25 -- Batch 117/ 842, training loss 0.3127939999103546\n",
      "Epoch 25 -- Batch 118/ 842, training loss 0.3040226399898529\n",
      "Epoch 25 -- Batch 119/ 842, training loss 0.32051190733909607\n",
      "Epoch 25 -- Batch 120/ 842, training loss 0.30518078804016113\n",
      "Epoch 25 -- Batch 121/ 842, training loss 0.3202195465564728\n",
      "Epoch 25 -- Batch 122/ 842, training loss 0.31992611289024353\n",
      "Epoch 25 -- Batch 123/ 842, training loss 0.3088001310825348\n",
      "Epoch 25 -- Batch 124/ 842, training loss 0.2933252453804016\n",
      "Epoch 25 -- Batch 125/ 842, training loss 0.307779461145401\n",
      "Epoch 25 -- Batch 126/ 842, training loss 0.3193287253379822\n",
      "Epoch 25 -- Batch 127/ 842, training loss 0.302979439496994\n",
      "Epoch 25 -- Batch 128/ 842, training loss 0.3066294491291046\n",
      "Epoch 25 -- Batch 129/ 842, training loss 0.3092823028564453\n",
      "Epoch 25 -- Batch 130/ 842, training loss 0.3076740503311157\n",
      "Epoch 25 -- Batch 131/ 842, training loss 0.3013975918292999\n",
      "Epoch 25 -- Batch 132/ 842, training loss 0.31708255410194397\n",
      "Epoch 25 -- Batch 133/ 842, training loss 0.3255532383918762\n",
      "Epoch 25 -- Batch 134/ 842, training loss 0.317013144493103\n",
      "Epoch 25 -- Batch 135/ 842, training loss 0.31369438767433167\n",
      "Epoch 25 -- Batch 136/ 842, training loss 0.30899086594581604\n",
      "Epoch 25 -- Batch 137/ 842, training loss 0.297648549079895\n",
      "Epoch 25 -- Batch 138/ 842, training loss 0.31320443749427795\n",
      "Epoch 25 -- Batch 139/ 842, training loss 0.3142562806606293\n",
      "Epoch 25 -- Batch 140/ 842, training loss 0.31804201006889343\n",
      "Epoch 25 -- Batch 141/ 842, training loss 0.31559622287750244\n",
      "Epoch 25 -- Batch 142/ 842, training loss 0.31940239667892456\n",
      "Epoch 25 -- Batch 143/ 842, training loss 0.307956725358963\n",
      "Epoch 25 -- Batch 144/ 842, training loss 0.3151611089706421\n",
      "Epoch 25 -- Batch 145/ 842, training loss 0.31035229563713074\n",
      "Epoch 25 -- Batch 146/ 842, training loss 0.30869150161743164\n",
      "Epoch 25 -- Batch 147/ 842, training loss 0.3029748499393463\n",
      "Epoch 25 -- Batch 148/ 842, training loss 0.3134176433086395\n",
      "Epoch 25 -- Batch 149/ 842, training loss 0.3120521605014801\n",
      "Epoch 25 -- Batch 150/ 842, training loss 0.3057408034801483\n",
      "Epoch 25 -- Batch 151/ 842, training loss 0.3133073151111603\n",
      "Epoch 25 -- Batch 152/ 842, training loss 0.3291114568710327\n",
      "Epoch 25 -- Batch 153/ 842, training loss 0.30658283829689026\n",
      "Epoch 25 -- Batch 154/ 842, training loss 0.3042815923690796\n",
      "Epoch 25 -- Batch 155/ 842, training loss 0.30957162380218506\n",
      "Epoch 25 -- Batch 156/ 842, training loss 0.31278741359710693\n",
      "Epoch 25 -- Batch 157/ 842, training loss 0.3114270567893982\n",
      "Epoch 25 -- Batch 158/ 842, training loss 0.3133094012737274\n",
      "Epoch 25 -- Batch 159/ 842, training loss 0.29927194118499756\n",
      "Epoch 25 -- Batch 160/ 842, training loss 0.31294864416122437\n",
      "Epoch 25 -- Batch 161/ 842, training loss 0.32761815190315247\n",
      "Epoch 25 -- Batch 162/ 842, training loss 0.3052545487880707\n",
      "Epoch 25 -- Batch 163/ 842, training loss 0.31381094455718994\n",
      "Epoch 25 -- Batch 164/ 842, training loss 0.3074078857898712\n",
      "Epoch 25 -- Batch 165/ 842, training loss 0.31200703978538513\n",
      "Epoch 25 -- Batch 166/ 842, training loss 0.30452316999435425\n",
      "Epoch 25 -- Batch 167/ 842, training loss 0.31773051619529724\n",
      "Epoch 25 -- Batch 168/ 842, training loss 0.3038162291049957\n",
      "Epoch 25 -- Batch 169/ 842, training loss 0.31195759773254395\n",
      "Epoch 25 -- Batch 170/ 842, training loss 0.3255481719970703\n",
      "Epoch 25 -- Batch 171/ 842, training loss 0.31473308801651\n",
      "Epoch 25 -- Batch 172/ 842, training loss 0.30873119831085205\n",
      "Epoch 25 -- Batch 173/ 842, training loss 0.32115980982780457\n",
      "Epoch 25 -- Batch 174/ 842, training loss 0.30980873107910156\n",
      "Epoch 25 -- Batch 175/ 842, training loss 0.32662323117256165\n",
      "Epoch 25 -- Batch 176/ 842, training loss 0.3131595253944397\n",
      "Epoch 25 -- Batch 177/ 842, training loss 0.3185230493545532\n",
      "Epoch 25 -- Batch 178/ 842, training loss 0.3112706243991852\n",
      "Epoch 25 -- Batch 179/ 842, training loss 0.31465306878089905\n",
      "Epoch 25 -- Batch 180/ 842, training loss 0.3148227632045746\n",
      "Epoch 25 -- Batch 181/ 842, training loss 0.3050086796283722\n",
      "Epoch 25 -- Batch 182/ 842, training loss 0.312887579202652\n",
      "Epoch 25 -- Batch 183/ 842, training loss 0.30338358879089355\n",
      "Epoch 25 -- Batch 184/ 842, training loss 0.3064481317996979\n",
      "Epoch 25 -- Batch 185/ 842, training loss 0.3212837874889374\n",
      "Epoch 25 -- Batch 186/ 842, training loss 0.31272754073143005\n",
      "Epoch 25 -- Batch 187/ 842, training loss 0.3095899224281311\n",
      "Epoch 25 -- Batch 188/ 842, training loss 0.31088775396347046\n",
      "Epoch 25 -- Batch 189/ 842, training loss 0.30814316868782043\n",
      "Epoch 25 -- Batch 190/ 842, training loss 0.30962827801704407\n",
      "Epoch 25 -- Batch 191/ 842, training loss 0.31293785572052\n",
      "Epoch 25 -- Batch 192/ 842, training loss 0.3225097358226776\n",
      "Epoch 25 -- Batch 193/ 842, training loss 0.31328144669532776\n",
      "Epoch 25 -- Batch 194/ 842, training loss 0.3235691487789154\n",
      "Epoch 25 -- Batch 195/ 842, training loss 0.3096410930156708\n",
      "Epoch 25 -- Batch 196/ 842, training loss 0.31168070435523987\n",
      "Epoch 25 -- Batch 197/ 842, training loss 0.31206053495407104\n",
      "Epoch 25 -- Batch 198/ 842, training loss 0.31420332193374634\n",
      "Epoch 25 -- Batch 199/ 842, training loss 0.31134453415870667\n",
      "Epoch 25 -- Batch 200/ 842, training loss 0.3166792690753937\n",
      "Epoch 25 -- Batch 201/ 842, training loss 0.31373947858810425\n",
      "Epoch 25 -- Batch 202/ 842, training loss 0.31711190938949585\n",
      "Epoch 25 -- Batch 203/ 842, training loss 0.3119005560874939\n",
      "Epoch 25 -- Batch 204/ 842, training loss 0.3101904094219208\n",
      "Epoch 25 -- Batch 205/ 842, training loss 0.3199408948421478\n",
      "Epoch 25 -- Batch 206/ 842, training loss 0.30760833621025085\n",
      "Epoch 25 -- Batch 207/ 842, training loss 0.3185572028160095\n",
      "Epoch 25 -- Batch 208/ 842, training loss 0.3098704516887665\n",
      "Epoch 25 -- Batch 209/ 842, training loss 0.3262353539466858\n",
      "Epoch 25 -- Batch 210/ 842, training loss 0.3136286437511444\n",
      "Epoch 25 -- Batch 211/ 842, training loss 0.3222352862358093\n",
      "Epoch 25 -- Batch 212/ 842, training loss 0.3208068907260895\n",
      "Epoch 25 -- Batch 213/ 842, training loss 0.3039498031139374\n",
      "Epoch 25 -- Batch 214/ 842, training loss 0.3037967085838318\n",
      "Epoch 25 -- Batch 215/ 842, training loss 0.30739060044288635\n",
      "Epoch 25 -- Batch 216/ 842, training loss 0.32079100608825684\n",
      "Epoch 25 -- Batch 217/ 842, training loss 0.31993234157562256\n",
      "Epoch 25 -- Batch 218/ 842, training loss 0.3119106888771057\n",
      "Epoch 25 -- Batch 219/ 842, training loss 0.3097495436668396\n",
      "Epoch 25 -- Batch 220/ 842, training loss 0.31169256567955017\n",
      "Epoch 25 -- Batch 221/ 842, training loss 0.3075600266456604\n",
      "Epoch 25 -- Batch 222/ 842, training loss 0.310386598110199\n",
      "Epoch 25 -- Batch 223/ 842, training loss 0.30807703733444214\n",
      "Epoch 25 -- Batch 224/ 842, training loss 0.31779220700263977\n",
      "Epoch 25 -- Batch 225/ 842, training loss 0.3200792074203491\n",
      "Epoch 25 -- Batch 226/ 842, training loss 0.3229771852493286\n",
      "Epoch 25 -- Batch 227/ 842, training loss 0.31449177861213684\n",
      "Epoch 25 -- Batch 228/ 842, training loss 0.3133010268211365\n",
      "Epoch 25 -- Batch 229/ 842, training loss 0.31939876079559326\n",
      "Epoch 25 -- Batch 230/ 842, training loss 0.3042650520801544\n",
      "Epoch 25 -- Batch 231/ 842, training loss 0.31908825039863586\n",
      "Epoch 25 -- Batch 232/ 842, training loss 0.31253260374069214\n",
      "Epoch 25 -- Batch 233/ 842, training loss 0.30675870180130005\n",
      "Epoch 25 -- Batch 234/ 842, training loss 0.3101382851600647\n",
      "Epoch 25 -- Batch 235/ 842, training loss 0.326022207736969\n",
      "Epoch 25 -- Batch 236/ 842, training loss 0.3106136620044708\n",
      "Epoch 25 -- Batch 237/ 842, training loss 0.3172340989112854\n",
      "Epoch 25 -- Batch 238/ 842, training loss 0.31646931171417236\n",
      "Epoch 25 -- Batch 239/ 842, training loss 0.3095887005329132\n",
      "Epoch 25 -- Batch 240/ 842, training loss 0.31894516944885254\n",
      "Epoch 25 -- Batch 241/ 842, training loss 0.31390100717544556\n",
      "Epoch 25 -- Batch 242/ 842, training loss 0.3113384246826172\n",
      "Epoch 25 -- Batch 243/ 842, training loss 0.3110600411891937\n",
      "Epoch 25 -- Batch 244/ 842, training loss 0.30758869647979736\n",
      "Epoch 25 -- Batch 245/ 842, training loss 0.30852383375167847\n",
      "Epoch 25 -- Batch 246/ 842, training loss 0.3068053126335144\n",
      "Epoch 25 -- Batch 247/ 842, training loss 0.307631254196167\n",
      "Epoch 25 -- Batch 248/ 842, training loss 0.30378788709640503\n",
      "Epoch 25 -- Batch 249/ 842, training loss 0.3047471046447754\n",
      "Epoch 25 -- Batch 250/ 842, training loss 0.2993626892566681\n",
      "Epoch 25 -- Batch 251/ 842, training loss 0.30398640036582947\n",
      "Epoch 25 -- Batch 252/ 842, training loss 0.31159234046936035\n",
      "Epoch 25 -- Batch 253/ 842, training loss 0.30464568734169006\n",
      "Epoch 25 -- Batch 254/ 842, training loss 0.31645849347114563\n",
      "Epoch 25 -- Batch 255/ 842, training loss 0.32616081833839417\n",
      "Epoch 25 -- Batch 256/ 842, training loss 0.30303582549095154\n",
      "Epoch 25 -- Batch 257/ 842, training loss 0.32677775621414185\n",
      "Epoch 25 -- Batch 258/ 842, training loss 0.3012080490589142\n",
      "Epoch 25 -- Batch 259/ 842, training loss 0.3106306791305542\n",
      "Epoch 25 -- Batch 260/ 842, training loss 0.31365349888801575\n",
      "Epoch 25 -- Batch 261/ 842, training loss 0.3146142363548279\n",
      "Epoch 25 -- Batch 262/ 842, training loss 0.3138830363750458\n",
      "Epoch 25 -- Batch 263/ 842, training loss 0.3145190477371216\n",
      "Epoch 25 -- Batch 264/ 842, training loss 0.3200038969516754\n",
      "Epoch 25 -- Batch 265/ 842, training loss 0.3103242814540863\n",
      "Epoch 25 -- Batch 266/ 842, training loss 0.30828431248664856\n",
      "Epoch 25 -- Batch 267/ 842, training loss 0.31400588154792786\n",
      "Epoch 25 -- Batch 268/ 842, training loss 0.30650731921195984\n",
      "Epoch 25 -- Batch 269/ 842, training loss 0.3137427568435669\n",
      "Epoch 25 -- Batch 270/ 842, training loss 0.3164042830467224\n",
      "Epoch 25 -- Batch 271/ 842, training loss 0.30832216143608093\n",
      "Epoch 25 -- Batch 272/ 842, training loss 0.3049246370792389\n",
      "Epoch 25 -- Batch 273/ 842, training loss 0.3132133185863495\n",
      "Epoch 25 -- Batch 274/ 842, training loss 0.311152845621109\n",
      "Epoch 25 -- Batch 275/ 842, training loss 0.3195774257183075\n",
      "Epoch 25 -- Batch 276/ 842, training loss 0.3147610127925873\n",
      "Epoch 25 -- Batch 277/ 842, training loss 0.3084774315357208\n",
      "Epoch 25 -- Batch 278/ 842, training loss 0.3034711182117462\n",
      "Epoch 25 -- Batch 279/ 842, training loss 0.3188367486000061\n",
      "Epoch 25 -- Batch 280/ 842, training loss 0.30781930685043335\n",
      "Epoch 25 -- Batch 281/ 842, training loss 0.30489596724510193\n",
      "Epoch 25 -- Batch 282/ 842, training loss 0.3078238070011139\n",
      "Epoch 25 -- Batch 283/ 842, training loss 0.31225866079330444\n",
      "Epoch 25 -- Batch 284/ 842, training loss 0.30822592973709106\n",
      "Epoch 25 -- Batch 285/ 842, training loss 0.3146623969078064\n",
      "Epoch 25 -- Batch 286/ 842, training loss 0.31508195400238037\n",
      "Epoch 25 -- Batch 287/ 842, training loss 0.31822824478149414\n",
      "Epoch 25 -- Batch 288/ 842, training loss 0.31413012742996216\n",
      "Epoch 25 -- Batch 289/ 842, training loss 0.31859099864959717\n",
      "Epoch 25 -- Batch 290/ 842, training loss 0.30511337518692017\n",
      "Epoch 25 -- Batch 291/ 842, training loss 0.313605397939682\n",
      "Epoch 25 -- Batch 292/ 842, training loss 0.31407323479652405\n",
      "Epoch 25 -- Batch 293/ 842, training loss 0.31114107370376587\n",
      "Epoch 25 -- Batch 294/ 842, training loss 0.318805068731308\n",
      "Epoch 25 -- Batch 295/ 842, training loss 0.3053923547267914\n",
      "Epoch 25 -- Batch 296/ 842, training loss 0.3123157322406769\n",
      "Epoch 25 -- Batch 297/ 842, training loss 0.3184410333633423\n",
      "Epoch 25 -- Batch 298/ 842, training loss 0.3144155740737915\n",
      "Epoch 25 -- Batch 299/ 842, training loss 0.3203050196170807\n",
      "Epoch 25 -- Batch 300/ 842, training loss 0.3056706488132477\n",
      "Epoch 25 -- Batch 301/ 842, training loss 0.3109034597873688\n",
      "Epoch 25 -- Batch 302/ 842, training loss 0.32115569710731506\n",
      "Epoch 25 -- Batch 303/ 842, training loss 0.3126795291900635\n",
      "Epoch 25 -- Batch 304/ 842, training loss 0.3107151985168457\n",
      "Epoch 25 -- Batch 305/ 842, training loss 0.31633517146110535\n",
      "Epoch 25 -- Batch 306/ 842, training loss 0.31541213393211365\n",
      "Epoch 25 -- Batch 307/ 842, training loss 0.3001919388771057\n",
      "Epoch 25 -- Batch 308/ 842, training loss 0.3038189113140106\n",
      "Epoch 25 -- Batch 309/ 842, training loss 0.3172954320907593\n",
      "Epoch 25 -- Batch 310/ 842, training loss 0.3124803304672241\n",
      "Epoch 25 -- Batch 311/ 842, training loss 0.32202455401420593\n",
      "Epoch 25 -- Batch 312/ 842, training loss 0.312210351228714\n",
      "Epoch 25 -- Batch 313/ 842, training loss 0.30729439854621887\n",
      "Epoch 25 -- Batch 314/ 842, training loss 0.3150918781757355\n",
      "Epoch 25 -- Batch 315/ 842, training loss 0.3153882622718811\n",
      "Epoch 25 -- Batch 316/ 842, training loss 0.318085253238678\n",
      "Epoch 25 -- Batch 317/ 842, training loss 0.3151494264602661\n",
      "Epoch 25 -- Batch 318/ 842, training loss 0.3085329830646515\n",
      "Epoch 25 -- Batch 319/ 842, training loss 0.3068847954273224\n",
      "Epoch 25 -- Batch 320/ 842, training loss 0.32563725113868713\n",
      "Epoch 25 -- Batch 321/ 842, training loss 0.31285738945007324\n",
      "Epoch 25 -- Batch 322/ 842, training loss 0.3120463788509369\n",
      "Epoch 25 -- Batch 323/ 842, training loss 0.32062649726867676\n",
      "Epoch 25 -- Batch 324/ 842, training loss 0.32314983010292053\n",
      "Epoch 25 -- Batch 325/ 842, training loss 0.32302767038345337\n",
      "Epoch 25 -- Batch 326/ 842, training loss 0.313845157623291\n",
      "Epoch 25 -- Batch 327/ 842, training loss 0.30670246481895447\n",
      "Epoch 25 -- Batch 328/ 842, training loss 0.3211442232131958\n",
      "Epoch 25 -- Batch 329/ 842, training loss 0.30713820457458496\n",
      "Epoch 25 -- Batch 330/ 842, training loss 0.322929710149765\n",
      "Epoch 25 -- Batch 331/ 842, training loss 0.3162989914417267\n",
      "Epoch 25 -- Batch 332/ 842, training loss 0.31229227781295776\n",
      "Epoch 25 -- Batch 333/ 842, training loss 0.31467148661613464\n",
      "Epoch 25 -- Batch 334/ 842, training loss 0.3126939535140991\n",
      "Epoch 25 -- Batch 335/ 842, training loss 0.3065031170845032\n",
      "Epoch 25 -- Batch 336/ 842, training loss 0.3224593997001648\n",
      "Epoch 25 -- Batch 337/ 842, training loss 0.33091530203819275\n",
      "Epoch 25 -- Batch 338/ 842, training loss 0.3079436719417572\n",
      "Epoch 25 -- Batch 339/ 842, training loss 0.31786543130874634\n",
      "Epoch 25 -- Batch 340/ 842, training loss 0.31327250599861145\n",
      "Epoch 25 -- Batch 341/ 842, training loss 0.3231840133666992\n",
      "Epoch 25 -- Batch 342/ 842, training loss 0.30816414952278137\n",
      "Epoch 25 -- Batch 343/ 842, training loss 0.30115464329719543\n",
      "Epoch 25 -- Batch 344/ 842, training loss 0.32029440999031067\n",
      "Epoch 25 -- Batch 345/ 842, training loss 0.312968909740448\n",
      "Epoch 25 -- Batch 346/ 842, training loss 0.31012487411499023\n",
      "Epoch 25 -- Batch 347/ 842, training loss 0.313300758600235\n",
      "Epoch 25 -- Batch 348/ 842, training loss 0.31289610266685486\n",
      "Epoch 25 -- Batch 349/ 842, training loss 0.31325381994247437\n",
      "Epoch 25 -- Batch 350/ 842, training loss 0.3087534010410309\n",
      "Epoch 25 -- Batch 351/ 842, training loss 0.3108471632003784\n",
      "Epoch 25 -- Batch 352/ 842, training loss 0.30377665162086487\n",
      "Epoch 25 -- Batch 353/ 842, training loss 0.30764636397361755\n",
      "Epoch 25 -- Batch 354/ 842, training loss 0.3082851767539978\n",
      "Epoch 25 -- Batch 355/ 842, training loss 0.31583163142204285\n",
      "Epoch 25 -- Batch 356/ 842, training loss 0.3083896338939667\n",
      "Epoch 25 -- Batch 357/ 842, training loss 0.31125399470329285\n",
      "Epoch 25 -- Batch 358/ 842, training loss 0.2986535131931305\n",
      "Epoch 25 -- Batch 359/ 842, training loss 0.32297879457473755\n",
      "Epoch 25 -- Batch 360/ 842, training loss 0.3194771111011505\n",
      "Epoch 25 -- Batch 361/ 842, training loss 0.313041090965271\n",
      "Epoch 25 -- Batch 362/ 842, training loss 0.3193575441837311\n",
      "Epoch 25 -- Batch 363/ 842, training loss 0.3074178099632263\n",
      "Epoch 25 -- Batch 364/ 842, training loss 0.31215792894363403\n",
      "Epoch 25 -- Batch 365/ 842, training loss 0.3031398355960846\n",
      "Epoch 25 -- Batch 366/ 842, training loss 0.3187478184700012\n",
      "Epoch 25 -- Batch 367/ 842, training loss 0.3165102005004883\n",
      "Epoch 25 -- Batch 368/ 842, training loss 0.31440362334251404\n",
      "Epoch 25 -- Batch 369/ 842, training loss 0.31443578004837036\n",
      "Epoch 25 -- Batch 370/ 842, training loss 0.3221454322338104\n",
      "Epoch 25 -- Batch 371/ 842, training loss 0.319909930229187\n",
      "Epoch 25 -- Batch 372/ 842, training loss 0.3208865225315094\n",
      "Epoch 25 -- Batch 373/ 842, training loss 0.30000385642051697\n",
      "Epoch 25 -- Batch 374/ 842, training loss 0.3258095383644104\n",
      "Epoch 25 -- Batch 375/ 842, training loss 0.3172754943370819\n",
      "Epoch 25 -- Batch 376/ 842, training loss 0.32520750164985657\n",
      "Epoch 25 -- Batch 377/ 842, training loss 0.3209398090839386\n",
      "Epoch 25 -- Batch 378/ 842, training loss 0.3054317533969879\n",
      "Epoch 25 -- Batch 379/ 842, training loss 0.3076944649219513\n",
      "Epoch 25 -- Batch 380/ 842, training loss 0.3228774964809418\n",
      "Epoch 25 -- Batch 381/ 842, training loss 0.32154715061187744\n",
      "Epoch 25 -- Batch 382/ 842, training loss 0.3154691159725189\n",
      "Epoch 25 -- Batch 383/ 842, training loss 0.31949150562286377\n",
      "Epoch 25 -- Batch 384/ 842, training loss 0.3151147663593292\n",
      "Epoch 25 -- Batch 385/ 842, training loss 0.2998308837413788\n",
      "Epoch 25 -- Batch 386/ 842, training loss 0.3226163387298584\n",
      "Epoch 25 -- Batch 387/ 842, training loss 0.31049296259880066\n",
      "Epoch 25 -- Batch 388/ 842, training loss 0.304838627576828\n",
      "Epoch 25 -- Batch 389/ 842, training loss 0.3063638508319855\n",
      "Epoch 25 -- Batch 390/ 842, training loss 0.3188699781894684\n",
      "Epoch 25 -- Batch 391/ 842, training loss 0.3110165297985077\n",
      "Epoch 25 -- Batch 392/ 842, training loss 0.3087255656719208\n",
      "Epoch 25 -- Batch 393/ 842, training loss 0.3186267018318176\n",
      "Epoch 25 -- Batch 394/ 842, training loss 0.3157544732093811\n",
      "Epoch 25 -- Batch 395/ 842, training loss 0.30979499220848083\n",
      "Epoch 25 -- Batch 396/ 842, training loss 0.31176653504371643\n",
      "Epoch 25 -- Batch 397/ 842, training loss 0.3229645788669586\n",
      "Epoch 25 -- Batch 398/ 842, training loss 0.31232818961143494\n",
      "Epoch 25 -- Batch 399/ 842, training loss 0.32357585430145264\n",
      "Epoch 25 -- Batch 400/ 842, training loss 0.30938634276390076\n",
      "Epoch 25 -- Batch 401/ 842, training loss 0.3107852041721344\n",
      "Epoch 25 -- Batch 402/ 842, training loss 0.32061147689819336\n",
      "Epoch 25 -- Batch 403/ 842, training loss 0.31177547574043274\n",
      "Epoch 25 -- Batch 404/ 842, training loss 0.31104400753974915\n",
      "Epoch 25 -- Batch 405/ 842, training loss 0.3119778335094452\n",
      "Epoch 25 -- Batch 406/ 842, training loss 0.31485387682914734\n",
      "Epoch 25 -- Batch 407/ 842, training loss 0.3174971640110016\n",
      "Epoch 25 -- Batch 408/ 842, training loss 0.31960639357566833\n",
      "Epoch 25 -- Batch 409/ 842, training loss 0.30538156628608704\n",
      "Epoch 25 -- Batch 410/ 842, training loss 0.3141845762729645\n",
      "Epoch 25 -- Batch 411/ 842, training loss 0.3052574694156647\n",
      "Epoch 25 -- Batch 412/ 842, training loss 0.32317429780960083\n",
      "Epoch 25 -- Batch 413/ 842, training loss 0.3217523396015167\n",
      "Epoch 25 -- Batch 414/ 842, training loss 0.30814361572265625\n",
      "Epoch 25 -- Batch 415/ 842, training loss 0.31671062111854553\n",
      "Epoch 25 -- Batch 416/ 842, training loss 0.31380128860473633\n",
      "Epoch 25 -- Batch 417/ 842, training loss 0.3184603750705719\n",
      "Epoch 25 -- Batch 418/ 842, training loss 0.3122727870941162\n",
      "Epoch 25 -- Batch 419/ 842, training loss 0.3111007511615753\n",
      "Epoch 25 -- Batch 420/ 842, training loss 0.3172090947628021\n",
      "Epoch 25 -- Batch 421/ 842, training loss 0.30364537239074707\n",
      "Epoch 25 -- Batch 422/ 842, training loss 0.31858211755752563\n",
      "Epoch 25 -- Batch 423/ 842, training loss 0.3175382912158966\n",
      "Epoch 25 -- Batch 424/ 842, training loss 0.3188638985157013\n",
      "Epoch 25 -- Batch 425/ 842, training loss 0.31032636761665344\n",
      "Epoch 25 -- Batch 426/ 842, training loss 0.31118711829185486\n",
      "Epoch 25 -- Batch 427/ 842, training loss 0.32425370812416077\n",
      "Epoch 25 -- Batch 428/ 842, training loss 0.316102534532547\n",
      "Epoch 25 -- Batch 429/ 842, training loss 0.33836957812309265\n",
      "Epoch 25 -- Batch 430/ 842, training loss 0.3091433644294739\n",
      "Epoch 25 -- Batch 431/ 842, training loss 0.31077203154563904\n",
      "Epoch 25 -- Batch 432/ 842, training loss 0.29259827733039856\n",
      "Epoch 25 -- Batch 433/ 842, training loss 0.31235113739967346\n",
      "Epoch 25 -- Batch 434/ 842, training loss 0.31815463304519653\n",
      "Epoch 25 -- Batch 435/ 842, training loss 0.3094521462917328\n",
      "Epoch 25 -- Batch 436/ 842, training loss 0.3296859860420227\n",
      "Epoch 25 -- Batch 437/ 842, training loss 0.30512934923171997\n",
      "Epoch 25 -- Batch 438/ 842, training loss 0.31899240612983704\n",
      "Epoch 25 -- Batch 439/ 842, training loss 0.32008957862854004\n",
      "Epoch 25 -- Batch 440/ 842, training loss 0.2996275722980499\n",
      "Epoch 25 -- Batch 441/ 842, training loss 0.3194728195667267\n",
      "Epoch 25 -- Batch 442/ 842, training loss 0.31823697686195374\n",
      "Epoch 25 -- Batch 443/ 842, training loss 0.31436800956726074\n",
      "Epoch 25 -- Batch 444/ 842, training loss 0.3179018795490265\n",
      "Epoch 25 -- Batch 445/ 842, training loss 0.32188981771469116\n",
      "Epoch 25 -- Batch 446/ 842, training loss 0.30695098638534546\n",
      "Epoch 25 -- Batch 447/ 842, training loss 0.3159398138523102\n",
      "Epoch 25 -- Batch 448/ 842, training loss 0.31711500883102417\n",
      "Epoch 25 -- Batch 449/ 842, training loss 0.30509063601493835\n",
      "Epoch 25 -- Batch 450/ 842, training loss 0.31859290599823\n",
      "Epoch 25 -- Batch 451/ 842, training loss 0.32316097617149353\n",
      "Epoch 25 -- Batch 452/ 842, training loss 0.3231636583805084\n",
      "Epoch 25 -- Batch 453/ 842, training loss 0.3072288930416107\n",
      "Epoch 25 -- Batch 454/ 842, training loss 0.31120771169662476\n",
      "Epoch 25 -- Batch 455/ 842, training loss 0.3090566396713257\n",
      "Epoch 25 -- Batch 456/ 842, training loss 0.3163439631462097\n",
      "Epoch 25 -- Batch 457/ 842, training loss 0.30603060126304626\n",
      "Epoch 25 -- Batch 458/ 842, training loss 0.30496659874916077\n",
      "Epoch 25 -- Batch 459/ 842, training loss 0.32312873005867004\n",
      "Epoch 25 -- Batch 460/ 842, training loss 0.320624977350235\n",
      "Epoch 25 -- Batch 461/ 842, training loss 0.3177591562271118\n",
      "Epoch 25 -- Batch 462/ 842, training loss 0.313662588596344\n",
      "Epoch 25 -- Batch 463/ 842, training loss 0.3173835873603821\n",
      "Epoch 25 -- Batch 464/ 842, training loss 0.31681662797927856\n",
      "Epoch 25 -- Batch 465/ 842, training loss 0.31724312901496887\n",
      "Epoch 25 -- Batch 466/ 842, training loss 0.31735149025917053\n",
      "Epoch 25 -- Batch 467/ 842, training loss 0.32521381974220276\n",
      "Epoch 25 -- Batch 468/ 842, training loss 0.3090037405490875\n",
      "Epoch 25 -- Batch 469/ 842, training loss 0.30941471457481384\n",
      "Epoch 25 -- Batch 470/ 842, training loss 0.31221842765808105\n",
      "Epoch 25 -- Batch 471/ 842, training loss 0.314678430557251\n",
      "Epoch 25 -- Batch 472/ 842, training loss 0.3013952970504761\n",
      "Epoch 25 -- Batch 473/ 842, training loss 0.3004617393016815\n",
      "Epoch 25 -- Batch 474/ 842, training loss 0.3065441846847534\n",
      "Epoch 25 -- Batch 475/ 842, training loss 0.302090048789978\n",
      "Epoch 25 -- Batch 476/ 842, training loss 0.31185388565063477\n",
      "Epoch 25 -- Batch 477/ 842, training loss 0.32508939504623413\n",
      "Epoch 25 -- Batch 478/ 842, training loss 0.31980013847351074\n",
      "Epoch 25 -- Batch 479/ 842, training loss 0.3118102252483368\n",
      "Epoch 25 -- Batch 480/ 842, training loss 0.30235499143600464\n",
      "Epoch 25 -- Batch 481/ 842, training loss 0.30400004982948303\n",
      "Epoch 25 -- Batch 482/ 842, training loss 0.3116680979728699\n",
      "Epoch 25 -- Batch 483/ 842, training loss 0.3243262767791748\n",
      "Epoch 25 -- Batch 484/ 842, training loss 0.31186482310295105\n",
      "Epoch 25 -- Batch 485/ 842, training loss 0.3233587443828583\n",
      "Epoch 25 -- Batch 486/ 842, training loss 0.31059831380844116\n",
      "Epoch 25 -- Batch 487/ 842, training loss 0.31772610545158386\n",
      "Epoch 25 -- Batch 488/ 842, training loss 0.30626922845840454\n",
      "Epoch 25 -- Batch 489/ 842, training loss 0.3134254515171051\n",
      "Epoch 25 -- Batch 490/ 842, training loss 0.32196369767189026\n",
      "Epoch 25 -- Batch 491/ 842, training loss 0.30269134044647217\n",
      "Epoch 25 -- Batch 492/ 842, training loss 0.31907352805137634\n",
      "Epoch 25 -- Batch 493/ 842, training loss 0.3161082863807678\n",
      "Epoch 25 -- Batch 494/ 842, training loss 0.3103119134902954\n",
      "Epoch 25 -- Batch 495/ 842, training loss 0.3171493411064148\n",
      "Epoch 25 -- Batch 496/ 842, training loss 0.314912885427475\n",
      "Epoch 25 -- Batch 497/ 842, training loss 0.3191360831260681\n",
      "Epoch 25 -- Batch 498/ 842, training loss 0.3196521997451782\n",
      "Epoch 25 -- Batch 499/ 842, training loss 0.31658613681793213\n",
      "Epoch 25 -- Batch 500/ 842, training loss 0.3170342445373535\n",
      "Epoch 25 -- Batch 501/ 842, training loss 0.31229469180107117\n",
      "Epoch 25 -- Batch 502/ 842, training loss 0.3247677981853485\n",
      "Epoch 25 -- Batch 503/ 842, training loss 0.32395777106285095\n",
      "Epoch 25 -- Batch 504/ 842, training loss 0.31630632281303406\n",
      "Epoch 25 -- Batch 505/ 842, training loss 0.3115623891353607\n",
      "Epoch 25 -- Batch 506/ 842, training loss 0.3161574602127075\n",
      "Epoch 25 -- Batch 507/ 842, training loss 0.30253949761390686\n",
      "Epoch 25 -- Batch 508/ 842, training loss 0.30844730138778687\n",
      "Epoch 25 -- Batch 509/ 842, training loss 0.32149630784988403\n",
      "Epoch 25 -- Batch 510/ 842, training loss 0.315284788608551\n",
      "Epoch 25 -- Batch 511/ 842, training loss 0.3197292387485504\n",
      "Epoch 25 -- Batch 512/ 842, training loss 0.3074352741241455\n",
      "Epoch 25 -- Batch 513/ 842, training loss 0.3036646842956543\n",
      "Epoch 25 -- Batch 514/ 842, training loss 0.2929675877094269\n",
      "Epoch 25 -- Batch 515/ 842, training loss 0.3130091726779938\n",
      "Epoch 25 -- Batch 516/ 842, training loss 0.31876739859580994\n",
      "Epoch 25 -- Batch 517/ 842, training loss 0.31994086503982544\n",
      "Epoch 25 -- Batch 518/ 842, training loss 0.3131175637245178\n",
      "Epoch 25 -- Batch 519/ 842, training loss 0.3179922103881836\n",
      "Epoch 25 -- Batch 520/ 842, training loss 0.3313717246055603\n",
      "Epoch 25 -- Batch 521/ 842, training loss 0.318041056394577\n",
      "Epoch 25 -- Batch 522/ 842, training loss 0.33128952980041504\n",
      "Epoch 25 -- Batch 523/ 842, training loss 0.3106485903263092\n",
      "Epoch 25 -- Batch 524/ 842, training loss 0.3200947046279907\n",
      "Epoch 25 -- Batch 525/ 842, training loss 0.327930748462677\n",
      "Epoch 25 -- Batch 526/ 842, training loss 0.3247273564338684\n",
      "Epoch 25 -- Batch 527/ 842, training loss 0.30653277039527893\n",
      "Epoch 25 -- Batch 528/ 842, training loss 0.3101016879081726\n",
      "Epoch 25 -- Batch 529/ 842, training loss 0.31989073753356934\n",
      "Epoch 25 -- Batch 530/ 842, training loss 0.310560017824173\n",
      "Epoch 25 -- Batch 531/ 842, training loss 0.3096485435962677\n",
      "Epoch 25 -- Batch 532/ 842, training loss 0.32007649540901184\n",
      "Epoch 25 -- Batch 533/ 842, training loss 0.32336491346359253\n",
      "Epoch 25 -- Batch 534/ 842, training loss 0.3263453245162964\n",
      "Epoch 25 -- Batch 535/ 842, training loss 0.30963823199272156\n",
      "Epoch 25 -- Batch 536/ 842, training loss 0.30600506067276\n",
      "Epoch 25 -- Batch 537/ 842, training loss 0.31107187271118164\n",
      "Epoch 25 -- Batch 538/ 842, training loss 0.31344369053840637\n",
      "Epoch 25 -- Batch 539/ 842, training loss 0.31688839197158813\n",
      "Epoch 25 -- Batch 540/ 842, training loss 0.3048189580440521\n",
      "Epoch 25 -- Batch 541/ 842, training loss 0.31392571330070496\n",
      "Epoch 25 -- Batch 542/ 842, training loss 0.32734912633895874\n",
      "Epoch 25 -- Batch 543/ 842, training loss 0.31619754433631897\n",
      "Epoch 25 -- Batch 544/ 842, training loss 0.3170425295829773\n",
      "Epoch 25 -- Batch 545/ 842, training loss 0.31254440546035767\n",
      "Epoch 25 -- Batch 546/ 842, training loss 0.3127516508102417\n",
      "Epoch 25 -- Batch 547/ 842, training loss 0.32313400506973267\n",
      "Epoch 25 -- Batch 548/ 842, training loss 0.31449273228645325\n",
      "Epoch 25 -- Batch 549/ 842, training loss 0.3155442476272583\n",
      "Epoch 25 -- Batch 550/ 842, training loss 0.3144814968109131\n",
      "Epoch 25 -- Batch 551/ 842, training loss 0.3165547251701355\n",
      "Epoch 25 -- Batch 552/ 842, training loss 0.3232324421405792\n",
      "Epoch 25 -- Batch 553/ 842, training loss 0.3093557059764862\n",
      "Epoch 25 -- Batch 554/ 842, training loss 0.30748435854911804\n",
      "Epoch 25 -- Batch 555/ 842, training loss 0.3085666000843048\n",
      "Epoch 25 -- Batch 556/ 842, training loss 0.3115081191062927\n",
      "Epoch 25 -- Batch 557/ 842, training loss 0.31364017724990845\n",
      "Epoch 25 -- Batch 558/ 842, training loss 0.31125780940055847\n",
      "Epoch 25 -- Batch 559/ 842, training loss 0.3152860999107361\n",
      "Epoch 25 -- Batch 560/ 842, training loss 0.3140535354614258\n",
      "Epoch 25 -- Batch 561/ 842, training loss 0.31331223249435425\n",
      "Epoch 25 -- Batch 562/ 842, training loss 0.30426591634750366\n",
      "Epoch 25 -- Batch 563/ 842, training loss 0.32251524925231934\n",
      "Epoch 25 -- Batch 564/ 842, training loss 0.32197582721710205\n",
      "Epoch 25 -- Batch 565/ 842, training loss 0.31402021646499634\n",
      "Epoch 25 -- Batch 566/ 842, training loss 0.2996435761451721\n",
      "Epoch 25 -- Batch 567/ 842, training loss 0.30167442560195923\n",
      "Epoch 25 -- Batch 568/ 842, training loss 0.3180994987487793\n",
      "Epoch 25 -- Batch 569/ 842, training loss 0.3118361830711365\n",
      "Epoch 25 -- Batch 570/ 842, training loss 0.3154946565628052\n",
      "Epoch 25 -- Batch 571/ 842, training loss 0.3061927258968353\n",
      "Epoch 25 -- Batch 572/ 842, training loss 0.3125465214252472\n",
      "Epoch 25 -- Batch 573/ 842, training loss 0.31373029947280884\n",
      "Epoch 25 -- Batch 574/ 842, training loss 0.31017792224884033\n",
      "Epoch 25 -- Batch 575/ 842, training loss 0.31454408168792725\n",
      "Epoch 25 -- Batch 576/ 842, training loss 0.30863699316978455\n",
      "Epoch 25 -- Batch 577/ 842, training loss 0.31398072838783264\n",
      "Epoch 25 -- Batch 578/ 842, training loss 0.31043267250061035\n",
      "Epoch 25 -- Batch 579/ 842, training loss 0.3165489435195923\n",
      "Epoch 25 -- Batch 580/ 842, training loss 0.3014543056488037\n",
      "Epoch 25 -- Batch 581/ 842, training loss 0.3142077624797821\n",
      "Epoch 25 -- Batch 582/ 842, training loss 0.30912095308303833\n",
      "Epoch 25 -- Batch 583/ 842, training loss 0.3237890899181366\n",
      "Epoch 25 -- Batch 584/ 842, training loss 0.31278473138809204\n",
      "Epoch 25 -- Batch 585/ 842, training loss 0.3172999322414398\n",
      "Epoch 25 -- Batch 586/ 842, training loss 0.3079122304916382\n",
      "Epoch 25 -- Batch 587/ 842, training loss 0.30337753891944885\n",
      "Epoch 25 -- Batch 588/ 842, training loss 0.3154149055480957\n",
      "Epoch 25 -- Batch 589/ 842, training loss 0.3299921452999115\n",
      "Epoch 25 -- Batch 590/ 842, training loss 0.30566081404685974\n",
      "Epoch 25 -- Batch 591/ 842, training loss 0.32270997762680054\n",
      "Epoch 25 -- Batch 592/ 842, training loss 0.31809794902801514\n",
      "Epoch 25 -- Batch 593/ 842, training loss 0.3182951807975769\n",
      "Epoch 25 -- Batch 594/ 842, training loss 0.31147149205207825\n",
      "Epoch 25 -- Batch 595/ 842, training loss 0.31986162066459656\n",
      "Epoch 25 -- Batch 596/ 842, training loss 0.31008630990982056\n",
      "Epoch 25 -- Batch 597/ 842, training loss 0.29187992215156555\n",
      "Epoch 25 -- Batch 598/ 842, training loss 0.31584131717681885\n",
      "Epoch 25 -- Batch 599/ 842, training loss 0.3089708089828491\n",
      "Epoch 25 -- Batch 600/ 842, training loss 0.3151495158672333\n",
      "Epoch 25 -- Batch 601/ 842, training loss 0.32075631618499756\n",
      "Epoch 25 -- Batch 602/ 842, training loss 0.30623894929885864\n",
      "Epoch 25 -- Batch 603/ 842, training loss 0.31239983439445496\n",
      "Epoch 25 -- Batch 604/ 842, training loss 0.3198360204696655\n",
      "Epoch 25 -- Batch 605/ 842, training loss 0.30656448006629944\n",
      "Epoch 25 -- Batch 606/ 842, training loss 0.3148590922355652\n",
      "Epoch 25 -- Batch 607/ 842, training loss 0.31214967370033264\n",
      "Epoch 25 -- Batch 608/ 842, training loss 0.32259780168533325\n",
      "Epoch 25 -- Batch 609/ 842, training loss 0.310937762260437\n",
      "Epoch 25 -- Batch 610/ 842, training loss 0.31606972217559814\n",
      "Epoch 25 -- Batch 611/ 842, training loss 0.3245886564254761\n",
      "Epoch 25 -- Batch 612/ 842, training loss 0.32221049070358276\n",
      "Epoch 25 -- Batch 613/ 842, training loss 0.31091368198394775\n",
      "Epoch 25 -- Batch 614/ 842, training loss 0.30999505519866943\n",
      "Epoch 25 -- Batch 615/ 842, training loss 0.32296255230903625\n",
      "Epoch 25 -- Batch 616/ 842, training loss 0.33341729640960693\n",
      "Epoch 25 -- Batch 617/ 842, training loss 0.3281247317790985\n",
      "Epoch 25 -- Batch 618/ 842, training loss 0.31218069791793823\n",
      "Epoch 25 -- Batch 619/ 842, training loss 0.3208926022052765\n",
      "Epoch 25 -- Batch 620/ 842, training loss 0.3231557309627533\n",
      "Epoch 25 -- Batch 621/ 842, training loss 0.32163190841674805\n",
      "Epoch 25 -- Batch 622/ 842, training loss 0.3141086995601654\n",
      "Epoch 25 -- Batch 623/ 842, training loss 0.3223230540752411\n",
      "Epoch 25 -- Batch 624/ 842, training loss 0.31436848640441895\n",
      "Epoch 25 -- Batch 625/ 842, training loss 0.3233867883682251\n",
      "Epoch 25 -- Batch 626/ 842, training loss 0.31529003381729126\n",
      "Epoch 25 -- Batch 627/ 842, training loss 0.31742677092552185\n",
      "Epoch 25 -- Batch 628/ 842, training loss 0.31305670738220215\n",
      "Epoch 25 -- Batch 629/ 842, training loss 0.3112016022205353\n",
      "Epoch 25 -- Batch 630/ 842, training loss 0.32139238715171814\n",
      "Epoch 25 -- Batch 631/ 842, training loss 0.31129032373428345\n",
      "Epoch 25 -- Batch 632/ 842, training loss 0.3219098448753357\n",
      "Epoch 25 -- Batch 633/ 842, training loss 0.31741631031036377\n",
      "Epoch 25 -- Batch 634/ 842, training loss 0.30130240321159363\n",
      "Epoch 25 -- Batch 635/ 842, training loss 0.30897507071495056\n",
      "Epoch 25 -- Batch 636/ 842, training loss 0.31122833490371704\n",
      "Epoch 25 -- Batch 637/ 842, training loss 0.30663371086120605\n",
      "Epoch 25 -- Batch 638/ 842, training loss 0.30823636054992676\n",
      "Epoch 25 -- Batch 639/ 842, training loss 0.32709887623786926\n",
      "Epoch 25 -- Batch 640/ 842, training loss 0.30841130018234253\n",
      "Epoch 25 -- Batch 641/ 842, training loss 0.2937409579753876\n",
      "Epoch 25 -- Batch 642/ 842, training loss 0.32052552700042725\n",
      "Epoch 25 -- Batch 643/ 842, training loss 0.30804312229156494\n",
      "Epoch 25 -- Batch 644/ 842, training loss 0.3299142122268677\n",
      "Epoch 25 -- Batch 645/ 842, training loss 0.324682354927063\n",
      "Epoch 25 -- Batch 646/ 842, training loss 0.31957218050956726\n",
      "Epoch 25 -- Batch 647/ 842, training loss 0.3287484347820282\n",
      "Epoch 25 -- Batch 648/ 842, training loss 0.3072313964366913\n",
      "Epoch 25 -- Batch 649/ 842, training loss 0.3150319755077362\n",
      "Epoch 25 -- Batch 650/ 842, training loss 0.3172208070755005\n",
      "Epoch 25 -- Batch 651/ 842, training loss 0.3268672227859497\n",
      "Epoch 25 -- Batch 652/ 842, training loss 0.3234347105026245\n",
      "Epoch 25 -- Batch 653/ 842, training loss 0.3262162208557129\n",
      "Epoch 25 -- Batch 654/ 842, training loss 0.2968047857284546\n",
      "Epoch 25 -- Batch 655/ 842, training loss 0.30335691571235657\n",
      "Epoch 25 -- Batch 656/ 842, training loss 0.30492860078811646\n",
      "Epoch 25 -- Batch 657/ 842, training loss 0.319869726896286\n",
      "Epoch 25 -- Batch 658/ 842, training loss 0.3183246850967407\n",
      "Epoch 25 -- Batch 659/ 842, training loss 0.32622185349464417\n",
      "Epoch 25 -- Batch 660/ 842, training loss 0.31962069869041443\n",
      "Epoch 25 -- Batch 661/ 842, training loss 0.3222186267375946\n",
      "Epoch 25 -- Batch 662/ 842, training loss 0.3225892186164856\n",
      "Epoch 25 -- Batch 663/ 842, training loss 0.30540692806243896\n",
      "Epoch 25 -- Batch 664/ 842, training loss 0.32792219519615173\n",
      "Epoch 25 -- Batch 665/ 842, training loss 0.30898213386535645\n",
      "Epoch 25 -- Batch 666/ 842, training loss 0.31756800413131714\n",
      "Epoch 25 -- Batch 667/ 842, training loss 0.318372905254364\n",
      "Epoch 25 -- Batch 668/ 842, training loss 0.3154223561286926\n",
      "Epoch 25 -- Batch 669/ 842, training loss 0.31086665391921997\n",
      "Epoch 25 -- Batch 670/ 842, training loss 0.32378876209259033\n",
      "Epoch 25 -- Batch 671/ 842, training loss 0.3101367652416229\n",
      "Epoch 25 -- Batch 672/ 842, training loss 0.3223051130771637\n",
      "Epoch 25 -- Batch 673/ 842, training loss 0.300972044467926\n",
      "Epoch 25 -- Batch 674/ 842, training loss 0.3049167990684509\n",
      "Epoch 25 -- Batch 675/ 842, training loss 0.3060862123966217\n",
      "Epoch 25 -- Batch 676/ 842, training loss 0.3140229880809784\n",
      "Epoch 25 -- Batch 677/ 842, training loss 0.30862510204315186\n",
      "Epoch 25 -- Batch 678/ 842, training loss 0.3149435818195343\n",
      "Epoch 25 -- Batch 679/ 842, training loss 0.31367361545562744\n",
      "Epoch 25 -- Batch 680/ 842, training loss 0.3074086606502533\n",
      "Epoch 25 -- Batch 681/ 842, training loss 0.3127024471759796\n",
      "Epoch 25 -- Batch 682/ 842, training loss 0.316863089799881\n",
      "Epoch 25 -- Batch 683/ 842, training loss 0.3159553110599518\n",
      "Epoch 25 -- Batch 684/ 842, training loss 0.31610625982284546\n",
      "Epoch 25 -- Batch 685/ 842, training loss 0.30710074305534363\n",
      "Epoch 25 -- Batch 686/ 842, training loss 0.32941368222236633\n",
      "Epoch 25 -- Batch 687/ 842, training loss 0.3184158205986023\n",
      "Epoch 25 -- Batch 688/ 842, training loss 0.3136409521102905\n",
      "Epoch 25 -- Batch 689/ 842, training loss 0.31191539764404297\n",
      "Epoch 25 -- Batch 690/ 842, training loss 0.3279447555541992\n",
      "Epoch 25 -- Batch 691/ 842, training loss 0.31817901134490967\n",
      "Epoch 25 -- Batch 692/ 842, training loss 0.3033006489276886\n",
      "Epoch 25 -- Batch 693/ 842, training loss 0.31889984011650085\n",
      "Epoch 25 -- Batch 694/ 842, training loss 0.3301345705986023\n",
      "Epoch 25 -- Batch 695/ 842, training loss 0.3178541660308838\n",
      "Epoch 25 -- Batch 696/ 842, training loss 0.31140273809432983\n",
      "Epoch 25 -- Batch 697/ 842, training loss 0.31342220306396484\n",
      "Epoch 25 -- Batch 698/ 842, training loss 0.3177092969417572\n",
      "Epoch 25 -- Batch 699/ 842, training loss 0.30971986055374146\n",
      "Epoch 25 -- Batch 700/ 842, training loss 0.31716853380203247\n",
      "Epoch 25 -- Batch 701/ 842, training loss 0.3298698961734772\n",
      "Epoch 25 -- Batch 702/ 842, training loss 0.3181549608707428\n",
      "Epoch 25 -- Batch 703/ 842, training loss 0.3002214729785919\n",
      "Epoch 25 -- Batch 704/ 842, training loss 0.3217133581638336\n",
      "Epoch 25 -- Batch 705/ 842, training loss 0.31466466188430786\n",
      "Epoch 25 -- Batch 706/ 842, training loss 0.31986889243125916\n",
      "Epoch 25 -- Batch 707/ 842, training loss 0.3103903830051422\n",
      "Epoch 25 -- Batch 708/ 842, training loss 0.3017996847629547\n",
      "Epoch 25 -- Batch 709/ 842, training loss 0.3100215494632721\n",
      "Epoch 25 -- Batch 710/ 842, training loss 0.31192952394485474\n",
      "Epoch 25 -- Batch 711/ 842, training loss 0.3049146234989166\n",
      "Epoch 25 -- Batch 712/ 842, training loss 0.3211795389652252\n",
      "Epoch 25 -- Batch 713/ 842, training loss 0.3170902729034424\n",
      "Epoch 25 -- Batch 714/ 842, training loss 0.3150525391101837\n",
      "Epoch 25 -- Batch 715/ 842, training loss 0.3208441436290741\n",
      "Epoch 25 -- Batch 716/ 842, training loss 0.31768834590911865\n",
      "Epoch 25 -- Batch 717/ 842, training loss 0.31113889813423157\n",
      "Epoch 25 -- Batch 718/ 842, training loss 0.3201335668563843\n",
      "Epoch 25 -- Batch 719/ 842, training loss 0.32732006907463074\n",
      "Epoch 25 -- Batch 720/ 842, training loss 0.31439828872680664\n",
      "Epoch 25 -- Batch 721/ 842, training loss 0.320500910282135\n",
      "Epoch 25 -- Batch 722/ 842, training loss 0.30220648646354675\n",
      "Epoch 25 -- Batch 723/ 842, training loss 0.317941278219223\n",
      "Epoch 25 -- Batch 724/ 842, training loss 0.3221510052680969\n",
      "Epoch 25 -- Batch 725/ 842, training loss 0.3183446228504181\n",
      "Epoch 25 -- Batch 726/ 842, training loss 0.3024202883243561\n",
      "Epoch 25 -- Batch 727/ 842, training loss 0.3213018476963043\n",
      "Epoch 25 -- Batch 728/ 842, training loss 0.31551986932754517\n",
      "Epoch 25 -- Batch 729/ 842, training loss 0.3186001479625702\n",
      "Epoch 25 -- Batch 730/ 842, training loss 0.317318856716156\n",
      "Epoch 25 -- Batch 731/ 842, training loss 0.312840074300766\n",
      "Epoch 25 -- Batch 732/ 842, training loss 0.31470727920532227\n",
      "Epoch 25 -- Batch 733/ 842, training loss 0.31439077854156494\n",
      "Epoch 25 -- Batch 734/ 842, training loss 0.3098529577255249\n",
      "Epoch 25 -- Batch 735/ 842, training loss 0.30676862597465515\n",
      "Epoch 25 -- Batch 736/ 842, training loss 0.3176494538784027\n",
      "Epoch 25 -- Batch 737/ 842, training loss 0.30272936820983887\n",
      "Epoch 25 -- Batch 738/ 842, training loss 0.31506529450416565\n",
      "Epoch 25 -- Batch 739/ 842, training loss 0.3104550540447235\n",
      "Epoch 25 -- Batch 740/ 842, training loss 0.3207932710647583\n",
      "Epoch 25 -- Batch 741/ 842, training loss 0.3237190544605255\n",
      "Epoch 25 -- Batch 742/ 842, training loss 0.322838693857193\n",
      "Epoch 25 -- Batch 743/ 842, training loss 0.29550719261169434\n",
      "Epoch 25 -- Batch 744/ 842, training loss 0.31181395053863525\n",
      "Epoch 25 -- Batch 745/ 842, training loss 0.3380117416381836\n",
      "Epoch 25 -- Batch 746/ 842, training loss 0.31943556666374207\n",
      "Epoch 25 -- Batch 747/ 842, training loss 0.3203093409538269\n",
      "Epoch 25 -- Batch 748/ 842, training loss 0.31196993589401245\n",
      "Epoch 25 -- Batch 749/ 842, training loss 0.3180827796459198\n",
      "Epoch 25 -- Batch 750/ 842, training loss 0.32007384300231934\n",
      "Epoch 25 -- Batch 751/ 842, training loss 0.3211243152618408\n",
      "Epoch 25 -- Batch 752/ 842, training loss 0.31020092964172363\n",
      "Epoch 25 -- Batch 753/ 842, training loss 0.32502156496047974\n",
      "Epoch 25 -- Batch 754/ 842, training loss 0.325213760137558\n",
      "Epoch 25 -- Batch 755/ 842, training loss 0.3162647485733032\n",
      "Epoch 25 -- Batch 756/ 842, training loss 0.31924769282341003\n",
      "Epoch 25 -- Batch 757/ 842, training loss 0.30726251006126404\n",
      "Epoch 25 -- Batch 758/ 842, training loss 0.3028814196586609\n",
      "Epoch 25 -- Batch 759/ 842, training loss 0.3083595931529999\n",
      "Epoch 25 -- Batch 760/ 842, training loss 0.30490270256996155\n",
      "Epoch 25 -- Batch 761/ 842, training loss 0.3132007122039795\n",
      "Epoch 25 -- Batch 762/ 842, training loss 0.305392324924469\n",
      "Epoch 25 -- Batch 763/ 842, training loss 0.3066772222518921\n",
      "Epoch 25 -- Batch 764/ 842, training loss 0.3209228813648224\n",
      "Epoch 25 -- Batch 765/ 842, training loss 0.3157373070716858\n",
      "Epoch 25 -- Batch 766/ 842, training loss 0.31221044063568115\n",
      "Epoch 25 -- Batch 767/ 842, training loss 0.31913408637046814\n",
      "Epoch 25 -- Batch 768/ 842, training loss 0.30860140919685364\n",
      "Epoch 25 -- Batch 769/ 842, training loss 0.3129359781742096\n",
      "Epoch 25 -- Batch 770/ 842, training loss 0.30818504095077515\n",
      "Epoch 25 -- Batch 771/ 842, training loss 0.3212091028690338\n",
      "Epoch 25 -- Batch 772/ 842, training loss 0.304553359746933\n",
      "Epoch 25 -- Batch 773/ 842, training loss 0.3206462264060974\n",
      "Epoch 25 -- Batch 774/ 842, training loss 0.31609681248664856\n",
      "Epoch 25 -- Batch 775/ 842, training loss 0.30779197812080383\n",
      "Epoch 25 -- Batch 776/ 842, training loss 0.3163207769393921\n",
      "Epoch 25 -- Batch 777/ 842, training loss 0.30541878938674927\n",
      "Epoch 25 -- Batch 778/ 842, training loss 0.311949759721756\n",
      "Epoch 25 -- Batch 779/ 842, training loss 0.3279375433921814\n",
      "Epoch 25 -- Batch 780/ 842, training loss 0.30458346009254456\n",
      "Epoch 25 -- Batch 781/ 842, training loss 0.31886881589889526\n",
      "Epoch 25 -- Batch 782/ 842, training loss 0.30579227209091187\n",
      "Epoch 25 -- Batch 783/ 842, training loss 0.30775755643844604\n",
      "Epoch 25 -- Batch 784/ 842, training loss 0.3037237823009491\n",
      "Epoch 25 -- Batch 785/ 842, training loss 0.33222827315330505\n",
      "Epoch 25 -- Batch 786/ 842, training loss 0.3081313371658325\n",
      "Epoch 25 -- Batch 787/ 842, training loss 0.32065629959106445\n",
      "Epoch 25 -- Batch 788/ 842, training loss 0.3114430904388428\n",
      "Epoch 25 -- Batch 789/ 842, training loss 0.30647140741348267\n",
      "Epoch 25 -- Batch 790/ 842, training loss 0.30210810899734497\n",
      "Epoch 25 -- Batch 791/ 842, training loss 0.31497660279273987\n",
      "Epoch 25 -- Batch 792/ 842, training loss 0.32471397519111633\n",
      "Epoch 25 -- Batch 793/ 842, training loss 0.31141015887260437\n",
      "Epoch 25 -- Batch 794/ 842, training loss 0.32041293382644653\n",
      "Epoch 25 -- Batch 795/ 842, training loss 0.3165247142314911\n",
      "Epoch 25 -- Batch 796/ 842, training loss 0.30889973044395447\n",
      "Epoch 25 -- Batch 797/ 842, training loss 0.3167206048965454\n",
      "Epoch 25 -- Batch 798/ 842, training loss 0.32483211159706116\n",
      "Epoch 25 -- Batch 799/ 842, training loss 0.32251808047294617\n",
      "Epoch 25 -- Batch 800/ 842, training loss 0.3227877616882324\n",
      "Epoch 25 -- Batch 801/ 842, training loss 0.31767523288726807\n",
      "Epoch 25 -- Batch 802/ 842, training loss 0.3218349814414978\n",
      "Epoch 25 -- Batch 803/ 842, training loss 0.3180498778820038\n",
      "Epoch 25 -- Batch 804/ 842, training loss 0.307874470949173\n",
      "Epoch 25 -- Batch 805/ 842, training loss 0.319811075925827\n",
      "Epoch 25 -- Batch 806/ 842, training loss 0.3279160261154175\n",
      "Epoch 25 -- Batch 807/ 842, training loss 0.3200128376483917\n",
      "Epoch 25 -- Batch 808/ 842, training loss 0.3178422451019287\n",
      "Epoch 25 -- Batch 809/ 842, training loss 0.3229242265224457\n",
      "Epoch 25 -- Batch 810/ 842, training loss 0.32413214445114136\n",
      "Epoch 25 -- Batch 811/ 842, training loss 0.3183155953884125\n",
      "Epoch 25 -- Batch 812/ 842, training loss 0.3184380531311035\n",
      "Epoch 25 -- Batch 813/ 842, training loss 0.3131468892097473\n",
      "Epoch 25 -- Batch 814/ 842, training loss 0.30783897638320923\n",
      "Epoch 25 -- Batch 815/ 842, training loss 0.31957492232322693\n",
      "Epoch 25 -- Batch 816/ 842, training loss 0.3145785629749298\n",
      "Epoch 25 -- Batch 817/ 842, training loss 0.3006640076637268\n",
      "Epoch 25 -- Batch 818/ 842, training loss 0.3099815249443054\n",
      "Epoch 25 -- Batch 819/ 842, training loss 0.3250854015350342\n",
      "Epoch 25 -- Batch 820/ 842, training loss 0.3089059591293335\n",
      "Epoch 25 -- Batch 821/ 842, training loss 0.31158187985420227\n",
      "Epoch 25 -- Batch 822/ 842, training loss 0.3138369917869568\n",
      "Epoch 25 -- Batch 823/ 842, training loss 0.30967628955841064\n",
      "Epoch 25 -- Batch 824/ 842, training loss 0.30888864398002625\n",
      "Epoch 25 -- Batch 825/ 842, training loss 0.31273984909057617\n",
      "Epoch 25 -- Batch 826/ 842, training loss 0.3179484009742737\n",
      "Epoch 25 -- Batch 827/ 842, training loss 0.32353881001472473\n",
      "Epoch 25 -- Batch 828/ 842, training loss 0.31541427969932556\n",
      "Epoch 25 -- Batch 829/ 842, training loss 0.3156183063983917\n",
      "Epoch 25 -- Batch 830/ 842, training loss 0.3178366720676422\n",
      "Epoch 25 -- Batch 831/ 842, training loss 0.31278374791145325\n",
      "Epoch 25 -- Batch 832/ 842, training loss 0.3224528431892395\n",
      "Epoch 25 -- Batch 833/ 842, training loss 0.3161374628543854\n",
      "Epoch 25 -- Batch 834/ 842, training loss 0.31473350524902344\n",
      "Epoch 25 -- Batch 835/ 842, training loss 0.31450918316841125\n",
      "Epoch 25 -- Batch 836/ 842, training loss 0.3120414614677429\n",
      "Epoch 25 -- Batch 837/ 842, training loss 0.30498698353767395\n",
      "Epoch 25 -- Batch 838/ 842, training loss 0.32339537143707275\n",
      "Epoch 25 -- Batch 839/ 842, training loss 0.31731393933296204\n",
      "Epoch 25 -- Batch 840/ 842, training loss 0.31189635396003723\n",
      "Epoch 25 -- Batch 841/ 842, training loss 0.3162468373775482\n",
      "Epoch 25 -- Batch 842/ 842, training loss 0.3086443543434143\n",
      "----------------------------------------------------------------------\n",
      "Epoch 25 -- Batch 1/ 94, validation loss 0.302260160446167\n",
      "Epoch 25 -- Batch 2/ 94, validation loss 0.2908650040626526\n",
      "Epoch 25 -- Batch 3/ 94, validation loss 0.30282288789749146\n",
      "Epoch 25 -- Batch 4/ 94, validation loss 0.3131544589996338\n",
      "Epoch 25 -- Batch 5/ 94, validation loss 0.3079341650009155\n",
      "Epoch 25 -- Batch 6/ 94, validation loss 0.30552396178245544\n",
      "Epoch 25 -- Batch 7/ 94, validation loss 0.3089613616466522\n",
      "Epoch 25 -- Batch 8/ 94, validation loss 0.3112654983997345\n",
      "Epoch 25 -- Batch 9/ 94, validation loss 0.29657673835754395\n",
      "Epoch 25 -- Batch 10/ 94, validation loss 0.3157028555870056\n",
      "Epoch 25 -- Batch 11/ 94, validation loss 0.30237168073654175\n",
      "Epoch 25 -- Batch 12/ 94, validation loss 0.3118544816970825\n",
      "Epoch 25 -- Batch 13/ 94, validation loss 0.2996244728565216\n",
      "Epoch 25 -- Batch 14/ 94, validation loss 0.31386956572532654\n",
      "Epoch 25 -- Batch 15/ 94, validation loss 0.30665135383605957\n",
      "Epoch 25 -- Batch 16/ 94, validation loss 0.29333868622779846\n",
      "Epoch 25 -- Batch 17/ 94, validation loss 0.2954258322715759\n",
      "Epoch 25 -- Batch 18/ 94, validation loss 0.30774596333503723\n",
      "Epoch 25 -- Batch 19/ 94, validation loss 0.29115918278694153\n",
      "Epoch 25 -- Batch 20/ 94, validation loss 0.3063679337501526\n",
      "Epoch 25 -- Batch 21/ 94, validation loss 0.30121809244155884\n",
      "Epoch 25 -- Batch 22/ 94, validation loss 0.3314841091632843\n",
      "Epoch 25 -- Batch 23/ 94, validation loss 0.30349621176719666\n",
      "Epoch 25 -- Batch 24/ 94, validation loss 0.2965332865715027\n",
      "Epoch 25 -- Batch 25/ 94, validation loss 0.31683266162872314\n",
      "Epoch 25 -- Batch 26/ 94, validation loss 0.2998374402523041\n",
      "Epoch 25 -- Batch 27/ 94, validation loss 0.31429240107536316\n",
      "Epoch 25 -- Batch 28/ 94, validation loss 0.30714115500450134\n",
      "Epoch 25 -- Batch 29/ 94, validation loss 0.3038991093635559\n",
      "Epoch 25 -- Batch 30/ 94, validation loss 0.3036159574985504\n",
      "Epoch 25 -- Batch 31/ 94, validation loss 0.3033861815929413\n",
      "Epoch 25 -- Batch 32/ 94, validation loss 0.30442678928375244\n",
      "Epoch 25 -- Batch 33/ 94, validation loss 0.29801875352859497\n",
      "Epoch 25 -- Batch 34/ 94, validation loss 0.29406416416168213\n",
      "Epoch 25 -- Batch 35/ 94, validation loss 0.3079514801502228\n",
      "Epoch 25 -- Batch 36/ 94, validation loss 0.30229440331459045\n",
      "Epoch 25 -- Batch 37/ 94, validation loss 0.2971448600292206\n",
      "Epoch 25 -- Batch 38/ 94, validation loss 0.30128493905067444\n",
      "Epoch 25 -- Batch 39/ 94, validation loss 0.31026989221572876\n",
      "Epoch 25 -- Batch 40/ 94, validation loss 0.2996279001235962\n",
      "Epoch 25 -- Batch 41/ 94, validation loss 0.29895052313804626\n",
      "Epoch 25 -- Batch 42/ 94, validation loss 0.3091771900653839\n",
      "Epoch 25 -- Batch 43/ 94, validation loss 0.29948490858078003\n",
      "Epoch 25 -- Batch 44/ 94, validation loss 0.2967018485069275\n",
      "Epoch 25 -- Batch 45/ 94, validation loss 0.31150439381599426\n",
      "Epoch 25 -- Batch 46/ 94, validation loss 0.30824828147888184\n",
      "Epoch 25 -- Batch 47/ 94, validation loss 0.2986951172351837\n",
      "Epoch 25 -- Batch 48/ 94, validation loss 0.31406712532043457\n",
      "Epoch 25 -- Batch 49/ 94, validation loss 0.31174153089523315\n",
      "Epoch 25 -- Batch 50/ 94, validation loss 0.29646965861320496\n",
      "Epoch 25 -- Batch 51/ 94, validation loss 0.31321457028388977\n",
      "Epoch 25 -- Batch 52/ 94, validation loss 0.2907174229621887\n",
      "Epoch 25 -- Batch 53/ 94, validation loss 0.2987043261528015\n",
      "Epoch 25 -- Batch 54/ 94, validation loss 0.3057331442832947\n",
      "Epoch 25 -- Batch 55/ 94, validation loss 0.33307304978370667\n",
      "Epoch 25 -- Batch 56/ 94, validation loss 0.2976033389568329\n",
      "Epoch 25 -- Batch 57/ 94, validation loss 0.29491639137268066\n",
      "Epoch 25 -- Batch 58/ 94, validation loss 0.31487900018692017\n",
      "Epoch 25 -- Batch 59/ 94, validation loss 0.3113267719745636\n",
      "Epoch 25 -- Batch 60/ 94, validation loss 0.33497029542922974\n",
      "Epoch 25 -- Batch 61/ 94, validation loss 0.30450439453125\n",
      "Epoch 25 -- Batch 62/ 94, validation loss 0.29390329122543335\n",
      "Epoch 25 -- Batch 63/ 94, validation loss 0.29059186577796936\n",
      "Epoch 25 -- Batch 64/ 94, validation loss 0.2985405921936035\n",
      "Epoch 25 -- Batch 65/ 94, validation loss 0.3096235394477844\n",
      "Epoch 25 -- Batch 66/ 94, validation loss 0.29891476035118103\n",
      "Epoch 25 -- Batch 67/ 94, validation loss 0.31589406728744507\n",
      "Epoch 25 -- Batch 68/ 94, validation loss 0.3086215853691101\n",
      "Epoch 25 -- Batch 69/ 94, validation loss 0.29983416199684143\n",
      "Epoch 25 -- Batch 70/ 94, validation loss 0.3042773902416229\n",
      "Epoch 25 -- Batch 71/ 94, validation loss 0.297336220741272\n",
      "Epoch 25 -- Batch 72/ 94, validation loss 0.3029266893863678\n",
      "Epoch 25 -- Batch 73/ 94, validation loss 0.31502312421798706\n",
      "Epoch 25 -- Batch 74/ 94, validation loss 0.30639955401420593\n",
      "Epoch 25 -- Batch 75/ 94, validation loss 0.30774474143981934\n",
      "Epoch 25 -- Batch 76/ 94, validation loss 0.2995508909225464\n",
      "Epoch 25 -- Batch 77/ 94, validation loss 0.2941506803035736\n",
      "Epoch 25 -- Batch 78/ 94, validation loss 0.3081803023815155\n",
      "Epoch 25 -- Batch 79/ 94, validation loss 0.2987597584724426\n",
      "Epoch 25 -- Batch 80/ 94, validation loss 0.31425243616104126\n",
      "Epoch 25 -- Batch 81/ 94, validation loss 0.30913209915161133\n",
      "Epoch 25 -- Batch 82/ 94, validation loss 0.30752837657928467\n",
      "Epoch 25 -- Batch 83/ 94, validation loss 0.30332228541374207\n",
      "Epoch 25 -- Batch 84/ 94, validation loss 0.29684609174728394\n",
      "Epoch 25 -- Batch 85/ 94, validation loss 0.3082685172557831\n",
      "Epoch 25 -- Batch 86/ 94, validation loss 0.29253631830215454\n",
      "Epoch 25 -- Batch 87/ 94, validation loss 0.30571457743644714\n",
      "Epoch 25 -- Batch 88/ 94, validation loss 0.31377720832824707\n",
      "Epoch 25 -- Batch 89/ 94, validation loss 0.3118817210197449\n",
      "Epoch 25 -- Batch 90/ 94, validation loss 0.2944199740886688\n",
      "Epoch 25 -- Batch 91/ 94, validation loss 0.2946111261844635\n",
      "Epoch 25 -- Batch 92/ 94, validation loss 0.3162270486354828\n",
      "Epoch 25 -- Batch 93/ 94, validation loss 0.3095606565475464\n",
      "Epoch 25 -- Batch 94/ 94, validation loss 0.2905825674533844\n",
      "----------------------------------------------------------------------\n",
      "Epoch 25 loss: Training 0.3134559988975525, Validation 0.2905825674533844\n",
      "----------------------------------------------------------------------\n",
      "Epoch 26/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:08:02] Explicit valence for atom # 4 O, 3, is greater than permitted\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 4 5 6 20 21 22 23\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 9 10\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 2 3 10\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 2 3 5 7 8 9 10 14 15 16 17 18 20\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 3 4 15\n",
      "[19:08:02] SMILES Parse Error: extra open parentheses for input: 'N#Cc1c(NC(=O)CN(CCCN2CCOCC2)sc2ccccc2n1'\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 9 10 21\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(OCC2CO2)c1-c1ccc(-c2ccccc2OC3CCCCC2)cc1'\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 1 3\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(F)c(NC(=O)C2C3C=CC4(O3)C2C(=O)N(C2CCC(C)CC2)C4C4C(=O)NC2=O)c1'\n",
      "[19:08:02] SMILES Parse Error: extra close parentheses while parsing: N=C(N)N=N/C=C/C=C/c1ccccc1)c1ccccc1\n",
      "[19:08:02] SMILES Parse Error: Failed parsing SMILES 'N=C(N)N=N/C=C/C=C/c1ccccc1)c1ccccc1' for input: 'N=C(N)N=N/C=C/C=C/c1ccccc1)c1ccccc1'\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C(=O)N(CCN(C)C)[C@@H]2C[C@@H]3C[C@H](c4cccc(OC)c4)S4C(=O)N3C2)c1'\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 1 2 13 18 26\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 6 8 9 10 11 12 13 14 15\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 5 6 7 16 17\n",
      "[19:08:02] SMILES Parse Error: syntax error while parsing: Cc1ccc(-n2nc3c(c2NC(=O)Cc2ccc(F)cc2)CS(=O)=)C3)cc1\n",
      "[19:08:02] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(-n2nc3c(c2NC(=O)Cc2ccc(F)cc2)CS(=O)=)C3)cc1' for input: 'Cc1ccc(-n2nc3c(c2NC(=O)Cc2ccc(F)cc2)CS(=O)=)C3)cc1'\n",
      "[19:08:02] non-ring atom 15 marked aromatic\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 20 21 22\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 1 2 5 6 7 8 9 10 11 12 28\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'CCSc1nnc(NC(=O)CSc2nnc(NC(=O)CC(C)C)s1)s1'\n",
      "[19:08:02] Conflicting single bond directions around double bond at index 1.\n",
      "[19:08:02]   BondStereo set to STEREONONE and single bond directions set to NONE.\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 23 25\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 1 2 21 22 24 25 26\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 4 27 28 29 30 31 32\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'CCOCCCn1c(SCC(=O)Nc2oc(C)c3c(C)n[nH]c(=O)c22)nnc1-c1ccncc1'\n",
      "[19:08:02] Explicit valence for atom # 16 O, 3, is greater than permitted\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'CC1CCCCC1NC(=O)C1CCN(S(=O)(=O)c2cccc3[nH]ccc2=O)CC1'\n",
      "[19:08:02] SMILES Parse Error: syntax error while parsing: =CCN1CCN(c2cc(Cl)c([N+](=O)[O-])cc2F)CC1C(=O)N1CCCC1\n",
      "[19:08:02] SMILES Parse Error: Failed parsing SMILES '=CCN1CCN(c2cc(Cl)c([N+](=O)[O-])cc2F)CC1C(=O)N1CCCC1' for input: '=CCN1CCN(c2cc(Cl)c([N+](=O)[O-])cc2F)CC1C(=O)N1CCCC1'\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 12 13 14\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 5\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 9 10 11 21 25\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 22 23 24 25 34\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'CC(C)C[C@@H](NC(=O)[C@@H]1CCCN1C(=O)CC1C(=O)NCCC1=O)N2C(=O)c1ccccc1'\n",
      "[19:08:02] Explicit valence for atom # 4 S, 7, is greater than permitted\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 18\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 1 2 3 10 12 19 20\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'CC(O)(CCl)C1CC1C(=O)OC1(Cl)Cl'\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 15 16 19\n",
      "[19:08:02] SMILES Parse Error: unclosed ring for input: 'CCOc1cc(C)nc(SCc2c(Cl)ccc3c2OC(C)(C)CCl)n1'\n",
      "[19:08:02] Explicit valence for atom # 3 C, 5, is greater than permitted\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 2 3 4 20 21\n",
      "[19:08:02] Can't kekulize mol.  Unkekulized atoms: 3 4 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 -- Batch 1/ 842, training loss 0.30417197942733765\n",
      "Epoch 26 -- Batch 2/ 842, training loss 0.3142158091068268\n",
      "Epoch 26 -- Batch 3/ 842, training loss 0.31422415375709534\n",
      "Epoch 26 -- Batch 4/ 842, training loss 0.30467814207077026\n",
      "Epoch 26 -- Batch 5/ 842, training loss 0.31406477093696594\n",
      "Epoch 26 -- Batch 6/ 842, training loss 0.3197287321090698\n",
      "Epoch 26 -- Batch 7/ 842, training loss 0.29786986112594604\n",
      "Epoch 26 -- Batch 8/ 842, training loss 0.3080611228942871\n",
      "Epoch 26 -- Batch 9/ 842, training loss 0.30594146251678467\n",
      "Epoch 26 -- Batch 10/ 842, training loss 0.3132709264755249\n",
      "Epoch 26 -- Batch 11/ 842, training loss 0.3219780921936035\n",
      "Epoch 26 -- Batch 12/ 842, training loss 0.3123544156551361\n",
      "Epoch 26 -- Batch 13/ 842, training loss 0.30510562658309937\n",
      "Epoch 26 -- Batch 14/ 842, training loss 0.3211993873119354\n",
      "Epoch 26 -- Batch 15/ 842, training loss 0.30149808526039124\n",
      "Epoch 26 -- Batch 16/ 842, training loss 0.31006136536598206\n",
      "Epoch 26 -- Batch 17/ 842, training loss 0.31367310881614685\n",
      "Epoch 26 -- Batch 18/ 842, training loss 0.31812024116516113\n",
      "Epoch 26 -- Batch 19/ 842, training loss 0.3089722990989685\n",
      "Epoch 26 -- Batch 20/ 842, training loss 0.30368828773498535\n",
      "Epoch 26 -- Batch 21/ 842, training loss 0.3027144968509674\n",
      "Epoch 26 -- Batch 22/ 842, training loss 0.30722948908805847\n",
      "Epoch 26 -- Batch 23/ 842, training loss 0.3139132261276245\n",
      "Epoch 26 -- Batch 24/ 842, training loss 0.3054971694946289\n",
      "Epoch 26 -- Batch 25/ 842, training loss 0.30953356623649597\n",
      "Epoch 26 -- Batch 26/ 842, training loss 0.30921903252601624\n",
      "Epoch 26 -- Batch 27/ 842, training loss 0.30476847290992737\n",
      "Epoch 26 -- Batch 28/ 842, training loss 0.31288954615592957\n",
      "Epoch 26 -- Batch 29/ 842, training loss 0.3220064640045166\n",
      "Epoch 26 -- Batch 30/ 842, training loss 0.30540546774864197\n",
      "Epoch 26 -- Batch 31/ 842, training loss 0.3098534047603607\n",
      "Epoch 26 -- Batch 32/ 842, training loss 0.31306642293930054\n",
      "Epoch 26 -- Batch 33/ 842, training loss 0.3094152510166168\n",
      "Epoch 26 -- Batch 34/ 842, training loss 0.3065570890903473\n",
      "Epoch 26 -- Batch 35/ 842, training loss 0.30468663573265076\n",
      "Epoch 26 -- Batch 36/ 842, training loss 0.2985183000564575\n",
      "Epoch 26 -- Batch 37/ 842, training loss 0.309847891330719\n",
      "Epoch 26 -- Batch 38/ 842, training loss 0.3182452321052551\n",
      "Epoch 26 -- Batch 39/ 842, training loss 0.3120470643043518\n",
      "Epoch 26 -- Batch 40/ 842, training loss 0.2957494258880615\n",
      "Epoch 26 -- Batch 41/ 842, training loss 0.3067445755004883\n",
      "Epoch 26 -- Batch 42/ 842, training loss 0.3116409182548523\n",
      "Epoch 26 -- Batch 43/ 842, training loss 0.30386683344841003\n",
      "Epoch 26 -- Batch 44/ 842, training loss 0.31006965041160583\n",
      "Epoch 26 -- Batch 45/ 842, training loss 0.3070980906486511\n",
      "Epoch 26 -- Batch 46/ 842, training loss 0.2986189126968384\n",
      "Epoch 26 -- Batch 47/ 842, training loss 0.30728989839553833\n",
      "Epoch 26 -- Batch 48/ 842, training loss 0.2969430387020111\n",
      "Epoch 26 -- Batch 49/ 842, training loss 0.3038104772567749\n",
      "Epoch 26 -- Batch 50/ 842, training loss 0.2982865273952484\n",
      "Epoch 26 -- Batch 51/ 842, training loss 0.3101942539215088\n",
      "Epoch 26 -- Batch 52/ 842, training loss 0.3005136549472809\n",
      "Epoch 26 -- Batch 53/ 842, training loss 0.30787089467048645\n",
      "Epoch 26 -- Batch 54/ 842, training loss 0.3184765875339508\n",
      "Epoch 26 -- Batch 55/ 842, training loss 0.30305981636047363\n",
      "Epoch 26 -- Batch 56/ 842, training loss 0.3072111904621124\n",
      "Epoch 26 -- Batch 57/ 842, training loss 0.31214219331741333\n",
      "Epoch 26 -- Batch 58/ 842, training loss 0.2932959496974945\n",
      "Epoch 26 -- Batch 59/ 842, training loss 0.31660717725753784\n",
      "Epoch 26 -- Batch 60/ 842, training loss 0.31352877616882324\n",
      "Epoch 26 -- Batch 61/ 842, training loss 0.3137906789779663\n",
      "Epoch 26 -- Batch 62/ 842, training loss 0.3089602589607239\n",
      "Epoch 26 -- Batch 63/ 842, training loss 0.3040443956851959\n",
      "Epoch 26 -- Batch 64/ 842, training loss 0.3089631497859955\n",
      "Epoch 26 -- Batch 65/ 842, training loss 0.31105268001556396\n",
      "Epoch 26 -- Batch 66/ 842, training loss 0.3043390214443207\n",
      "Epoch 26 -- Batch 67/ 842, training loss 0.3150596022605896\n",
      "Epoch 26 -- Batch 68/ 842, training loss 0.30009904503822327\n",
      "Epoch 26 -- Batch 69/ 842, training loss 0.3080401122570038\n",
      "Epoch 26 -- Batch 70/ 842, training loss 0.3123692274093628\n",
      "Epoch 26 -- Batch 71/ 842, training loss 0.3042701184749603\n",
      "Epoch 26 -- Batch 72/ 842, training loss 0.30836477875709534\n",
      "Epoch 26 -- Batch 73/ 842, training loss 0.3167453110218048\n",
      "Epoch 26 -- Batch 74/ 842, training loss 0.3182862102985382\n",
      "Epoch 26 -- Batch 75/ 842, training loss 0.3059343993663788\n",
      "Epoch 26 -- Batch 76/ 842, training loss 0.3101974129676819\n",
      "Epoch 26 -- Batch 77/ 842, training loss 0.31417837738990784\n",
      "Epoch 26 -- Batch 78/ 842, training loss 0.3066713213920593\n",
      "Epoch 26 -- Batch 79/ 842, training loss 0.3073537349700928\n",
      "Epoch 26 -- Batch 80/ 842, training loss 0.30848994851112366\n",
      "Epoch 26 -- Batch 81/ 842, training loss 0.3073641061782837\n",
      "Epoch 26 -- Batch 82/ 842, training loss 0.30706459283828735\n",
      "Epoch 26 -- Batch 83/ 842, training loss 0.31471049785614014\n",
      "Epoch 26 -- Batch 84/ 842, training loss 0.32346421480178833\n",
      "Epoch 26 -- Batch 85/ 842, training loss 0.30510658025741577\n",
      "Epoch 26 -- Batch 86/ 842, training loss 0.3114151358604431\n",
      "Epoch 26 -- Batch 87/ 842, training loss 0.30742835998535156\n",
      "Epoch 26 -- Batch 88/ 842, training loss 0.30634137988090515\n",
      "Epoch 26 -- Batch 89/ 842, training loss 0.3029271960258484\n",
      "Epoch 26 -- Batch 90/ 842, training loss 0.3056381940841675\n",
      "Epoch 26 -- Batch 91/ 842, training loss 0.3083555996417999\n",
      "Epoch 26 -- Batch 92/ 842, training loss 0.3173362910747528\n",
      "Epoch 26 -- Batch 93/ 842, training loss 0.3157880902290344\n",
      "Epoch 26 -- Batch 94/ 842, training loss 0.30579283833503723\n",
      "Epoch 26 -- Batch 95/ 842, training loss 0.32738038897514343\n",
      "Epoch 26 -- Batch 96/ 842, training loss 0.3037371039390564\n",
      "Epoch 26 -- Batch 97/ 842, training loss 0.31059250235557556\n",
      "Epoch 26 -- Batch 98/ 842, training loss 0.309140682220459\n",
      "Epoch 26 -- Batch 99/ 842, training loss 0.3186366856098175\n",
      "Epoch 26 -- Batch 100/ 842, training loss 0.310868501663208\n",
      "Epoch 26 -- Batch 101/ 842, training loss 0.3038751482963562\n",
      "Epoch 26 -- Batch 102/ 842, training loss 0.302936315536499\n",
      "Epoch 26 -- Batch 103/ 842, training loss 0.30396223068237305\n",
      "Epoch 26 -- Batch 104/ 842, training loss 0.2989293932914734\n",
      "Epoch 26 -- Batch 105/ 842, training loss 0.30320775508880615\n",
      "Epoch 26 -- Batch 106/ 842, training loss 0.30332469940185547\n",
      "Epoch 26 -- Batch 107/ 842, training loss 0.30296942591667175\n",
      "Epoch 26 -- Batch 108/ 842, training loss 0.32030585408210754\n",
      "Epoch 26 -- Batch 109/ 842, training loss 0.30638477206230164\n",
      "Epoch 26 -- Batch 110/ 842, training loss 0.30712753534317017\n",
      "Epoch 26 -- Batch 111/ 842, training loss 0.31482189893722534\n",
      "Epoch 26 -- Batch 112/ 842, training loss 0.3172215223312378\n",
      "Epoch 26 -- Batch 113/ 842, training loss 0.30538150668144226\n",
      "Epoch 26 -- Batch 114/ 842, training loss 0.30803942680358887\n",
      "Epoch 26 -- Batch 115/ 842, training loss 0.303096204996109\n",
      "Epoch 26 -- Batch 116/ 842, training loss 0.3125423491001129\n",
      "Epoch 26 -- Batch 117/ 842, training loss 0.31718072295188904\n",
      "Epoch 26 -- Batch 118/ 842, training loss 0.31361839175224304\n",
      "Epoch 26 -- Batch 119/ 842, training loss 0.30727583169937134\n",
      "Epoch 26 -- Batch 120/ 842, training loss 0.30236706137657166\n",
      "Epoch 26 -- Batch 121/ 842, training loss 0.2995772957801819\n",
      "Epoch 26 -- Batch 122/ 842, training loss 0.30897870659828186\n",
      "Epoch 26 -- Batch 123/ 842, training loss 0.3008511960506439\n",
      "Epoch 26 -- Batch 124/ 842, training loss 0.3063407838344574\n",
      "Epoch 26 -- Batch 125/ 842, training loss 0.3044147193431854\n",
      "Epoch 26 -- Batch 126/ 842, training loss 0.31093665957450867\n",
      "Epoch 26 -- Batch 127/ 842, training loss 0.29780885577201843\n",
      "Epoch 26 -- Batch 128/ 842, training loss 0.3123754560947418\n",
      "Epoch 26 -- Batch 129/ 842, training loss 0.2978718876838684\n",
      "Epoch 26 -- Batch 130/ 842, training loss 0.2964659333229065\n",
      "Epoch 26 -- Batch 131/ 842, training loss 0.31615620851516724\n",
      "Epoch 26 -- Batch 132/ 842, training loss 0.3198681175708771\n",
      "Epoch 26 -- Batch 133/ 842, training loss 0.3019043803215027\n",
      "Epoch 26 -- Batch 134/ 842, training loss 0.3046363890171051\n",
      "Epoch 26 -- Batch 135/ 842, training loss 0.29995793104171753\n",
      "Epoch 26 -- Batch 136/ 842, training loss 0.3067951500415802\n",
      "Epoch 26 -- Batch 137/ 842, training loss 0.307282954454422\n",
      "Epoch 26 -- Batch 138/ 842, training loss 0.3119790554046631\n",
      "Epoch 26 -- Batch 139/ 842, training loss 0.31902244687080383\n",
      "Epoch 26 -- Batch 140/ 842, training loss 0.3122841417789459\n",
      "Epoch 26 -- Batch 141/ 842, training loss 0.3189478814601898\n",
      "Epoch 26 -- Batch 142/ 842, training loss 0.3123553395271301\n",
      "Epoch 26 -- Batch 143/ 842, training loss 0.30910128355026245\n",
      "Epoch 26 -- Batch 144/ 842, training loss 0.31546512246131897\n",
      "Epoch 26 -- Batch 145/ 842, training loss 0.2974405586719513\n",
      "Epoch 26 -- Batch 146/ 842, training loss 0.3146471381187439\n",
      "Epoch 26 -- Batch 147/ 842, training loss 0.31053420901298523\n",
      "Epoch 26 -- Batch 148/ 842, training loss 0.3171059489250183\n",
      "Epoch 26 -- Batch 149/ 842, training loss 0.3028760850429535\n",
      "Epoch 26 -- Batch 150/ 842, training loss 0.3112890124320984\n",
      "Epoch 26 -- Batch 151/ 842, training loss 0.30441418290138245\n",
      "Epoch 26 -- Batch 152/ 842, training loss 0.31097131967544556\n",
      "Epoch 26 -- Batch 153/ 842, training loss 0.3145322799682617\n",
      "Epoch 26 -- Batch 154/ 842, training loss 0.3032788038253784\n",
      "Epoch 26 -- Batch 155/ 842, training loss 0.297600120306015\n",
      "Epoch 26 -- Batch 156/ 842, training loss 0.3174766004085541\n",
      "Epoch 26 -- Batch 157/ 842, training loss 0.3137959837913513\n",
      "Epoch 26 -- Batch 158/ 842, training loss 0.30738160014152527\n",
      "Epoch 26 -- Batch 159/ 842, training loss 0.30876579880714417\n",
      "Epoch 26 -- Batch 160/ 842, training loss 0.30209988355636597\n",
      "Epoch 26 -- Batch 161/ 842, training loss 0.3013448119163513\n",
      "Epoch 26 -- Batch 162/ 842, training loss 0.30390164256095886\n",
      "Epoch 26 -- Batch 163/ 842, training loss 0.3130286931991577\n",
      "Epoch 26 -- Batch 164/ 842, training loss 0.318895548582077\n",
      "Epoch 26 -- Batch 165/ 842, training loss 0.31441429257392883\n",
      "Epoch 26 -- Batch 166/ 842, training loss 0.3107873499393463\n",
      "Epoch 26 -- Batch 167/ 842, training loss 0.31173133850097656\n",
      "Epoch 26 -- Batch 168/ 842, training loss 0.2975078523159027\n",
      "Epoch 26 -- Batch 169/ 842, training loss 0.30540409684181213\n",
      "Epoch 26 -- Batch 170/ 842, training loss 0.3087855577468872\n",
      "Epoch 26 -- Batch 171/ 842, training loss 0.31163159012794495\n",
      "Epoch 26 -- Batch 172/ 842, training loss 0.3139263093471527\n",
      "Epoch 26 -- Batch 173/ 842, training loss 0.3015550374984741\n",
      "Epoch 26 -- Batch 174/ 842, training loss 0.3140869140625\n",
      "Epoch 26 -- Batch 175/ 842, training loss 0.31090301275253296\n",
      "Epoch 26 -- Batch 176/ 842, training loss 0.3142927289009094\n",
      "Epoch 26 -- Batch 177/ 842, training loss 0.3130079209804535\n",
      "Epoch 26 -- Batch 178/ 842, training loss 0.3007979393005371\n",
      "Epoch 26 -- Batch 179/ 842, training loss 0.31122535467147827\n",
      "Epoch 26 -- Batch 180/ 842, training loss 0.3165389895439148\n",
      "Epoch 26 -- Batch 181/ 842, training loss 0.30959638953208923\n",
      "Epoch 26 -- Batch 182/ 842, training loss 0.31151753664016724\n",
      "Epoch 26 -- Batch 183/ 842, training loss 0.3165167570114136\n",
      "Epoch 26 -- Batch 184/ 842, training loss 0.30485737323760986\n",
      "Epoch 26 -- Batch 185/ 842, training loss 0.315483957529068\n",
      "Epoch 26 -- Batch 186/ 842, training loss 0.30249646306037903\n",
      "Epoch 26 -- Batch 187/ 842, training loss 0.30725935101509094\n",
      "Epoch 26 -- Batch 188/ 842, training loss 0.3218957185745239\n",
      "Epoch 26 -- Batch 189/ 842, training loss 0.31870296597480774\n",
      "Epoch 26 -- Batch 190/ 842, training loss 0.31132054328918457\n",
      "Epoch 26 -- Batch 191/ 842, training loss 0.30369073152542114\n",
      "Epoch 26 -- Batch 192/ 842, training loss 0.30913272500038147\n",
      "Epoch 26 -- Batch 193/ 842, training loss 0.32563626766204834\n",
      "Epoch 26 -- Batch 194/ 842, training loss 0.3115077316761017\n",
      "Epoch 26 -- Batch 195/ 842, training loss 0.32152456045150757\n",
      "Epoch 26 -- Batch 196/ 842, training loss 0.30765894055366516\n",
      "Epoch 26 -- Batch 197/ 842, training loss 0.3029266893863678\n",
      "Epoch 26 -- Batch 198/ 842, training loss 0.3071739971637726\n",
      "Epoch 26 -- Batch 199/ 842, training loss 0.3046417534351349\n",
      "Epoch 26 -- Batch 200/ 842, training loss 0.3039131760597229\n",
      "Epoch 26 -- Batch 201/ 842, training loss 0.3080892562866211\n",
      "Epoch 26 -- Batch 202/ 842, training loss 0.32098618149757385\n",
      "Epoch 26 -- Batch 203/ 842, training loss 0.31660112738609314\n",
      "Epoch 26 -- Batch 204/ 842, training loss 0.31887438893318176\n",
      "Epoch 26 -- Batch 205/ 842, training loss 0.31328943371772766\n",
      "Epoch 26 -- Batch 206/ 842, training loss 0.29862332344055176\n",
      "Epoch 26 -- Batch 207/ 842, training loss 0.3014964163303375\n",
      "Epoch 26 -- Batch 208/ 842, training loss 0.31040069460868835\n",
      "Epoch 26 -- Batch 209/ 842, training loss 0.31074705719947815\n",
      "Epoch 26 -- Batch 210/ 842, training loss 0.31089985370635986\n",
      "Epoch 26 -- Batch 211/ 842, training loss 0.29748693108558655\n",
      "Epoch 26 -- Batch 212/ 842, training loss 0.31687724590301514\n",
      "Epoch 26 -- Batch 213/ 842, training loss 0.3110079765319824\n",
      "Epoch 26 -- Batch 214/ 842, training loss 0.3075428307056427\n",
      "Epoch 26 -- Batch 215/ 842, training loss 0.31432849168777466\n",
      "Epoch 26 -- Batch 216/ 842, training loss 0.3141811788082123\n",
      "Epoch 26 -- Batch 217/ 842, training loss 0.31246811151504517\n",
      "Epoch 26 -- Batch 218/ 842, training loss 0.32090574502944946\n",
      "Epoch 26 -- Batch 219/ 842, training loss 0.30670228600502014\n",
      "Epoch 26 -- Batch 220/ 842, training loss 0.3094693124294281\n",
      "Epoch 26 -- Batch 221/ 842, training loss 0.3094669282436371\n",
      "Epoch 26 -- Batch 222/ 842, training loss 0.3230243921279907\n",
      "Epoch 26 -- Batch 223/ 842, training loss 0.30078864097595215\n",
      "Epoch 26 -- Batch 224/ 842, training loss 0.3046712577342987\n",
      "Epoch 26 -- Batch 225/ 842, training loss 0.3252308964729309\n",
      "Epoch 26 -- Batch 226/ 842, training loss 0.3108580708503723\n",
      "Epoch 26 -- Batch 227/ 842, training loss 0.3122238516807556\n",
      "Epoch 26 -- Batch 228/ 842, training loss 0.32507362961769104\n",
      "Epoch 26 -- Batch 229/ 842, training loss 0.31157636642456055\n",
      "Epoch 26 -- Batch 230/ 842, training loss 0.31352120637893677\n",
      "Epoch 26 -- Batch 231/ 842, training loss 0.31280526518821716\n",
      "Epoch 26 -- Batch 232/ 842, training loss 0.30050480365753174\n",
      "Epoch 26 -- Batch 233/ 842, training loss 0.30521664023399353\n",
      "Epoch 26 -- Batch 234/ 842, training loss 0.29981729388237\n",
      "Epoch 26 -- Batch 235/ 842, training loss 0.3007251024246216\n",
      "Epoch 26 -- Batch 236/ 842, training loss 0.30257347226142883\n",
      "Epoch 26 -- Batch 237/ 842, training loss 0.3080371618270874\n",
      "Epoch 26 -- Batch 238/ 842, training loss 0.31310513615608215\n",
      "Epoch 26 -- Batch 239/ 842, training loss 0.3137582242488861\n",
      "Epoch 26 -- Batch 240/ 842, training loss 0.3082834482192993\n",
      "Epoch 26 -- Batch 241/ 842, training loss 0.300674170255661\n",
      "Epoch 26 -- Batch 242/ 842, training loss 0.3000016510486603\n",
      "Epoch 26 -- Batch 243/ 842, training loss 0.3045192360877991\n",
      "Epoch 26 -- Batch 244/ 842, training loss 0.3277714252471924\n",
      "Epoch 26 -- Batch 245/ 842, training loss 0.3133595287799835\n",
      "Epoch 26 -- Batch 246/ 842, training loss 0.30559229850769043\n",
      "Epoch 26 -- Batch 247/ 842, training loss 0.3223206698894501\n",
      "Epoch 26 -- Batch 248/ 842, training loss 0.310478150844574\n",
      "Epoch 26 -- Batch 249/ 842, training loss 0.3075285255908966\n",
      "Epoch 26 -- Batch 250/ 842, training loss 0.3054327070713043\n",
      "Epoch 26 -- Batch 251/ 842, training loss 0.3147515654563904\n",
      "Epoch 26 -- Batch 252/ 842, training loss 0.29914721846580505\n",
      "Epoch 26 -- Batch 253/ 842, training loss 0.3067167103290558\n",
      "Epoch 26 -- Batch 254/ 842, training loss 0.316135972738266\n",
      "Epoch 26 -- Batch 255/ 842, training loss 0.2999182641506195\n",
      "Epoch 26 -- Batch 256/ 842, training loss 0.31582292914390564\n",
      "Epoch 26 -- Batch 257/ 842, training loss 0.3028053343296051\n",
      "Epoch 26 -- Batch 258/ 842, training loss 0.3146575391292572\n",
      "Epoch 26 -- Batch 259/ 842, training loss 0.31397193670272827\n",
      "Epoch 26 -- Batch 260/ 842, training loss 0.3047666847705841\n",
      "Epoch 26 -- Batch 261/ 842, training loss 0.3214978575706482\n",
      "Epoch 26 -- Batch 262/ 842, training loss 0.30238911509513855\n",
      "Epoch 26 -- Batch 263/ 842, training loss 0.3031370937824249\n",
      "Epoch 26 -- Batch 264/ 842, training loss 0.3138532042503357\n",
      "Epoch 26 -- Batch 265/ 842, training loss 0.3153596818447113\n",
      "Epoch 26 -- Batch 266/ 842, training loss 0.3071559965610504\n",
      "Epoch 26 -- Batch 267/ 842, training loss 0.3028985261917114\n",
      "Epoch 26 -- Batch 268/ 842, training loss 0.31646010279655457\n",
      "Epoch 26 -- Batch 269/ 842, training loss 0.3084307014942169\n",
      "Epoch 26 -- Batch 270/ 842, training loss 0.3000648021697998\n",
      "Epoch 26 -- Batch 271/ 842, training loss 0.3018149435520172\n",
      "Epoch 26 -- Batch 272/ 842, training loss 0.31475821137428284\n",
      "Epoch 26 -- Batch 273/ 842, training loss 0.3034955859184265\n",
      "Epoch 26 -- Batch 274/ 842, training loss 0.30183616280555725\n",
      "Epoch 26 -- Batch 275/ 842, training loss 0.3093540072441101\n",
      "Epoch 26 -- Batch 276/ 842, training loss 0.31658563017845154\n",
      "Epoch 26 -- Batch 277/ 842, training loss 0.3122345805168152\n",
      "Epoch 26 -- Batch 278/ 842, training loss 0.3033634126186371\n",
      "Epoch 26 -- Batch 279/ 842, training loss 0.2987954914569855\n",
      "Epoch 26 -- Batch 280/ 842, training loss 0.30019015073776245\n",
      "Epoch 26 -- Batch 281/ 842, training loss 0.30948689579963684\n",
      "Epoch 26 -- Batch 282/ 842, training loss 0.3108114004135132\n",
      "Epoch 26 -- Batch 283/ 842, training loss 0.3098168671131134\n",
      "Epoch 26 -- Batch 284/ 842, training loss 0.3013690412044525\n",
      "Epoch 26 -- Batch 285/ 842, training loss 0.31692638993263245\n",
      "Epoch 26 -- Batch 286/ 842, training loss 0.30554133653640747\n",
      "Epoch 26 -- Batch 287/ 842, training loss 0.3050765097141266\n",
      "Epoch 26 -- Batch 288/ 842, training loss 0.299091637134552\n",
      "Epoch 26 -- Batch 289/ 842, training loss 0.3145982325077057\n",
      "Epoch 26 -- Batch 290/ 842, training loss 0.3144494295120239\n",
      "Epoch 26 -- Batch 291/ 842, training loss 0.32248201966285706\n",
      "Epoch 26 -- Batch 292/ 842, training loss 0.3000718057155609\n",
      "Epoch 26 -- Batch 293/ 842, training loss 0.3074968755245209\n",
      "Epoch 26 -- Batch 294/ 842, training loss 0.3135341703891754\n",
      "Epoch 26 -- Batch 295/ 842, training loss 0.3054342269897461\n",
      "Epoch 26 -- Batch 296/ 842, training loss 0.30533137917518616\n",
      "Epoch 26 -- Batch 297/ 842, training loss 0.3100339472293854\n",
      "Epoch 26 -- Batch 298/ 842, training loss 0.3144723176956177\n",
      "Epoch 26 -- Batch 299/ 842, training loss 0.3117648661136627\n",
      "Epoch 26 -- Batch 300/ 842, training loss 0.30006274580955505\n",
      "Epoch 26 -- Batch 301/ 842, training loss 0.2991710901260376\n",
      "Epoch 26 -- Batch 302/ 842, training loss 0.312528520822525\n",
      "Epoch 26 -- Batch 303/ 842, training loss 0.31017372012138367\n",
      "Epoch 26 -- Batch 304/ 842, training loss 0.30890190601348877\n",
      "Epoch 26 -- Batch 305/ 842, training loss 0.31301289796829224\n",
      "Epoch 26 -- Batch 306/ 842, training loss 0.30578359961509705\n",
      "Epoch 26 -- Batch 307/ 842, training loss 0.3155445158481598\n",
      "Epoch 26 -- Batch 308/ 842, training loss 0.31100785732269287\n",
      "Epoch 26 -- Batch 309/ 842, training loss 0.2915974259376526\n",
      "Epoch 26 -- Batch 310/ 842, training loss 0.3039312958717346\n",
      "Epoch 26 -- Batch 311/ 842, training loss 0.3023183047771454\n",
      "Epoch 26 -- Batch 312/ 842, training loss 0.30370789766311646\n",
      "Epoch 26 -- Batch 313/ 842, training loss 0.30139780044555664\n",
      "Epoch 26 -- Batch 314/ 842, training loss 0.3062666654586792\n",
      "Epoch 26 -- Batch 315/ 842, training loss 0.3036543130874634\n",
      "Epoch 26 -- Batch 316/ 842, training loss 0.31960487365722656\n",
      "Epoch 26 -- Batch 317/ 842, training loss 0.31712231040000916\n",
      "Epoch 26 -- Batch 318/ 842, training loss 0.3152948021888733\n",
      "Epoch 26 -- Batch 319/ 842, training loss 0.32753053307533264\n",
      "Epoch 26 -- Batch 320/ 842, training loss 0.30575671792030334\n",
      "Epoch 26 -- Batch 321/ 842, training loss 0.30711305141448975\n",
      "Epoch 26 -- Batch 322/ 842, training loss 0.3122216463088989\n",
      "Epoch 26 -- Batch 323/ 842, training loss 0.32660382986068726\n",
      "Epoch 26 -- Batch 324/ 842, training loss 0.31298205256462097\n",
      "Epoch 26 -- Batch 325/ 842, training loss 0.3112550675868988\n",
      "Epoch 26 -- Batch 326/ 842, training loss 0.3179309368133545\n",
      "Epoch 26 -- Batch 327/ 842, training loss 0.303056001663208\n",
      "Epoch 26 -- Batch 328/ 842, training loss 0.3014654219150543\n",
      "Epoch 26 -- Batch 329/ 842, training loss 0.31072235107421875\n",
      "Epoch 26 -- Batch 330/ 842, training loss 0.31031233072280884\n",
      "Epoch 26 -- Batch 331/ 842, training loss 0.31693100929260254\n",
      "Epoch 26 -- Batch 332/ 842, training loss 0.3041324317455292\n",
      "Epoch 26 -- Batch 333/ 842, training loss 0.3106248378753662\n",
      "Epoch 26 -- Batch 334/ 842, training loss 0.31151366233825684\n",
      "Epoch 26 -- Batch 335/ 842, training loss 0.3108355700969696\n",
      "Epoch 26 -- Batch 336/ 842, training loss 0.301449179649353\n",
      "Epoch 26 -- Batch 337/ 842, training loss 0.3144378960132599\n",
      "Epoch 26 -- Batch 338/ 842, training loss 0.30471351742744446\n",
      "Epoch 26 -- Batch 339/ 842, training loss 0.3145718276500702\n",
      "Epoch 26 -- Batch 340/ 842, training loss 0.3071361184120178\n",
      "Epoch 26 -- Batch 341/ 842, training loss 0.3026180565357208\n",
      "Epoch 26 -- Batch 342/ 842, training loss 0.3194524645805359\n",
      "Epoch 26 -- Batch 343/ 842, training loss 0.31311920285224915\n",
      "Epoch 26 -- Batch 344/ 842, training loss 0.32503950595855713\n",
      "Epoch 26 -- Batch 345/ 842, training loss 0.3056468069553375\n",
      "Epoch 26 -- Batch 346/ 842, training loss 0.30363449454307556\n",
      "Epoch 26 -- Batch 347/ 842, training loss 0.31147077679634094\n",
      "Epoch 26 -- Batch 348/ 842, training loss 0.3127317428588867\n",
      "Epoch 26 -- Batch 349/ 842, training loss 0.29832619428634644\n",
      "Epoch 26 -- Batch 350/ 842, training loss 0.2996119558811188\n",
      "Epoch 26 -- Batch 351/ 842, training loss 0.31596314907073975\n",
      "Epoch 26 -- Batch 352/ 842, training loss 0.3027951717376709\n",
      "Epoch 26 -- Batch 353/ 842, training loss 0.3167087137699127\n",
      "Epoch 26 -- Batch 354/ 842, training loss 0.3043823838233948\n",
      "Epoch 26 -- Batch 355/ 842, training loss 0.3174184262752533\n",
      "Epoch 26 -- Batch 356/ 842, training loss 0.3115740418434143\n",
      "Epoch 26 -- Batch 357/ 842, training loss 0.2938370704650879\n",
      "Epoch 26 -- Batch 358/ 842, training loss 0.30956268310546875\n",
      "Epoch 26 -- Batch 359/ 842, training loss 0.3147396147251129\n",
      "Epoch 26 -- Batch 360/ 842, training loss 0.3204144835472107\n",
      "Epoch 26 -- Batch 361/ 842, training loss 0.31541842222213745\n",
      "Epoch 26 -- Batch 362/ 842, training loss 0.31666892766952515\n",
      "Epoch 26 -- Batch 363/ 842, training loss 0.30686914920806885\n",
      "Epoch 26 -- Batch 364/ 842, training loss 0.3190392851829529\n",
      "Epoch 26 -- Batch 365/ 842, training loss 0.29435110092163086\n",
      "Epoch 26 -- Batch 366/ 842, training loss 0.31403765082359314\n",
      "Epoch 26 -- Batch 367/ 842, training loss 0.31309545040130615\n",
      "Epoch 26 -- Batch 368/ 842, training loss 0.30966636538505554\n",
      "Epoch 26 -- Batch 369/ 842, training loss 0.3207415044307709\n",
      "Epoch 26 -- Batch 370/ 842, training loss 0.31029778718948364\n",
      "Epoch 26 -- Batch 371/ 842, training loss 0.31385505199432373\n",
      "Epoch 26 -- Batch 372/ 842, training loss 0.2990334630012512\n",
      "Epoch 26 -- Batch 373/ 842, training loss 0.30441775918006897\n",
      "Epoch 26 -- Batch 374/ 842, training loss 0.3050236999988556\n",
      "Epoch 26 -- Batch 375/ 842, training loss 0.30750778317451477\n",
      "Epoch 26 -- Batch 376/ 842, training loss 0.3144182860851288\n",
      "Epoch 26 -- Batch 377/ 842, training loss 0.30412009358406067\n",
      "Epoch 26 -- Batch 378/ 842, training loss 0.3085189461708069\n",
      "Epoch 26 -- Batch 379/ 842, training loss 0.3183889091014862\n",
      "Epoch 26 -- Batch 380/ 842, training loss 0.31012025475502014\n",
      "Epoch 26 -- Batch 381/ 842, training loss 0.3053826689720154\n",
      "Epoch 26 -- Batch 382/ 842, training loss 0.3140205144882202\n",
      "Epoch 26 -- Batch 383/ 842, training loss 0.30532386898994446\n",
      "Epoch 26 -- Batch 384/ 842, training loss 0.2996247112751007\n",
      "Epoch 26 -- Batch 385/ 842, training loss 0.3093353807926178\n",
      "Epoch 26 -- Batch 386/ 842, training loss 0.3076741099357605\n",
      "Epoch 26 -- Batch 387/ 842, training loss 0.3171616494655609\n",
      "Epoch 26 -- Batch 388/ 842, training loss 0.3130911886692047\n",
      "Epoch 26 -- Batch 389/ 842, training loss 0.3242470920085907\n",
      "Epoch 26 -- Batch 390/ 842, training loss 0.31832966208457947\n",
      "Epoch 26 -- Batch 391/ 842, training loss 0.32339581847190857\n",
      "Epoch 26 -- Batch 392/ 842, training loss 0.3074471354484558\n",
      "Epoch 26 -- Batch 393/ 842, training loss 0.3123002052307129\n",
      "Epoch 26 -- Batch 394/ 842, training loss 0.30537325143814087\n",
      "Epoch 26 -- Batch 395/ 842, training loss 0.3087177276611328\n",
      "Epoch 26 -- Batch 396/ 842, training loss 0.3117152452468872\n",
      "Epoch 26 -- Batch 397/ 842, training loss 0.3131345808506012\n",
      "Epoch 26 -- Batch 398/ 842, training loss 0.31197285652160645\n",
      "Epoch 26 -- Batch 399/ 842, training loss 0.31419429183006287\n",
      "Epoch 26 -- Batch 400/ 842, training loss 0.32075369358062744\n",
      "Epoch 26 -- Batch 401/ 842, training loss 0.3071756660938263\n",
      "Epoch 26 -- Batch 402/ 842, training loss 0.312050461769104\n",
      "Epoch 26 -- Batch 403/ 842, training loss 0.3206920921802521\n",
      "Epoch 26 -- Batch 404/ 842, training loss 0.3072742819786072\n",
      "Epoch 26 -- Batch 405/ 842, training loss 0.3149254024028778\n",
      "Epoch 26 -- Batch 406/ 842, training loss 0.30562031269073486\n",
      "Epoch 26 -- Batch 407/ 842, training loss 0.31457042694091797\n",
      "Epoch 26 -- Batch 408/ 842, training loss 0.3156687021255493\n",
      "Epoch 26 -- Batch 409/ 842, training loss 0.3086918294429779\n",
      "Epoch 26 -- Batch 410/ 842, training loss 0.3191833198070526\n",
      "Epoch 26 -- Batch 411/ 842, training loss 0.2952933609485626\n",
      "Epoch 26 -- Batch 412/ 842, training loss 0.32099637389183044\n",
      "Epoch 26 -- Batch 413/ 842, training loss 0.32200074195861816\n",
      "Epoch 26 -- Batch 414/ 842, training loss 0.29842087626457214\n",
      "Epoch 26 -- Batch 415/ 842, training loss 0.31240877509117126\n",
      "Epoch 26 -- Batch 416/ 842, training loss 0.3098302185535431\n",
      "Epoch 26 -- Batch 417/ 842, training loss 0.31759577989578247\n",
      "Epoch 26 -- Batch 418/ 842, training loss 0.3121107816696167\n",
      "Epoch 26 -- Batch 419/ 842, training loss 0.319970041513443\n",
      "Epoch 26 -- Batch 420/ 842, training loss 0.31500551104545593\n",
      "Epoch 26 -- Batch 421/ 842, training loss 0.3156168758869171\n",
      "Epoch 26 -- Batch 422/ 842, training loss 0.31572040915489197\n",
      "Epoch 26 -- Batch 423/ 842, training loss 0.3081091642379761\n",
      "Epoch 26 -- Batch 424/ 842, training loss 0.30127188563346863\n",
      "Epoch 26 -- Batch 425/ 842, training loss 0.315906286239624\n",
      "Epoch 26 -- Batch 426/ 842, training loss 0.32619503140449524\n",
      "Epoch 26 -- Batch 427/ 842, training loss 0.3103524148464203\n",
      "Epoch 26 -- Batch 428/ 842, training loss 0.3171084523200989\n",
      "Epoch 26 -- Batch 429/ 842, training loss 0.31152087450027466\n",
      "Epoch 26 -- Batch 430/ 842, training loss 0.3195555806159973\n",
      "Epoch 26 -- Batch 431/ 842, training loss 0.31488338112831116\n",
      "Epoch 26 -- Batch 432/ 842, training loss 0.31170645356178284\n",
      "Epoch 26 -- Batch 433/ 842, training loss 0.30962255597114563\n",
      "Epoch 26 -- Batch 434/ 842, training loss 0.3094823658466339\n",
      "Epoch 26 -- Batch 435/ 842, training loss 0.31254053115844727\n",
      "Epoch 26 -- Batch 436/ 842, training loss 0.30328455567359924\n",
      "Epoch 26 -- Batch 437/ 842, training loss 0.30834463238716125\n",
      "Epoch 26 -- Batch 438/ 842, training loss 0.3073262870311737\n",
      "Epoch 26 -- Batch 439/ 842, training loss 0.3195423185825348\n",
      "Epoch 26 -- Batch 440/ 842, training loss 0.3070397973060608\n",
      "Epoch 26 -- Batch 441/ 842, training loss 0.3164746165275574\n",
      "Epoch 26 -- Batch 442/ 842, training loss 0.31817835569381714\n",
      "Epoch 26 -- Batch 443/ 842, training loss 0.31049591302871704\n",
      "Epoch 26 -- Batch 444/ 842, training loss 0.3233471214771271\n",
      "Epoch 26 -- Batch 445/ 842, training loss 0.31530073285102844\n",
      "Epoch 26 -- Batch 446/ 842, training loss 0.3165217936038971\n",
      "Epoch 26 -- Batch 447/ 842, training loss 0.32706212997436523\n",
      "Epoch 26 -- Batch 448/ 842, training loss 0.3208785653114319\n",
      "Epoch 26 -- Batch 449/ 842, training loss 0.3079221248626709\n",
      "Epoch 26 -- Batch 450/ 842, training loss 0.3094741106033325\n",
      "Epoch 26 -- Batch 451/ 842, training loss 0.31378430128097534\n",
      "Epoch 26 -- Batch 452/ 842, training loss 0.32195761799812317\n",
      "Epoch 26 -- Batch 453/ 842, training loss 0.3034103512763977\n",
      "Epoch 26 -- Batch 454/ 842, training loss 0.30859628319740295\n",
      "Epoch 26 -- Batch 455/ 842, training loss 0.3238547444343567\n",
      "Epoch 26 -- Batch 456/ 842, training loss 0.3065294623374939\n",
      "Epoch 26 -- Batch 457/ 842, training loss 0.31808966398239136\n",
      "Epoch 26 -- Batch 458/ 842, training loss 0.31506937742233276\n",
      "Epoch 26 -- Batch 459/ 842, training loss 0.3242345154285431\n",
      "Epoch 26 -- Batch 460/ 842, training loss 0.30111411213874817\n",
      "Epoch 26 -- Batch 461/ 842, training loss 0.30769026279449463\n",
      "Epoch 26 -- Batch 462/ 842, training loss 0.32237914204597473\n",
      "Epoch 26 -- Batch 463/ 842, training loss 0.31687313318252563\n",
      "Epoch 26 -- Batch 464/ 842, training loss 0.31301820278167725\n",
      "Epoch 26 -- Batch 465/ 842, training loss 0.3160260021686554\n",
      "Epoch 26 -- Batch 466/ 842, training loss 0.3079138398170471\n",
      "Epoch 26 -- Batch 467/ 842, training loss 0.3143467307090759\n",
      "Epoch 26 -- Batch 468/ 842, training loss 0.3171044588088989\n",
      "Epoch 26 -- Batch 469/ 842, training loss 0.30704233050346375\n",
      "Epoch 26 -- Batch 470/ 842, training loss 0.31308987736701965\n",
      "Epoch 26 -- Batch 471/ 842, training loss 0.3214316964149475\n",
      "Epoch 26 -- Batch 472/ 842, training loss 0.3208200931549072\n",
      "Epoch 26 -- Batch 473/ 842, training loss 0.3111885190010071\n",
      "Epoch 26 -- Batch 474/ 842, training loss 0.3022439479827881\n",
      "Epoch 26 -- Batch 475/ 842, training loss 0.3243319094181061\n",
      "Epoch 26 -- Batch 476/ 842, training loss 0.3117758631706238\n",
      "Epoch 26 -- Batch 477/ 842, training loss 0.30453482270240784\n",
      "Epoch 26 -- Batch 478/ 842, training loss 0.31185346841812134\n",
      "Epoch 26 -- Batch 479/ 842, training loss 0.30221059918403625\n",
      "Epoch 26 -- Batch 480/ 842, training loss 0.30399706959724426\n",
      "Epoch 26 -- Batch 481/ 842, training loss 0.31713154911994934\n",
      "Epoch 26 -- Batch 482/ 842, training loss 0.31122785806655884\n",
      "Epoch 26 -- Batch 483/ 842, training loss 0.3116380274295807\n",
      "Epoch 26 -- Batch 484/ 842, training loss 0.3152167499065399\n",
      "Epoch 26 -- Batch 485/ 842, training loss 0.31145042181015015\n",
      "Epoch 26 -- Batch 486/ 842, training loss 0.3129822909832001\n",
      "Epoch 26 -- Batch 487/ 842, training loss 0.3097674548625946\n",
      "Epoch 26 -- Batch 488/ 842, training loss 0.3140007257461548\n",
      "Epoch 26 -- Batch 489/ 842, training loss 0.29811203479766846\n",
      "Epoch 26 -- Batch 490/ 842, training loss 0.3124341070652008\n",
      "Epoch 26 -- Batch 491/ 842, training loss 0.3204372227191925\n",
      "Epoch 26 -- Batch 492/ 842, training loss 0.304185688495636\n",
      "Epoch 26 -- Batch 493/ 842, training loss 0.31630316376686096\n",
      "Epoch 26 -- Batch 494/ 842, training loss 0.30988115072250366\n",
      "Epoch 26 -- Batch 495/ 842, training loss 0.309128075838089\n",
      "Epoch 26 -- Batch 496/ 842, training loss 0.31413859128952026\n",
      "Epoch 26 -- Batch 497/ 842, training loss 0.3354514539241791\n",
      "Epoch 26 -- Batch 498/ 842, training loss 0.30194398760795593\n",
      "Epoch 26 -- Batch 499/ 842, training loss 0.3212868869304657\n",
      "Epoch 26 -- Batch 500/ 842, training loss 0.3071773946285248\n",
      "Epoch 26 -- Batch 501/ 842, training loss 0.30760011076927185\n",
      "Epoch 26 -- Batch 502/ 842, training loss 0.3199678063392639\n",
      "Epoch 26 -- Batch 503/ 842, training loss 0.3271130323410034\n",
      "Epoch 26 -- Batch 504/ 842, training loss 0.3192726671695709\n",
      "Epoch 26 -- Batch 505/ 842, training loss 0.31841138005256653\n",
      "Epoch 26 -- Batch 506/ 842, training loss 0.3219318389892578\n",
      "Epoch 26 -- Batch 507/ 842, training loss 0.3256809115409851\n",
      "Epoch 26 -- Batch 508/ 842, training loss 0.30786940455436707\n",
      "Epoch 26 -- Batch 509/ 842, training loss 0.31023088097572327\n",
      "Epoch 26 -- Batch 510/ 842, training loss 0.31112706661224365\n",
      "Epoch 26 -- Batch 511/ 842, training loss 0.3076692819595337\n",
      "Epoch 26 -- Batch 512/ 842, training loss 0.3091081976890564\n",
      "Epoch 26 -- Batch 513/ 842, training loss 0.31052476167678833\n",
      "Epoch 26 -- Batch 514/ 842, training loss 0.3220711052417755\n",
      "Epoch 26 -- Batch 515/ 842, training loss 0.31370824575424194\n",
      "Epoch 26 -- Batch 516/ 842, training loss 0.32670485973358154\n",
      "Epoch 26 -- Batch 517/ 842, training loss 0.3166266679763794\n",
      "Epoch 26 -- Batch 518/ 842, training loss 0.3141160011291504\n",
      "Epoch 26 -- Batch 519/ 842, training loss 0.3041131794452667\n",
      "Epoch 26 -- Batch 520/ 842, training loss 0.31881335377693176\n",
      "Epoch 26 -- Batch 521/ 842, training loss 0.32743313908576965\n",
      "Epoch 26 -- Batch 522/ 842, training loss 0.3178723156452179\n",
      "Epoch 26 -- Batch 523/ 842, training loss 0.31872299313545227\n",
      "Epoch 26 -- Batch 524/ 842, training loss 0.31773149967193604\n",
      "Epoch 26 -- Batch 525/ 842, training loss 0.3197516202926636\n",
      "Epoch 26 -- Batch 526/ 842, training loss 0.31967228651046753\n",
      "Epoch 26 -- Batch 527/ 842, training loss 0.31542572379112244\n",
      "Epoch 26 -- Batch 528/ 842, training loss 0.3144306540489197\n",
      "Epoch 26 -- Batch 529/ 842, training loss 0.319070428609848\n",
      "Epoch 26 -- Batch 530/ 842, training loss 0.30558687448501587\n",
      "Epoch 26 -- Batch 531/ 842, training loss 0.31534188985824585\n",
      "Epoch 26 -- Batch 532/ 842, training loss 0.29961463809013367\n",
      "Epoch 26 -- Batch 533/ 842, training loss 0.3096809387207031\n",
      "Epoch 26 -- Batch 534/ 842, training loss 0.3233361542224884\n",
      "Epoch 26 -- Batch 535/ 842, training loss 0.33413225412368774\n",
      "Epoch 26 -- Batch 536/ 842, training loss 0.31438198685646057\n",
      "Epoch 26 -- Batch 537/ 842, training loss 0.30517205595970154\n",
      "Epoch 26 -- Batch 538/ 842, training loss 0.30940479040145874\n",
      "Epoch 26 -- Batch 539/ 842, training loss 0.3194815516471863\n",
      "Epoch 26 -- Batch 540/ 842, training loss 0.30146002769470215\n",
      "Epoch 26 -- Batch 541/ 842, training loss 0.3248754143714905\n",
      "Epoch 26 -- Batch 542/ 842, training loss 0.3063807189464569\n",
      "Epoch 26 -- Batch 543/ 842, training loss 0.31422701478004456\n",
      "Epoch 26 -- Batch 544/ 842, training loss 0.31366628408432007\n",
      "Epoch 26 -- Batch 545/ 842, training loss 0.3131279945373535\n",
      "Epoch 26 -- Batch 546/ 842, training loss 0.31553512811660767\n",
      "Epoch 26 -- Batch 547/ 842, training loss 0.3091939091682434\n",
      "Epoch 26 -- Batch 548/ 842, training loss 0.30181884765625\n",
      "Epoch 26 -- Batch 549/ 842, training loss 0.31269824504852295\n",
      "Epoch 26 -- Batch 550/ 842, training loss 0.3209574222564697\n",
      "Epoch 26 -- Batch 551/ 842, training loss 0.3117719888687134\n",
      "Epoch 26 -- Batch 552/ 842, training loss 0.3208727240562439\n",
      "Epoch 26 -- Batch 553/ 842, training loss 0.3149528205394745\n",
      "Epoch 26 -- Batch 554/ 842, training loss 0.30468374490737915\n",
      "Epoch 26 -- Batch 555/ 842, training loss 0.3074306845664978\n",
      "Epoch 26 -- Batch 556/ 842, training loss 0.3134828209877014\n",
      "Epoch 26 -- Batch 557/ 842, training loss 0.31969135999679565\n",
      "Epoch 26 -- Batch 558/ 842, training loss 0.30876755714416504\n",
      "Epoch 26 -- Batch 559/ 842, training loss 0.31916090846061707\n",
      "Epoch 26 -- Batch 560/ 842, training loss 0.30805858969688416\n",
      "Epoch 26 -- Batch 561/ 842, training loss 0.3086605668067932\n",
      "Epoch 26 -- Batch 562/ 842, training loss 0.3180790841579437\n",
      "Epoch 26 -- Batch 563/ 842, training loss 0.3010953366756439\n",
      "Epoch 26 -- Batch 564/ 842, training loss 0.3009909391403198\n",
      "Epoch 26 -- Batch 565/ 842, training loss 0.31222811341285706\n",
      "Epoch 26 -- Batch 566/ 842, training loss 0.3038260042667389\n",
      "Epoch 26 -- Batch 567/ 842, training loss 0.30921608209609985\n",
      "Epoch 26 -- Batch 568/ 842, training loss 0.3128257691860199\n",
      "Epoch 26 -- Batch 569/ 842, training loss 0.31244319677352905\n",
      "Epoch 26 -- Batch 570/ 842, training loss 0.3154132664203644\n",
      "Epoch 26 -- Batch 571/ 842, training loss 0.3051309585571289\n",
      "Epoch 26 -- Batch 572/ 842, training loss 0.3101579546928406\n",
      "Epoch 26 -- Batch 573/ 842, training loss 0.3195030689239502\n",
      "Epoch 26 -- Batch 574/ 842, training loss 0.3048732280731201\n",
      "Epoch 26 -- Batch 575/ 842, training loss 0.320321261882782\n",
      "Epoch 26 -- Batch 576/ 842, training loss 0.2939644455909729\n",
      "Epoch 26 -- Batch 577/ 842, training loss 0.30695605278015137\n",
      "Epoch 26 -- Batch 578/ 842, training loss 0.3226092457771301\n",
      "Epoch 26 -- Batch 579/ 842, training loss 0.31530389189720154\n",
      "Epoch 26 -- Batch 580/ 842, training loss 0.3067322075366974\n",
      "Epoch 26 -- Batch 581/ 842, training loss 0.31372058391571045\n",
      "Epoch 26 -- Batch 582/ 842, training loss 0.3169606328010559\n",
      "Epoch 26 -- Batch 583/ 842, training loss 0.3103468716144562\n",
      "Epoch 26 -- Batch 584/ 842, training loss 0.32606086134910583\n",
      "Epoch 26 -- Batch 585/ 842, training loss 0.30657270550727844\n",
      "Epoch 26 -- Batch 586/ 842, training loss 0.3103863000869751\n",
      "Epoch 26 -- Batch 587/ 842, training loss 0.30899643898010254\n",
      "Epoch 26 -- Batch 588/ 842, training loss 0.3168146014213562\n",
      "Epoch 26 -- Batch 589/ 842, training loss 0.3153775632381439\n",
      "Epoch 26 -- Batch 590/ 842, training loss 0.3108637034893036\n",
      "Epoch 26 -- Batch 591/ 842, training loss 0.3264978229999542\n",
      "Epoch 26 -- Batch 592/ 842, training loss 0.31312304735183716\n",
      "Epoch 26 -- Batch 593/ 842, training loss 0.30502012372016907\n",
      "Epoch 26 -- Batch 594/ 842, training loss 0.3096071183681488\n",
      "Epoch 26 -- Batch 595/ 842, training loss 0.3194456994533539\n",
      "Epoch 26 -- Batch 596/ 842, training loss 0.3064669370651245\n",
      "Epoch 26 -- Batch 597/ 842, training loss 0.3125772774219513\n",
      "Epoch 26 -- Batch 598/ 842, training loss 0.3003027141094208\n",
      "Epoch 26 -- Batch 599/ 842, training loss 0.31153252720832825\n",
      "Epoch 26 -- Batch 600/ 842, training loss 0.31195512413978577\n",
      "Epoch 26 -- Batch 601/ 842, training loss 0.30934715270996094\n",
      "Epoch 26 -- Batch 602/ 842, training loss 0.3179285526275635\n",
      "Epoch 26 -- Batch 603/ 842, training loss 0.3172888457775116\n",
      "Epoch 26 -- Batch 604/ 842, training loss 0.3036806583404541\n",
      "Epoch 26 -- Batch 605/ 842, training loss 0.30570003390312195\n",
      "Epoch 26 -- Batch 606/ 842, training loss 0.3212815523147583\n",
      "Epoch 26 -- Batch 607/ 842, training loss 0.308188796043396\n",
      "Epoch 26 -- Batch 608/ 842, training loss 0.32215604186058044\n",
      "Epoch 26 -- Batch 609/ 842, training loss 0.30999910831451416\n",
      "Epoch 26 -- Batch 610/ 842, training loss 0.31478172540664673\n",
      "Epoch 26 -- Batch 611/ 842, training loss 0.3158412575721741\n",
      "Epoch 26 -- Batch 612/ 842, training loss 0.3275372385978699\n",
      "Epoch 26 -- Batch 613/ 842, training loss 0.3000938892364502\n",
      "Epoch 26 -- Batch 614/ 842, training loss 0.3273965120315552\n",
      "Epoch 26 -- Batch 615/ 842, training loss 0.3031226694583893\n",
      "Epoch 26 -- Batch 616/ 842, training loss 0.3216041326522827\n",
      "Epoch 26 -- Batch 617/ 842, training loss 0.31501832604408264\n",
      "Epoch 26 -- Batch 618/ 842, training loss 0.30641111731529236\n",
      "Epoch 26 -- Batch 619/ 842, training loss 0.30806005001068115\n",
      "Epoch 26 -- Batch 620/ 842, training loss 0.3123014569282532\n",
      "Epoch 26 -- Batch 621/ 842, training loss 0.31104668974876404\n",
      "Epoch 26 -- Batch 622/ 842, training loss 0.31520959734916687\n",
      "Epoch 26 -- Batch 623/ 842, training loss 0.31295108795166016\n",
      "Epoch 26 -- Batch 624/ 842, training loss 0.3004242777824402\n",
      "Epoch 26 -- Batch 625/ 842, training loss 0.30994173884391785\n",
      "Epoch 26 -- Batch 626/ 842, training loss 0.31883206963539124\n",
      "Epoch 26 -- Batch 627/ 842, training loss 0.30018293857574463\n",
      "Epoch 26 -- Batch 628/ 842, training loss 0.3022565245628357\n",
      "Epoch 26 -- Batch 629/ 842, training loss 0.31315505504608154\n",
      "Epoch 26 -- Batch 630/ 842, training loss 0.3102688193321228\n",
      "Epoch 26 -- Batch 631/ 842, training loss 0.32256045937538147\n",
      "Epoch 26 -- Batch 632/ 842, training loss 0.31242144107818604\n",
      "Epoch 26 -- Batch 633/ 842, training loss 0.3231022357940674\n",
      "Epoch 26 -- Batch 634/ 842, training loss 0.32152119278907776\n",
      "Epoch 26 -- Batch 635/ 842, training loss 0.3162447214126587\n",
      "Epoch 26 -- Batch 636/ 842, training loss 0.3041428327560425\n",
      "Epoch 26 -- Batch 637/ 842, training loss 0.311126708984375\n",
      "Epoch 26 -- Batch 638/ 842, training loss 0.30451634526252747\n",
      "Epoch 26 -- Batch 639/ 842, training loss 0.3180731534957886\n",
      "Epoch 26 -- Batch 640/ 842, training loss 0.3238641917705536\n",
      "Epoch 26 -- Batch 641/ 842, training loss 0.32745248079299927\n",
      "Epoch 26 -- Batch 642/ 842, training loss 0.31614089012145996\n",
      "Epoch 26 -- Batch 643/ 842, training loss 0.3097231388092041\n",
      "Epoch 26 -- Batch 644/ 842, training loss 0.31352680921554565\n",
      "Epoch 26 -- Batch 645/ 842, training loss 0.31778913736343384\n",
      "Epoch 26 -- Batch 646/ 842, training loss 0.3183969557285309\n",
      "Epoch 26 -- Batch 647/ 842, training loss 0.309617817401886\n",
      "Epoch 26 -- Batch 648/ 842, training loss 0.31460830569267273\n",
      "Epoch 26 -- Batch 649/ 842, training loss 0.3140903413295746\n",
      "Epoch 26 -- Batch 650/ 842, training loss 0.31783658266067505\n",
      "Epoch 26 -- Batch 651/ 842, training loss 0.31119388341903687\n",
      "Epoch 26 -- Batch 652/ 842, training loss 0.3090749979019165\n",
      "Epoch 26 -- Batch 653/ 842, training loss 0.30278894305229187\n",
      "Epoch 26 -- Batch 654/ 842, training loss 0.3180597126483917\n",
      "Epoch 26 -- Batch 655/ 842, training loss 0.3119329512119293\n",
      "Epoch 26 -- Batch 656/ 842, training loss 0.3277963399887085\n",
      "Epoch 26 -- Batch 657/ 842, training loss 0.3030610978603363\n",
      "Epoch 26 -- Batch 658/ 842, training loss 0.3108471930027008\n",
      "Epoch 26 -- Batch 659/ 842, training loss 0.30885592103004456\n",
      "Epoch 26 -- Batch 660/ 842, training loss 0.307058185338974\n",
      "Epoch 26 -- Batch 661/ 842, training loss 0.3185184895992279\n",
      "Epoch 26 -- Batch 662/ 842, training loss 0.31344202160835266\n",
      "Epoch 26 -- Batch 663/ 842, training loss 0.31723761558532715\n",
      "Epoch 26 -- Batch 664/ 842, training loss 0.3071965277194977\n",
      "Epoch 26 -- Batch 665/ 842, training loss 0.3042593002319336\n",
      "Epoch 26 -- Batch 666/ 842, training loss 0.3084488809108734\n",
      "Epoch 26 -- Batch 667/ 842, training loss 0.3231288194656372\n",
      "Epoch 26 -- Batch 668/ 842, training loss 0.30970779061317444\n",
      "Epoch 26 -- Batch 669/ 842, training loss 0.3032669425010681\n",
      "Epoch 26 -- Batch 670/ 842, training loss 0.3046802282333374\n",
      "Epoch 26 -- Batch 671/ 842, training loss 0.3065283000469208\n",
      "Epoch 26 -- Batch 672/ 842, training loss 0.3200293183326721\n",
      "Epoch 26 -- Batch 673/ 842, training loss 0.3054368197917938\n",
      "Epoch 26 -- Batch 674/ 842, training loss 0.31027284264564514\n",
      "Epoch 26 -- Batch 675/ 842, training loss 0.316422700881958\n",
      "Epoch 26 -- Batch 676/ 842, training loss 0.31565386056900024\n",
      "Epoch 26 -- Batch 677/ 842, training loss 0.3204239308834076\n",
      "Epoch 26 -- Batch 678/ 842, training loss 0.3221786320209503\n",
      "Epoch 26 -- Batch 679/ 842, training loss 0.3010104298591614\n",
      "Epoch 26 -- Batch 680/ 842, training loss 0.31408679485321045\n",
      "Epoch 26 -- Batch 681/ 842, training loss 0.31189632415771484\n",
      "Epoch 26 -- Batch 682/ 842, training loss 0.30838581919670105\n",
      "Epoch 26 -- Batch 683/ 842, training loss 0.3176690340042114\n",
      "Epoch 26 -- Batch 684/ 842, training loss 0.32169705629348755\n",
      "Epoch 26 -- Batch 685/ 842, training loss 0.3162432014942169\n",
      "Epoch 26 -- Batch 686/ 842, training loss 0.31959614157676697\n",
      "Epoch 26 -- Batch 687/ 842, training loss 0.3282279372215271\n",
      "Epoch 26 -- Batch 688/ 842, training loss 0.3035520911216736\n",
      "Epoch 26 -- Batch 689/ 842, training loss 0.30647212266921997\n",
      "Epoch 26 -- Batch 690/ 842, training loss 0.30927130579948425\n",
      "Epoch 26 -- Batch 691/ 842, training loss 0.31521403789520264\n",
      "Epoch 26 -- Batch 692/ 842, training loss 0.3171173334121704\n",
      "Epoch 26 -- Batch 693/ 842, training loss 0.3113778531551361\n",
      "Epoch 26 -- Batch 694/ 842, training loss 0.3086962401866913\n",
      "Epoch 26 -- Batch 695/ 842, training loss 0.3055710792541504\n",
      "Epoch 26 -- Batch 696/ 842, training loss 0.3010440170764923\n",
      "Epoch 26 -- Batch 697/ 842, training loss 0.3122647702693939\n",
      "Epoch 26 -- Batch 698/ 842, training loss 0.311122328042984\n",
      "Epoch 26 -- Batch 699/ 842, training loss 0.32356396317481995\n",
      "Epoch 26 -- Batch 700/ 842, training loss 0.32197248935699463\n",
      "Epoch 26 -- Batch 701/ 842, training loss 0.30461588501930237\n",
      "Epoch 26 -- Batch 702/ 842, training loss 0.31813400983810425\n",
      "Epoch 26 -- Batch 703/ 842, training loss 0.30894994735717773\n",
      "Epoch 26 -- Batch 704/ 842, training loss 0.31775757670402527\n",
      "Epoch 26 -- Batch 705/ 842, training loss 0.3150582015514374\n",
      "Epoch 26 -- Batch 706/ 842, training loss 0.3044290244579315\n",
      "Epoch 26 -- Batch 707/ 842, training loss 0.3162645697593689\n",
      "Epoch 26 -- Batch 708/ 842, training loss 0.3135020434856415\n",
      "Epoch 26 -- Batch 709/ 842, training loss 0.310090035200119\n",
      "Epoch 26 -- Batch 710/ 842, training loss 0.30864307284355164\n",
      "Epoch 26 -- Batch 711/ 842, training loss 0.32086673378944397\n",
      "Epoch 26 -- Batch 712/ 842, training loss 0.3134361803531647\n",
      "Epoch 26 -- Batch 713/ 842, training loss 0.3112691044807434\n",
      "Epoch 26 -- Batch 714/ 842, training loss 0.314755380153656\n",
      "Epoch 26 -- Batch 715/ 842, training loss 0.3098272681236267\n",
      "Epoch 26 -- Batch 716/ 842, training loss 0.31083640456199646\n",
      "Epoch 26 -- Batch 717/ 842, training loss 0.326251357793808\n",
      "Epoch 26 -- Batch 718/ 842, training loss 0.3004305362701416\n",
      "Epoch 26 -- Batch 719/ 842, training loss 0.3188859224319458\n",
      "Epoch 26 -- Batch 720/ 842, training loss 0.31619054079055786\n",
      "Epoch 26 -- Batch 721/ 842, training loss 0.3221515119075775\n",
      "Epoch 26 -- Batch 722/ 842, training loss 0.30634286999702454\n",
      "Epoch 26 -- Batch 723/ 842, training loss 0.31685391068458557\n",
      "Epoch 26 -- Batch 724/ 842, training loss 0.31500253081321716\n",
      "Epoch 26 -- Batch 725/ 842, training loss 0.31909671425819397\n",
      "Epoch 26 -- Batch 726/ 842, training loss 0.31263768672943115\n",
      "Epoch 26 -- Batch 727/ 842, training loss 0.3075735867023468\n",
      "Epoch 26 -- Batch 728/ 842, training loss 0.3151642382144928\n",
      "Epoch 26 -- Batch 729/ 842, training loss 0.30259090662002563\n",
      "Epoch 26 -- Batch 730/ 842, training loss 0.31478384137153625\n",
      "Epoch 26 -- Batch 731/ 842, training loss 0.31140899658203125\n",
      "Epoch 26 -- Batch 732/ 842, training loss 0.30787235498428345\n",
      "Epoch 26 -- Batch 733/ 842, training loss 0.3198283910751343\n",
      "Epoch 26 -- Batch 734/ 842, training loss 0.31749844551086426\n",
      "Epoch 26 -- Batch 735/ 842, training loss 0.3089931011199951\n",
      "Epoch 26 -- Batch 736/ 842, training loss 0.3139133155345917\n",
      "Epoch 26 -- Batch 737/ 842, training loss 0.312171071767807\n",
      "Epoch 26 -- Batch 738/ 842, training loss 0.3227938115596771\n",
      "Epoch 26 -- Batch 739/ 842, training loss 0.3204880952835083\n",
      "Epoch 26 -- Batch 740/ 842, training loss 0.3216306269168854\n",
      "Epoch 26 -- Batch 741/ 842, training loss 0.316812127828598\n",
      "Epoch 26 -- Batch 742/ 842, training loss 0.322151243686676\n",
      "Epoch 26 -- Batch 743/ 842, training loss 0.31269606947898865\n",
      "Epoch 26 -- Batch 744/ 842, training loss 0.3002099096775055\n",
      "Epoch 26 -- Batch 745/ 842, training loss 0.3119416832923889\n",
      "Epoch 26 -- Batch 746/ 842, training loss 0.31493228673934937\n",
      "Epoch 26 -- Batch 747/ 842, training loss 0.31297212839126587\n",
      "Epoch 26 -- Batch 748/ 842, training loss 0.311018705368042\n",
      "Epoch 26 -- Batch 749/ 842, training loss 0.327100932598114\n",
      "Epoch 26 -- Batch 750/ 842, training loss 0.3162766098976135\n",
      "Epoch 26 -- Batch 751/ 842, training loss 0.31197845935821533\n",
      "Epoch 26 -- Batch 752/ 842, training loss 0.31650519371032715\n",
      "Epoch 26 -- Batch 753/ 842, training loss 0.3097509443759918\n",
      "Epoch 26 -- Batch 754/ 842, training loss 0.31078770756721497\n",
      "Epoch 26 -- Batch 755/ 842, training loss 0.31556642055511475\n",
      "Epoch 26 -- Batch 756/ 842, training loss 0.3196413815021515\n",
      "Epoch 26 -- Batch 757/ 842, training loss 0.3274877667427063\n",
      "Epoch 26 -- Batch 758/ 842, training loss 0.3072194755077362\n",
      "Epoch 26 -- Batch 759/ 842, training loss 0.3124881088733673\n",
      "Epoch 26 -- Batch 760/ 842, training loss 0.3100990951061249\n",
      "Epoch 26 -- Batch 761/ 842, training loss 0.32282963395118713\n",
      "Epoch 26 -- Batch 762/ 842, training loss 0.3059804439544678\n",
      "Epoch 26 -- Batch 763/ 842, training loss 0.3182125389575958\n",
      "Epoch 26 -- Batch 764/ 842, training loss 0.32600924372673035\n",
      "Epoch 26 -- Batch 765/ 842, training loss 0.31717294454574585\n",
      "Epoch 26 -- Batch 766/ 842, training loss 0.3052619397640228\n",
      "Epoch 26 -- Batch 767/ 842, training loss 0.3169045150279999\n",
      "Epoch 26 -- Batch 768/ 842, training loss 0.3048436939716339\n",
      "Epoch 26 -- Batch 769/ 842, training loss 0.3191951811313629\n",
      "Epoch 26 -- Batch 770/ 842, training loss 0.31926479935646057\n",
      "Epoch 26 -- Batch 771/ 842, training loss 0.30844923853874207\n",
      "Epoch 26 -- Batch 772/ 842, training loss 0.3052709698677063\n",
      "Epoch 26 -- Batch 773/ 842, training loss 0.3095119893550873\n",
      "Epoch 26 -- Batch 774/ 842, training loss 0.2998696267604828\n",
      "Epoch 26 -- Batch 775/ 842, training loss 0.3147253096103668\n",
      "Epoch 26 -- Batch 776/ 842, training loss 0.3108365535736084\n",
      "Epoch 26 -- Batch 777/ 842, training loss 0.31599748134613037\n",
      "Epoch 26 -- Batch 778/ 842, training loss 0.32129430770874023\n",
      "Epoch 26 -- Batch 779/ 842, training loss 0.31191179156303406\n",
      "Epoch 26 -- Batch 780/ 842, training loss 0.321819543838501\n",
      "Epoch 26 -- Batch 781/ 842, training loss 0.32056236267089844\n",
      "Epoch 26 -- Batch 782/ 842, training loss 0.3154866397380829\n",
      "Epoch 26 -- Batch 783/ 842, training loss 0.3076774775981903\n",
      "Epoch 26 -- Batch 784/ 842, training loss 0.3135493993759155\n",
      "Epoch 26 -- Batch 785/ 842, training loss 0.3159366846084595\n",
      "Epoch 26 -- Batch 786/ 842, training loss 0.3113318383693695\n",
      "Epoch 26 -- Batch 787/ 842, training loss 0.3183102607727051\n",
      "Epoch 26 -- Batch 788/ 842, training loss 0.3127328157424927\n",
      "Epoch 26 -- Batch 789/ 842, training loss 0.32436177134513855\n",
      "Epoch 26 -- Batch 790/ 842, training loss 0.30283430218696594\n",
      "Epoch 26 -- Batch 791/ 842, training loss 0.33056557178497314\n",
      "Epoch 26 -- Batch 792/ 842, training loss 0.3098975718021393\n",
      "Epoch 26 -- Batch 793/ 842, training loss 0.3121775984764099\n",
      "Epoch 26 -- Batch 794/ 842, training loss 0.32574892044067383\n",
      "Epoch 26 -- Batch 795/ 842, training loss 0.31189051270484924\n",
      "Epoch 26 -- Batch 796/ 842, training loss 0.308635950088501\n",
      "Epoch 26 -- Batch 797/ 842, training loss 0.3145665228366852\n",
      "Epoch 26 -- Batch 798/ 842, training loss 0.3289283514022827\n",
      "Epoch 26 -- Batch 799/ 842, training loss 0.3147602081298828\n",
      "Epoch 26 -- Batch 800/ 842, training loss 0.31383970379829407\n",
      "Epoch 26 -- Batch 801/ 842, training loss 0.312787264585495\n",
      "Epoch 26 -- Batch 802/ 842, training loss 0.3098655343055725\n",
      "Epoch 26 -- Batch 803/ 842, training loss 0.32365837693214417\n",
      "Epoch 26 -- Batch 804/ 842, training loss 0.3075445294380188\n",
      "Epoch 26 -- Batch 805/ 842, training loss 0.31571856141090393\n",
      "Epoch 26 -- Batch 806/ 842, training loss 0.3225782513618469\n",
      "Epoch 26 -- Batch 807/ 842, training loss 0.3050890266895294\n",
      "Epoch 26 -- Batch 808/ 842, training loss 0.32504719495773315\n",
      "Epoch 26 -- Batch 809/ 842, training loss 0.3090789020061493\n",
      "Epoch 26 -- Batch 810/ 842, training loss 0.3206767141819\n",
      "Epoch 26 -- Batch 811/ 842, training loss 0.3167247772216797\n",
      "Epoch 26 -- Batch 812/ 842, training loss 0.3176206648349762\n",
      "Epoch 26 -- Batch 813/ 842, training loss 0.3146837651729584\n",
      "Epoch 26 -- Batch 814/ 842, training loss 0.3180573582649231\n",
      "Epoch 26 -- Batch 815/ 842, training loss 0.32047635316848755\n",
      "Epoch 26 -- Batch 816/ 842, training loss 0.3023274838924408\n",
      "Epoch 26 -- Batch 817/ 842, training loss 0.31318774819374084\n",
      "Epoch 26 -- Batch 818/ 842, training loss 0.29402363300323486\n",
      "Epoch 26 -- Batch 819/ 842, training loss 0.3065439760684967\n",
      "Epoch 26 -- Batch 820/ 842, training loss 0.31086286902427673\n",
      "Epoch 26 -- Batch 821/ 842, training loss 0.3198891580104828\n",
      "Epoch 26 -- Batch 822/ 842, training loss 0.3225032687187195\n",
      "Epoch 26 -- Batch 823/ 842, training loss 0.31699007749557495\n",
      "Epoch 26 -- Batch 824/ 842, training loss 0.3158526122570038\n",
      "Epoch 26 -- Batch 825/ 842, training loss 0.3186768591403961\n",
      "Epoch 26 -- Batch 826/ 842, training loss 0.31332927942276\n",
      "Epoch 26 -- Batch 827/ 842, training loss 0.3227996528148651\n",
      "Epoch 26 -- Batch 828/ 842, training loss 0.31487685441970825\n",
      "Epoch 26 -- Batch 829/ 842, training loss 0.3036274015903473\n",
      "Epoch 26 -- Batch 830/ 842, training loss 0.327602744102478\n",
      "Epoch 26 -- Batch 831/ 842, training loss 0.2999468743801117\n",
      "Epoch 26 -- Batch 832/ 842, training loss 0.32271450757980347\n",
      "Epoch 26 -- Batch 833/ 842, training loss 0.3140352666378021\n",
      "Epoch 26 -- Batch 834/ 842, training loss 0.3157341480255127\n",
      "Epoch 26 -- Batch 835/ 842, training loss 0.3208644390106201\n",
      "Epoch 26 -- Batch 836/ 842, training loss 0.32237371802330017\n",
      "Epoch 26 -- Batch 837/ 842, training loss 0.31672948598861694\n",
      "Epoch 26 -- Batch 838/ 842, training loss 0.322396457195282\n",
      "Epoch 26 -- Batch 839/ 842, training loss 0.3253731429576874\n",
      "Epoch 26 -- Batch 840/ 842, training loss 0.3126989006996155\n",
      "Epoch 26 -- Batch 841/ 842, training loss 0.3056333661079407\n",
      "Epoch 26 -- Batch 842/ 842, training loss 0.35730069875717163\n",
      "----------------------------------------------------------------------\n",
      "Epoch 26 -- Batch 1/ 94, validation loss 0.30809876322746277\n",
      "Epoch 26 -- Batch 2/ 94, validation loss 0.3057654798030853\n",
      "Epoch 26 -- Batch 3/ 94, validation loss 0.30809953808784485\n",
      "Epoch 26 -- Batch 4/ 94, validation loss 0.29762449860572815\n",
      "Epoch 26 -- Batch 5/ 94, validation loss 0.30779528617858887\n",
      "Epoch 26 -- Batch 6/ 94, validation loss 0.3043743968009949\n",
      "Epoch 26 -- Batch 7/ 94, validation loss 0.3158275783061981\n",
      "Epoch 26 -- Batch 8/ 94, validation loss 0.29775816202163696\n",
      "Epoch 26 -- Batch 9/ 94, validation loss 0.30709370970726013\n",
      "Epoch 26 -- Batch 10/ 94, validation loss 0.2943691611289978\n",
      "Epoch 26 -- Batch 11/ 94, validation loss 0.3094770908355713\n",
      "Epoch 26 -- Batch 12/ 94, validation loss 0.2992464005947113\n",
      "Epoch 26 -- Batch 13/ 94, validation loss 0.29635006189346313\n",
      "Epoch 26 -- Batch 14/ 94, validation loss 0.2952284812927246\n",
      "Epoch 26 -- Batch 15/ 94, validation loss 0.29688236117362976\n",
      "Epoch 26 -- Batch 16/ 94, validation loss 0.2987813949584961\n",
      "Epoch 26 -- Batch 17/ 94, validation loss 0.30190417170524597\n",
      "Epoch 26 -- Batch 18/ 94, validation loss 0.31142979860305786\n",
      "Epoch 26 -- Batch 19/ 94, validation loss 0.30829599499702454\n",
      "Epoch 26 -- Batch 20/ 94, validation loss 0.3063794672489166\n",
      "Epoch 26 -- Batch 21/ 94, validation loss 0.35163524746894836\n",
      "Epoch 26 -- Batch 22/ 94, validation loss 0.3191395699977875\n",
      "Epoch 26 -- Batch 23/ 94, validation loss 0.30686861276626587\n",
      "Epoch 26 -- Batch 24/ 94, validation loss 0.3086140751838684\n",
      "Epoch 26 -- Batch 25/ 94, validation loss 0.3069126009941101\n",
      "Epoch 26 -- Batch 26/ 94, validation loss 0.3072862923145294\n",
      "Epoch 26 -- Batch 27/ 94, validation loss 0.29605183005332947\n",
      "Epoch 26 -- Batch 28/ 94, validation loss 0.2983276844024658\n",
      "Epoch 26 -- Batch 29/ 94, validation loss 0.2969348132610321\n",
      "Epoch 26 -- Batch 30/ 94, validation loss 0.30307266116142273\n",
      "Epoch 26 -- Batch 31/ 94, validation loss 0.30853310227394104\n",
      "Epoch 26 -- Batch 32/ 94, validation loss 0.2858317494392395\n",
      "Epoch 26 -- Batch 33/ 94, validation loss 0.30194681882858276\n",
      "Epoch 26 -- Batch 34/ 94, validation loss 0.29980313777923584\n",
      "Epoch 26 -- Batch 35/ 94, validation loss 0.30044668912887573\n",
      "Epoch 26 -- Batch 36/ 94, validation loss 0.30672410130500793\n",
      "Epoch 26 -- Batch 37/ 94, validation loss 0.3093584477901459\n",
      "Epoch 26 -- Batch 38/ 94, validation loss 0.29979661107063293\n",
      "Epoch 26 -- Batch 39/ 94, validation loss 0.3006502091884613\n",
      "Epoch 26 -- Batch 40/ 94, validation loss 0.3007403016090393\n",
      "Epoch 26 -- Batch 41/ 94, validation loss 0.29959017038345337\n",
      "Epoch 26 -- Batch 42/ 94, validation loss 0.3093947768211365\n",
      "Epoch 26 -- Batch 43/ 94, validation loss 0.3022628724575043\n",
      "Epoch 26 -- Batch 44/ 94, validation loss 0.30635136365890503\n",
      "Epoch 26 -- Batch 45/ 94, validation loss 0.2985188961029053\n",
      "Epoch 26 -- Batch 46/ 94, validation loss 0.31038904190063477\n",
      "Epoch 26 -- Batch 47/ 94, validation loss 0.3149321377277374\n",
      "Epoch 26 -- Batch 48/ 94, validation loss 0.29191163182258606\n",
      "Epoch 26 -- Batch 49/ 94, validation loss 0.29389306902885437\n",
      "Epoch 26 -- Batch 50/ 94, validation loss 0.2995723783969879\n",
      "Epoch 26 -- Batch 51/ 94, validation loss 0.3026535212993622\n",
      "Epoch 26 -- Batch 52/ 94, validation loss 0.3026811480522156\n",
      "Epoch 26 -- Batch 53/ 94, validation loss 0.30035510659217834\n",
      "Epoch 26 -- Batch 54/ 94, validation loss 0.2969624698162079\n",
      "Epoch 26 -- Batch 55/ 94, validation loss 0.3078151345252991\n",
      "Epoch 26 -- Batch 56/ 94, validation loss 0.3047593832015991\n",
      "Epoch 26 -- Batch 57/ 94, validation loss 0.32447192072868347\n",
      "Epoch 26 -- Batch 58/ 94, validation loss 0.2961985766887665\n",
      "Epoch 26 -- Batch 59/ 94, validation loss 0.29727280139923096\n",
      "Epoch 26 -- Batch 60/ 94, validation loss 0.30325835943222046\n",
      "Epoch 26 -- Batch 61/ 94, validation loss 0.30978041887283325\n",
      "Epoch 26 -- Batch 62/ 94, validation loss 0.31549906730651855\n",
      "Epoch 26 -- Batch 63/ 94, validation loss 0.3176596760749817\n",
      "Epoch 26 -- Batch 64/ 94, validation loss 0.31228718161582947\n",
      "Epoch 26 -- Batch 65/ 94, validation loss 0.29651209712028503\n",
      "Epoch 26 -- Batch 66/ 94, validation loss 0.3085378408432007\n",
      "Epoch 26 -- Batch 67/ 94, validation loss 0.3010646402835846\n",
      "Epoch 26 -- Batch 68/ 94, validation loss 0.29777392745018005\n",
      "Epoch 26 -- Batch 69/ 94, validation loss 0.3018900454044342\n",
      "Epoch 26 -- Batch 70/ 94, validation loss 0.29966768622398376\n",
      "Epoch 26 -- Batch 71/ 94, validation loss 0.295818567276001\n",
      "Epoch 26 -- Batch 72/ 94, validation loss 0.3134078085422516\n",
      "Epoch 26 -- Batch 73/ 94, validation loss 0.3318387567996979\n",
      "Epoch 26 -- Batch 74/ 94, validation loss 0.33331891894340515\n",
      "Epoch 26 -- Batch 75/ 94, validation loss 0.29273301362991333\n",
      "Epoch 26 -- Batch 76/ 94, validation loss 0.3025703430175781\n",
      "Epoch 26 -- Batch 77/ 94, validation loss 0.3018662929534912\n",
      "Epoch 26 -- Batch 78/ 94, validation loss 0.2976931631565094\n",
      "Epoch 26 -- Batch 79/ 94, validation loss 0.29908090829849243\n",
      "Epoch 26 -- Batch 80/ 94, validation loss 0.2934516668319702\n",
      "Epoch 26 -- Batch 81/ 94, validation loss 0.29215890169143677\n",
      "Epoch 26 -- Batch 82/ 94, validation loss 0.32132411003112793\n",
      "Epoch 26 -- Batch 83/ 94, validation loss 0.3097147047519684\n",
      "Epoch 26 -- Batch 84/ 94, validation loss 0.2890514135360718\n",
      "Epoch 26 -- Batch 85/ 94, validation loss 0.29808303713798523\n",
      "Epoch 26 -- Batch 86/ 94, validation loss 0.29421645402908325\n",
      "Epoch 26 -- Batch 87/ 94, validation loss 0.29264599084854126\n",
      "Epoch 26 -- Batch 88/ 94, validation loss 0.30363473296165466\n",
      "Epoch 26 -- Batch 89/ 94, validation loss 0.3059310019016266\n",
      "Epoch 26 -- Batch 90/ 94, validation loss 0.2979021966457367\n",
      "Epoch 26 -- Batch 91/ 94, validation loss 0.3044479787349701\n",
      "Epoch 26 -- Batch 92/ 94, validation loss 0.3042392432689667\n",
      "Epoch 26 -- Batch 93/ 94, validation loss 0.3035770356655121\n",
      "Epoch 26 -- Batch 94/ 94, validation loss 0.30366250872612\n",
      "----------------------------------------------------------------------\n",
      "Epoch 26 loss: Training 0.311617374420166, Validation 0.30366250872612\n",
      "----------------------------------------------------------------------\n",
      "Epoch 27/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 18 21\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 24 25 26\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 9\n",
      "[19:08:12] SMILES Parse Error: extra close parentheses while parsing: CC1(C)Oc2ccc3cccc(=O)[n+]24)cc1Cl\n",
      "[19:08:12] SMILES Parse Error: Failed parsing SMILES 'CC1(C)Oc2ccc3cccc(=O)[n+]24)cc1Cl' for input: 'CC1(C)Oc2ccc3cccc(=O)[n+]24)cc1Cl'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[19:08:12] Explicit valence for atom # 9 C, 5, is greater than permitted\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 17\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 27\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CC1(C)Cc2c(c(N3CCOCC3)nc3sc4c(-c5cccc5c5ccccc55)cnc4c23)CO1'\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CCN(CC)S(=O)(=O)c1ccc(OC)c(N2CCN(C(=O)C34CC5CC(CC(C5)C2)C4)CC2)c1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 12 13 17 18 22\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3 17 18\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 2 3 7 8 9\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CC1CCC2NC(=O)N(c3ccc(Cl)cc3)C(=O)[C@]23C1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 30 31 32\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 24\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2CCc3c(nc(N4CCCCC4C)[nH]c2=O)C2)cc1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 5 7 8 9 27\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CCC(=O)Nc1nc2c(c3c1c(=O)oc1cccc(OC)c1CC)CC2'\n",
      "[19:08:12] SMILES Parse Error: extra open parentheses for input: 'CC/C(=C(/c1ccc(OC)cc1)c1csc(/C=N/NC(=O)CSCC(=O)O)c1C'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 2 3 4 8 21\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 5 6 17 18 19 20 21 22 23 24 25 26 27\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14 15 16 17 18 19 25\n",
      "[19:08:12] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(CNS(=O)(=O)c2cc(N=Nc3c(O)[nH]c3ccccc23)ccc1O'\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CC1CCc2c(sc3nc(CN4CCOCC4)nc(N(CC#N)C4)c23)C1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 19\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'O=C(/C=C/c1c(Cl)cccc1Cl)c1cc2c3c(c1)CCC(=O)N2'\n",
      "[19:08:12] SMILES Parse Error: extra open parentheses for input: 'O=C1CCC[C@@H]2Cc3cc4c(cc3N1CCCN(Cc2ccc(F)cc2)C[C@H]14'\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'O=C(CN(CC1CC1)CC(O)CS2)Nc1ccc(Cl)cc1Cl'\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'Cc1cc2c(c(=O)n1CCN(C)C)C(=O)c1ccccc1Br'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 17 18 19\n",
      "[19:08:12] Explicit valence for atom # 5 Br, 2, is greater than permitted\n",
      "[19:08:12] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1cnc2n1CC[C@H](CC(=O)N1CCN(c3ccccc3)CC1)[C@@H]2C1CC1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 2 3 20\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Nc2ccn3c(n2)cc2c(-c2ccco2)nn3C)cc1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 1 3 4 5 6 7 23 24 25\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'C/C=C(/C)C(=O)OC1CCC2(C)C(=CCC3C2CC2(C)C(C)CCC22)C1'\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2C[C@H](c3ccc(OC)c(O)c3)OC2CCOC(C)C2)cc1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 3 4 20\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 20\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CC1CN(C(=O)C2CC23CCN4C(=O)C2CCCC3)CCN1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 18 19 20 24 25\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CC(=O)OC1CCC2(C)C(CCC32C(=O)NCCCN2CCCN=C2C)C1'\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(F)cc1-c1ccccc1C1SCc1ccccc2F)C1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14 15 16 17 26\n",
      "[19:08:12] non-ring atom 1 marked aromatic\n",
      "[19:08:12] SMILES Parse Error: unclosed ring for input: 'CCc1nc(-c2nnnn3CC2CCCN(CC2CC=CCC2)CC2)ccc1[N+](=O)[O-]'\n",
      "[19:08:12] SMILES Parse Error: extra open parentheses for input: 'CCOC(=O)C(NC(C)=O)(Nc1ccc(S(=O)(=O)Nc2nccs2)cc1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 4 13 14 18 19 24 25 26\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 20 21 22 24 27\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 3 4 5 19 20\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 11 12 13\n",
      "[19:08:12] SMILES Parse Error: extra close parentheses while parsing: O=c1c2cc3c(=O)n(C4CCS(=O)(=O)N4)ccc3[nH]c2=O)oc1\n",
      "[19:08:12] SMILES Parse Error: Failed parsing SMILES 'O=c1c2cc3c(=O)n(C4CCS(=O)(=O)N4)ccc3[nH]c2=O)oc1' for input: 'O=c1c2cc3c(=O)n(C4CCS(=O)(=O)N4)ccc3[nH]c2=O)oc1'\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 9 10 17 18 32 33 34 35 36\n",
      "[19:08:12] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 -- Batch 1/ 842, training loss 0.3072579801082611\n",
      "Epoch 27 -- Batch 2/ 842, training loss 0.2980761229991913\n",
      "Epoch 27 -- Batch 3/ 842, training loss 0.3063768446445465\n",
      "Epoch 27 -- Batch 4/ 842, training loss 0.30846646428108215\n",
      "Epoch 27 -- Batch 5/ 842, training loss 0.31508150696754456\n",
      "Epoch 27 -- Batch 6/ 842, training loss 0.31293633580207825\n",
      "Epoch 27 -- Batch 7/ 842, training loss 0.305373877286911\n",
      "Epoch 27 -- Batch 8/ 842, training loss 0.3042360544204712\n",
      "Epoch 27 -- Batch 9/ 842, training loss 0.3126601576805115\n",
      "Epoch 27 -- Batch 10/ 842, training loss 0.3080982267856598\n",
      "Epoch 27 -- Batch 11/ 842, training loss 0.3055137097835541\n",
      "Epoch 27 -- Batch 12/ 842, training loss 0.3016073405742645\n",
      "Epoch 27 -- Batch 13/ 842, training loss 0.3054662346839905\n",
      "Epoch 27 -- Batch 14/ 842, training loss 0.3084569275379181\n",
      "Epoch 27 -- Batch 15/ 842, training loss 0.3080443739891052\n",
      "Epoch 27 -- Batch 16/ 842, training loss 0.3099311292171478\n",
      "Epoch 27 -- Batch 17/ 842, training loss 0.3039758801460266\n",
      "Epoch 27 -- Batch 18/ 842, training loss 0.3004341721534729\n",
      "Epoch 27 -- Batch 19/ 842, training loss 0.31366100907325745\n",
      "Epoch 27 -- Batch 20/ 842, training loss 0.30489581823349\n",
      "Epoch 27 -- Batch 21/ 842, training loss 0.3180210590362549\n",
      "Epoch 27 -- Batch 22/ 842, training loss 0.31311431527137756\n",
      "Epoch 27 -- Batch 23/ 842, training loss 0.2958511710166931\n",
      "Epoch 27 -- Batch 24/ 842, training loss 0.3059675395488739\n",
      "Epoch 27 -- Batch 25/ 842, training loss 0.3223048448562622\n",
      "Epoch 27 -- Batch 26/ 842, training loss 0.3108995854854584\n",
      "Epoch 27 -- Batch 27/ 842, training loss 0.3064950108528137\n",
      "Epoch 27 -- Batch 28/ 842, training loss 0.30615559220314026\n",
      "Epoch 27 -- Batch 29/ 842, training loss 0.3022076487541199\n",
      "Epoch 27 -- Batch 30/ 842, training loss 0.30186304450035095\n",
      "Epoch 27 -- Batch 31/ 842, training loss 0.3061807453632355\n",
      "Epoch 27 -- Batch 32/ 842, training loss 0.31116998195648193\n",
      "Epoch 27 -- Batch 33/ 842, training loss 0.31232935190200806\n",
      "Epoch 27 -- Batch 34/ 842, training loss 0.30162179470062256\n",
      "Epoch 27 -- Batch 35/ 842, training loss 0.31650274991989136\n",
      "Epoch 27 -- Batch 36/ 842, training loss 0.3171650469303131\n",
      "Epoch 27 -- Batch 37/ 842, training loss 0.2984687089920044\n",
      "Epoch 27 -- Batch 38/ 842, training loss 0.3099946677684784\n",
      "Epoch 27 -- Batch 39/ 842, training loss 0.29635903239250183\n",
      "Epoch 27 -- Batch 40/ 842, training loss 0.306772381067276\n",
      "Epoch 27 -- Batch 41/ 842, training loss 0.30380311608314514\n",
      "Epoch 27 -- Batch 42/ 842, training loss 0.3047405183315277\n",
      "Epoch 27 -- Batch 43/ 842, training loss 0.30756688117980957\n",
      "Epoch 27 -- Batch 44/ 842, training loss 0.30563485622406006\n",
      "Epoch 27 -- Batch 45/ 842, training loss 0.3087042272090912\n",
      "Epoch 27 -- Batch 46/ 842, training loss 0.31076496839523315\n",
      "Epoch 27 -- Batch 47/ 842, training loss 0.3028867244720459\n",
      "Epoch 27 -- Batch 48/ 842, training loss 0.2976660430431366\n",
      "Epoch 27 -- Batch 49/ 842, training loss 0.304807186126709\n",
      "Epoch 27 -- Batch 50/ 842, training loss 0.3140025734901428\n",
      "Epoch 27 -- Batch 51/ 842, training loss 0.3132701814174652\n",
      "Epoch 27 -- Batch 52/ 842, training loss 0.29804837703704834\n",
      "Epoch 27 -- Batch 53/ 842, training loss 0.30806922912597656\n",
      "Epoch 27 -- Batch 54/ 842, training loss 0.3003690838813782\n",
      "Epoch 27 -- Batch 55/ 842, training loss 0.30913200974464417\n",
      "Epoch 27 -- Batch 56/ 842, training loss 0.2919718325138092\n",
      "Epoch 27 -- Batch 57/ 842, training loss 0.3043317198753357\n",
      "Epoch 27 -- Batch 58/ 842, training loss 0.30392134189605713\n",
      "Epoch 27 -- Batch 59/ 842, training loss 0.30562105774879456\n",
      "Epoch 27 -- Batch 60/ 842, training loss 0.3055464029312134\n",
      "Epoch 27 -- Batch 61/ 842, training loss 0.310791552066803\n",
      "Epoch 27 -- Batch 62/ 842, training loss 0.3043438792228699\n",
      "Epoch 27 -- Batch 63/ 842, training loss 0.3069257438182831\n",
      "Epoch 27 -- Batch 64/ 842, training loss 0.3102705776691437\n",
      "Epoch 27 -- Batch 65/ 842, training loss 0.3127386271953583\n",
      "Epoch 27 -- Batch 66/ 842, training loss 0.3064076900482178\n",
      "Epoch 27 -- Batch 67/ 842, training loss 0.3013549745082855\n",
      "Epoch 27 -- Batch 68/ 842, training loss 0.3053697347640991\n",
      "Epoch 27 -- Batch 69/ 842, training loss 0.3000224828720093\n",
      "Epoch 27 -- Batch 70/ 842, training loss 0.30359846353530884\n",
      "Epoch 27 -- Batch 71/ 842, training loss 0.30718228220939636\n",
      "Epoch 27 -- Batch 72/ 842, training loss 0.30260202288627625\n",
      "Epoch 27 -- Batch 73/ 842, training loss 0.30961382389068604\n",
      "Epoch 27 -- Batch 74/ 842, training loss 0.30671456456184387\n",
      "Epoch 27 -- Batch 75/ 842, training loss 0.3117665946483612\n",
      "Epoch 27 -- Batch 76/ 842, training loss 0.3100219964981079\n",
      "Epoch 27 -- Batch 77/ 842, training loss 0.3057626187801361\n",
      "Epoch 27 -- Batch 78/ 842, training loss 0.3117237091064453\n",
      "Epoch 27 -- Batch 79/ 842, training loss 0.30740097165107727\n",
      "Epoch 27 -- Batch 80/ 842, training loss 0.3079163730144501\n",
      "Epoch 27 -- Batch 81/ 842, training loss 0.3109046518802643\n",
      "Epoch 27 -- Batch 82/ 842, training loss 0.30679023265838623\n",
      "Epoch 27 -- Batch 83/ 842, training loss 0.3117007613182068\n",
      "Epoch 27 -- Batch 84/ 842, training loss 0.3097463846206665\n",
      "Epoch 27 -- Batch 85/ 842, training loss 0.31711170077323914\n",
      "Epoch 27 -- Batch 86/ 842, training loss 0.3219488859176636\n",
      "Epoch 27 -- Batch 87/ 842, training loss 0.29757845401763916\n",
      "Epoch 27 -- Batch 88/ 842, training loss 0.30981871485710144\n",
      "Epoch 27 -- Batch 89/ 842, training loss 0.30875179171562195\n",
      "Epoch 27 -- Batch 90/ 842, training loss 0.2984793782234192\n",
      "Epoch 27 -- Batch 91/ 842, training loss 0.30679818987846375\n",
      "Epoch 27 -- Batch 92/ 842, training loss 0.313521146774292\n",
      "Epoch 27 -- Batch 93/ 842, training loss 0.3100796937942505\n",
      "Epoch 27 -- Batch 94/ 842, training loss 0.30513057112693787\n",
      "Epoch 27 -- Batch 95/ 842, training loss 0.30424070358276367\n",
      "Epoch 27 -- Batch 96/ 842, training loss 0.31635159254074097\n",
      "Epoch 27 -- Batch 97/ 842, training loss 0.30444061756134033\n",
      "Epoch 27 -- Batch 98/ 842, training loss 0.2987118661403656\n",
      "Epoch 27 -- Batch 99/ 842, training loss 0.2989440858364105\n",
      "Epoch 27 -- Batch 100/ 842, training loss 0.299932062625885\n",
      "Epoch 27 -- Batch 101/ 842, training loss 0.2932911515235901\n",
      "Epoch 27 -- Batch 102/ 842, training loss 0.2980252802371979\n",
      "Epoch 27 -- Batch 103/ 842, training loss 0.3054924011230469\n",
      "Epoch 27 -- Batch 104/ 842, training loss 0.3141632378101349\n",
      "Epoch 27 -- Batch 105/ 842, training loss 0.2979392409324646\n",
      "Epoch 27 -- Batch 106/ 842, training loss 0.3061983287334442\n",
      "Epoch 27 -- Batch 107/ 842, training loss 0.30062010884284973\n",
      "Epoch 27 -- Batch 108/ 842, training loss 0.30301955342292786\n",
      "Epoch 27 -- Batch 109/ 842, training loss 0.3012523949146271\n",
      "Epoch 27 -- Batch 110/ 842, training loss 0.30177730321884155\n",
      "Epoch 27 -- Batch 111/ 842, training loss 0.3019120693206787\n",
      "Epoch 27 -- Batch 112/ 842, training loss 0.3087584376335144\n",
      "Epoch 27 -- Batch 113/ 842, training loss 0.3158469796180725\n",
      "Epoch 27 -- Batch 114/ 842, training loss 0.3046683669090271\n",
      "Epoch 27 -- Batch 115/ 842, training loss 0.3203575611114502\n",
      "Epoch 27 -- Batch 116/ 842, training loss 0.3064422011375427\n",
      "Epoch 27 -- Batch 117/ 842, training loss 0.3138928711414337\n",
      "Epoch 27 -- Batch 118/ 842, training loss 0.31153175234794617\n",
      "Epoch 27 -- Batch 119/ 842, training loss 0.31418266892433167\n",
      "Epoch 27 -- Batch 120/ 842, training loss 0.31005823612213135\n",
      "Epoch 27 -- Batch 121/ 842, training loss 0.30149874091148376\n",
      "Epoch 27 -- Batch 122/ 842, training loss 0.3024900555610657\n",
      "Epoch 27 -- Batch 123/ 842, training loss 0.3281504809856415\n",
      "Epoch 27 -- Batch 124/ 842, training loss 0.311026394367218\n",
      "Epoch 27 -- Batch 125/ 842, training loss 0.3155447542667389\n",
      "Epoch 27 -- Batch 126/ 842, training loss 0.3066803812980652\n",
      "Epoch 27 -- Batch 127/ 842, training loss 0.29512691497802734\n",
      "Epoch 27 -- Batch 128/ 842, training loss 0.31044918298721313\n",
      "Epoch 27 -- Batch 129/ 842, training loss 0.3077513873577118\n",
      "Epoch 27 -- Batch 130/ 842, training loss 0.3112346827983856\n",
      "Epoch 27 -- Batch 131/ 842, training loss 0.30013105273246765\n",
      "Epoch 27 -- Batch 132/ 842, training loss 0.31583237648010254\n",
      "Epoch 27 -- Batch 133/ 842, training loss 0.31195583939552307\n",
      "Epoch 27 -- Batch 134/ 842, training loss 0.3204023540019989\n",
      "Epoch 27 -- Batch 135/ 842, training loss 0.307748019695282\n",
      "Epoch 27 -- Batch 136/ 842, training loss 0.2913483679294586\n",
      "Epoch 27 -- Batch 137/ 842, training loss 0.30146118998527527\n",
      "Epoch 27 -- Batch 138/ 842, training loss 0.3149033188819885\n",
      "Epoch 27 -- Batch 139/ 842, training loss 0.3117039203643799\n",
      "Epoch 27 -- Batch 140/ 842, training loss 0.3110320568084717\n",
      "Epoch 27 -- Batch 141/ 842, training loss 0.3126193881034851\n",
      "Epoch 27 -- Batch 142/ 842, training loss 0.30733999609947205\n",
      "Epoch 27 -- Batch 143/ 842, training loss 0.3128203749656677\n",
      "Epoch 27 -- Batch 144/ 842, training loss 0.3171336054801941\n",
      "Epoch 27 -- Batch 145/ 842, training loss 0.3086320757865906\n",
      "Epoch 27 -- Batch 146/ 842, training loss 0.3142125606536865\n",
      "Epoch 27 -- Batch 147/ 842, training loss 0.315406858921051\n",
      "Epoch 27 -- Batch 148/ 842, training loss 0.31217288970947266\n",
      "Epoch 27 -- Batch 149/ 842, training loss 0.3030816614627838\n",
      "Epoch 27 -- Batch 150/ 842, training loss 0.30778270959854126\n",
      "Epoch 27 -- Batch 151/ 842, training loss 0.3064650595188141\n",
      "Epoch 27 -- Batch 152/ 842, training loss 0.3079550266265869\n",
      "Epoch 27 -- Batch 153/ 842, training loss 0.3168227970600128\n",
      "Epoch 27 -- Batch 154/ 842, training loss 0.3058238923549652\n",
      "Epoch 27 -- Batch 155/ 842, training loss 0.31303659081459045\n",
      "Epoch 27 -- Batch 156/ 842, training loss 0.3287283480167389\n",
      "Epoch 27 -- Batch 157/ 842, training loss 0.3118305206298828\n",
      "Epoch 27 -- Batch 158/ 842, training loss 0.30109187960624695\n",
      "Epoch 27 -- Batch 159/ 842, training loss 0.29942458868026733\n",
      "Epoch 27 -- Batch 160/ 842, training loss 0.294726699590683\n",
      "Epoch 27 -- Batch 161/ 842, training loss 0.31297382712364197\n",
      "Epoch 27 -- Batch 162/ 842, training loss 0.3213556706905365\n",
      "Epoch 27 -- Batch 163/ 842, training loss 0.3038302958011627\n",
      "Epoch 27 -- Batch 164/ 842, training loss 0.3210586905479431\n",
      "Epoch 27 -- Batch 165/ 842, training loss 0.30864277482032776\n",
      "Epoch 27 -- Batch 166/ 842, training loss 0.31410670280456543\n",
      "Epoch 27 -- Batch 167/ 842, training loss 0.3013794422149658\n",
      "Epoch 27 -- Batch 168/ 842, training loss 0.31404659152030945\n",
      "Epoch 27 -- Batch 169/ 842, training loss 0.29954060912132263\n",
      "Epoch 27 -- Batch 170/ 842, training loss 0.3044651746749878\n",
      "Epoch 27 -- Batch 171/ 842, training loss 0.30303195118904114\n",
      "Epoch 27 -- Batch 172/ 842, training loss 0.3067486882209778\n",
      "Epoch 27 -- Batch 173/ 842, training loss 0.3057701885700226\n",
      "Epoch 27 -- Batch 174/ 842, training loss 0.30535802245140076\n",
      "Epoch 27 -- Batch 175/ 842, training loss 0.3038424551486969\n",
      "Epoch 27 -- Batch 176/ 842, training loss 0.3051745891571045\n",
      "Epoch 27 -- Batch 177/ 842, training loss 0.29765933752059937\n",
      "Epoch 27 -- Batch 178/ 842, training loss 0.29874494671821594\n",
      "Epoch 27 -- Batch 179/ 842, training loss 0.31261929869651794\n",
      "Epoch 27 -- Batch 180/ 842, training loss 0.30431991815567017\n",
      "Epoch 27 -- Batch 181/ 842, training loss 0.3150872588157654\n",
      "Epoch 27 -- Batch 182/ 842, training loss 0.30490487813949585\n",
      "Epoch 27 -- Batch 183/ 842, training loss 0.3013265132904053\n",
      "Epoch 27 -- Batch 184/ 842, training loss 0.30458223819732666\n",
      "Epoch 27 -- Batch 185/ 842, training loss 0.3020297586917877\n",
      "Epoch 27 -- Batch 186/ 842, training loss 0.31868764758110046\n",
      "Epoch 27 -- Batch 187/ 842, training loss 0.30473563075065613\n",
      "Epoch 27 -- Batch 188/ 842, training loss 0.3086964786052704\n",
      "Epoch 27 -- Batch 189/ 842, training loss 0.311215877532959\n",
      "Epoch 27 -- Batch 190/ 842, training loss 0.31209999322891235\n",
      "Epoch 27 -- Batch 191/ 842, training loss 0.3105030953884125\n",
      "Epoch 27 -- Batch 192/ 842, training loss 0.3090880811214447\n",
      "Epoch 27 -- Batch 193/ 842, training loss 0.30897265672683716\n",
      "Epoch 27 -- Batch 194/ 842, training loss 0.3114985525608063\n",
      "Epoch 27 -- Batch 195/ 842, training loss 0.31432852149009705\n",
      "Epoch 27 -- Batch 196/ 842, training loss 0.3123508393764496\n",
      "Epoch 27 -- Batch 197/ 842, training loss 0.305001437664032\n",
      "Epoch 27 -- Batch 198/ 842, training loss 0.3149656355381012\n",
      "Epoch 27 -- Batch 199/ 842, training loss 0.3123784065246582\n",
      "Epoch 27 -- Batch 200/ 842, training loss 0.3082132637500763\n",
      "Epoch 27 -- Batch 201/ 842, training loss 0.3122497797012329\n",
      "Epoch 27 -- Batch 202/ 842, training loss 0.30948588252067566\n",
      "Epoch 27 -- Batch 203/ 842, training loss 0.31291937828063965\n",
      "Epoch 27 -- Batch 204/ 842, training loss 0.3110318183898926\n",
      "Epoch 27 -- Batch 205/ 842, training loss 0.3082433342933655\n",
      "Epoch 27 -- Batch 206/ 842, training loss 0.3139551877975464\n",
      "Epoch 27 -- Batch 207/ 842, training loss 0.31041356921195984\n",
      "Epoch 27 -- Batch 208/ 842, training loss 0.30115601420402527\n",
      "Epoch 27 -- Batch 209/ 842, training loss 0.3117075264453888\n",
      "Epoch 27 -- Batch 210/ 842, training loss 0.30490514636039734\n",
      "Epoch 27 -- Batch 211/ 842, training loss 0.29125064611434937\n",
      "Epoch 27 -- Batch 212/ 842, training loss 0.3109162747859955\n",
      "Epoch 27 -- Batch 213/ 842, training loss 0.3002609610557556\n",
      "Epoch 27 -- Batch 214/ 842, training loss 0.3087921440601349\n",
      "Epoch 27 -- Batch 215/ 842, training loss 0.3105558156967163\n",
      "Epoch 27 -- Batch 216/ 842, training loss 0.3046724200248718\n",
      "Epoch 27 -- Batch 217/ 842, training loss 0.30386415123939514\n",
      "Epoch 27 -- Batch 218/ 842, training loss 0.3026808798313141\n",
      "Epoch 27 -- Batch 219/ 842, training loss 0.3054980933666229\n",
      "Epoch 27 -- Batch 220/ 842, training loss 0.31341585516929626\n",
      "Epoch 27 -- Batch 221/ 842, training loss 0.309905469417572\n",
      "Epoch 27 -- Batch 222/ 842, training loss 0.31406161189079285\n",
      "Epoch 27 -- Batch 223/ 842, training loss 0.2952605187892914\n",
      "Epoch 27 -- Batch 224/ 842, training loss 0.3029305934906006\n",
      "Epoch 27 -- Batch 225/ 842, training loss 0.3062182664871216\n",
      "Epoch 27 -- Batch 226/ 842, training loss 0.31436237692832947\n",
      "Epoch 27 -- Batch 227/ 842, training loss 0.3125306963920593\n",
      "Epoch 27 -- Batch 228/ 842, training loss 0.3117181360721588\n",
      "Epoch 27 -- Batch 229/ 842, training loss 0.30258429050445557\n",
      "Epoch 27 -- Batch 230/ 842, training loss 0.3095448911190033\n",
      "Epoch 27 -- Batch 231/ 842, training loss 0.31049373745918274\n",
      "Epoch 27 -- Batch 232/ 842, training loss 0.31041768193244934\n",
      "Epoch 27 -- Batch 233/ 842, training loss 0.301876425743103\n",
      "Epoch 27 -- Batch 234/ 842, training loss 0.3051888048648834\n",
      "Epoch 27 -- Batch 235/ 842, training loss 0.31322234869003296\n",
      "Epoch 27 -- Batch 236/ 842, training loss 0.311266154050827\n",
      "Epoch 27 -- Batch 237/ 842, training loss 0.31075477600097656\n",
      "Epoch 27 -- Batch 238/ 842, training loss 0.30392715334892273\n",
      "Epoch 27 -- Batch 239/ 842, training loss 0.3171485364437103\n",
      "Epoch 27 -- Batch 240/ 842, training loss 0.316729873418808\n",
      "Epoch 27 -- Batch 241/ 842, training loss 0.29107216000556946\n",
      "Epoch 27 -- Batch 242/ 842, training loss 0.304823100566864\n",
      "Epoch 27 -- Batch 243/ 842, training loss 0.3112504780292511\n",
      "Epoch 27 -- Batch 244/ 842, training loss 0.30743077397346497\n",
      "Epoch 27 -- Batch 245/ 842, training loss 0.30587002635002136\n",
      "Epoch 27 -- Batch 246/ 842, training loss 0.3072892725467682\n",
      "Epoch 27 -- Batch 247/ 842, training loss 0.3111743927001953\n",
      "Epoch 27 -- Batch 248/ 842, training loss 0.31313851475715637\n",
      "Epoch 27 -- Batch 249/ 842, training loss 0.30089497566223145\n",
      "Epoch 27 -- Batch 250/ 842, training loss 0.30795449018478394\n",
      "Epoch 27 -- Batch 251/ 842, training loss 0.31196096539497375\n",
      "Epoch 27 -- Batch 252/ 842, training loss 0.29994556307792664\n",
      "Epoch 27 -- Batch 253/ 842, training loss 0.3122117519378662\n",
      "Epoch 27 -- Batch 254/ 842, training loss 0.30536630749702454\n",
      "Epoch 27 -- Batch 255/ 842, training loss 0.31756603717803955\n",
      "Epoch 27 -- Batch 256/ 842, training loss 0.33417972922325134\n",
      "Epoch 27 -- Batch 257/ 842, training loss 0.30416375398635864\n",
      "Epoch 27 -- Batch 258/ 842, training loss 0.313223272562027\n",
      "Epoch 27 -- Batch 259/ 842, training loss 0.30637994408607483\n",
      "Epoch 27 -- Batch 260/ 842, training loss 0.3055371940135956\n",
      "Epoch 27 -- Batch 261/ 842, training loss 0.3083980977535248\n",
      "Epoch 27 -- Batch 262/ 842, training loss 0.30605995655059814\n",
      "Epoch 27 -- Batch 263/ 842, training loss 0.3120805323123932\n",
      "Epoch 27 -- Batch 264/ 842, training loss 0.30952686071395874\n",
      "Epoch 27 -- Batch 265/ 842, training loss 0.3129349648952484\n",
      "Epoch 27 -- Batch 266/ 842, training loss 0.3032257854938507\n",
      "Epoch 27 -- Batch 267/ 842, training loss 0.3104020953178406\n",
      "Epoch 27 -- Batch 268/ 842, training loss 0.3116413950920105\n",
      "Epoch 27 -- Batch 269/ 842, training loss 0.3182857930660248\n",
      "Epoch 27 -- Batch 270/ 842, training loss 0.31326228380203247\n",
      "Epoch 27 -- Batch 271/ 842, training loss 0.2983754277229309\n",
      "Epoch 27 -- Batch 272/ 842, training loss 0.3053637742996216\n",
      "Epoch 27 -- Batch 273/ 842, training loss 0.3111753463745117\n",
      "Epoch 27 -- Batch 274/ 842, training loss 0.3109106123447418\n",
      "Epoch 27 -- Batch 275/ 842, training loss 0.31222739815711975\n",
      "Epoch 27 -- Batch 276/ 842, training loss 0.3173392415046692\n",
      "Epoch 27 -- Batch 277/ 842, training loss 0.3118279278278351\n",
      "Epoch 27 -- Batch 278/ 842, training loss 0.31697919964790344\n",
      "Epoch 27 -- Batch 279/ 842, training loss 0.3069070875644684\n",
      "Epoch 27 -- Batch 280/ 842, training loss 0.30946749448776245\n",
      "Epoch 27 -- Batch 281/ 842, training loss 0.311960369348526\n",
      "Epoch 27 -- Batch 282/ 842, training loss 0.3107554316520691\n",
      "Epoch 27 -- Batch 283/ 842, training loss 0.30535364151000977\n",
      "Epoch 27 -- Batch 284/ 842, training loss 0.3177448809146881\n",
      "Epoch 27 -- Batch 285/ 842, training loss 0.31489548087120056\n",
      "Epoch 27 -- Batch 286/ 842, training loss 0.31019967794418335\n",
      "Epoch 27 -- Batch 287/ 842, training loss 0.30868735909461975\n",
      "Epoch 27 -- Batch 288/ 842, training loss 0.3079605996608734\n",
      "Epoch 27 -- Batch 289/ 842, training loss 0.3170260787010193\n",
      "Epoch 27 -- Batch 290/ 842, training loss 0.31687039136886597\n",
      "Epoch 27 -- Batch 291/ 842, training loss 0.31574466824531555\n",
      "Epoch 27 -- Batch 292/ 842, training loss 0.3234759569168091\n",
      "Epoch 27 -- Batch 293/ 842, training loss 0.3199154734611511\n",
      "Epoch 27 -- Batch 294/ 842, training loss 0.3073470890522003\n",
      "Epoch 27 -- Batch 295/ 842, training loss 0.3099132180213928\n",
      "Epoch 27 -- Batch 296/ 842, training loss 0.30725106596946716\n",
      "Epoch 27 -- Batch 297/ 842, training loss 0.3046521544456482\n",
      "Epoch 27 -- Batch 298/ 842, training loss 0.3013651967048645\n",
      "Epoch 27 -- Batch 299/ 842, training loss 0.3162938952445984\n",
      "Epoch 27 -- Batch 300/ 842, training loss 0.3127632439136505\n",
      "Epoch 27 -- Batch 301/ 842, training loss 0.3092758357524872\n",
      "Epoch 27 -- Batch 302/ 842, training loss 0.3050609529018402\n",
      "Epoch 27 -- Batch 303/ 842, training loss 0.2999242842197418\n",
      "Epoch 27 -- Batch 304/ 842, training loss 0.309981107711792\n",
      "Epoch 27 -- Batch 305/ 842, training loss 0.30369505286216736\n",
      "Epoch 27 -- Batch 306/ 842, training loss 0.31061333417892456\n",
      "Epoch 27 -- Batch 307/ 842, training loss 0.30238214135169983\n",
      "Epoch 27 -- Batch 308/ 842, training loss 0.3081733286380768\n",
      "Epoch 27 -- Batch 309/ 842, training loss 0.3065849542617798\n",
      "Epoch 27 -- Batch 310/ 842, training loss 0.30823037028312683\n",
      "Epoch 27 -- Batch 311/ 842, training loss 0.3031485080718994\n",
      "Epoch 27 -- Batch 312/ 842, training loss 0.29727211594581604\n",
      "Epoch 27 -- Batch 313/ 842, training loss 0.3067823052406311\n",
      "Epoch 27 -- Batch 314/ 842, training loss 0.30498576164245605\n",
      "Epoch 27 -- Batch 315/ 842, training loss 0.3105049133300781\n",
      "Epoch 27 -- Batch 316/ 842, training loss 0.3005838692188263\n",
      "Epoch 27 -- Batch 317/ 842, training loss 0.3070078492164612\n",
      "Epoch 27 -- Batch 318/ 842, training loss 0.31634899973869324\n",
      "Epoch 27 -- Batch 319/ 842, training loss 0.3140421509742737\n",
      "Epoch 27 -- Batch 320/ 842, training loss 0.3009365499019623\n",
      "Epoch 27 -- Batch 321/ 842, training loss 0.29865145683288574\n",
      "Epoch 27 -- Batch 322/ 842, training loss 0.30626538395881653\n",
      "Epoch 27 -- Batch 323/ 842, training loss 0.31549111008644104\n",
      "Epoch 27 -- Batch 324/ 842, training loss 0.31220149993896484\n",
      "Epoch 27 -- Batch 325/ 842, training loss 0.305641233921051\n",
      "Epoch 27 -- Batch 326/ 842, training loss 0.2898862659931183\n",
      "Epoch 27 -- Batch 327/ 842, training loss 0.3076827824115753\n",
      "Epoch 27 -- Batch 328/ 842, training loss 0.29924461245536804\n",
      "Epoch 27 -- Batch 329/ 842, training loss 0.3143250346183777\n",
      "Epoch 27 -- Batch 330/ 842, training loss 0.31060975790023804\n",
      "Epoch 27 -- Batch 331/ 842, training loss 0.3167363107204437\n",
      "Epoch 27 -- Batch 332/ 842, training loss 0.30979618430137634\n",
      "Epoch 27 -- Batch 333/ 842, training loss 0.3077149987220764\n",
      "Epoch 27 -- Batch 334/ 842, training loss 0.29818353056907654\n",
      "Epoch 27 -- Batch 335/ 842, training loss 0.30661678314208984\n",
      "Epoch 27 -- Batch 336/ 842, training loss 0.3168952763080597\n",
      "Epoch 27 -- Batch 337/ 842, training loss 0.30708858370780945\n",
      "Epoch 27 -- Batch 338/ 842, training loss 0.30910930037498474\n",
      "Epoch 27 -- Batch 339/ 842, training loss 0.29980799555778503\n",
      "Epoch 27 -- Batch 340/ 842, training loss 0.3021838963031769\n",
      "Epoch 27 -- Batch 341/ 842, training loss 0.29703179001808167\n",
      "Epoch 27 -- Batch 342/ 842, training loss 0.3037102222442627\n",
      "Epoch 27 -- Batch 343/ 842, training loss 0.31935349106788635\n",
      "Epoch 27 -- Batch 344/ 842, training loss 0.3200405538082123\n",
      "Epoch 27 -- Batch 345/ 842, training loss 0.3039992153644562\n",
      "Epoch 27 -- Batch 346/ 842, training loss 0.3082921504974365\n",
      "Epoch 27 -- Batch 347/ 842, training loss 0.3153342306613922\n",
      "Epoch 27 -- Batch 348/ 842, training loss 0.31555041670799255\n",
      "Epoch 27 -- Batch 349/ 842, training loss 0.30349454283714294\n",
      "Epoch 27 -- Batch 350/ 842, training loss 0.31487637758255005\n",
      "Epoch 27 -- Batch 351/ 842, training loss 0.3138626217842102\n",
      "Epoch 27 -- Batch 352/ 842, training loss 0.308240681886673\n",
      "Epoch 27 -- Batch 353/ 842, training loss 0.3121240735054016\n",
      "Epoch 27 -- Batch 354/ 842, training loss 0.3095318377017975\n",
      "Epoch 27 -- Batch 355/ 842, training loss 0.3074530363082886\n",
      "Epoch 27 -- Batch 356/ 842, training loss 0.30856865644454956\n",
      "Epoch 27 -- Batch 357/ 842, training loss 0.3186456859111786\n",
      "Epoch 27 -- Batch 358/ 842, training loss 0.3084818124771118\n",
      "Epoch 27 -- Batch 359/ 842, training loss 0.3094039857387543\n",
      "Epoch 27 -- Batch 360/ 842, training loss 0.301329106092453\n",
      "Epoch 27 -- Batch 361/ 842, training loss 0.32103848457336426\n",
      "Epoch 27 -- Batch 362/ 842, training loss 0.29494455456733704\n",
      "Epoch 27 -- Batch 363/ 842, training loss 0.30521395802497864\n",
      "Epoch 27 -- Batch 364/ 842, training loss 0.3111177682876587\n",
      "Epoch 27 -- Batch 365/ 842, training loss 0.30647891759872437\n",
      "Epoch 27 -- Batch 366/ 842, training loss 0.3130609691143036\n",
      "Epoch 27 -- Batch 367/ 842, training loss 0.3072041869163513\n",
      "Epoch 27 -- Batch 368/ 842, training loss 0.3088843822479248\n",
      "Epoch 27 -- Batch 369/ 842, training loss 0.30393460392951965\n",
      "Epoch 27 -- Batch 370/ 842, training loss 0.31047767400741577\n",
      "Epoch 27 -- Batch 371/ 842, training loss 0.31207728385925293\n",
      "Epoch 27 -- Batch 372/ 842, training loss 0.30563345551490784\n",
      "Epoch 27 -- Batch 373/ 842, training loss 0.30413681268692017\n",
      "Epoch 27 -- Batch 374/ 842, training loss 0.31057751178741455\n",
      "Epoch 27 -- Batch 375/ 842, training loss 0.29808279871940613\n",
      "Epoch 27 -- Batch 376/ 842, training loss 0.3217756748199463\n",
      "Epoch 27 -- Batch 377/ 842, training loss 0.3179866373538971\n",
      "Epoch 27 -- Batch 378/ 842, training loss 0.29950863122940063\n",
      "Epoch 27 -- Batch 379/ 842, training loss 0.3091808259487152\n",
      "Epoch 27 -- Batch 380/ 842, training loss 0.2955186367034912\n",
      "Epoch 27 -- Batch 381/ 842, training loss 0.32072219252586365\n",
      "Epoch 27 -- Batch 382/ 842, training loss 0.3205845057964325\n",
      "Epoch 27 -- Batch 383/ 842, training loss 0.3100639581680298\n",
      "Epoch 27 -- Batch 384/ 842, training loss 0.32461968064308167\n",
      "Epoch 27 -- Batch 385/ 842, training loss 0.3185921907424927\n",
      "Epoch 27 -- Batch 386/ 842, training loss 0.30475476384162903\n",
      "Epoch 27 -- Batch 387/ 842, training loss 0.3100191056728363\n",
      "Epoch 27 -- Batch 388/ 842, training loss 0.30821049213409424\n",
      "Epoch 27 -- Batch 389/ 842, training loss 0.31985533237457275\n",
      "Epoch 27 -- Batch 390/ 842, training loss 0.30367013812065125\n",
      "Epoch 27 -- Batch 391/ 842, training loss 0.312992125749588\n",
      "Epoch 27 -- Batch 392/ 842, training loss 0.3193944990634918\n",
      "Epoch 27 -- Batch 393/ 842, training loss 0.31545019149780273\n",
      "Epoch 27 -- Batch 394/ 842, training loss 0.3109065592288971\n",
      "Epoch 27 -- Batch 395/ 842, training loss 0.31475216150283813\n",
      "Epoch 27 -- Batch 396/ 842, training loss 0.3066621422767639\n",
      "Epoch 27 -- Batch 397/ 842, training loss 0.3139299154281616\n",
      "Epoch 27 -- Batch 398/ 842, training loss 0.31685173511505127\n",
      "Epoch 27 -- Batch 399/ 842, training loss 0.30189797282218933\n",
      "Epoch 27 -- Batch 400/ 842, training loss 0.3071579039096832\n",
      "Epoch 27 -- Batch 401/ 842, training loss 0.3209213614463806\n",
      "Epoch 27 -- Batch 402/ 842, training loss 0.2963595390319824\n",
      "Epoch 27 -- Batch 403/ 842, training loss 0.31248581409454346\n",
      "Epoch 27 -- Batch 404/ 842, training loss 0.3147139251232147\n",
      "Epoch 27 -- Batch 405/ 842, training loss 0.3204450309276581\n",
      "Epoch 27 -- Batch 406/ 842, training loss 0.31718483567237854\n",
      "Epoch 27 -- Batch 407/ 842, training loss 0.30191290378570557\n",
      "Epoch 27 -- Batch 408/ 842, training loss 0.3055563271045685\n",
      "Epoch 27 -- Batch 409/ 842, training loss 0.3106328248977661\n",
      "Epoch 27 -- Batch 410/ 842, training loss 0.30826336145401\n",
      "Epoch 27 -- Batch 411/ 842, training loss 0.30284151434898376\n",
      "Epoch 27 -- Batch 412/ 842, training loss 0.31410515308380127\n",
      "Epoch 27 -- Batch 413/ 842, training loss 0.31327497959136963\n",
      "Epoch 27 -- Batch 414/ 842, training loss 0.30919739603996277\n",
      "Epoch 27 -- Batch 415/ 842, training loss 0.29606667160987854\n",
      "Epoch 27 -- Batch 416/ 842, training loss 0.30814221501350403\n",
      "Epoch 27 -- Batch 417/ 842, training loss 0.30497878789901733\n",
      "Epoch 27 -- Batch 418/ 842, training loss 0.3092122972011566\n",
      "Epoch 27 -- Batch 419/ 842, training loss 0.2983025312423706\n",
      "Epoch 27 -- Batch 420/ 842, training loss 0.30911263823509216\n",
      "Epoch 27 -- Batch 421/ 842, training loss 0.3073730170726776\n",
      "Epoch 27 -- Batch 422/ 842, training loss 0.30811965465545654\n",
      "Epoch 27 -- Batch 423/ 842, training loss 0.29372674226760864\n",
      "Epoch 27 -- Batch 424/ 842, training loss 0.312486857175827\n",
      "Epoch 27 -- Batch 425/ 842, training loss 0.3110290467739105\n",
      "Epoch 27 -- Batch 426/ 842, training loss 0.3140312135219574\n",
      "Epoch 27 -- Batch 427/ 842, training loss 0.31273290514945984\n",
      "Epoch 27 -- Batch 428/ 842, training loss 0.2992075979709625\n",
      "Epoch 27 -- Batch 429/ 842, training loss 0.30713149905204773\n",
      "Epoch 27 -- Batch 430/ 842, training loss 0.3178481459617615\n",
      "Epoch 27 -- Batch 431/ 842, training loss 0.30895501375198364\n",
      "Epoch 27 -- Batch 432/ 842, training loss 0.3064098656177521\n",
      "Epoch 27 -- Batch 433/ 842, training loss 0.303742378950119\n",
      "Epoch 27 -- Batch 434/ 842, training loss 0.3160112202167511\n",
      "Epoch 27 -- Batch 435/ 842, training loss 0.31888145208358765\n",
      "Epoch 27 -- Batch 436/ 842, training loss 0.31368884444236755\n",
      "Epoch 27 -- Batch 437/ 842, training loss 0.30732259154319763\n",
      "Epoch 27 -- Batch 438/ 842, training loss 0.3091840147972107\n",
      "Epoch 27 -- Batch 439/ 842, training loss 0.31280508637428284\n",
      "Epoch 27 -- Batch 440/ 842, training loss 0.3018965721130371\n",
      "Epoch 27 -- Batch 441/ 842, training loss 0.30412599444389343\n",
      "Epoch 27 -- Batch 442/ 842, training loss 0.31418007612228394\n",
      "Epoch 27 -- Batch 443/ 842, training loss 0.3048759400844574\n",
      "Epoch 27 -- Batch 444/ 842, training loss 0.315814346075058\n",
      "Epoch 27 -- Batch 445/ 842, training loss 0.3119194507598877\n",
      "Epoch 27 -- Batch 446/ 842, training loss 0.3126668632030487\n",
      "Epoch 27 -- Batch 447/ 842, training loss 0.2930474281311035\n",
      "Epoch 27 -- Batch 448/ 842, training loss 0.3124401867389679\n",
      "Epoch 27 -- Batch 449/ 842, training loss 0.30841413140296936\n",
      "Epoch 27 -- Batch 450/ 842, training loss 0.30671456456184387\n",
      "Epoch 27 -- Batch 451/ 842, training loss 0.31309691071510315\n",
      "Epoch 27 -- Batch 452/ 842, training loss 0.30988162755966187\n",
      "Epoch 27 -- Batch 453/ 842, training loss 0.3034241497516632\n",
      "Epoch 27 -- Batch 454/ 842, training loss 0.3015109598636627\n",
      "Epoch 27 -- Batch 455/ 842, training loss 0.3219214379787445\n",
      "Epoch 27 -- Batch 456/ 842, training loss 0.311387836933136\n",
      "Epoch 27 -- Batch 457/ 842, training loss 0.3161490559577942\n",
      "Epoch 27 -- Batch 458/ 842, training loss 0.31494471430778503\n",
      "Epoch 27 -- Batch 459/ 842, training loss 0.3060649633407593\n",
      "Epoch 27 -- Batch 460/ 842, training loss 0.30670368671417236\n",
      "Epoch 27 -- Batch 461/ 842, training loss 0.309960275888443\n",
      "Epoch 27 -- Batch 462/ 842, training loss 0.3132069408893585\n",
      "Epoch 27 -- Batch 463/ 842, training loss 0.3046810030937195\n",
      "Epoch 27 -- Batch 464/ 842, training loss 0.30571478605270386\n",
      "Epoch 27 -- Batch 465/ 842, training loss 0.31972208619117737\n",
      "Epoch 27 -- Batch 466/ 842, training loss 0.3204720914363861\n",
      "Epoch 27 -- Batch 467/ 842, training loss 0.31142300367355347\n",
      "Epoch 27 -- Batch 468/ 842, training loss 0.303676575422287\n",
      "Epoch 27 -- Batch 469/ 842, training loss 0.30701449513435364\n",
      "Epoch 27 -- Batch 470/ 842, training loss 0.3111054599285126\n",
      "Epoch 27 -- Batch 471/ 842, training loss 0.30539095401763916\n",
      "Epoch 27 -- Batch 472/ 842, training loss 0.30565375089645386\n",
      "Epoch 27 -- Batch 473/ 842, training loss 0.3116562068462372\n",
      "Epoch 27 -- Batch 474/ 842, training loss 0.296737402677536\n",
      "Epoch 27 -- Batch 475/ 842, training loss 0.3174094557762146\n",
      "Epoch 27 -- Batch 476/ 842, training loss 0.31558260321617126\n",
      "Epoch 27 -- Batch 477/ 842, training loss 0.3162367343902588\n",
      "Epoch 27 -- Batch 478/ 842, training loss 0.3060668706893921\n",
      "Epoch 27 -- Batch 479/ 842, training loss 0.3095543384552002\n",
      "Epoch 27 -- Batch 480/ 842, training loss 0.32097727060317993\n",
      "Epoch 27 -- Batch 481/ 842, training loss 0.30888259410858154\n",
      "Epoch 27 -- Batch 482/ 842, training loss 0.31325438618659973\n",
      "Epoch 27 -- Batch 483/ 842, training loss 0.3109516203403473\n",
      "Epoch 27 -- Batch 484/ 842, training loss 0.2973531484603882\n",
      "Epoch 27 -- Batch 485/ 842, training loss 0.30726054310798645\n",
      "Epoch 27 -- Batch 486/ 842, training loss 0.31024307012557983\n",
      "Epoch 27 -- Batch 487/ 842, training loss 0.31122538447380066\n",
      "Epoch 27 -- Batch 488/ 842, training loss 0.32131436467170715\n",
      "Epoch 27 -- Batch 489/ 842, training loss 0.3189021348953247\n",
      "Epoch 27 -- Batch 490/ 842, training loss 0.29864466190338135\n",
      "Epoch 27 -- Batch 491/ 842, training loss 0.3202109634876251\n",
      "Epoch 27 -- Batch 492/ 842, training loss 0.3066147565841675\n",
      "Epoch 27 -- Batch 493/ 842, training loss 0.3091631531715393\n",
      "Epoch 27 -- Batch 494/ 842, training loss 0.3061925768852234\n",
      "Epoch 27 -- Batch 495/ 842, training loss 0.3036782741546631\n",
      "Epoch 27 -- Batch 496/ 842, training loss 0.3151799738407135\n",
      "Epoch 27 -- Batch 497/ 842, training loss 0.3151353597640991\n",
      "Epoch 27 -- Batch 498/ 842, training loss 0.3052977919578552\n",
      "Epoch 27 -- Batch 499/ 842, training loss 0.31692808866500854\n",
      "Epoch 27 -- Batch 500/ 842, training loss 0.31442850828170776\n",
      "Epoch 27 -- Batch 501/ 842, training loss 0.31612783670425415\n",
      "Epoch 27 -- Batch 502/ 842, training loss 0.3080415427684784\n",
      "Epoch 27 -- Batch 503/ 842, training loss 0.3117869794368744\n",
      "Epoch 27 -- Batch 504/ 842, training loss 0.3145811855792999\n",
      "Epoch 27 -- Batch 505/ 842, training loss 0.3153083026409149\n",
      "Epoch 27 -- Batch 506/ 842, training loss 0.30153998732566833\n",
      "Epoch 27 -- Batch 507/ 842, training loss 0.31786924600601196\n",
      "Epoch 27 -- Batch 508/ 842, training loss 0.31245091557502747\n",
      "Epoch 27 -- Batch 509/ 842, training loss 0.3038735091686249\n",
      "Epoch 27 -- Batch 510/ 842, training loss 0.30784884095191956\n",
      "Epoch 27 -- Batch 511/ 842, training loss 0.3112057149410248\n",
      "Epoch 27 -- Batch 512/ 842, training loss 0.30789002776145935\n",
      "Epoch 27 -- Batch 513/ 842, training loss 0.3091880977153778\n",
      "Epoch 27 -- Batch 514/ 842, training loss 0.3139818608760834\n",
      "Epoch 27 -- Batch 515/ 842, training loss 0.2971036434173584\n",
      "Epoch 27 -- Batch 516/ 842, training loss 0.31841254234313965\n",
      "Epoch 27 -- Batch 517/ 842, training loss 0.3156011402606964\n",
      "Epoch 27 -- Batch 518/ 842, training loss 0.31909334659576416\n",
      "Epoch 27 -- Batch 519/ 842, training loss 0.3197459876537323\n",
      "Epoch 27 -- Batch 520/ 842, training loss 0.3170105218887329\n",
      "Epoch 27 -- Batch 521/ 842, training loss 0.29731839895248413\n",
      "Epoch 27 -- Batch 522/ 842, training loss 0.3109683692455292\n",
      "Epoch 27 -- Batch 523/ 842, training loss 0.30089712142944336\n",
      "Epoch 27 -- Batch 524/ 842, training loss 0.32276955246925354\n",
      "Epoch 27 -- Batch 525/ 842, training loss 0.30468568205833435\n",
      "Epoch 27 -- Batch 526/ 842, training loss 0.3117600381374359\n",
      "Epoch 27 -- Batch 527/ 842, training loss 0.31894606351852417\n",
      "Epoch 27 -- Batch 528/ 842, training loss 0.30916470289230347\n",
      "Epoch 27 -- Batch 529/ 842, training loss 0.3059685230255127\n",
      "Epoch 27 -- Batch 530/ 842, training loss 0.3087901175022125\n",
      "Epoch 27 -- Batch 531/ 842, training loss 0.30812785029411316\n",
      "Epoch 27 -- Batch 532/ 842, training loss 0.318060040473938\n",
      "Epoch 27 -- Batch 533/ 842, training loss 0.3033370077610016\n",
      "Epoch 27 -- Batch 534/ 842, training loss 0.3192638158798218\n",
      "Epoch 27 -- Batch 535/ 842, training loss 0.3114794194698334\n",
      "Epoch 27 -- Batch 536/ 842, training loss 0.32665228843688965\n",
      "Epoch 27 -- Batch 537/ 842, training loss 0.31614792346954346\n",
      "Epoch 27 -- Batch 538/ 842, training loss 0.31486740708351135\n",
      "Epoch 27 -- Batch 539/ 842, training loss 0.30739372968673706\n",
      "Epoch 27 -- Batch 540/ 842, training loss 0.317035436630249\n",
      "Epoch 27 -- Batch 541/ 842, training loss 0.3142367899417877\n",
      "Epoch 27 -- Batch 542/ 842, training loss 0.29583051800727844\n",
      "Epoch 27 -- Batch 543/ 842, training loss 0.31749266386032104\n",
      "Epoch 27 -- Batch 544/ 842, training loss 0.30773356556892395\n",
      "Epoch 27 -- Batch 545/ 842, training loss 0.3146606683731079\n",
      "Epoch 27 -- Batch 546/ 842, training loss 0.3126471936702728\n",
      "Epoch 27 -- Batch 547/ 842, training loss 0.31449687480926514\n",
      "Epoch 27 -- Batch 548/ 842, training loss 0.3058159649372101\n",
      "Epoch 27 -- Batch 549/ 842, training loss 0.314530611038208\n",
      "Epoch 27 -- Batch 550/ 842, training loss 0.3117353022098541\n",
      "Epoch 27 -- Batch 551/ 842, training loss 0.31695863604545593\n",
      "Epoch 27 -- Batch 552/ 842, training loss 0.30318745970726013\n",
      "Epoch 27 -- Batch 553/ 842, training loss 0.30555617809295654\n",
      "Epoch 27 -- Batch 554/ 842, training loss 0.31115832924842834\n",
      "Epoch 27 -- Batch 555/ 842, training loss 0.3128531277179718\n",
      "Epoch 27 -- Batch 556/ 842, training loss 0.311350017786026\n",
      "Epoch 27 -- Batch 557/ 842, training loss 0.31636297702789307\n",
      "Epoch 27 -- Batch 558/ 842, training loss 0.3166274428367615\n",
      "Epoch 27 -- Batch 559/ 842, training loss 0.30940183997154236\n",
      "Epoch 27 -- Batch 560/ 842, training loss 0.3065459132194519\n",
      "Epoch 27 -- Batch 561/ 842, training loss 0.3171465992927551\n",
      "Epoch 27 -- Batch 562/ 842, training loss 0.30273357033729553\n",
      "Epoch 27 -- Batch 563/ 842, training loss 0.3155671954154968\n",
      "Epoch 27 -- Batch 564/ 842, training loss 0.3144324719905853\n",
      "Epoch 27 -- Batch 565/ 842, training loss 0.31796595454216003\n",
      "Epoch 27 -- Batch 566/ 842, training loss 0.3025836944580078\n",
      "Epoch 27 -- Batch 567/ 842, training loss 0.31349772214889526\n",
      "Epoch 27 -- Batch 568/ 842, training loss 0.3075152039527893\n",
      "Epoch 27 -- Batch 569/ 842, training loss 0.2958497703075409\n",
      "Epoch 27 -- Batch 570/ 842, training loss 0.3143288493156433\n",
      "Epoch 27 -- Batch 571/ 842, training loss 0.31616848707199097\n",
      "Epoch 27 -- Batch 572/ 842, training loss 0.30359897017478943\n",
      "Epoch 27 -- Batch 573/ 842, training loss 0.3001571595668793\n",
      "Epoch 27 -- Batch 574/ 842, training loss 0.3165700137615204\n",
      "Epoch 27 -- Batch 575/ 842, training loss 0.3121124505996704\n",
      "Epoch 27 -- Batch 576/ 842, training loss 0.3101547658443451\n",
      "Epoch 27 -- Batch 577/ 842, training loss 0.3139762580394745\n",
      "Epoch 27 -- Batch 578/ 842, training loss 0.31119367480278015\n",
      "Epoch 27 -- Batch 579/ 842, training loss 0.3106013834476471\n",
      "Epoch 27 -- Batch 580/ 842, training loss 0.3166143596172333\n",
      "Epoch 27 -- Batch 581/ 842, training loss 0.31129464507102966\n",
      "Epoch 27 -- Batch 582/ 842, training loss 0.30207422375679016\n",
      "Epoch 27 -- Batch 583/ 842, training loss 0.31653791666030884\n",
      "Epoch 27 -- Batch 584/ 842, training loss 0.3193788230419159\n",
      "Epoch 27 -- Batch 585/ 842, training loss 0.3044242262840271\n",
      "Epoch 27 -- Batch 586/ 842, training loss 0.31468039751052856\n",
      "Epoch 27 -- Batch 587/ 842, training loss 0.314586877822876\n",
      "Epoch 27 -- Batch 588/ 842, training loss 0.3219006061553955\n",
      "Epoch 27 -- Batch 589/ 842, training loss 0.31220829486846924\n",
      "Epoch 27 -- Batch 590/ 842, training loss 0.3075971305370331\n",
      "Epoch 27 -- Batch 591/ 842, training loss 0.30595484375953674\n",
      "Epoch 27 -- Batch 592/ 842, training loss 0.31919506192207336\n",
      "Epoch 27 -- Batch 593/ 842, training loss 0.3111618757247925\n",
      "Epoch 27 -- Batch 594/ 842, training loss 0.307693213224411\n",
      "Epoch 27 -- Batch 595/ 842, training loss 0.3108062744140625\n",
      "Epoch 27 -- Batch 596/ 842, training loss 0.3106696903705597\n",
      "Epoch 27 -- Batch 597/ 842, training loss 0.3135180175304413\n",
      "Epoch 27 -- Batch 598/ 842, training loss 0.3218352496623993\n",
      "Epoch 27 -- Batch 599/ 842, training loss 0.3124741017818451\n",
      "Epoch 27 -- Batch 600/ 842, training loss 0.31152594089508057\n",
      "Epoch 27 -- Batch 601/ 842, training loss 0.29423025250434875\n",
      "Epoch 27 -- Batch 602/ 842, training loss 0.29732951521873474\n",
      "Epoch 27 -- Batch 603/ 842, training loss 0.3078107535839081\n",
      "Epoch 27 -- Batch 604/ 842, training loss 0.3139486014842987\n",
      "Epoch 27 -- Batch 605/ 842, training loss 0.30737683176994324\n",
      "Epoch 27 -- Batch 606/ 842, training loss 0.3092329800128937\n",
      "Epoch 27 -- Batch 607/ 842, training loss 0.3123853802680969\n",
      "Epoch 27 -- Batch 608/ 842, training loss 0.31939515471458435\n",
      "Epoch 27 -- Batch 609/ 842, training loss 0.3149174749851227\n",
      "Epoch 27 -- Batch 610/ 842, training loss 0.31393784284591675\n",
      "Epoch 27 -- Batch 611/ 842, training loss 0.3088577389717102\n",
      "Epoch 27 -- Batch 612/ 842, training loss 0.31884679198265076\n",
      "Epoch 27 -- Batch 613/ 842, training loss 0.3169250786304474\n",
      "Epoch 27 -- Batch 614/ 842, training loss 0.30607014894485474\n",
      "Epoch 27 -- Batch 615/ 842, training loss 0.30369409918785095\n",
      "Epoch 27 -- Batch 616/ 842, training loss 0.3079545795917511\n",
      "Epoch 27 -- Batch 617/ 842, training loss 0.30647966265678406\n",
      "Epoch 27 -- Batch 618/ 842, training loss 0.3259284198284149\n",
      "Epoch 27 -- Batch 619/ 842, training loss 0.3193061649799347\n",
      "Epoch 27 -- Batch 620/ 842, training loss 0.31661248207092285\n",
      "Epoch 27 -- Batch 621/ 842, training loss 0.3109182119369507\n",
      "Epoch 27 -- Batch 622/ 842, training loss 0.3045811653137207\n",
      "Epoch 27 -- Batch 623/ 842, training loss 0.31315097212791443\n",
      "Epoch 27 -- Batch 624/ 842, training loss 0.3172285556793213\n",
      "Epoch 27 -- Batch 625/ 842, training loss 0.30219563841819763\n",
      "Epoch 27 -- Batch 626/ 842, training loss 0.3010791838169098\n",
      "Epoch 27 -- Batch 627/ 842, training loss 0.3048878312110901\n",
      "Epoch 27 -- Batch 628/ 842, training loss 0.30830129981040955\n",
      "Epoch 27 -- Batch 629/ 842, training loss 0.3163003623485565\n",
      "Epoch 27 -- Batch 630/ 842, training loss 0.3080095052719116\n",
      "Epoch 27 -- Batch 631/ 842, training loss 0.32771000266075134\n",
      "Epoch 27 -- Batch 632/ 842, training loss 0.3143393099308014\n",
      "Epoch 27 -- Batch 633/ 842, training loss 0.3122125267982483\n",
      "Epoch 27 -- Batch 634/ 842, training loss 0.3032737672328949\n",
      "Epoch 27 -- Batch 635/ 842, training loss 0.31172794103622437\n",
      "Epoch 27 -- Batch 636/ 842, training loss 0.3138521611690521\n",
      "Epoch 27 -- Batch 637/ 842, training loss 0.3159962594509125\n",
      "Epoch 27 -- Batch 638/ 842, training loss 0.32037585973739624\n",
      "Epoch 27 -- Batch 639/ 842, training loss 0.3223307430744171\n",
      "Epoch 27 -- Batch 640/ 842, training loss 0.3078566789627075\n",
      "Epoch 27 -- Batch 641/ 842, training loss 0.3127913475036621\n",
      "Epoch 27 -- Batch 642/ 842, training loss 0.3183450996875763\n",
      "Epoch 27 -- Batch 643/ 842, training loss 0.319498747587204\n",
      "Epoch 27 -- Batch 644/ 842, training loss 0.2944425046443939\n",
      "Epoch 27 -- Batch 645/ 842, training loss 0.3162520229816437\n",
      "Epoch 27 -- Batch 646/ 842, training loss 0.3107747435569763\n",
      "Epoch 27 -- Batch 647/ 842, training loss 0.30867496132850647\n",
      "Epoch 27 -- Batch 648/ 842, training loss 0.3168684244155884\n",
      "Epoch 27 -- Batch 649/ 842, training loss 0.31322628259658813\n",
      "Epoch 27 -- Batch 650/ 842, training loss 0.31326282024383545\n",
      "Epoch 27 -- Batch 651/ 842, training loss 0.32227224111557007\n",
      "Epoch 27 -- Batch 652/ 842, training loss 0.3090916574001312\n",
      "Epoch 27 -- Batch 653/ 842, training loss 0.3187428414821625\n",
      "Epoch 27 -- Batch 654/ 842, training loss 0.3108106553554535\n",
      "Epoch 27 -- Batch 655/ 842, training loss 0.30681994557380676\n",
      "Epoch 27 -- Batch 656/ 842, training loss 0.3014029860496521\n",
      "Epoch 27 -- Batch 657/ 842, training loss 0.31280040740966797\n",
      "Epoch 27 -- Batch 658/ 842, training loss 0.30711162090301514\n",
      "Epoch 27 -- Batch 659/ 842, training loss 0.31652915477752686\n",
      "Epoch 27 -- Batch 660/ 842, training loss 0.31859636306762695\n",
      "Epoch 27 -- Batch 661/ 842, training loss 0.30198222398757935\n",
      "Epoch 27 -- Batch 662/ 842, training loss 0.3037331998348236\n",
      "Epoch 27 -- Batch 663/ 842, training loss 0.31446266174316406\n",
      "Epoch 27 -- Batch 664/ 842, training loss 0.30286529660224915\n",
      "Epoch 27 -- Batch 665/ 842, training loss 0.31062066555023193\n",
      "Epoch 27 -- Batch 666/ 842, training loss 0.32081592082977295\n",
      "Epoch 27 -- Batch 667/ 842, training loss 0.3069130480289459\n",
      "Epoch 27 -- Batch 668/ 842, training loss 0.2990378141403198\n",
      "Epoch 27 -- Batch 669/ 842, training loss 0.30886170268058777\n",
      "Epoch 27 -- Batch 670/ 842, training loss 0.3090399503707886\n",
      "Epoch 27 -- Batch 671/ 842, training loss 0.2998287081718445\n",
      "Epoch 27 -- Batch 672/ 842, training loss 0.31400278210639954\n",
      "Epoch 27 -- Batch 673/ 842, training loss 0.30884450674057007\n",
      "Epoch 27 -- Batch 674/ 842, training loss 0.2982836663722992\n",
      "Epoch 27 -- Batch 675/ 842, training loss 0.31587836146354675\n",
      "Epoch 27 -- Batch 676/ 842, training loss 0.3131968080997467\n",
      "Epoch 27 -- Batch 677/ 842, training loss 0.3119712173938751\n",
      "Epoch 27 -- Batch 678/ 842, training loss 0.3148002326488495\n",
      "Epoch 27 -- Batch 679/ 842, training loss 0.3275725245475769\n",
      "Epoch 27 -- Batch 680/ 842, training loss 0.3061715364456177\n",
      "Epoch 27 -- Batch 681/ 842, training loss 0.3087279796600342\n",
      "Epoch 27 -- Batch 682/ 842, training loss 0.30495157837867737\n",
      "Epoch 27 -- Batch 683/ 842, training loss 0.32495471835136414\n",
      "Epoch 27 -- Batch 684/ 842, training loss 0.31399405002593994\n",
      "Epoch 27 -- Batch 685/ 842, training loss 0.3218669891357422\n",
      "Epoch 27 -- Batch 686/ 842, training loss 0.30319133400917053\n",
      "Epoch 27 -- Batch 687/ 842, training loss 0.30558544397354126\n",
      "Epoch 27 -- Batch 688/ 842, training loss 0.3125709295272827\n",
      "Epoch 27 -- Batch 689/ 842, training loss 0.3083994686603546\n",
      "Epoch 27 -- Batch 690/ 842, training loss 0.3076978921890259\n",
      "Epoch 27 -- Batch 691/ 842, training loss 0.31787943840026855\n",
      "Epoch 27 -- Batch 692/ 842, training loss 0.3080320358276367\n",
      "Epoch 27 -- Batch 693/ 842, training loss 0.3140134811401367\n",
      "Epoch 27 -- Batch 694/ 842, training loss 0.31166741251945496\n",
      "Epoch 27 -- Batch 695/ 842, training loss 0.30778801441192627\n",
      "Epoch 27 -- Batch 696/ 842, training loss 0.31189069151878357\n",
      "Epoch 27 -- Batch 697/ 842, training loss 0.3106135427951813\n",
      "Epoch 27 -- Batch 698/ 842, training loss 0.30198943614959717\n",
      "Epoch 27 -- Batch 699/ 842, training loss 0.3084137737751007\n",
      "Epoch 27 -- Batch 700/ 842, training loss 0.30438122153282166\n",
      "Epoch 27 -- Batch 701/ 842, training loss 0.30571457743644714\n",
      "Epoch 27 -- Batch 702/ 842, training loss 0.31567612290382385\n",
      "Epoch 27 -- Batch 703/ 842, training loss 0.317726731300354\n",
      "Epoch 27 -- Batch 704/ 842, training loss 0.3051614761352539\n",
      "Epoch 27 -- Batch 705/ 842, training loss 0.307633638381958\n",
      "Epoch 27 -- Batch 706/ 842, training loss 0.32455962896347046\n",
      "Epoch 27 -- Batch 707/ 842, training loss 0.30765247344970703\n",
      "Epoch 27 -- Batch 708/ 842, training loss 0.3135756850242615\n",
      "Epoch 27 -- Batch 709/ 842, training loss 0.3022526204586029\n",
      "Epoch 27 -- Batch 710/ 842, training loss 0.3081660270690918\n",
      "Epoch 27 -- Batch 711/ 842, training loss 0.3131932020187378\n",
      "Epoch 27 -- Batch 712/ 842, training loss 0.3167378306388855\n",
      "Epoch 27 -- Batch 713/ 842, training loss 0.31547752022743225\n",
      "Epoch 27 -- Batch 714/ 842, training loss 0.303420752286911\n",
      "Epoch 27 -- Batch 715/ 842, training loss 0.31409549713134766\n",
      "Epoch 27 -- Batch 716/ 842, training loss 0.30933085083961487\n",
      "Epoch 27 -- Batch 717/ 842, training loss 0.30937498807907104\n",
      "Epoch 27 -- Batch 718/ 842, training loss 0.3077479600906372\n",
      "Epoch 27 -- Batch 719/ 842, training loss 0.3107498288154602\n",
      "Epoch 27 -- Batch 720/ 842, training loss 0.3119047284126282\n",
      "Epoch 27 -- Batch 721/ 842, training loss 0.3156603276729584\n",
      "Epoch 27 -- Batch 722/ 842, training loss 0.3072032630443573\n",
      "Epoch 27 -- Batch 723/ 842, training loss 0.3117913603782654\n",
      "Epoch 27 -- Batch 724/ 842, training loss 0.3072104752063751\n",
      "Epoch 27 -- Batch 725/ 842, training loss 0.3067627251148224\n",
      "Epoch 27 -- Batch 726/ 842, training loss 0.32845625281333923\n",
      "Epoch 27 -- Batch 727/ 842, training loss 0.30774977803230286\n",
      "Epoch 27 -- Batch 728/ 842, training loss 0.3071267604827881\n",
      "Epoch 27 -- Batch 729/ 842, training loss 0.3153356611728668\n",
      "Epoch 27 -- Batch 730/ 842, training loss 0.3070874512195587\n",
      "Epoch 27 -- Batch 731/ 842, training loss 0.3197888433933258\n",
      "Epoch 27 -- Batch 732/ 842, training loss 0.30464547872543335\n",
      "Epoch 27 -- Batch 733/ 842, training loss 0.31331825256347656\n",
      "Epoch 27 -- Batch 734/ 842, training loss 0.31717649102211\n",
      "Epoch 27 -- Batch 735/ 842, training loss 0.29467183351516724\n",
      "Epoch 27 -- Batch 736/ 842, training loss 0.3173397183418274\n",
      "Epoch 27 -- Batch 737/ 842, training loss 0.2951892614364624\n",
      "Epoch 27 -- Batch 738/ 842, training loss 0.3002900779247284\n",
      "Epoch 27 -- Batch 739/ 842, training loss 0.3027213215827942\n",
      "Epoch 27 -- Batch 740/ 842, training loss 0.3051912188529968\n",
      "Epoch 27 -- Batch 741/ 842, training loss 0.32678332924842834\n",
      "Epoch 27 -- Batch 742/ 842, training loss 0.30572131276130676\n",
      "Epoch 27 -- Batch 743/ 842, training loss 0.3075169324874878\n",
      "Epoch 27 -- Batch 744/ 842, training loss 0.32072991132736206\n",
      "Epoch 27 -- Batch 745/ 842, training loss 0.3145526349544525\n",
      "Epoch 27 -- Batch 746/ 842, training loss 0.3100477457046509\n",
      "Epoch 27 -- Batch 747/ 842, training loss 0.32063227891921997\n",
      "Epoch 27 -- Batch 748/ 842, training loss 0.3051014840602875\n",
      "Epoch 27 -- Batch 749/ 842, training loss 0.31125739216804504\n",
      "Epoch 27 -- Batch 750/ 842, training loss 0.31600672006607056\n",
      "Epoch 27 -- Batch 751/ 842, training loss 0.29704663157463074\n",
      "Epoch 27 -- Batch 752/ 842, training loss 0.30414798855781555\n",
      "Epoch 27 -- Batch 753/ 842, training loss 0.31345221400260925\n",
      "Epoch 27 -- Batch 754/ 842, training loss 0.30621930956840515\n",
      "Epoch 27 -- Batch 755/ 842, training loss 0.31960800290107727\n",
      "Epoch 27 -- Batch 756/ 842, training loss 0.3133917450904846\n",
      "Epoch 27 -- Batch 757/ 842, training loss 0.3072679042816162\n",
      "Epoch 27 -- Batch 758/ 842, training loss 0.31012099981307983\n",
      "Epoch 27 -- Batch 759/ 842, training loss 0.31832414865493774\n",
      "Epoch 27 -- Batch 760/ 842, training loss 0.3228841722011566\n",
      "Epoch 27 -- Batch 761/ 842, training loss 0.3113047480583191\n",
      "Epoch 27 -- Batch 762/ 842, training loss 0.2999849319458008\n",
      "Epoch 27 -- Batch 763/ 842, training loss 0.30211323499679565\n",
      "Epoch 27 -- Batch 764/ 842, training loss 0.30172258615493774\n",
      "Epoch 27 -- Batch 765/ 842, training loss 0.3149006962776184\n",
      "Epoch 27 -- Batch 766/ 842, training loss 0.31931933760643005\n",
      "Epoch 27 -- Batch 767/ 842, training loss 0.3146767020225525\n",
      "Epoch 27 -- Batch 768/ 842, training loss 0.3138968050479889\n",
      "Epoch 27 -- Batch 769/ 842, training loss 0.30839571356773376\n",
      "Epoch 27 -- Batch 770/ 842, training loss 0.3129408061504364\n",
      "Epoch 27 -- Batch 771/ 842, training loss 0.3100728690624237\n",
      "Epoch 27 -- Batch 772/ 842, training loss 0.3074604570865631\n",
      "Epoch 27 -- Batch 773/ 842, training loss 0.3190809488296509\n",
      "Epoch 27 -- Batch 774/ 842, training loss 0.29946035146713257\n",
      "Epoch 27 -- Batch 775/ 842, training loss 0.3160021901130676\n",
      "Epoch 27 -- Batch 776/ 842, training loss 0.31072479486465454\n",
      "Epoch 27 -- Batch 777/ 842, training loss 0.3200763761997223\n",
      "Epoch 27 -- Batch 778/ 842, training loss 0.31458866596221924\n",
      "Epoch 27 -- Batch 779/ 842, training loss 0.3176177740097046\n",
      "Epoch 27 -- Batch 780/ 842, training loss 0.321427583694458\n",
      "Epoch 27 -- Batch 781/ 842, training loss 0.311450332403183\n",
      "Epoch 27 -- Batch 782/ 842, training loss 0.3112242817878723\n",
      "Epoch 27 -- Batch 783/ 842, training loss 0.31624794006347656\n",
      "Epoch 27 -- Batch 784/ 842, training loss 0.3108727037906647\n",
      "Epoch 27 -- Batch 785/ 842, training loss 0.3182106018066406\n",
      "Epoch 27 -- Batch 786/ 842, training loss 0.30989137291908264\n",
      "Epoch 27 -- Batch 787/ 842, training loss 0.32498544454574585\n",
      "Epoch 27 -- Batch 788/ 842, training loss 0.3139175772666931\n",
      "Epoch 27 -- Batch 789/ 842, training loss 0.3051939904689789\n",
      "Epoch 27 -- Batch 790/ 842, training loss 0.3213701844215393\n",
      "Epoch 27 -- Batch 791/ 842, training loss 0.30484968423843384\n",
      "Epoch 27 -- Batch 792/ 842, training loss 0.3081962764263153\n",
      "Epoch 27 -- Batch 793/ 842, training loss 0.3118341267108917\n",
      "Epoch 27 -- Batch 794/ 842, training loss 0.29665544629096985\n",
      "Epoch 27 -- Batch 795/ 842, training loss 0.3020796775817871\n",
      "Epoch 27 -- Batch 796/ 842, training loss 0.3170730173587799\n",
      "Epoch 27 -- Batch 797/ 842, training loss 0.3072029650211334\n",
      "Epoch 27 -- Batch 798/ 842, training loss 0.3094342052936554\n",
      "Epoch 27 -- Batch 799/ 842, training loss 0.30520594120025635\n",
      "Epoch 27 -- Batch 800/ 842, training loss 0.3123076856136322\n",
      "Epoch 27 -- Batch 801/ 842, training loss 0.29789844155311584\n",
      "Epoch 27 -- Batch 802/ 842, training loss 0.3042076826095581\n",
      "Epoch 27 -- Batch 803/ 842, training loss 0.30771782994270325\n",
      "Epoch 27 -- Batch 804/ 842, training loss 0.3078572452068329\n",
      "Epoch 27 -- Batch 805/ 842, training loss 0.318174809217453\n",
      "Epoch 27 -- Batch 806/ 842, training loss 0.3126049339771271\n",
      "Epoch 27 -- Batch 807/ 842, training loss 0.31113073229789734\n",
      "Epoch 27 -- Batch 808/ 842, training loss 0.3130188584327698\n",
      "Epoch 27 -- Batch 809/ 842, training loss 0.31901147961616516\n",
      "Epoch 27 -- Batch 810/ 842, training loss 0.3105512261390686\n",
      "Epoch 27 -- Batch 811/ 842, training loss 0.31532004475593567\n",
      "Epoch 27 -- Batch 812/ 842, training loss 0.3146592378616333\n",
      "Epoch 27 -- Batch 813/ 842, training loss 0.3086661696434021\n",
      "Epoch 27 -- Batch 814/ 842, training loss 0.311416357755661\n",
      "Epoch 27 -- Batch 815/ 842, training loss 0.3125975430011749\n",
      "Epoch 27 -- Batch 816/ 842, training loss 0.31664565205574036\n",
      "Epoch 27 -- Batch 817/ 842, training loss 0.30939874053001404\n",
      "Epoch 27 -- Batch 818/ 842, training loss 0.3284223675727844\n",
      "Epoch 27 -- Batch 819/ 842, training loss 0.31502416729927063\n",
      "Epoch 27 -- Batch 820/ 842, training loss 0.3078787326812744\n",
      "Epoch 27 -- Batch 821/ 842, training loss 0.31234633922576904\n",
      "Epoch 27 -- Batch 822/ 842, training loss 0.31218668818473816\n",
      "Epoch 27 -- Batch 823/ 842, training loss 0.3177849054336548\n",
      "Epoch 27 -- Batch 824/ 842, training loss 0.3237069249153137\n",
      "Epoch 27 -- Batch 825/ 842, training loss 0.3185986280441284\n",
      "Epoch 27 -- Batch 826/ 842, training loss 0.3290010988712311\n",
      "Epoch 27 -- Batch 827/ 842, training loss 0.3087061047554016\n",
      "Epoch 27 -- Batch 828/ 842, training loss 0.30374330282211304\n",
      "Epoch 27 -- Batch 829/ 842, training loss 0.3043452203273773\n",
      "Epoch 27 -- Batch 830/ 842, training loss 0.30822470784187317\n",
      "Epoch 27 -- Batch 831/ 842, training loss 0.3218783438205719\n",
      "Epoch 27 -- Batch 832/ 842, training loss 0.3041119873523712\n",
      "Epoch 27 -- Batch 833/ 842, training loss 0.3030799329280853\n",
      "Epoch 27 -- Batch 834/ 842, training loss 0.3150549530982971\n",
      "Epoch 27 -- Batch 835/ 842, training loss 0.31276482343673706\n",
      "Epoch 27 -- Batch 836/ 842, training loss 0.3159002661705017\n",
      "Epoch 27 -- Batch 837/ 842, training loss 0.3205457329750061\n",
      "Epoch 27 -- Batch 838/ 842, training loss 0.32020270824432373\n",
      "Epoch 27 -- Batch 839/ 842, training loss 0.32172828912734985\n",
      "Epoch 27 -- Batch 840/ 842, training loss 0.324317067861557\n",
      "Epoch 27 -- Batch 841/ 842, training loss 0.3115104138851166\n",
      "Epoch 27 -- Batch 842/ 842, training loss 0.3188513517379761\n",
      "----------------------------------------------------------------------\n",
      "Epoch 27 -- Batch 1/ 94, validation loss 0.29561296105384827\n",
      "Epoch 27 -- Batch 2/ 94, validation loss 0.3017352223396301\n",
      "Epoch 27 -- Batch 3/ 94, validation loss 0.31795039772987366\n",
      "Epoch 27 -- Batch 4/ 94, validation loss 0.2877291142940521\n",
      "Epoch 27 -- Batch 5/ 94, validation loss 0.30879801511764526\n",
      "Epoch 27 -- Batch 6/ 94, validation loss 0.298397421836853\n",
      "Epoch 27 -- Batch 7/ 94, validation loss 0.30136236548423767\n",
      "Epoch 27 -- Batch 8/ 94, validation loss 0.3007906675338745\n",
      "Epoch 27 -- Batch 9/ 94, validation loss 0.30205246806144714\n",
      "Epoch 27 -- Batch 10/ 94, validation loss 0.30060893297195435\n",
      "Epoch 27 -- Batch 11/ 94, validation loss 0.3063196539878845\n",
      "Epoch 27 -- Batch 12/ 94, validation loss 0.30827903747558594\n",
      "Epoch 27 -- Batch 13/ 94, validation loss 0.2947379946708679\n",
      "Epoch 27 -- Batch 14/ 94, validation loss 0.29852837324142456\n",
      "Epoch 27 -- Batch 15/ 94, validation loss 0.30373236536979675\n",
      "Epoch 27 -- Batch 16/ 94, validation loss 0.28375044465065\n",
      "Epoch 27 -- Batch 17/ 94, validation loss 0.3389052748680115\n",
      "Epoch 27 -- Batch 18/ 94, validation loss 0.29172468185424805\n",
      "Epoch 27 -- Batch 19/ 94, validation loss 0.29487016797065735\n",
      "Epoch 27 -- Batch 20/ 94, validation loss 0.3056573271751404\n",
      "Epoch 27 -- Batch 21/ 94, validation loss 0.2974955141544342\n",
      "Epoch 27 -- Batch 22/ 94, validation loss 0.30269646644592285\n",
      "Epoch 27 -- Batch 23/ 94, validation loss 0.30513566732406616\n",
      "Epoch 27 -- Batch 24/ 94, validation loss 0.29302582144737244\n",
      "Epoch 27 -- Batch 25/ 94, validation loss 0.30831319093704224\n",
      "Epoch 27 -- Batch 26/ 94, validation loss 0.2959166467189789\n",
      "Epoch 27 -- Batch 27/ 94, validation loss 0.2950751483440399\n",
      "Epoch 27 -- Batch 28/ 94, validation loss 0.2991434335708618\n",
      "Epoch 27 -- Batch 29/ 94, validation loss 0.3026900291442871\n",
      "Epoch 27 -- Batch 30/ 94, validation loss 0.3084448575973511\n",
      "Epoch 27 -- Batch 31/ 94, validation loss 0.2978735566139221\n",
      "Epoch 27 -- Batch 32/ 94, validation loss 0.29964953660964966\n",
      "Epoch 27 -- Batch 33/ 94, validation loss 0.3091558814048767\n",
      "Epoch 27 -- Batch 34/ 94, validation loss 0.29167628288269043\n",
      "Epoch 27 -- Batch 35/ 94, validation loss 0.29552987217903137\n",
      "Epoch 27 -- Batch 36/ 94, validation loss 0.2952800691127777\n",
      "Epoch 27 -- Batch 37/ 94, validation loss 0.2961236536502838\n",
      "Epoch 27 -- Batch 38/ 94, validation loss 0.29915347695350647\n",
      "Epoch 27 -- Batch 39/ 94, validation loss 0.2953040599822998\n",
      "Epoch 27 -- Batch 40/ 94, validation loss 0.29782208800315857\n",
      "Epoch 27 -- Batch 41/ 94, validation loss 0.3093734085559845\n",
      "Epoch 27 -- Batch 42/ 94, validation loss 0.3039173483848572\n",
      "Epoch 27 -- Batch 43/ 94, validation loss 0.3086465299129486\n",
      "Epoch 27 -- Batch 44/ 94, validation loss 0.2874126732349396\n",
      "Epoch 27 -- Batch 45/ 94, validation loss 0.3037548363208771\n",
      "Epoch 27 -- Batch 46/ 94, validation loss 0.2965857684612274\n",
      "Epoch 27 -- Batch 47/ 94, validation loss 0.29953157901763916\n",
      "Epoch 27 -- Batch 48/ 94, validation loss 0.2995910048484802\n",
      "Epoch 27 -- Batch 49/ 94, validation loss 0.30762985348701477\n",
      "Epoch 27 -- Batch 50/ 94, validation loss 0.303329199552536\n",
      "Epoch 27 -- Batch 51/ 94, validation loss 0.29815807938575745\n",
      "Epoch 27 -- Batch 52/ 94, validation loss 0.2965173125267029\n",
      "Epoch 27 -- Batch 53/ 94, validation loss 0.28884345293045044\n",
      "Epoch 27 -- Batch 54/ 94, validation loss 0.3051924705505371\n",
      "Epoch 27 -- Batch 55/ 94, validation loss 0.3105998933315277\n",
      "Epoch 27 -- Batch 56/ 94, validation loss 0.29098597168922424\n",
      "Epoch 27 -- Batch 57/ 94, validation loss 0.30610597133636475\n",
      "Epoch 27 -- Batch 58/ 94, validation loss 0.30023494362831116\n",
      "Epoch 27 -- Batch 59/ 94, validation loss 0.2956984341144562\n",
      "Epoch 27 -- Batch 60/ 94, validation loss 0.3021140992641449\n",
      "Epoch 27 -- Batch 61/ 94, validation loss 0.2971397042274475\n",
      "Epoch 27 -- Batch 62/ 94, validation loss 0.3062725365161896\n",
      "Epoch 27 -- Batch 63/ 94, validation loss 0.2997458875179291\n",
      "Epoch 27 -- Batch 64/ 94, validation loss 0.29841384291648865\n",
      "Epoch 27 -- Batch 65/ 94, validation loss 0.2924598455429077\n",
      "Epoch 27 -- Batch 66/ 94, validation loss 0.2927660644054413\n",
      "Epoch 27 -- Batch 67/ 94, validation loss 0.29315629601478577\n",
      "Epoch 27 -- Batch 68/ 94, validation loss 0.3133706748485565\n",
      "Epoch 27 -- Batch 69/ 94, validation loss 0.2977485954761505\n",
      "Epoch 27 -- Batch 70/ 94, validation loss 0.28975844383239746\n",
      "Epoch 27 -- Batch 71/ 94, validation loss 0.2971402108669281\n",
      "Epoch 27 -- Batch 72/ 94, validation loss 0.31059613823890686\n",
      "Epoch 27 -- Batch 73/ 94, validation loss 0.30699893832206726\n",
      "Epoch 27 -- Batch 74/ 94, validation loss 0.2995462417602539\n",
      "Epoch 27 -- Batch 75/ 94, validation loss 0.30230116844177246\n",
      "Epoch 27 -- Batch 76/ 94, validation loss 0.2954303026199341\n",
      "Epoch 27 -- Batch 77/ 94, validation loss 0.29508650302886963\n",
      "Epoch 27 -- Batch 78/ 94, validation loss 0.3094545602798462\n",
      "Epoch 27 -- Batch 79/ 94, validation loss 0.3345756232738495\n",
      "Epoch 27 -- Batch 80/ 94, validation loss 0.3000682592391968\n",
      "Epoch 27 -- Batch 81/ 94, validation loss 0.2953660786151886\n",
      "Epoch 27 -- Batch 82/ 94, validation loss 0.3016560673713684\n",
      "Epoch 27 -- Batch 83/ 94, validation loss 0.3059948682785034\n",
      "Epoch 27 -- Batch 84/ 94, validation loss 0.30238640308380127\n",
      "Epoch 27 -- Batch 85/ 94, validation loss 0.29570117592811584\n",
      "Epoch 27 -- Batch 86/ 94, validation loss 0.286439448595047\n",
      "Epoch 27 -- Batch 87/ 94, validation loss 0.3005838096141815\n",
      "Epoch 27 -- Batch 88/ 94, validation loss 0.2933019995689392\n",
      "Epoch 27 -- Batch 89/ 94, validation loss 0.3045506477355957\n",
      "Epoch 27 -- Batch 90/ 94, validation loss 0.2953856289386749\n",
      "Epoch 27 -- Batch 91/ 94, validation loss 0.30747178196907043\n",
      "Epoch 27 -- Batch 92/ 94, validation loss 0.30306118726730347\n",
      "Epoch 27 -- Batch 93/ 94, validation loss 0.29534289240837097\n",
      "Epoch 27 -- Batch 94/ 94, validation loss 0.3130333721637726\n",
      "----------------------------------------------------------------------\n",
      "Epoch 27 loss: Training 0.3098025918006897, Validation 0.3130333721637726\n",
      "----------------------------------------------------------------------\n",
      "Epoch 28/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:08:23] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc(F)cc1NC(=O)CSc1nnc(-c2cc(Cl)ccc2Cl)n1N)c1ccccc1\n",
      "[19:08:23] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(F)cc1NC(=O)CSc1nnc(-c2cc(Cl)ccc2Cl)n1N)c1ccccc1' for input: 'Cc1ccc(F)cc1NC(=O)CSc1nnc(-c2cc(Cl)ccc2Cl)n1N)c1ccccc1'\n",
      "[19:08:23] SMILES Parse Error: extra close parentheses while parsing: c1ccc2c3c([nH]c2c1)c1nc-n1CCc1ccccc1)C2\n",
      "[19:08:23] SMILES Parse Error: Failed parsing SMILES 'c1ccc2c3c([nH]c2c1)c1nc-n1CCc1ccccc1)C2' for input: 'c1ccc2c3c([nH]c2c1)c1nc-n1CCc1ccccc1)C2'\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 14 15 25\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 5 7 25\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 2 3 5 6 19\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 27 28 30 31\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 26 27\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'C1CCC(C(=O)NCc2nnc3n2CCN(Cc2ccc4c(c2)OCO4)CC2)CC1'\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 10 20 21\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 5 6 16 18 20\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7 8 22 24 25\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'CCCNc1ncnc2c1c1nc(Nc1cccc(C(=O)OC)cc1)nn2C'\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 13 14 15\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 8 9 10\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'Cc1n[nH]c(C)c1-c1cccc(C(=O)NCC2CCN(C(=O)C3CC3CCC3)C2)c1'\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 2 3 5 6 21\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 1 2 15\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 2 3 5\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(=O)Nc2c3c(nn2-c2cccc(C(F)(F)F)c2)CS(=O)(=O)C2)cc1C'\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'Cn1cnc(S(=O)(=O)N2C[C@H]3[C@@H]4CC[C@H](C4)[C@@H]3[C@@H](C)[C@@H]2C2)c1'\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 11 21 22 23 24 25 26\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'O=C(CCCSc1ccc(NC(=O)C2COc2ccccc2)nn1)OCCc1ccccc1'\n",
      "[19:08:23] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'CC(C)CC1CCC(NC(=O)C(c2ccc3c(c2)OCO3)-2c(O)ccc2N1)C1'\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'CCCC12CN3CC(CCC)(c3ccc(Br)cc3)C1C2=O'\n",
      "[19:08:23] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C)c1OC(=O)CSC1=NSC2=S'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 -- Batch 1/ 842, training loss 0.2962436378002167\n",
      "Epoch 28 -- Batch 2/ 842, training loss 0.3059232532978058\n",
      "Epoch 28 -- Batch 3/ 842, training loss 0.3032916784286499\n",
      "Epoch 28 -- Batch 4/ 842, training loss 0.29936355352401733\n",
      "Epoch 28 -- Batch 5/ 842, training loss 0.30717000365257263\n",
      "Epoch 28 -- Batch 6/ 842, training loss 0.30118489265441895\n",
      "Epoch 28 -- Batch 7/ 842, training loss 0.31719139218330383\n",
      "Epoch 28 -- Batch 8/ 842, training loss 0.3036789298057556\n",
      "Epoch 28 -- Batch 9/ 842, training loss 0.31465965509414673\n",
      "Epoch 28 -- Batch 10/ 842, training loss 0.30576133728027344\n",
      "Epoch 28 -- Batch 11/ 842, training loss 0.3071347177028656\n",
      "Epoch 28 -- Batch 12/ 842, training loss 0.3067638576030731\n",
      "Epoch 28 -- Batch 13/ 842, training loss 0.3026958107948303\n",
      "Epoch 28 -- Batch 14/ 842, training loss 0.3067464232444763\n",
      "Epoch 28 -- Batch 15/ 842, training loss 0.30458277463912964\n",
      "Epoch 28 -- Batch 16/ 842, training loss 0.3087851405143738\n",
      "Epoch 28 -- Batch 17/ 842, training loss 0.309292197227478\n",
      "Epoch 28 -- Batch 18/ 842, training loss 0.31011033058166504\n",
      "Epoch 28 -- Batch 19/ 842, training loss 0.3112446665763855\n",
      "Epoch 28 -- Batch 20/ 842, training loss 0.30435022711753845\n",
      "Epoch 28 -- Batch 21/ 842, training loss 0.30768460035324097\n",
      "Epoch 28 -- Batch 22/ 842, training loss 0.3070330321788788\n",
      "Epoch 28 -- Batch 23/ 842, training loss 0.30328404903411865\n",
      "Epoch 28 -- Batch 24/ 842, training loss 0.31179603934288025\n",
      "Epoch 28 -- Batch 25/ 842, training loss 0.30161917209625244\n",
      "Epoch 28 -- Batch 26/ 842, training loss 0.30730393528938293\n",
      "Epoch 28 -- Batch 27/ 842, training loss 0.3057762086391449\n",
      "Epoch 28 -- Batch 28/ 842, training loss 0.31337082386016846\n",
      "Epoch 28 -- Batch 29/ 842, training loss 0.2996669411659241\n",
      "Epoch 28 -- Batch 30/ 842, training loss 0.3062022626399994\n",
      "Epoch 28 -- Batch 31/ 842, training loss 0.2983072102069855\n",
      "Epoch 28 -- Batch 32/ 842, training loss 0.30862221121788025\n",
      "Epoch 28 -- Batch 33/ 842, training loss 0.31252357363700867\n",
      "Epoch 28 -- Batch 34/ 842, training loss 0.2957253158092499\n",
      "Epoch 28 -- Batch 35/ 842, training loss 0.3165688216686249\n",
      "Epoch 28 -- Batch 36/ 842, training loss 0.3054334819316864\n",
      "Epoch 28 -- Batch 37/ 842, training loss 0.3039145767688751\n",
      "Epoch 28 -- Batch 38/ 842, training loss 0.31253743171691895\n",
      "Epoch 28 -- Batch 39/ 842, training loss 0.3008235991001129\n",
      "Epoch 28 -- Batch 40/ 842, training loss 0.3030782639980316\n",
      "Epoch 28 -- Batch 41/ 842, training loss 0.30393001437187195\n",
      "Epoch 28 -- Batch 42/ 842, training loss 0.29419809579849243\n",
      "Epoch 28 -- Batch 43/ 842, training loss 0.29756245017051697\n",
      "Epoch 28 -- Batch 44/ 842, training loss 0.2975144684314728\n",
      "Epoch 28 -- Batch 45/ 842, training loss 0.3073790967464447\n",
      "Epoch 28 -- Batch 46/ 842, training loss 0.304429829120636\n",
      "Epoch 28 -- Batch 47/ 842, training loss 0.30996307730674744\n",
      "Epoch 28 -- Batch 48/ 842, training loss 0.3090812861919403\n",
      "Epoch 28 -- Batch 49/ 842, training loss 0.3186047375202179\n",
      "Epoch 28 -- Batch 50/ 842, training loss 0.2994356155395508\n",
      "Epoch 28 -- Batch 51/ 842, training loss 0.31565284729003906\n",
      "Epoch 28 -- Batch 52/ 842, training loss 0.3050980269908905\n",
      "Epoch 28 -- Batch 53/ 842, training loss 0.2965584695339203\n",
      "Epoch 28 -- Batch 54/ 842, training loss 0.2947697937488556\n",
      "Epoch 28 -- Batch 55/ 842, training loss 0.314402312040329\n",
      "Epoch 28 -- Batch 56/ 842, training loss 0.3025825321674347\n",
      "Epoch 28 -- Batch 57/ 842, training loss 0.3113366663455963\n",
      "Epoch 28 -- Batch 58/ 842, training loss 0.3096860349178314\n",
      "Epoch 28 -- Batch 59/ 842, training loss 0.30470457673072815\n",
      "Epoch 28 -- Batch 60/ 842, training loss 0.30729976296424866\n",
      "Epoch 28 -- Batch 61/ 842, training loss 0.3035586476325989\n",
      "Epoch 28 -- Batch 62/ 842, training loss 0.291964590549469\n",
      "Epoch 28 -- Batch 63/ 842, training loss 0.30671006441116333\n",
      "Epoch 28 -- Batch 64/ 842, training loss 0.3084927499294281\n",
      "Epoch 28 -- Batch 65/ 842, training loss 0.30812400579452515\n",
      "Epoch 28 -- Batch 66/ 842, training loss 0.3051835298538208\n",
      "Epoch 28 -- Batch 67/ 842, training loss 0.3093622326850891\n",
      "Epoch 28 -- Batch 68/ 842, training loss 0.3068207800388336\n",
      "Epoch 28 -- Batch 69/ 842, training loss 0.30818668007850647\n",
      "Epoch 28 -- Batch 70/ 842, training loss 0.31191229820251465\n",
      "Epoch 28 -- Batch 71/ 842, training loss 0.29402104020118713\n",
      "Epoch 28 -- Batch 72/ 842, training loss 0.3132353723049164\n",
      "Epoch 28 -- Batch 73/ 842, training loss 0.3123311400413513\n",
      "Epoch 28 -- Batch 74/ 842, training loss 0.2983848750591278\n",
      "Epoch 28 -- Batch 75/ 842, training loss 0.3140678107738495\n",
      "Epoch 28 -- Batch 76/ 842, training loss 0.3092013895511627\n",
      "Epoch 28 -- Batch 77/ 842, training loss 0.30355897545814514\n",
      "Epoch 28 -- Batch 78/ 842, training loss 0.30447155237197876\n",
      "Epoch 28 -- Batch 79/ 842, training loss 0.30909720063209534\n",
      "Epoch 28 -- Batch 80/ 842, training loss 0.29586389660835266\n",
      "Epoch 28 -- Batch 81/ 842, training loss 0.3107927143573761\n",
      "Epoch 28 -- Batch 82/ 842, training loss 0.3070356249809265\n",
      "Epoch 28 -- Batch 83/ 842, training loss 0.29474830627441406\n",
      "Epoch 28 -- Batch 84/ 842, training loss 0.3002417981624603\n",
      "Epoch 28 -- Batch 85/ 842, training loss 0.31251415610313416\n",
      "Epoch 28 -- Batch 86/ 842, training loss 0.29833486676216125\n",
      "Epoch 28 -- Batch 87/ 842, training loss 0.3083917498588562\n",
      "Epoch 28 -- Batch 88/ 842, training loss 0.30355727672576904\n",
      "Epoch 28 -- Batch 89/ 842, training loss 0.3022466003894806\n",
      "Epoch 28 -- Batch 90/ 842, training loss 0.3096393048763275\n",
      "Epoch 28 -- Batch 91/ 842, training loss 0.29803013801574707\n",
      "Epoch 28 -- Batch 92/ 842, training loss 0.3038954436779022\n",
      "Epoch 28 -- Batch 93/ 842, training loss 0.2958337664604187\n",
      "Epoch 28 -- Batch 94/ 842, training loss 0.2944326400756836\n",
      "Epoch 28 -- Batch 95/ 842, training loss 0.30750182271003723\n",
      "Epoch 28 -- Batch 96/ 842, training loss 0.2957429587841034\n",
      "Epoch 28 -- Batch 97/ 842, training loss 0.3213619589805603\n",
      "Epoch 28 -- Batch 98/ 842, training loss 0.3079334497451782\n",
      "Epoch 28 -- Batch 99/ 842, training loss 0.3118308186531067\n",
      "Epoch 28 -- Batch 100/ 842, training loss 0.2975514233112335\n",
      "Epoch 28 -- Batch 101/ 842, training loss 0.29681462049484253\n",
      "Epoch 28 -- Batch 102/ 842, training loss 0.3083174526691437\n",
      "Epoch 28 -- Batch 103/ 842, training loss 0.30828264355659485\n",
      "Epoch 28 -- Batch 104/ 842, training loss 0.2951538562774658\n",
      "Epoch 28 -- Batch 105/ 842, training loss 0.3051440715789795\n",
      "Epoch 28 -- Batch 106/ 842, training loss 0.30713117122650146\n",
      "Epoch 28 -- Batch 107/ 842, training loss 0.30572497844696045\n",
      "Epoch 28 -- Batch 108/ 842, training loss 0.3066893517971039\n",
      "Epoch 28 -- Batch 109/ 842, training loss 0.3115499019622803\n",
      "Epoch 28 -- Batch 110/ 842, training loss 0.2999548316001892\n",
      "Epoch 28 -- Batch 111/ 842, training loss 0.30024453997612\n",
      "Epoch 28 -- Batch 112/ 842, training loss 0.3083516061306\n",
      "Epoch 28 -- Batch 113/ 842, training loss 0.2918825149536133\n",
      "Epoch 28 -- Batch 114/ 842, training loss 0.2994842231273651\n",
      "Epoch 28 -- Batch 115/ 842, training loss 0.3008214831352234\n",
      "Epoch 28 -- Batch 116/ 842, training loss 0.29699862003326416\n",
      "Epoch 28 -- Batch 117/ 842, training loss 0.319717675447464\n",
      "Epoch 28 -- Batch 118/ 842, training loss 0.3028414249420166\n",
      "Epoch 28 -- Batch 119/ 842, training loss 0.29687759280204773\n",
      "Epoch 28 -- Batch 120/ 842, training loss 0.30011701583862305\n",
      "Epoch 28 -- Batch 121/ 842, training loss 0.31595030426979065\n",
      "Epoch 28 -- Batch 122/ 842, training loss 0.3052099645137787\n",
      "Epoch 28 -- Batch 123/ 842, training loss 0.29997751116752625\n",
      "Epoch 28 -- Batch 124/ 842, training loss 0.3011440932750702\n",
      "Epoch 28 -- Batch 125/ 842, training loss 0.2922758460044861\n",
      "Epoch 28 -- Batch 126/ 842, training loss 0.30317559838294983\n",
      "Epoch 28 -- Batch 127/ 842, training loss 0.3184354305267334\n",
      "Epoch 28 -- Batch 128/ 842, training loss 0.3086867928504944\n",
      "Epoch 28 -- Batch 129/ 842, training loss 0.31240352988243103\n",
      "Epoch 28 -- Batch 130/ 842, training loss 0.3052898049354553\n",
      "Epoch 28 -- Batch 131/ 842, training loss 0.31131428480148315\n",
      "Epoch 28 -- Batch 132/ 842, training loss 0.3103795647621155\n",
      "Epoch 28 -- Batch 133/ 842, training loss 0.31308242678642273\n",
      "Epoch 28 -- Batch 134/ 842, training loss 0.31010112166404724\n",
      "Epoch 28 -- Batch 135/ 842, training loss 0.3077714741230011\n",
      "Epoch 28 -- Batch 136/ 842, training loss 0.3060602843761444\n",
      "Epoch 28 -- Batch 137/ 842, training loss 0.2950199842453003\n",
      "Epoch 28 -- Batch 138/ 842, training loss 0.2987872362136841\n",
      "Epoch 28 -- Batch 139/ 842, training loss 0.2953311800956726\n",
      "Epoch 28 -- Batch 140/ 842, training loss 0.2946116328239441\n",
      "Epoch 28 -- Batch 141/ 842, training loss 0.30790430307388306\n",
      "Epoch 28 -- Batch 142/ 842, training loss 0.3043633699417114\n",
      "Epoch 28 -- Batch 143/ 842, training loss 0.29851219058036804\n",
      "Epoch 28 -- Batch 144/ 842, training loss 0.2999582290649414\n",
      "Epoch 28 -- Batch 145/ 842, training loss 0.30898016691207886\n",
      "Epoch 28 -- Batch 146/ 842, training loss 0.29890549182891846\n",
      "Epoch 28 -- Batch 147/ 842, training loss 0.2983965575695038\n",
      "Epoch 28 -- Batch 148/ 842, training loss 0.2995784282684326\n",
      "Epoch 28 -- Batch 149/ 842, training loss 0.3164191246032715\n",
      "Epoch 28 -- Batch 150/ 842, training loss 0.31279686093330383\n",
      "Epoch 28 -- Batch 151/ 842, training loss 0.30782896280288696\n",
      "Epoch 28 -- Batch 152/ 842, training loss 0.30499887466430664\n",
      "Epoch 28 -- Batch 153/ 842, training loss 0.3089808225631714\n",
      "Epoch 28 -- Batch 154/ 842, training loss 0.3027631938457489\n",
      "Epoch 28 -- Batch 155/ 842, training loss 0.30443572998046875\n",
      "Epoch 28 -- Batch 156/ 842, training loss 0.3027845621109009\n",
      "Epoch 28 -- Batch 157/ 842, training loss 0.30523034930229187\n",
      "Epoch 28 -- Batch 158/ 842, training loss 0.2981967031955719\n",
      "Epoch 28 -- Batch 159/ 842, training loss 0.3137381374835968\n",
      "Epoch 28 -- Batch 160/ 842, training loss 0.30429738759994507\n",
      "Epoch 28 -- Batch 161/ 842, training loss 0.30279579758644104\n",
      "Epoch 28 -- Batch 162/ 842, training loss 0.2995277941226959\n",
      "Epoch 28 -- Batch 163/ 842, training loss 0.3166283667087555\n",
      "Epoch 28 -- Batch 164/ 842, training loss 0.30568253993988037\n",
      "Epoch 28 -- Batch 165/ 842, training loss 0.31200292706489563\n",
      "Epoch 28 -- Batch 166/ 842, training loss 0.31068336963653564\n",
      "Epoch 28 -- Batch 167/ 842, training loss 0.3058200776576996\n",
      "Epoch 28 -- Batch 168/ 842, training loss 0.30576643347740173\n",
      "Epoch 28 -- Batch 169/ 842, training loss 0.31580850481987\n",
      "Epoch 28 -- Batch 170/ 842, training loss 0.29793187975883484\n",
      "Epoch 28 -- Batch 171/ 842, training loss 0.30240607261657715\n",
      "Epoch 28 -- Batch 172/ 842, training loss 0.30809149146080017\n",
      "Epoch 28 -- Batch 173/ 842, training loss 0.31310972571372986\n",
      "Epoch 28 -- Batch 174/ 842, training loss 0.30477434396743774\n",
      "Epoch 28 -- Batch 175/ 842, training loss 0.3000469505786896\n",
      "Epoch 28 -- Batch 176/ 842, training loss 0.2995011508464813\n",
      "Epoch 28 -- Batch 177/ 842, training loss 0.29569199681282043\n",
      "Epoch 28 -- Batch 178/ 842, training loss 0.3071479797363281\n",
      "Epoch 28 -- Batch 179/ 842, training loss 0.3015064001083374\n",
      "Epoch 28 -- Batch 180/ 842, training loss 0.2941582500934601\n",
      "Epoch 28 -- Batch 181/ 842, training loss 0.31208738684654236\n",
      "Epoch 28 -- Batch 182/ 842, training loss 0.3005595803260803\n",
      "Epoch 28 -- Batch 183/ 842, training loss 0.3144342005252838\n",
      "Epoch 28 -- Batch 184/ 842, training loss 0.3055151402950287\n",
      "Epoch 28 -- Batch 185/ 842, training loss 0.3123169243335724\n",
      "Epoch 28 -- Batch 186/ 842, training loss 0.29625681042671204\n",
      "Epoch 28 -- Batch 187/ 842, training loss 0.3158579170703888\n",
      "Epoch 28 -- Batch 188/ 842, training loss 0.30953624844551086\n",
      "Epoch 28 -- Batch 189/ 842, training loss 0.3022412061691284\n",
      "Epoch 28 -- Batch 190/ 842, training loss 0.3075382113456726\n",
      "Epoch 28 -- Batch 191/ 842, training loss 0.3058140277862549\n",
      "Epoch 28 -- Batch 192/ 842, training loss 0.30601081252098083\n",
      "Epoch 28 -- Batch 193/ 842, training loss 0.312165230512619\n",
      "Epoch 28 -- Batch 194/ 842, training loss 0.31238746643066406\n",
      "Epoch 28 -- Batch 195/ 842, training loss 0.3070543110370636\n",
      "Epoch 28 -- Batch 196/ 842, training loss 0.30121007561683655\n",
      "Epoch 28 -- Batch 197/ 842, training loss 0.31702154874801636\n",
      "Epoch 28 -- Batch 198/ 842, training loss 0.31118813157081604\n",
      "Epoch 28 -- Batch 199/ 842, training loss 0.30617329478263855\n",
      "Epoch 28 -- Batch 200/ 842, training loss 0.3080485761165619\n",
      "Epoch 28 -- Batch 201/ 842, training loss 0.30616533756256104\n",
      "Epoch 28 -- Batch 202/ 842, training loss 0.31308314204216003\n",
      "Epoch 28 -- Batch 203/ 842, training loss 0.31335875391960144\n",
      "Epoch 28 -- Batch 204/ 842, training loss 0.3038281798362732\n",
      "Epoch 28 -- Batch 205/ 842, training loss 0.29465848207473755\n",
      "Epoch 28 -- Batch 206/ 842, training loss 0.3014615476131439\n",
      "Epoch 28 -- Batch 207/ 842, training loss 0.2973344624042511\n",
      "Epoch 28 -- Batch 208/ 842, training loss 0.30810001492500305\n",
      "Epoch 28 -- Batch 209/ 842, training loss 0.3100147247314453\n",
      "Epoch 28 -- Batch 210/ 842, training loss 0.31179478764533997\n",
      "Epoch 28 -- Batch 211/ 842, training loss 0.30618056654930115\n",
      "Epoch 28 -- Batch 212/ 842, training loss 0.3013960123062134\n",
      "Epoch 28 -- Batch 213/ 842, training loss 0.29558730125427246\n",
      "Epoch 28 -- Batch 214/ 842, training loss 0.296414852142334\n",
      "Epoch 28 -- Batch 215/ 842, training loss 0.29532390832901\n",
      "Epoch 28 -- Batch 216/ 842, training loss 0.3221640884876251\n",
      "Epoch 28 -- Batch 217/ 842, training loss 0.3088228106498718\n",
      "Epoch 28 -- Batch 218/ 842, training loss 0.30448803305625916\n",
      "Epoch 28 -- Batch 219/ 842, training loss 0.3094044327735901\n",
      "Epoch 28 -- Batch 220/ 842, training loss 0.30868685245513916\n",
      "Epoch 28 -- Batch 221/ 842, training loss 0.3090498745441437\n",
      "Epoch 28 -- Batch 222/ 842, training loss 0.311389297246933\n",
      "Epoch 28 -- Batch 223/ 842, training loss 0.3140152096748352\n",
      "Epoch 28 -- Batch 224/ 842, training loss 0.3079984784126282\n",
      "Epoch 28 -- Batch 225/ 842, training loss 0.29549041390419006\n",
      "Epoch 28 -- Batch 226/ 842, training loss 0.31804168224334717\n",
      "Epoch 28 -- Batch 227/ 842, training loss 0.3040187656879425\n",
      "Epoch 28 -- Batch 228/ 842, training loss 0.30682018399238586\n",
      "Epoch 28 -- Batch 229/ 842, training loss 0.30509302020072937\n",
      "Epoch 28 -- Batch 230/ 842, training loss 0.3128111660480499\n",
      "Epoch 28 -- Batch 231/ 842, training loss 0.298069030046463\n",
      "Epoch 28 -- Batch 232/ 842, training loss 0.31103911995887756\n",
      "Epoch 28 -- Batch 233/ 842, training loss 0.31135252118110657\n",
      "Epoch 28 -- Batch 234/ 842, training loss 0.3060462474822998\n",
      "Epoch 28 -- Batch 235/ 842, training loss 0.3101683557033539\n",
      "Epoch 28 -- Batch 236/ 842, training loss 0.30285388231277466\n",
      "Epoch 28 -- Batch 237/ 842, training loss 0.31085753440856934\n",
      "Epoch 28 -- Batch 238/ 842, training loss 0.31810933351516724\n",
      "Epoch 28 -- Batch 239/ 842, training loss 0.30495980381965637\n",
      "Epoch 28 -- Batch 240/ 842, training loss 0.3076742887496948\n",
      "Epoch 28 -- Batch 241/ 842, training loss 0.32995232939720154\n",
      "Epoch 28 -- Batch 242/ 842, training loss 0.3067164421081543\n",
      "Epoch 28 -- Batch 243/ 842, training loss 0.3107587397098541\n",
      "Epoch 28 -- Batch 244/ 842, training loss 0.3041500151157379\n",
      "Epoch 28 -- Batch 245/ 842, training loss 0.2954937219619751\n",
      "Epoch 28 -- Batch 246/ 842, training loss 0.28597816824913025\n",
      "Epoch 28 -- Batch 247/ 842, training loss 0.3039931058883667\n",
      "Epoch 28 -- Batch 248/ 842, training loss 0.31038302183151245\n",
      "Epoch 28 -- Batch 249/ 842, training loss 0.30444812774658203\n",
      "Epoch 28 -- Batch 250/ 842, training loss 0.3105199635028839\n",
      "Epoch 28 -- Batch 251/ 842, training loss 0.30438992381095886\n",
      "Epoch 28 -- Batch 252/ 842, training loss 0.2948710322380066\n",
      "Epoch 28 -- Batch 253/ 842, training loss 0.2997715473175049\n",
      "Epoch 28 -- Batch 254/ 842, training loss 0.291456401348114\n",
      "Epoch 28 -- Batch 255/ 842, training loss 0.30599892139434814\n",
      "Epoch 28 -- Batch 256/ 842, training loss 0.3059155344963074\n",
      "Epoch 28 -- Batch 257/ 842, training loss 0.303320974111557\n",
      "Epoch 28 -- Batch 258/ 842, training loss 0.3057154715061188\n",
      "Epoch 28 -- Batch 259/ 842, training loss 0.2953041195869446\n",
      "Epoch 28 -- Batch 260/ 842, training loss 0.31233447790145874\n",
      "Epoch 28 -- Batch 261/ 842, training loss 0.3005845248699188\n",
      "Epoch 28 -- Batch 262/ 842, training loss 0.31216302514076233\n",
      "Epoch 28 -- Batch 263/ 842, training loss 0.3046720027923584\n",
      "Epoch 28 -- Batch 264/ 842, training loss 0.3158823251724243\n",
      "Epoch 28 -- Batch 265/ 842, training loss 0.31082239747047424\n",
      "Epoch 28 -- Batch 266/ 842, training loss 0.31260767579078674\n",
      "Epoch 28 -- Batch 267/ 842, training loss 0.30550727248191833\n",
      "Epoch 28 -- Batch 268/ 842, training loss 0.3075851798057556\n",
      "Epoch 28 -- Batch 269/ 842, training loss 0.30881184339523315\n",
      "Epoch 28 -- Batch 270/ 842, training loss 0.3058103621006012\n",
      "Epoch 28 -- Batch 271/ 842, training loss 0.3025939166545868\n",
      "Epoch 28 -- Batch 272/ 842, training loss 0.30537331104278564\n",
      "Epoch 28 -- Batch 273/ 842, training loss 0.3114795684814453\n",
      "Epoch 28 -- Batch 274/ 842, training loss 0.3026657998561859\n",
      "Epoch 28 -- Batch 275/ 842, training loss 0.3021879196166992\n",
      "Epoch 28 -- Batch 276/ 842, training loss 0.31748825311660767\n",
      "Epoch 28 -- Batch 277/ 842, training loss 0.3101118803024292\n",
      "Epoch 28 -- Batch 278/ 842, training loss 0.3139205873012543\n",
      "Epoch 28 -- Batch 279/ 842, training loss 0.30096009373664856\n",
      "Epoch 28 -- Batch 280/ 842, training loss 0.29972630739212036\n",
      "Epoch 28 -- Batch 281/ 842, training loss 0.3013557195663452\n",
      "Epoch 28 -- Batch 282/ 842, training loss 0.312664657831192\n",
      "Epoch 28 -- Batch 283/ 842, training loss 0.2954828441143036\n",
      "Epoch 28 -- Batch 284/ 842, training loss 0.3081546425819397\n",
      "Epoch 28 -- Batch 285/ 842, training loss 0.30915307998657227\n",
      "Epoch 28 -- Batch 286/ 842, training loss 0.31072139739990234\n",
      "Epoch 28 -- Batch 287/ 842, training loss 0.30354779958724976\n",
      "Epoch 28 -- Batch 288/ 842, training loss 0.31224122643470764\n",
      "Epoch 28 -- Batch 289/ 842, training loss 0.30799558758735657\n",
      "Epoch 28 -- Batch 290/ 842, training loss 0.30698785185813904\n",
      "Epoch 28 -- Batch 291/ 842, training loss 0.31023895740509033\n",
      "Epoch 28 -- Batch 292/ 842, training loss 0.30879706144332886\n",
      "Epoch 28 -- Batch 293/ 842, training loss 0.2984906733036041\n",
      "Epoch 28 -- Batch 294/ 842, training loss 0.30391740798950195\n",
      "Epoch 28 -- Batch 295/ 842, training loss 0.29726943373680115\n",
      "Epoch 28 -- Batch 296/ 842, training loss 0.30621644854545593\n",
      "Epoch 28 -- Batch 297/ 842, training loss 0.31332889199256897\n",
      "Epoch 28 -- Batch 298/ 842, training loss 0.2999563217163086\n",
      "Epoch 28 -- Batch 299/ 842, training loss 0.3075754642486572\n",
      "Epoch 28 -- Batch 300/ 842, training loss 0.29690656065940857\n",
      "Epoch 28 -- Batch 301/ 842, training loss 0.3121993839740753\n",
      "Epoch 28 -- Batch 302/ 842, training loss 0.29668277502059937\n",
      "Epoch 28 -- Batch 303/ 842, training loss 0.3081571161746979\n",
      "Epoch 28 -- Batch 304/ 842, training loss 0.29966866970062256\n",
      "Epoch 28 -- Batch 305/ 842, training loss 0.3058890104293823\n",
      "Epoch 28 -- Batch 306/ 842, training loss 0.30591005086898804\n",
      "Epoch 28 -- Batch 307/ 842, training loss 0.3195537030696869\n",
      "Epoch 28 -- Batch 308/ 842, training loss 0.3019449710845947\n",
      "Epoch 28 -- Batch 309/ 842, training loss 0.3163551092147827\n",
      "Epoch 28 -- Batch 310/ 842, training loss 0.31104975938796997\n",
      "Epoch 28 -- Batch 311/ 842, training loss 0.3031895160675049\n",
      "Epoch 28 -- Batch 312/ 842, training loss 0.31011316180229187\n",
      "Epoch 28 -- Batch 313/ 842, training loss 0.31827589869499207\n",
      "Epoch 28 -- Batch 314/ 842, training loss 0.309524267911911\n",
      "Epoch 28 -- Batch 315/ 842, training loss 0.29657140374183655\n",
      "Epoch 28 -- Batch 316/ 842, training loss 0.3144857883453369\n",
      "Epoch 28 -- Batch 317/ 842, training loss 0.31039366126060486\n",
      "Epoch 28 -- Batch 318/ 842, training loss 0.3033726215362549\n",
      "Epoch 28 -- Batch 319/ 842, training loss 0.3088921010494232\n",
      "Epoch 28 -- Batch 320/ 842, training loss 0.3142760694026947\n",
      "Epoch 28 -- Batch 321/ 842, training loss 0.30134740471839905\n",
      "Epoch 28 -- Batch 322/ 842, training loss 0.3055994510650635\n",
      "Epoch 28 -- Batch 323/ 842, training loss 0.3045942485332489\n",
      "Epoch 28 -- Batch 324/ 842, training loss 0.31360286474227905\n",
      "Epoch 28 -- Batch 325/ 842, training loss 0.3045099377632141\n",
      "Epoch 28 -- Batch 326/ 842, training loss 0.32119932770729065\n",
      "Epoch 28 -- Batch 327/ 842, training loss 0.2992958724498749\n",
      "Epoch 28 -- Batch 328/ 842, training loss 0.30967891216278076\n",
      "Epoch 28 -- Batch 329/ 842, training loss 0.3038438856601715\n",
      "Epoch 28 -- Batch 330/ 842, training loss 0.3072272539138794\n",
      "Epoch 28 -- Batch 331/ 842, training loss 0.2925511598587036\n",
      "Epoch 28 -- Batch 332/ 842, training loss 0.30337294936180115\n",
      "Epoch 28 -- Batch 333/ 842, training loss 0.31760141253471375\n",
      "Epoch 28 -- Batch 334/ 842, training loss 0.30730462074279785\n",
      "Epoch 28 -- Batch 335/ 842, training loss 0.31360864639282227\n",
      "Epoch 28 -- Batch 336/ 842, training loss 0.29880622029304504\n",
      "Epoch 28 -- Batch 337/ 842, training loss 0.29289335012435913\n",
      "Epoch 28 -- Batch 338/ 842, training loss 0.30618545413017273\n",
      "Epoch 28 -- Batch 339/ 842, training loss 0.3076877295970917\n",
      "Epoch 28 -- Batch 340/ 842, training loss 0.3055589497089386\n",
      "Epoch 28 -- Batch 341/ 842, training loss 0.30274733901023865\n",
      "Epoch 28 -- Batch 342/ 842, training loss 0.31408292055130005\n",
      "Epoch 28 -- Batch 343/ 842, training loss 0.30262571573257446\n",
      "Epoch 28 -- Batch 344/ 842, training loss 0.2988691031932831\n",
      "Epoch 28 -- Batch 345/ 842, training loss 0.30635154247283936\n",
      "Epoch 28 -- Batch 346/ 842, training loss 0.3219805359840393\n",
      "Epoch 28 -- Batch 347/ 842, training loss 0.3109375536441803\n",
      "Epoch 28 -- Batch 348/ 842, training loss 0.29863253235816956\n",
      "Epoch 28 -- Batch 349/ 842, training loss 0.31463027000427246\n",
      "Epoch 28 -- Batch 350/ 842, training loss 0.3044193685054779\n",
      "Epoch 28 -- Batch 351/ 842, training loss 0.3052014708518982\n",
      "Epoch 28 -- Batch 352/ 842, training loss 0.3092381954193115\n",
      "Epoch 28 -- Batch 353/ 842, training loss 0.3144817054271698\n",
      "Epoch 28 -- Batch 354/ 842, training loss 0.3127858638763428\n",
      "Epoch 28 -- Batch 355/ 842, training loss 0.30865478515625\n",
      "Epoch 28 -- Batch 356/ 842, training loss 0.31705790758132935\n",
      "Epoch 28 -- Batch 357/ 842, training loss 0.3137694299221039\n",
      "Epoch 28 -- Batch 358/ 842, training loss 0.312725305557251\n",
      "Epoch 28 -- Batch 359/ 842, training loss 0.3030332922935486\n",
      "Epoch 28 -- Batch 360/ 842, training loss 0.29768574237823486\n",
      "Epoch 28 -- Batch 361/ 842, training loss 0.30362042784690857\n",
      "Epoch 28 -- Batch 362/ 842, training loss 0.31392717361450195\n",
      "Epoch 28 -- Batch 363/ 842, training loss 0.30633801221847534\n",
      "Epoch 28 -- Batch 364/ 842, training loss 0.2998882234096527\n",
      "Epoch 28 -- Batch 365/ 842, training loss 0.3036177158355713\n",
      "Epoch 28 -- Batch 366/ 842, training loss 0.31054946780204773\n",
      "Epoch 28 -- Batch 367/ 842, training loss 0.2980351448059082\n",
      "Epoch 28 -- Batch 368/ 842, training loss 0.30332380533218384\n",
      "Epoch 28 -- Batch 369/ 842, training loss 0.3130094110965729\n",
      "Epoch 28 -- Batch 370/ 842, training loss 0.30033767223358154\n",
      "Epoch 28 -- Batch 371/ 842, training loss 0.30322620272636414\n",
      "Epoch 28 -- Batch 372/ 842, training loss 0.313493937253952\n",
      "Epoch 28 -- Batch 373/ 842, training loss 0.32008588314056396\n",
      "Epoch 28 -- Batch 374/ 842, training loss 0.3009103834629059\n",
      "Epoch 28 -- Batch 375/ 842, training loss 0.30556532740592957\n",
      "Epoch 28 -- Batch 376/ 842, training loss 0.3007048964500427\n",
      "Epoch 28 -- Batch 377/ 842, training loss 0.30955296754837036\n",
      "Epoch 28 -- Batch 378/ 842, training loss 0.3119906485080719\n",
      "Epoch 28 -- Batch 379/ 842, training loss 0.2903733253479004\n",
      "Epoch 28 -- Batch 380/ 842, training loss 0.3128875494003296\n",
      "Epoch 28 -- Batch 381/ 842, training loss 0.2997262179851532\n",
      "Epoch 28 -- Batch 382/ 842, training loss 0.3032691776752472\n",
      "Epoch 28 -- Batch 383/ 842, training loss 0.29953238368034363\n",
      "Epoch 28 -- Batch 384/ 842, training loss 0.3008202910423279\n",
      "Epoch 28 -- Batch 385/ 842, training loss 0.3015831708908081\n",
      "Epoch 28 -- Batch 386/ 842, training loss 0.30121731758117676\n",
      "Epoch 28 -- Batch 387/ 842, training loss 0.29873818159103394\n",
      "Epoch 28 -- Batch 388/ 842, training loss 0.31949129700660706\n",
      "Epoch 28 -- Batch 389/ 842, training loss 0.29845887422561646\n",
      "Epoch 28 -- Batch 390/ 842, training loss 0.300375759601593\n",
      "Epoch 28 -- Batch 391/ 842, training loss 0.3067747950553894\n",
      "Epoch 28 -- Batch 392/ 842, training loss 0.3125464618206024\n",
      "Epoch 28 -- Batch 393/ 842, training loss 0.30643752217292786\n",
      "Epoch 28 -- Batch 394/ 842, training loss 0.2973475456237793\n",
      "Epoch 28 -- Batch 395/ 842, training loss 0.3172098994255066\n",
      "Epoch 28 -- Batch 396/ 842, training loss 0.30848997831344604\n",
      "Epoch 28 -- Batch 397/ 842, training loss 0.30534473061561584\n",
      "Epoch 28 -- Batch 398/ 842, training loss 0.3036772608757019\n",
      "Epoch 28 -- Batch 399/ 842, training loss 0.3054587244987488\n",
      "Epoch 28 -- Batch 400/ 842, training loss 0.30697089433670044\n",
      "Epoch 28 -- Batch 401/ 842, training loss 0.3118075430393219\n",
      "Epoch 28 -- Batch 402/ 842, training loss 0.30464065074920654\n",
      "Epoch 28 -- Batch 403/ 842, training loss 0.3113357126712799\n",
      "Epoch 28 -- Batch 404/ 842, training loss 0.30881842970848083\n",
      "Epoch 28 -- Batch 405/ 842, training loss 0.31289997696876526\n",
      "Epoch 28 -- Batch 406/ 842, training loss 0.3134596347808838\n",
      "Epoch 28 -- Batch 407/ 842, training loss 0.31318989396095276\n",
      "Epoch 28 -- Batch 408/ 842, training loss 0.3061290383338928\n",
      "Epoch 28 -- Batch 409/ 842, training loss 0.3049207627773285\n",
      "Epoch 28 -- Batch 410/ 842, training loss 0.2994686961174011\n",
      "Epoch 28 -- Batch 411/ 842, training loss 0.3141983151435852\n",
      "Epoch 28 -- Batch 412/ 842, training loss 0.2977244555950165\n",
      "Epoch 28 -- Batch 413/ 842, training loss 0.31060245633125305\n",
      "Epoch 28 -- Batch 414/ 842, training loss 0.31231915950775146\n",
      "Epoch 28 -- Batch 415/ 842, training loss 0.3196692168712616\n",
      "Epoch 28 -- Batch 416/ 842, training loss 0.31374409794807434\n",
      "Epoch 28 -- Batch 417/ 842, training loss 0.30646124482154846\n",
      "Epoch 28 -- Batch 418/ 842, training loss 0.3192942142486572\n",
      "Epoch 28 -- Batch 419/ 842, training loss 0.306702196598053\n",
      "Epoch 28 -- Batch 420/ 842, training loss 0.31173476576805115\n",
      "Epoch 28 -- Batch 421/ 842, training loss 0.30830827355384827\n",
      "Epoch 28 -- Batch 422/ 842, training loss 0.3062194883823395\n",
      "Epoch 28 -- Batch 423/ 842, training loss 0.3200140595436096\n",
      "Epoch 28 -- Batch 424/ 842, training loss 0.30516237020492554\n",
      "Epoch 28 -- Batch 425/ 842, training loss 0.3001076579093933\n",
      "Epoch 28 -- Batch 426/ 842, training loss 0.316752552986145\n",
      "Epoch 28 -- Batch 427/ 842, training loss 0.31576967239379883\n",
      "Epoch 28 -- Batch 428/ 842, training loss 0.3009810447692871\n",
      "Epoch 28 -- Batch 429/ 842, training loss 0.3225197196006775\n",
      "Epoch 28 -- Batch 430/ 842, training loss 0.3090039789676666\n",
      "Epoch 28 -- Batch 431/ 842, training loss 0.327382355928421\n",
      "Epoch 28 -- Batch 432/ 842, training loss 0.30567070841789246\n",
      "Epoch 28 -- Batch 433/ 842, training loss 0.29716575145721436\n",
      "Epoch 28 -- Batch 434/ 842, training loss 0.3170832395553589\n",
      "Epoch 28 -- Batch 435/ 842, training loss 0.30244413018226624\n",
      "Epoch 28 -- Batch 436/ 842, training loss 0.302674263715744\n",
      "Epoch 28 -- Batch 437/ 842, training loss 0.31606990098953247\n",
      "Epoch 28 -- Batch 438/ 842, training loss 0.3056296706199646\n",
      "Epoch 28 -- Batch 439/ 842, training loss 0.322415828704834\n",
      "Epoch 28 -- Batch 440/ 842, training loss 0.3030678927898407\n",
      "Epoch 28 -- Batch 441/ 842, training loss 0.30116379261016846\n",
      "Epoch 28 -- Batch 442/ 842, training loss 0.2998194098472595\n",
      "Epoch 28 -- Batch 443/ 842, training loss 0.3085877001285553\n",
      "Epoch 28 -- Batch 444/ 842, training loss 0.3125942647457123\n",
      "Epoch 28 -- Batch 445/ 842, training loss 0.3063422441482544\n",
      "Epoch 28 -- Batch 446/ 842, training loss 0.30784472823143005\n",
      "Epoch 28 -- Batch 447/ 842, training loss 0.31999677419662476\n",
      "Epoch 28 -- Batch 448/ 842, training loss 0.31395453214645386\n",
      "Epoch 28 -- Batch 449/ 842, training loss 0.30114439129829407\n",
      "Epoch 28 -- Batch 450/ 842, training loss 0.31866103410720825\n",
      "Epoch 28 -- Batch 451/ 842, training loss 0.30801305174827576\n",
      "Epoch 28 -- Batch 452/ 842, training loss 0.3126438558101654\n",
      "Epoch 28 -- Batch 453/ 842, training loss 0.3096764087677002\n",
      "Epoch 28 -- Batch 454/ 842, training loss 0.3205048739910126\n",
      "Epoch 28 -- Batch 455/ 842, training loss 0.3111504018306732\n",
      "Epoch 28 -- Batch 456/ 842, training loss 0.3036055862903595\n",
      "Epoch 28 -- Batch 457/ 842, training loss 0.307023823261261\n",
      "Epoch 28 -- Batch 458/ 842, training loss 0.3140106201171875\n",
      "Epoch 28 -- Batch 459/ 842, training loss 0.31508713960647583\n",
      "Epoch 28 -- Batch 460/ 842, training loss 0.30480435490608215\n",
      "Epoch 28 -- Batch 461/ 842, training loss 0.3113585412502289\n",
      "Epoch 28 -- Batch 462/ 842, training loss 0.3113960027694702\n",
      "Epoch 28 -- Batch 463/ 842, training loss 0.31310272216796875\n",
      "Epoch 28 -- Batch 464/ 842, training loss 0.29639697074890137\n",
      "Epoch 28 -- Batch 465/ 842, training loss 0.3017987012863159\n",
      "Epoch 28 -- Batch 466/ 842, training loss 0.311628133058548\n",
      "Epoch 28 -- Batch 467/ 842, training loss 0.3153330981731415\n",
      "Epoch 28 -- Batch 468/ 842, training loss 0.31162741780281067\n",
      "Epoch 28 -- Batch 469/ 842, training loss 0.3125075101852417\n",
      "Epoch 28 -- Batch 470/ 842, training loss 0.30601122975349426\n",
      "Epoch 28 -- Batch 471/ 842, training loss 0.31018057465553284\n",
      "Epoch 28 -- Batch 472/ 842, training loss 0.3068409860134125\n",
      "Epoch 28 -- Batch 473/ 842, training loss 0.3193495571613312\n",
      "Epoch 28 -- Batch 474/ 842, training loss 0.31594499945640564\n",
      "Epoch 28 -- Batch 475/ 842, training loss 0.30727988481521606\n",
      "Epoch 28 -- Batch 476/ 842, training loss 0.31437113881111145\n",
      "Epoch 28 -- Batch 477/ 842, training loss 0.3102329969406128\n",
      "Epoch 28 -- Batch 478/ 842, training loss 0.3113574683666229\n",
      "Epoch 28 -- Batch 479/ 842, training loss 0.31622079014778137\n",
      "Epoch 28 -- Batch 480/ 842, training loss 0.29811808466911316\n",
      "Epoch 28 -- Batch 481/ 842, training loss 0.3091391623020172\n",
      "Epoch 28 -- Batch 482/ 842, training loss 0.31263983249664307\n",
      "Epoch 28 -- Batch 483/ 842, training loss 0.30557340383529663\n",
      "Epoch 28 -- Batch 484/ 842, training loss 0.3082124888896942\n",
      "Epoch 28 -- Batch 485/ 842, training loss 0.3176302909851074\n",
      "Epoch 28 -- Batch 486/ 842, training loss 0.3089333176612854\n",
      "Epoch 28 -- Batch 487/ 842, training loss 0.3097131848335266\n",
      "Epoch 28 -- Batch 488/ 842, training loss 0.3215782046318054\n",
      "Epoch 28 -- Batch 489/ 842, training loss 0.2993490397930145\n",
      "Epoch 28 -- Batch 490/ 842, training loss 0.30372610688209534\n",
      "Epoch 28 -- Batch 491/ 842, training loss 0.3146999478340149\n",
      "Epoch 28 -- Batch 492/ 842, training loss 0.31761929392814636\n",
      "Epoch 28 -- Batch 493/ 842, training loss 0.2992062270641327\n",
      "Epoch 28 -- Batch 494/ 842, training loss 0.3121984899044037\n",
      "Epoch 28 -- Batch 495/ 842, training loss 0.30890458822250366\n",
      "Epoch 28 -- Batch 496/ 842, training loss 0.3130043148994446\n",
      "Epoch 28 -- Batch 497/ 842, training loss 0.3085237741470337\n",
      "Epoch 28 -- Batch 498/ 842, training loss 0.3104769289493561\n",
      "Epoch 28 -- Batch 499/ 842, training loss 0.30867698788642883\n",
      "Epoch 28 -- Batch 500/ 842, training loss 0.3122568130493164\n",
      "Epoch 28 -- Batch 501/ 842, training loss 0.31061530113220215\n",
      "Epoch 28 -- Batch 502/ 842, training loss 0.3191238045692444\n",
      "Epoch 28 -- Batch 503/ 842, training loss 0.2975791096687317\n",
      "Epoch 28 -- Batch 504/ 842, training loss 0.30175545811653137\n",
      "Epoch 28 -- Batch 505/ 842, training loss 0.31175538897514343\n",
      "Epoch 28 -- Batch 506/ 842, training loss 0.3146798610687256\n",
      "Epoch 28 -- Batch 507/ 842, training loss 0.3076600134372711\n",
      "Epoch 28 -- Batch 508/ 842, training loss 0.3143365979194641\n",
      "Epoch 28 -- Batch 509/ 842, training loss 0.3042783737182617\n",
      "Epoch 28 -- Batch 510/ 842, training loss 0.31475427746772766\n",
      "Epoch 28 -- Batch 511/ 842, training loss 0.306549996137619\n",
      "Epoch 28 -- Batch 512/ 842, training loss 0.3118402063846588\n",
      "Epoch 28 -- Batch 513/ 842, training loss 0.2996561527252197\n",
      "Epoch 28 -- Batch 514/ 842, training loss 0.31259438395500183\n",
      "Epoch 28 -- Batch 515/ 842, training loss 0.31990161538124084\n",
      "Epoch 28 -- Batch 516/ 842, training loss 0.3117017149925232\n",
      "Epoch 28 -- Batch 517/ 842, training loss 0.3065478205680847\n",
      "Epoch 28 -- Batch 518/ 842, training loss 0.31399691104888916\n",
      "Epoch 28 -- Batch 519/ 842, training loss 0.31846919655799866\n",
      "Epoch 28 -- Batch 520/ 842, training loss 0.3185272812843323\n",
      "Epoch 28 -- Batch 521/ 842, training loss 0.3046093285083771\n",
      "Epoch 28 -- Batch 522/ 842, training loss 0.30617862939834595\n",
      "Epoch 28 -- Batch 523/ 842, training loss 0.3002433180809021\n",
      "Epoch 28 -- Batch 524/ 842, training loss 0.30624881386756897\n",
      "Epoch 28 -- Batch 525/ 842, training loss 0.3124295771121979\n",
      "Epoch 28 -- Batch 526/ 842, training loss 0.3057365119457245\n",
      "Epoch 28 -- Batch 527/ 842, training loss 0.2995274066925049\n",
      "Epoch 28 -- Batch 528/ 842, training loss 0.3022041320800781\n",
      "Epoch 28 -- Batch 529/ 842, training loss 0.3089050054550171\n",
      "Epoch 28 -- Batch 530/ 842, training loss 0.3142939805984497\n",
      "Epoch 28 -- Batch 531/ 842, training loss 0.2924537658691406\n",
      "Epoch 28 -- Batch 532/ 842, training loss 0.3188309371471405\n",
      "Epoch 28 -- Batch 533/ 842, training loss 0.31525689363479614\n",
      "Epoch 28 -- Batch 534/ 842, training loss 0.30316293239593506\n",
      "Epoch 28 -- Batch 535/ 842, training loss 0.3077179789543152\n",
      "Epoch 28 -- Batch 536/ 842, training loss 0.30170938372612\n",
      "Epoch 28 -- Batch 537/ 842, training loss 0.30266550183296204\n",
      "Epoch 28 -- Batch 538/ 842, training loss 0.31420305371284485\n",
      "Epoch 28 -- Batch 539/ 842, training loss 0.31182679533958435\n",
      "Epoch 28 -- Batch 540/ 842, training loss 0.30375370383262634\n",
      "Epoch 28 -- Batch 541/ 842, training loss 0.28893646597862244\n",
      "Epoch 28 -- Batch 542/ 842, training loss 0.30052876472473145\n",
      "Epoch 28 -- Batch 543/ 842, training loss 0.3132188320159912\n",
      "Epoch 28 -- Batch 544/ 842, training loss 0.31277865171432495\n",
      "Epoch 28 -- Batch 545/ 842, training loss 0.29972195625305176\n",
      "Epoch 28 -- Batch 546/ 842, training loss 0.3137357234954834\n",
      "Epoch 28 -- Batch 547/ 842, training loss 0.3057512640953064\n",
      "Epoch 28 -- Batch 548/ 842, training loss 0.31517940759658813\n",
      "Epoch 28 -- Batch 549/ 842, training loss 0.317273885011673\n",
      "Epoch 28 -- Batch 550/ 842, training loss 0.31174546480178833\n",
      "Epoch 28 -- Batch 551/ 842, training loss 0.29611751437187195\n",
      "Epoch 28 -- Batch 552/ 842, training loss 0.3082018196582794\n",
      "Epoch 28 -- Batch 553/ 842, training loss 0.31180766224861145\n",
      "Epoch 28 -- Batch 554/ 842, training loss 0.31473252177238464\n",
      "Epoch 28 -- Batch 555/ 842, training loss 0.30662691593170166\n",
      "Epoch 28 -- Batch 556/ 842, training loss 0.3116943836212158\n",
      "Epoch 28 -- Batch 557/ 842, training loss 0.30641669034957886\n",
      "Epoch 28 -- Batch 558/ 842, training loss 0.29935258626937866\n",
      "Epoch 28 -- Batch 559/ 842, training loss 0.2959103584289551\n",
      "Epoch 28 -- Batch 560/ 842, training loss 0.2982236444950104\n",
      "Epoch 28 -- Batch 561/ 842, training loss 0.31115782260894775\n",
      "Epoch 28 -- Batch 562/ 842, training loss 0.30914855003356934\n",
      "Epoch 28 -- Batch 563/ 842, training loss 0.3102182149887085\n",
      "Epoch 28 -- Batch 564/ 842, training loss 0.3026135563850403\n",
      "Epoch 28 -- Batch 565/ 842, training loss 0.2993575632572174\n",
      "Epoch 28 -- Batch 566/ 842, training loss 0.30532488226890564\n",
      "Epoch 28 -- Batch 567/ 842, training loss 0.30340057611465454\n",
      "Epoch 28 -- Batch 568/ 842, training loss 0.29839929938316345\n",
      "Epoch 28 -- Batch 569/ 842, training loss 0.3099179267883301\n",
      "Epoch 28 -- Batch 570/ 842, training loss 0.3018917143344879\n",
      "Epoch 28 -- Batch 571/ 842, training loss 0.32881712913513184\n",
      "Epoch 28 -- Batch 572/ 842, training loss 0.3027398884296417\n",
      "Epoch 28 -- Batch 573/ 842, training loss 0.3098720908164978\n",
      "Epoch 28 -- Batch 574/ 842, training loss 0.31878426671028137\n",
      "Epoch 28 -- Batch 575/ 842, training loss 0.31470930576324463\n",
      "Epoch 28 -- Batch 576/ 842, training loss 0.31498992443084717\n",
      "Epoch 28 -- Batch 577/ 842, training loss 0.312347412109375\n",
      "Epoch 28 -- Batch 578/ 842, training loss 0.3045021891593933\n",
      "Epoch 28 -- Batch 579/ 842, training loss 0.3166040778160095\n",
      "Epoch 28 -- Batch 580/ 842, training loss 0.30336838960647583\n",
      "Epoch 28 -- Batch 581/ 842, training loss 0.3053515553474426\n",
      "Epoch 28 -- Batch 582/ 842, training loss 0.2976604402065277\n",
      "Epoch 28 -- Batch 583/ 842, training loss 0.3154093325138092\n",
      "Epoch 28 -- Batch 584/ 842, training loss 0.3011632561683655\n",
      "Epoch 28 -- Batch 585/ 842, training loss 0.31061407923698425\n",
      "Epoch 28 -- Batch 586/ 842, training loss 0.31310880184173584\n",
      "Epoch 28 -- Batch 587/ 842, training loss 0.3191593587398529\n",
      "Epoch 28 -- Batch 588/ 842, training loss 0.30825912952423096\n",
      "Epoch 28 -- Batch 589/ 842, training loss 0.3103366196155548\n",
      "Epoch 28 -- Batch 590/ 842, training loss 0.3133662939071655\n",
      "Epoch 28 -- Batch 591/ 842, training loss 0.30163905024528503\n",
      "Epoch 28 -- Batch 592/ 842, training loss 0.30333757400512695\n",
      "Epoch 28 -- Batch 593/ 842, training loss 0.3159100413322449\n",
      "Epoch 28 -- Batch 594/ 842, training loss 0.30787932872772217\n",
      "Epoch 28 -- Batch 595/ 842, training loss 0.30774250626564026\n",
      "Epoch 28 -- Batch 596/ 842, training loss 0.30236682295799255\n",
      "Epoch 28 -- Batch 597/ 842, training loss 0.32048556208610535\n",
      "Epoch 28 -- Batch 598/ 842, training loss 0.3040247857570648\n",
      "Epoch 28 -- Batch 599/ 842, training loss 0.31196972727775574\n",
      "Epoch 28 -- Batch 600/ 842, training loss 0.302033931016922\n",
      "Epoch 28 -- Batch 601/ 842, training loss 0.3041442036628723\n",
      "Epoch 28 -- Batch 602/ 842, training loss 0.3084772229194641\n",
      "Epoch 28 -- Batch 603/ 842, training loss 0.3077443540096283\n",
      "Epoch 28 -- Batch 604/ 842, training loss 0.308707058429718\n",
      "Epoch 28 -- Batch 605/ 842, training loss 0.30423253774642944\n",
      "Epoch 28 -- Batch 606/ 842, training loss 0.31195124983787537\n",
      "Epoch 28 -- Batch 607/ 842, training loss 0.32902660965919495\n",
      "Epoch 28 -- Batch 608/ 842, training loss 0.31255069375038147\n",
      "Epoch 28 -- Batch 609/ 842, training loss 0.31207919120788574\n",
      "Epoch 28 -- Batch 610/ 842, training loss 0.31352463364601135\n",
      "Epoch 28 -- Batch 611/ 842, training loss 0.31867679953575134\n",
      "Epoch 28 -- Batch 612/ 842, training loss 0.31542518734931946\n",
      "Epoch 28 -- Batch 613/ 842, training loss 0.3028753101825714\n",
      "Epoch 28 -- Batch 614/ 842, training loss 0.31726887822151184\n",
      "Epoch 28 -- Batch 615/ 842, training loss 0.3090088367462158\n",
      "Epoch 28 -- Batch 616/ 842, training loss 0.3078605532646179\n",
      "Epoch 28 -- Batch 617/ 842, training loss 0.30625662207603455\n",
      "Epoch 28 -- Batch 618/ 842, training loss 0.31285518407821655\n",
      "Epoch 28 -- Batch 619/ 842, training loss 0.30626875162124634\n",
      "Epoch 28 -- Batch 620/ 842, training loss 0.30562326312065125\n",
      "Epoch 28 -- Batch 621/ 842, training loss 0.3123423755168915\n",
      "Epoch 28 -- Batch 622/ 842, training loss 0.30559754371643066\n",
      "Epoch 28 -- Batch 623/ 842, training loss 0.311301052570343\n",
      "Epoch 28 -- Batch 624/ 842, training loss 0.3189150094985962\n",
      "Epoch 28 -- Batch 625/ 842, training loss 0.3053160011768341\n",
      "Epoch 28 -- Batch 626/ 842, training loss 0.2988331913948059\n",
      "Epoch 28 -- Batch 627/ 842, training loss 0.30907654762268066\n",
      "Epoch 28 -- Batch 628/ 842, training loss 0.30168458819389343\n",
      "Epoch 28 -- Batch 629/ 842, training loss 0.2940302789211273\n",
      "Epoch 28 -- Batch 630/ 842, training loss 0.30503374338150024\n",
      "Epoch 28 -- Batch 631/ 842, training loss 0.3107295036315918\n",
      "Epoch 28 -- Batch 632/ 842, training loss 0.31366801261901855\n",
      "Epoch 28 -- Batch 633/ 842, training loss 0.2986421585083008\n",
      "Epoch 28 -- Batch 634/ 842, training loss 0.3103921711444855\n",
      "Epoch 28 -- Batch 635/ 842, training loss 0.2980771064758301\n",
      "Epoch 28 -- Batch 636/ 842, training loss 0.3188285827636719\n",
      "Epoch 28 -- Batch 637/ 842, training loss 0.30443075299263\n",
      "Epoch 28 -- Batch 638/ 842, training loss 0.31828543543815613\n",
      "Epoch 28 -- Batch 639/ 842, training loss 0.31391510367393494\n",
      "Epoch 28 -- Batch 640/ 842, training loss 0.3113113343715668\n",
      "Epoch 28 -- Batch 641/ 842, training loss 0.3118537664413452\n",
      "Epoch 28 -- Batch 642/ 842, training loss 0.30853238701820374\n",
      "Epoch 28 -- Batch 643/ 842, training loss 0.3117913007736206\n",
      "Epoch 28 -- Batch 644/ 842, training loss 0.3035042881965637\n",
      "Epoch 28 -- Batch 645/ 842, training loss 0.30031105875968933\n",
      "Epoch 28 -- Batch 646/ 842, training loss 0.30256184935569763\n",
      "Epoch 28 -- Batch 647/ 842, training loss 0.303169310092926\n",
      "Epoch 28 -- Batch 648/ 842, training loss 0.3098851442337036\n",
      "Epoch 28 -- Batch 649/ 842, training loss 0.3167307376861572\n",
      "Epoch 28 -- Batch 650/ 842, training loss 0.30518463253974915\n",
      "Epoch 28 -- Batch 651/ 842, training loss 0.3075626492500305\n",
      "Epoch 28 -- Batch 652/ 842, training loss 0.3110000491142273\n",
      "Epoch 28 -- Batch 653/ 842, training loss 0.3050456941127777\n",
      "Epoch 28 -- Batch 654/ 842, training loss 0.31371137499809265\n",
      "Epoch 28 -- Batch 655/ 842, training loss 0.3085309863090515\n",
      "Epoch 28 -- Batch 656/ 842, training loss 0.3153037428855896\n",
      "Epoch 28 -- Batch 657/ 842, training loss 0.30833062529563904\n",
      "Epoch 28 -- Batch 658/ 842, training loss 0.31343019008636475\n",
      "Epoch 28 -- Batch 659/ 842, training loss 0.3031747341156006\n",
      "Epoch 28 -- Batch 660/ 842, training loss 0.30979371070861816\n",
      "Epoch 28 -- Batch 661/ 842, training loss 0.30025583505630493\n",
      "Epoch 28 -- Batch 662/ 842, training loss 0.3088661730289459\n",
      "Epoch 28 -- Batch 663/ 842, training loss 0.30940452218055725\n",
      "Epoch 28 -- Batch 664/ 842, training loss 0.3189726173877716\n",
      "Epoch 28 -- Batch 665/ 842, training loss 0.32276612520217896\n",
      "Epoch 28 -- Batch 666/ 842, training loss 0.300027459859848\n",
      "Epoch 28 -- Batch 667/ 842, training loss 0.31766200065612793\n",
      "Epoch 28 -- Batch 668/ 842, training loss 0.31191420555114746\n",
      "Epoch 28 -- Batch 669/ 842, training loss 0.30450454354286194\n",
      "Epoch 28 -- Batch 670/ 842, training loss 0.30427777767181396\n",
      "Epoch 28 -- Batch 671/ 842, training loss 0.31757110357284546\n",
      "Epoch 28 -- Batch 672/ 842, training loss 0.29922062158584595\n",
      "Epoch 28 -- Batch 673/ 842, training loss 0.3180679678916931\n",
      "Epoch 28 -- Batch 674/ 842, training loss 0.31354233622550964\n",
      "Epoch 28 -- Batch 675/ 842, training loss 0.3126089572906494\n",
      "Epoch 28 -- Batch 676/ 842, training loss 0.30429160594940186\n",
      "Epoch 28 -- Batch 677/ 842, training loss 0.2970580756664276\n",
      "Epoch 28 -- Batch 678/ 842, training loss 0.3099725842475891\n",
      "Epoch 28 -- Batch 679/ 842, training loss 0.31644949316978455\n",
      "Epoch 28 -- Batch 680/ 842, training loss 0.3095216751098633\n",
      "Epoch 28 -- Batch 681/ 842, training loss 0.31226593255996704\n",
      "Epoch 28 -- Batch 682/ 842, training loss 0.30672937631607056\n",
      "Epoch 28 -- Batch 683/ 842, training loss 0.30683237314224243\n",
      "Epoch 28 -- Batch 684/ 842, training loss 0.3210364282131195\n",
      "Epoch 28 -- Batch 685/ 842, training loss 0.3011951446533203\n",
      "Epoch 28 -- Batch 686/ 842, training loss 0.31331688165664673\n",
      "Epoch 28 -- Batch 687/ 842, training loss 0.30287420749664307\n",
      "Epoch 28 -- Batch 688/ 842, training loss 0.31995755434036255\n",
      "Epoch 28 -- Batch 689/ 842, training loss 0.31297293305397034\n",
      "Epoch 28 -- Batch 690/ 842, training loss 0.2997177541255951\n",
      "Epoch 28 -- Batch 691/ 842, training loss 0.3018273711204529\n",
      "Epoch 28 -- Batch 692/ 842, training loss 0.30584031343460083\n",
      "Epoch 28 -- Batch 693/ 842, training loss 0.30864599347114563\n",
      "Epoch 28 -- Batch 694/ 842, training loss 0.30906984210014343\n",
      "Epoch 28 -- Batch 695/ 842, training loss 0.30112969875335693\n",
      "Epoch 28 -- Batch 696/ 842, training loss 0.30155277252197266\n",
      "Epoch 28 -- Batch 697/ 842, training loss 0.31179022789001465\n",
      "Epoch 28 -- Batch 698/ 842, training loss 0.3081996440887451\n",
      "Epoch 28 -- Batch 699/ 842, training loss 0.3154713213443756\n",
      "Epoch 28 -- Batch 700/ 842, training loss 0.3089113235473633\n",
      "Epoch 28 -- Batch 701/ 842, training loss 0.29504555463790894\n",
      "Epoch 28 -- Batch 702/ 842, training loss 0.30434900522232056\n",
      "Epoch 28 -- Batch 703/ 842, training loss 0.302788645029068\n",
      "Epoch 28 -- Batch 704/ 842, training loss 0.31890204548835754\n",
      "Epoch 28 -- Batch 705/ 842, training loss 0.30364173650741577\n",
      "Epoch 28 -- Batch 706/ 842, training loss 0.31385570764541626\n",
      "Epoch 28 -- Batch 707/ 842, training loss 0.31149306893348694\n",
      "Epoch 28 -- Batch 708/ 842, training loss 0.30828797817230225\n",
      "Epoch 28 -- Batch 709/ 842, training loss 0.3124799132347107\n",
      "Epoch 28 -- Batch 710/ 842, training loss 0.3035522997379303\n",
      "Epoch 28 -- Batch 711/ 842, training loss 0.3158608078956604\n",
      "Epoch 28 -- Batch 712/ 842, training loss 0.3106216490268707\n",
      "Epoch 28 -- Batch 713/ 842, training loss 0.3139707148075104\n",
      "Epoch 28 -- Batch 714/ 842, training loss 0.3015207350254059\n",
      "Epoch 28 -- Batch 715/ 842, training loss 0.3122376799583435\n",
      "Epoch 28 -- Batch 716/ 842, training loss 0.2984882891178131\n",
      "Epoch 28 -- Batch 717/ 842, training loss 0.31425729393959045\n",
      "Epoch 28 -- Batch 718/ 842, training loss 0.3022696375846863\n",
      "Epoch 28 -- Batch 719/ 842, training loss 0.306636244058609\n",
      "Epoch 28 -- Batch 720/ 842, training loss 0.3097458481788635\n",
      "Epoch 28 -- Batch 721/ 842, training loss 0.30219361186027527\n",
      "Epoch 28 -- Batch 722/ 842, training loss 0.31200000643730164\n",
      "Epoch 28 -- Batch 723/ 842, training loss 0.303367018699646\n",
      "Epoch 28 -- Batch 724/ 842, training loss 0.30106621980667114\n",
      "Epoch 28 -- Batch 725/ 842, training loss 0.3112061023712158\n",
      "Epoch 28 -- Batch 726/ 842, training loss 0.3164544403553009\n",
      "Epoch 28 -- Batch 727/ 842, training loss 0.30364248156547546\n",
      "Epoch 28 -- Batch 728/ 842, training loss 0.31111547350883484\n",
      "Epoch 28 -- Batch 729/ 842, training loss 0.3116164207458496\n",
      "Epoch 28 -- Batch 730/ 842, training loss 0.3080027103424072\n",
      "Epoch 28 -- Batch 731/ 842, training loss 0.30893850326538086\n",
      "Epoch 28 -- Batch 732/ 842, training loss 0.3102266490459442\n",
      "Epoch 28 -- Batch 733/ 842, training loss 0.3124714493751526\n",
      "Epoch 28 -- Batch 734/ 842, training loss 0.3070884048938751\n",
      "Epoch 28 -- Batch 735/ 842, training loss 0.30518442392349243\n",
      "Epoch 28 -- Batch 736/ 842, training loss 0.3135222792625427\n",
      "Epoch 28 -- Batch 737/ 842, training loss 0.32514089345932007\n",
      "Epoch 28 -- Batch 738/ 842, training loss 0.3075183033943176\n",
      "Epoch 28 -- Batch 739/ 842, training loss 0.30897417664527893\n",
      "Epoch 28 -- Batch 740/ 842, training loss 0.30287110805511475\n",
      "Epoch 28 -- Batch 741/ 842, training loss 0.2980619966983795\n",
      "Epoch 28 -- Batch 742/ 842, training loss 0.30805790424346924\n",
      "Epoch 28 -- Batch 743/ 842, training loss 0.30933135747909546\n",
      "Epoch 28 -- Batch 744/ 842, training loss 0.31348511576652527\n",
      "Epoch 28 -- Batch 745/ 842, training loss 0.313048392534256\n",
      "Epoch 28 -- Batch 746/ 842, training loss 0.3019271492958069\n",
      "Epoch 28 -- Batch 747/ 842, training loss 0.3034220337867737\n",
      "Epoch 28 -- Batch 748/ 842, training loss 0.3093838095664978\n",
      "Epoch 28 -- Batch 749/ 842, training loss 0.30825698375701904\n",
      "Epoch 28 -- Batch 750/ 842, training loss 0.3141170144081116\n",
      "Epoch 28 -- Batch 751/ 842, training loss 0.30884528160095215\n",
      "Epoch 28 -- Batch 752/ 842, training loss 0.29983991384506226\n",
      "Epoch 28 -- Batch 753/ 842, training loss 0.3159457743167877\n",
      "Epoch 28 -- Batch 754/ 842, training loss 0.31436240673065186\n",
      "Epoch 28 -- Batch 755/ 842, training loss 0.3131222426891327\n",
      "Epoch 28 -- Batch 756/ 842, training loss 0.3031625747680664\n",
      "Epoch 28 -- Batch 757/ 842, training loss 0.31321781873703003\n",
      "Epoch 28 -- Batch 758/ 842, training loss 0.3069179952144623\n",
      "Epoch 28 -- Batch 759/ 842, training loss 0.3191397190093994\n",
      "Epoch 28 -- Batch 760/ 842, training loss 0.3088856339454651\n",
      "Epoch 28 -- Batch 761/ 842, training loss 0.3064468502998352\n",
      "Epoch 28 -- Batch 762/ 842, training loss 0.30551984906196594\n",
      "Epoch 28 -- Batch 763/ 842, training loss 0.3118593692779541\n",
      "Epoch 28 -- Batch 764/ 842, training loss 0.3156406879425049\n",
      "Epoch 28 -- Batch 765/ 842, training loss 0.3068678677082062\n",
      "Epoch 28 -- Batch 766/ 842, training loss 0.30210772156715393\n",
      "Epoch 28 -- Batch 767/ 842, training loss 0.2957944869995117\n",
      "Epoch 28 -- Batch 768/ 842, training loss 0.3059982657432556\n",
      "Epoch 28 -- Batch 769/ 842, training loss 0.3145727217197418\n",
      "Epoch 28 -- Batch 770/ 842, training loss 0.3139982521533966\n",
      "Epoch 28 -- Batch 771/ 842, training loss 0.3091438412666321\n",
      "Epoch 28 -- Batch 772/ 842, training loss 0.308174729347229\n",
      "Epoch 28 -- Batch 773/ 842, training loss 0.3145456910133362\n",
      "Epoch 28 -- Batch 774/ 842, training loss 0.31349048018455505\n",
      "Epoch 28 -- Batch 775/ 842, training loss 0.3075774312019348\n",
      "Epoch 28 -- Batch 776/ 842, training loss 0.31191444396972656\n",
      "Epoch 28 -- Batch 777/ 842, training loss 0.29637107253074646\n",
      "Epoch 28 -- Batch 778/ 842, training loss 0.3097672462463379\n",
      "Epoch 28 -- Batch 779/ 842, training loss 0.3237791657447815\n",
      "Epoch 28 -- Batch 780/ 842, training loss 0.31131434440612793\n",
      "Epoch 28 -- Batch 781/ 842, training loss 0.3096967339515686\n",
      "Epoch 28 -- Batch 782/ 842, training loss 0.3098156154155731\n",
      "Epoch 28 -- Batch 783/ 842, training loss 0.3134448826313019\n",
      "Epoch 28 -- Batch 784/ 842, training loss 0.31522029638290405\n",
      "Epoch 28 -- Batch 785/ 842, training loss 0.2988150119781494\n",
      "Epoch 28 -- Batch 786/ 842, training loss 0.31392189860343933\n",
      "Epoch 28 -- Batch 787/ 842, training loss 0.30354130268096924\n",
      "Epoch 28 -- Batch 788/ 842, training loss 0.3021501302719116\n",
      "Epoch 28 -- Batch 789/ 842, training loss 0.31211650371551514\n",
      "Epoch 28 -- Batch 790/ 842, training loss 0.3140816390514374\n",
      "Epoch 28 -- Batch 791/ 842, training loss 0.3048968017101288\n",
      "Epoch 28 -- Batch 792/ 842, training loss 0.3007091283798218\n",
      "Epoch 28 -- Batch 793/ 842, training loss 0.3136754035949707\n",
      "Epoch 28 -- Batch 794/ 842, training loss 0.3009485900402069\n",
      "Epoch 28 -- Batch 795/ 842, training loss 0.31290918588638306\n",
      "Epoch 28 -- Batch 796/ 842, training loss 0.306025892496109\n",
      "Epoch 28 -- Batch 797/ 842, training loss 0.30492788553237915\n",
      "Epoch 28 -- Batch 798/ 842, training loss 0.3112790286540985\n",
      "Epoch 28 -- Batch 799/ 842, training loss 0.2990606129169464\n",
      "Epoch 28 -- Batch 800/ 842, training loss 0.30373287200927734\n",
      "Epoch 28 -- Batch 801/ 842, training loss 0.32192060351371765\n",
      "Epoch 28 -- Batch 802/ 842, training loss 0.31005537509918213\n",
      "Epoch 28 -- Batch 803/ 842, training loss 0.30370619893074036\n",
      "Epoch 28 -- Batch 804/ 842, training loss 0.3044488728046417\n",
      "Epoch 28 -- Batch 805/ 842, training loss 0.3116840422153473\n",
      "Epoch 28 -- Batch 806/ 842, training loss 0.3129819631576538\n",
      "Epoch 28 -- Batch 807/ 842, training loss 0.3119717538356781\n",
      "Epoch 28 -- Batch 808/ 842, training loss 0.3105010986328125\n",
      "Epoch 28 -- Batch 809/ 842, training loss 0.30856606364250183\n",
      "Epoch 28 -- Batch 810/ 842, training loss 0.31702592968940735\n",
      "Epoch 28 -- Batch 811/ 842, training loss 0.3106132745742798\n",
      "Epoch 28 -- Batch 812/ 842, training loss 0.3154216408729553\n",
      "Epoch 28 -- Batch 813/ 842, training loss 0.3237702250480652\n",
      "Epoch 28 -- Batch 814/ 842, training loss 0.3139559328556061\n",
      "Epoch 28 -- Batch 815/ 842, training loss 0.29575759172439575\n",
      "Epoch 28 -- Batch 816/ 842, training loss 0.30824026465415955\n",
      "Epoch 28 -- Batch 817/ 842, training loss 0.31952542066574097\n",
      "Epoch 28 -- Batch 818/ 842, training loss 0.30108416080474854\n",
      "Epoch 28 -- Batch 819/ 842, training loss 0.3248204290866852\n",
      "Epoch 28 -- Batch 820/ 842, training loss 0.31898850202560425\n",
      "Epoch 28 -- Batch 821/ 842, training loss 0.31549790501594543\n",
      "Epoch 28 -- Batch 822/ 842, training loss 0.32156965136528015\n",
      "Epoch 28 -- Batch 823/ 842, training loss 0.3037894368171692\n",
      "Epoch 28 -- Batch 824/ 842, training loss 0.3176334798336029\n",
      "Epoch 28 -- Batch 825/ 842, training loss 0.3013631999492645\n",
      "Epoch 28 -- Batch 826/ 842, training loss 0.31083762645721436\n",
      "Epoch 28 -- Batch 827/ 842, training loss 0.32644128799438477\n",
      "Epoch 28 -- Batch 828/ 842, training loss 0.315856009721756\n",
      "Epoch 28 -- Batch 829/ 842, training loss 0.30915212631225586\n",
      "Epoch 28 -- Batch 830/ 842, training loss 0.3033019006252289\n",
      "Epoch 28 -- Batch 831/ 842, training loss 0.30645716190338135\n",
      "Epoch 28 -- Batch 832/ 842, training loss 0.3060886859893799\n",
      "Epoch 28 -- Batch 833/ 842, training loss 0.31513485312461853\n",
      "Epoch 28 -- Batch 834/ 842, training loss 0.3192709982395172\n",
      "Epoch 28 -- Batch 835/ 842, training loss 0.31479182839393616\n",
      "Epoch 28 -- Batch 836/ 842, training loss 0.3118511736392975\n",
      "Epoch 28 -- Batch 837/ 842, training loss 0.3160097002983093\n",
      "Epoch 28 -- Batch 838/ 842, training loss 0.30879998207092285\n",
      "Epoch 28 -- Batch 839/ 842, training loss 0.31825903058052063\n",
      "Epoch 28 -- Batch 840/ 842, training loss 0.3067963123321533\n",
      "Epoch 28 -- Batch 841/ 842, training loss 0.31190595030784607\n",
      "Epoch 28 -- Batch 842/ 842, training loss 0.28495079278945923\n",
      "----------------------------------------------------------------------\n",
      "Epoch 28 -- Batch 1/ 94, validation loss 0.2964947819709778\n",
      "Epoch 28 -- Batch 2/ 94, validation loss 0.2977854609489441\n",
      "Epoch 28 -- Batch 3/ 94, validation loss 0.29473167657852173\n",
      "Epoch 28 -- Batch 4/ 94, validation loss 0.3156353235244751\n",
      "Epoch 28 -- Batch 5/ 94, validation loss 0.311130553483963\n",
      "Epoch 28 -- Batch 6/ 94, validation loss 0.30086636543273926\n",
      "Epoch 28 -- Batch 7/ 94, validation loss 0.30076080560684204\n",
      "Epoch 28 -- Batch 8/ 94, validation loss 0.2918194532394409\n",
      "Epoch 28 -- Batch 9/ 94, validation loss 0.31538331508636475\n",
      "Epoch 28 -- Batch 10/ 94, validation loss 0.32676827907562256\n",
      "Epoch 28 -- Batch 11/ 94, validation loss 0.3066074848175049\n",
      "Epoch 28 -- Batch 12/ 94, validation loss 0.30570271611213684\n",
      "Epoch 28 -- Batch 13/ 94, validation loss 0.2948920428752899\n",
      "Epoch 28 -- Batch 14/ 94, validation loss 0.2967965602874756\n",
      "Epoch 28 -- Batch 15/ 94, validation loss 0.29443469643592834\n",
      "Epoch 28 -- Batch 16/ 94, validation loss 0.298898845911026\n",
      "Epoch 28 -- Batch 17/ 94, validation loss 0.2905588448047638\n",
      "Epoch 28 -- Batch 18/ 94, validation loss 0.2986452877521515\n",
      "Epoch 28 -- Batch 19/ 94, validation loss 0.29109758138656616\n",
      "Epoch 28 -- Batch 20/ 94, validation loss 0.290410578250885\n",
      "Epoch 28 -- Batch 21/ 94, validation loss 0.3038126230239868\n",
      "Epoch 28 -- Batch 22/ 94, validation loss 0.3002481162548065\n",
      "Epoch 28 -- Batch 23/ 94, validation loss 0.30888450145721436\n",
      "Epoch 28 -- Batch 24/ 94, validation loss 0.3161403238773346\n",
      "Epoch 28 -- Batch 25/ 94, validation loss 0.2920154631137848\n",
      "Epoch 28 -- Batch 26/ 94, validation loss 0.3046974241733551\n",
      "Epoch 28 -- Batch 27/ 94, validation loss 0.3013799488544464\n",
      "Epoch 28 -- Batch 28/ 94, validation loss 0.30710843205451965\n",
      "Epoch 28 -- Batch 29/ 94, validation loss 0.29349642992019653\n",
      "Epoch 28 -- Batch 30/ 94, validation loss 0.29432883858680725\n",
      "Epoch 28 -- Batch 31/ 94, validation loss 0.29383501410484314\n",
      "Epoch 28 -- Batch 32/ 94, validation loss 0.29782432317733765\n",
      "Epoch 28 -- Batch 33/ 94, validation loss 0.2977187931537628\n",
      "Epoch 28 -- Batch 34/ 94, validation loss 0.2968094050884247\n",
      "Epoch 28 -- Batch 35/ 94, validation loss 0.29073020815849304\n",
      "Epoch 28 -- Batch 36/ 94, validation loss 0.2966737449169159\n",
      "Epoch 28 -- Batch 37/ 94, validation loss 0.3079982399940491\n",
      "Epoch 28 -- Batch 38/ 94, validation loss 0.29826563596725464\n",
      "Epoch 28 -- Batch 39/ 94, validation loss 0.289010614156723\n",
      "Epoch 28 -- Batch 40/ 94, validation loss 0.29995182156562805\n",
      "Epoch 28 -- Batch 41/ 94, validation loss 0.29338473081588745\n",
      "Epoch 28 -- Batch 42/ 94, validation loss 0.3063354194164276\n",
      "Epoch 28 -- Batch 43/ 94, validation loss 0.2992153465747833\n",
      "Epoch 28 -- Batch 44/ 94, validation loss 0.29645392298698425\n",
      "Epoch 28 -- Batch 45/ 94, validation loss 0.298540860414505\n",
      "Epoch 28 -- Batch 46/ 94, validation loss 0.30068448185920715\n",
      "Epoch 28 -- Batch 47/ 94, validation loss 0.29534730315208435\n",
      "Epoch 28 -- Batch 48/ 94, validation loss 0.2900421917438507\n",
      "Epoch 28 -- Batch 49/ 94, validation loss 0.29825684428215027\n",
      "Epoch 28 -- Batch 50/ 94, validation loss 0.2980542778968811\n",
      "Epoch 28 -- Batch 51/ 94, validation loss 0.29525357484817505\n",
      "Epoch 28 -- Batch 52/ 94, validation loss 0.28838494420051575\n",
      "Epoch 28 -- Batch 53/ 94, validation loss 0.30741220712661743\n",
      "Epoch 28 -- Batch 54/ 94, validation loss 0.2991127073764801\n",
      "Epoch 28 -- Batch 55/ 94, validation loss 0.3028574287891388\n",
      "Epoch 28 -- Batch 56/ 94, validation loss 0.29910483956336975\n",
      "Epoch 28 -- Batch 57/ 94, validation loss 0.28672268986701965\n",
      "Epoch 28 -- Batch 58/ 94, validation loss 0.3164134919643402\n",
      "Epoch 28 -- Batch 59/ 94, validation loss 0.3002343475818634\n",
      "Epoch 28 -- Batch 60/ 94, validation loss 0.3198275566101074\n",
      "Epoch 28 -- Batch 61/ 94, validation loss 0.2944084405899048\n",
      "Epoch 28 -- Batch 62/ 94, validation loss 0.30041077733039856\n",
      "Epoch 28 -- Batch 63/ 94, validation loss 0.30672696232795715\n",
      "Epoch 28 -- Batch 64/ 94, validation loss 0.299606055021286\n",
      "Epoch 28 -- Batch 65/ 94, validation loss 0.28566721081733704\n",
      "Epoch 28 -- Batch 66/ 94, validation loss 0.29731616377830505\n",
      "Epoch 28 -- Batch 67/ 94, validation loss 0.30420368909835815\n",
      "Epoch 28 -- Batch 68/ 94, validation loss 0.2943192422389984\n",
      "Epoch 28 -- Batch 69/ 94, validation loss 0.29306721687316895\n",
      "Epoch 28 -- Batch 70/ 94, validation loss 0.3033662736415863\n",
      "Epoch 28 -- Batch 71/ 94, validation loss 0.29298052191734314\n",
      "Epoch 28 -- Batch 72/ 94, validation loss 0.2975994348526001\n",
      "Epoch 28 -- Batch 73/ 94, validation loss 0.2937983274459839\n",
      "Epoch 28 -- Batch 74/ 94, validation loss 0.29885318875312805\n",
      "Epoch 28 -- Batch 75/ 94, validation loss 0.2962397634983063\n",
      "Epoch 28 -- Batch 76/ 94, validation loss 0.2983272671699524\n",
      "Epoch 28 -- Batch 77/ 94, validation loss 0.29207006096839905\n",
      "Epoch 28 -- Batch 78/ 94, validation loss 0.3075622618198395\n",
      "Epoch 28 -- Batch 79/ 94, validation loss 0.2989484667778015\n",
      "Epoch 28 -- Batch 80/ 94, validation loss 0.2967730164527893\n",
      "Epoch 28 -- Batch 81/ 94, validation loss 0.2940346300601959\n",
      "Epoch 28 -- Batch 82/ 94, validation loss 0.30742353200912476\n",
      "Epoch 28 -- Batch 83/ 94, validation loss 0.3005377948284149\n",
      "Epoch 28 -- Batch 84/ 94, validation loss 0.30475106835365295\n",
      "Epoch 28 -- Batch 85/ 94, validation loss 0.29568204283714294\n",
      "Epoch 28 -- Batch 86/ 94, validation loss 0.29416850209236145\n",
      "Epoch 28 -- Batch 87/ 94, validation loss 0.2972067594528198\n",
      "Epoch 28 -- Batch 88/ 94, validation loss 0.2968090772628784\n",
      "Epoch 28 -- Batch 89/ 94, validation loss 0.28752973675727844\n",
      "Epoch 28 -- Batch 90/ 94, validation loss 0.33591341972351074\n",
      "Epoch 28 -- Batch 91/ 94, validation loss 0.29602381587028503\n",
      "Epoch 28 -- Batch 92/ 94, validation loss 0.3013942837715149\n",
      "Epoch 28 -- Batch 93/ 94, validation loss 0.30131232738494873\n",
      "Epoch 28 -- Batch 94/ 94, validation loss 0.29389476776123047\n",
      "----------------------------------------------------------------------\n",
      "Epoch 28 loss: Training 0.307689368724823, Validation 0.2938947379589081\n",
      "----------------------------------------------------------------------\n",
      "Epoch 29/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 9 10 11 18 20\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'Cn1c(=O)c2c3c(ncn2CC(=O)OCC(=O)Nc2ccc(Br)cc2)n(C)c1=O'\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1(O)C2CCCCC2C1(O)C(=O)N(C)C2CCCCC1'\n",
      "[19:08:33] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 12 27\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'c1ccc(COC2C(C3COC4(CCCCC4)O3)OC3OC4(CCCCC4)OC3[C@H]2C(=O)N(CCC(F)(F)F)C3=O)cc1'\n",
      "[19:08:33] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:08:33] SMILES Parse Error: syntax error while parsing: Cc1cccc(-n2nc3c(c2NC(=O)Cc2ccc(O)c(OC)c2)CS(=O)==)C3)c1C\n",
      "[19:08:33] SMILES Parse Error: Failed parsing SMILES 'Cc1cccc(-n2nc3c(c2NC(=O)Cc2ccc(O)c(OC)c2)CS(=O)==)C3)c1C' for input: 'Cc1cccc(-n2nc3c(c2NC(=O)Cc2ccc(O)c(OC)c2)CS(=O)==)C3)c1C'\n",
      "[19:08:33] Explicit valence for atom # 9 C, 5, is greater than permitted\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[19:08:33] SMILES Parse Error: extra open parentheses for input: 'CCc1sc(N2[C@@H](C#N)[C@@H](c3ccccc3-c3nccc(C)c3C2=O)n[n+]2cccc(F)c21'\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 8 9 11 13 15\n",
      "[19:08:33] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 19 20 21\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'C[C@@]12C=CCC(=O)C=C1C(C1[C@@H](CO)C1=O)[C@@H]2C(=O)OC2(C)O'\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(-c2ccc([C@@H]3[C@H](C#N)N(C(=O)C4CCC3)[C@H]3CO)cc2)cc1'\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 8 9 10 21 22 23 24\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccn2c(=O)cc(CSc4nnc(Cc5ccccc5)nn4n3)c2cccc12)c1ccccc1'\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 3 4 17\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'COCCOP1(=O)C(C)=C(N)O2'\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 22 23 24 25 27\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'CC(C)c1ccc(NC(=O)CN2C[C@H]3CC=C[C@H]3[C@H]3C23CC4)cc2c1'\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1cccc(-c2ccc(C3c4ccccc4C(N)=O)=N2)c1'\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 2 11 12 27 28 29 30 31 32\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 10 17 18 19 31 32 33\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 16 18\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 6 8 9 10 28 29\n",
      "[19:08:33] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C2CC(=O)Nc3cc4c(cc33)OCCO4)s1'\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 5 6 7 34 36\n",
      "[19:08:33] Can't kekulize mol.  Unkekulized atoms: 9 10 13 26 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 -- Batch 1/ 842, training loss 0.295406311750412\n",
      "Epoch 29 -- Batch 2/ 842, training loss 0.3022112548351288\n",
      "Epoch 29 -- Batch 3/ 842, training loss 0.29400551319122314\n",
      "Epoch 29 -- Batch 4/ 842, training loss 0.30828857421875\n",
      "Epoch 29 -- Batch 5/ 842, training loss 0.3039529323577881\n",
      "Epoch 29 -- Batch 6/ 842, training loss 0.29846230149269104\n",
      "Epoch 29 -- Batch 7/ 842, training loss 0.30116215348243713\n",
      "Epoch 29 -- Batch 8/ 842, training loss 0.3059957027435303\n",
      "Epoch 29 -- Batch 9/ 842, training loss 0.30876341462135315\n",
      "Epoch 29 -- Batch 10/ 842, training loss 0.3003529906272888\n",
      "Epoch 29 -- Batch 11/ 842, training loss 0.30735528469085693\n",
      "Epoch 29 -- Batch 12/ 842, training loss 0.3174561560153961\n",
      "Epoch 29 -- Batch 13/ 842, training loss 0.30606985092163086\n",
      "Epoch 29 -- Batch 14/ 842, training loss 0.3028392791748047\n",
      "Epoch 29 -- Batch 15/ 842, training loss 0.29791346192359924\n",
      "Epoch 29 -- Batch 16/ 842, training loss 0.29539093375205994\n",
      "Epoch 29 -- Batch 17/ 842, training loss 0.30689162015914917\n",
      "Epoch 29 -- Batch 18/ 842, training loss 0.3027507960796356\n",
      "Epoch 29 -- Batch 19/ 842, training loss 0.3044964075088501\n",
      "Epoch 29 -- Batch 20/ 842, training loss 0.30449366569519043\n",
      "Epoch 29 -- Batch 21/ 842, training loss 0.30798473954200745\n",
      "Epoch 29 -- Batch 22/ 842, training loss 0.3163491189479828\n",
      "Epoch 29 -- Batch 23/ 842, training loss 0.3029495179653168\n",
      "Epoch 29 -- Batch 24/ 842, training loss 0.30574363470077515\n",
      "Epoch 29 -- Batch 25/ 842, training loss 0.3059888482093811\n",
      "Epoch 29 -- Batch 26/ 842, training loss 0.3031716048717499\n",
      "Epoch 29 -- Batch 27/ 842, training loss 0.29872366786003113\n",
      "Epoch 29 -- Batch 28/ 842, training loss 0.298373818397522\n",
      "Epoch 29 -- Batch 29/ 842, training loss 0.3114809989929199\n",
      "Epoch 29 -- Batch 30/ 842, training loss 0.3081422448158264\n",
      "Epoch 29 -- Batch 31/ 842, training loss 0.30429837107658386\n",
      "Epoch 29 -- Batch 32/ 842, training loss 0.3145081400871277\n",
      "Epoch 29 -- Batch 33/ 842, training loss 0.2981090247631073\n",
      "Epoch 29 -- Batch 34/ 842, training loss 0.29735350608825684\n",
      "Epoch 29 -- Batch 35/ 842, training loss 0.30941829085350037\n",
      "Epoch 29 -- Batch 36/ 842, training loss 0.30569425225257874\n",
      "Epoch 29 -- Batch 37/ 842, training loss 0.3087429106235504\n",
      "Epoch 29 -- Batch 38/ 842, training loss 0.30233481526374817\n",
      "Epoch 29 -- Batch 39/ 842, training loss 0.3050048351287842\n",
      "Epoch 29 -- Batch 40/ 842, training loss 0.29870152473449707\n",
      "Epoch 29 -- Batch 41/ 842, training loss 0.2968677282333374\n",
      "Epoch 29 -- Batch 42/ 842, training loss 0.29370394349098206\n",
      "Epoch 29 -- Batch 43/ 842, training loss 0.31690967082977295\n",
      "Epoch 29 -- Batch 44/ 842, training loss 0.29444020986557007\n",
      "Epoch 29 -- Batch 45/ 842, training loss 0.3088253140449524\n",
      "Epoch 29 -- Batch 46/ 842, training loss 0.3028578758239746\n",
      "Epoch 29 -- Batch 47/ 842, training loss 0.3091592788696289\n",
      "Epoch 29 -- Batch 48/ 842, training loss 0.3093174695968628\n",
      "Epoch 29 -- Batch 49/ 842, training loss 0.29506823420524597\n",
      "Epoch 29 -- Batch 50/ 842, training loss 0.3029167056083679\n",
      "Epoch 29 -- Batch 51/ 842, training loss 0.29697954654693604\n",
      "Epoch 29 -- Batch 52/ 842, training loss 0.30961382389068604\n",
      "Epoch 29 -- Batch 53/ 842, training loss 0.29804667830467224\n",
      "Epoch 29 -- Batch 54/ 842, training loss 0.293447345495224\n",
      "Epoch 29 -- Batch 55/ 842, training loss 0.3012579381465912\n",
      "Epoch 29 -- Batch 56/ 842, training loss 0.2944483458995819\n",
      "Epoch 29 -- Batch 57/ 842, training loss 0.29688477516174316\n",
      "Epoch 29 -- Batch 58/ 842, training loss 0.3009108901023865\n",
      "Epoch 29 -- Batch 59/ 842, training loss 0.3027118444442749\n",
      "Epoch 29 -- Batch 60/ 842, training loss 0.28247278928756714\n",
      "Epoch 29 -- Batch 61/ 842, training loss 0.2927873134613037\n",
      "Epoch 29 -- Batch 62/ 842, training loss 0.2972696125507355\n",
      "Epoch 29 -- Batch 63/ 842, training loss 0.29871052503585815\n",
      "Epoch 29 -- Batch 64/ 842, training loss 0.29843807220458984\n",
      "Epoch 29 -- Batch 65/ 842, training loss 0.30949288606643677\n",
      "Epoch 29 -- Batch 66/ 842, training loss 0.3019425570964813\n",
      "Epoch 29 -- Batch 67/ 842, training loss 0.30327683687210083\n",
      "Epoch 29 -- Batch 68/ 842, training loss 0.2986169159412384\n",
      "Epoch 29 -- Batch 69/ 842, training loss 0.29040929675102234\n",
      "Epoch 29 -- Batch 70/ 842, training loss 0.3123980760574341\n",
      "Epoch 29 -- Batch 71/ 842, training loss 0.3188435137271881\n",
      "Epoch 29 -- Batch 72/ 842, training loss 0.2933332324028015\n",
      "Epoch 29 -- Batch 73/ 842, training loss 0.30914369225502014\n",
      "Epoch 29 -- Batch 74/ 842, training loss 0.30818161368370056\n",
      "Epoch 29 -- Batch 75/ 842, training loss 0.3006836771965027\n",
      "Epoch 29 -- Batch 76/ 842, training loss 0.2988155484199524\n",
      "Epoch 29 -- Batch 77/ 842, training loss 0.30926644802093506\n",
      "Epoch 29 -- Batch 78/ 842, training loss 0.30274736881256104\n",
      "Epoch 29 -- Batch 79/ 842, training loss 0.31469252705574036\n",
      "Epoch 29 -- Batch 80/ 842, training loss 0.2894790768623352\n",
      "Epoch 29 -- Batch 81/ 842, training loss 0.31375497579574585\n",
      "Epoch 29 -- Batch 82/ 842, training loss 0.2864987552165985\n",
      "Epoch 29 -- Batch 83/ 842, training loss 0.3172219693660736\n",
      "Epoch 29 -- Batch 84/ 842, training loss 0.30237606167793274\n",
      "Epoch 29 -- Batch 85/ 842, training loss 0.3130504786968231\n",
      "Epoch 29 -- Batch 86/ 842, training loss 0.30909299850463867\n",
      "Epoch 29 -- Batch 87/ 842, training loss 0.30524271726608276\n",
      "Epoch 29 -- Batch 88/ 842, training loss 0.3041449785232544\n",
      "Epoch 29 -- Batch 89/ 842, training loss 0.2978159189224243\n",
      "Epoch 29 -- Batch 90/ 842, training loss 0.2982599139213562\n",
      "Epoch 29 -- Batch 91/ 842, training loss 0.3038315773010254\n",
      "Epoch 29 -- Batch 92/ 842, training loss 0.3075580596923828\n",
      "Epoch 29 -- Batch 93/ 842, training loss 0.30174946784973145\n",
      "Epoch 29 -- Batch 94/ 842, training loss 0.30323395133018494\n",
      "Epoch 29 -- Batch 95/ 842, training loss 0.31711283326148987\n",
      "Epoch 29 -- Batch 96/ 842, training loss 0.3095422387123108\n",
      "Epoch 29 -- Batch 97/ 842, training loss 0.3058542013168335\n",
      "Epoch 29 -- Batch 98/ 842, training loss 0.3027411699295044\n",
      "Epoch 29 -- Batch 99/ 842, training loss 0.3162868618965149\n",
      "Epoch 29 -- Batch 100/ 842, training loss 0.2987670302391052\n",
      "Epoch 29 -- Batch 101/ 842, training loss 0.29704779386520386\n",
      "Epoch 29 -- Batch 102/ 842, training loss 0.29537108540534973\n",
      "Epoch 29 -- Batch 103/ 842, training loss 0.3022589385509491\n",
      "Epoch 29 -- Batch 104/ 842, training loss 0.304992139339447\n",
      "Epoch 29 -- Batch 105/ 842, training loss 0.29586222767829895\n",
      "Epoch 29 -- Batch 106/ 842, training loss 0.30988603830337524\n",
      "Epoch 29 -- Batch 107/ 842, training loss 0.2984582781791687\n",
      "Epoch 29 -- Batch 108/ 842, training loss 0.3115285336971283\n",
      "Epoch 29 -- Batch 109/ 842, training loss 0.3029616177082062\n",
      "Epoch 29 -- Batch 110/ 842, training loss 0.3057694137096405\n",
      "Epoch 29 -- Batch 111/ 842, training loss 0.3124327063560486\n",
      "Epoch 29 -- Batch 112/ 842, training loss 0.3021496832370758\n",
      "Epoch 29 -- Batch 113/ 842, training loss 0.2983817458152771\n",
      "Epoch 29 -- Batch 114/ 842, training loss 0.30266690254211426\n",
      "Epoch 29 -- Batch 115/ 842, training loss 0.30804184079170227\n",
      "Epoch 29 -- Batch 116/ 842, training loss 0.2964652478694916\n",
      "Epoch 29 -- Batch 117/ 842, training loss 0.30741751194000244\n",
      "Epoch 29 -- Batch 118/ 842, training loss 0.3020680546760559\n",
      "Epoch 29 -- Batch 119/ 842, training loss 0.30966049432754517\n",
      "Epoch 29 -- Batch 120/ 842, training loss 0.30698585510253906\n",
      "Epoch 29 -- Batch 121/ 842, training loss 0.295732706785202\n",
      "Epoch 29 -- Batch 122/ 842, training loss 0.2957596480846405\n",
      "Epoch 29 -- Batch 123/ 842, training loss 0.30386942625045776\n",
      "Epoch 29 -- Batch 124/ 842, training loss 0.30826514959335327\n",
      "Epoch 29 -- Batch 125/ 842, training loss 0.3121943473815918\n",
      "Epoch 29 -- Batch 126/ 842, training loss 0.2975630760192871\n",
      "Epoch 29 -- Batch 127/ 842, training loss 0.3064528703689575\n",
      "Epoch 29 -- Batch 128/ 842, training loss 0.31287992000579834\n",
      "Epoch 29 -- Batch 129/ 842, training loss 0.3009694516658783\n",
      "Epoch 29 -- Batch 130/ 842, training loss 0.29084503650665283\n",
      "Epoch 29 -- Batch 131/ 842, training loss 0.3128311038017273\n",
      "Epoch 29 -- Batch 132/ 842, training loss 0.29756835103034973\n",
      "Epoch 29 -- Batch 133/ 842, training loss 0.3017016053199768\n",
      "Epoch 29 -- Batch 134/ 842, training loss 0.3068801760673523\n",
      "Epoch 29 -- Batch 135/ 842, training loss 0.30681565403938293\n",
      "Epoch 29 -- Batch 136/ 842, training loss 0.29942047595977783\n",
      "Epoch 29 -- Batch 137/ 842, training loss 0.3049604296684265\n",
      "Epoch 29 -- Batch 138/ 842, training loss 0.31213584542274475\n",
      "Epoch 29 -- Batch 139/ 842, training loss 0.30171406269073486\n",
      "Epoch 29 -- Batch 140/ 842, training loss 0.29532262682914734\n",
      "Epoch 29 -- Batch 141/ 842, training loss 0.300336629152298\n",
      "Epoch 29 -- Batch 142/ 842, training loss 0.3013284206390381\n",
      "Epoch 29 -- Batch 143/ 842, training loss 0.3065158426761627\n",
      "Epoch 29 -- Batch 144/ 842, training loss 0.2959968149662018\n",
      "Epoch 29 -- Batch 145/ 842, training loss 0.29709097743034363\n",
      "Epoch 29 -- Batch 146/ 842, training loss 0.31213995814323425\n",
      "Epoch 29 -- Batch 147/ 842, training loss 0.29644548892974854\n",
      "Epoch 29 -- Batch 148/ 842, training loss 0.3056839108467102\n",
      "Epoch 29 -- Batch 149/ 842, training loss 0.30565133690834045\n",
      "Epoch 29 -- Batch 150/ 842, training loss 0.3012775778770447\n",
      "Epoch 29 -- Batch 151/ 842, training loss 0.30343493819236755\n",
      "Epoch 29 -- Batch 152/ 842, training loss 0.30154716968536377\n",
      "Epoch 29 -- Batch 153/ 842, training loss 0.30449169874191284\n",
      "Epoch 29 -- Batch 154/ 842, training loss 0.3055291473865509\n",
      "Epoch 29 -- Batch 155/ 842, training loss 0.30040544271469116\n",
      "Epoch 29 -- Batch 156/ 842, training loss 0.300354927778244\n",
      "Epoch 29 -- Batch 157/ 842, training loss 0.3121717572212219\n",
      "Epoch 29 -- Batch 158/ 842, training loss 0.30563634634017944\n",
      "Epoch 29 -- Batch 159/ 842, training loss 0.30483272671699524\n",
      "Epoch 29 -- Batch 160/ 842, training loss 0.30163171887397766\n",
      "Epoch 29 -- Batch 161/ 842, training loss 0.30309784412384033\n",
      "Epoch 29 -- Batch 162/ 842, training loss 0.3136656880378723\n",
      "Epoch 29 -- Batch 163/ 842, training loss 0.31903135776519775\n",
      "Epoch 29 -- Batch 164/ 842, training loss 0.2838408350944519\n",
      "Epoch 29 -- Batch 165/ 842, training loss 0.3010229468345642\n",
      "Epoch 29 -- Batch 166/ 842, training loss 0.31346797943115234\n",
      "Epoch 29 -- Batch 167/ 842, training loss 0.3044193387031555\n",
      "Epoch 29 -- Batch 168/ 842, training loss 0.30812931060791016\n",
      "Epoch 29 -- Batch 169/ 842, training loss 0.2992793619632721\n",
      "Epoch 29 -- Batch 170/ 842, training loss 0.29331374168395996\n",
      "Epoch 29 -- Batch 171/ 842, training loss 0.3000726103782654\n",
      "Epoch 29 -- Batch 172/ 842, training loss 0.2988942265510559\n",
      "Epoch 29 -- Batch 173/ 842, training loss 0.30602404475212097\n",
      "Epoch 29 -- Batch 174/ 842, training loss 0.29349103569984436\n",
      "Epoch 29 -- Batch 175/ 842, training loss 0.31400078535079956\n",
      "Epoch 29 -- Batch 176/ 842, training loss 0.30559056997299194\n",
      "Epoch 29 -- Batch 177/ 842, training loss 0.3108501136302948\n",
      "Epoch 29 -- Batch 178/ 842, training loss 0.3073720932006836\n",
      "Epoch 29 -- Batch 179/ 842, training loss 0.3047206699848175\n",
      "Epoch 29 -- Batch 180/ 842, training loss 0.307040274143219\n",
      "Epoch 29 -- Batch 181/ 842, training loss 0.2948242723941803\n",
      "Epoch 29 -- Batch 182/ 842, training loss 0.31123408675193787\n",
      "Epoch 29 -- Batch 183/ 842, training loss 0.3048161268234253\n",
      "Epoch 29 -- Batch 184/ 842, training loss 0.30940261483192444\n",
      "Epoch 29 -- Batch 185/ 842, training loss 0.30676034092903137\n",
      "Epoch 29 -- Batch 186/ 842, training loss 0.30376875400543213\n",
      "Epoch 29 -- Batch 187/ 842, training loss 0.3005770146846771\n",
      "Epoch 29 -- Batch 188/ 842, training loss 0.3074352443218231\n",
      "Epoch 29 -- Batch 189/ 842, training loss 0.2902119755744934\n",
      "Epoch 29 -- Batch 190/ 842, training loss 0.3002076745033264\n",
      "Epoch 29 -- Batch 191/ 842, training loss 0.30372241139411926\n",
      "Epoch 29 -- Batch 192/ 842, training loss 0.31109246611595154\n",
      "Epoch 29 -- Batch 193/ 842, training loss 0.3153262734413147\n",
      "Epoch 29 -- Batch 194/ 842, training loss 0.2977639436721802\n",
      "Epoch 29 -- Batch 195/ 842, training loss 0.30464693903923035\n",
      "Epoch 29 -- Batch 196/ 842, training loss 0.3029504120349884\n",
      "Epoch 29 -- Batch 197/ 842, training loss 0.2971019148826599\n",
      "Epoch 29 -- Batch 198/ 842, training loss 0.30201345682144165\n",
      "Epoch 29 -- Batch 199/ 842, training loss 0.309329628944397\n",
      "Epoch 29 -- Batch 200/ 842, training loss 0.3016553819179535\n",
      "Epoch 29 -- Batch 201/ 842, training loss 0.3005456328392029\n",
      "Epoch 29 -- Batch 202/ 842, training loss 0.30675771832466125\n",
      "Epoch 29 -- Batch 203/ 842, training loss 0.28843972086906433\n",
      "Epoch 29 -- Batch 204/ 842, training loss 0.3161851167678833\n",
      "Epoch 29 -- Batch 205/ 842, training loss 0.3154902756214142\n",
      "Epoch 29 -- Batch 206/ 842, training loss 0.3074529767036438\n",
      "Epoch 29 -- Batch 207/ 842, training loss 0.29429128766059875\n",
      "Epoch 29 -- Batch 208/ 842, training loss 0.30274727940559387\n",
      "Epoch 29 -- Batch 209/ 842, training loss 0.2993345260620117\n",
      "Epoch 29 -- Batch 210/ 842, training loss 0.307227224111557\n",
      "Epoch 29 -- Batch 211/ 842, training loss 0.29481640458106995\n",
      "Epoch 29 -- Batch 212/ 842, training loss 0.3067306578159332\n",
      "Epoch 29 -- Batch 213/ 842, training loss 0.29876264929771423\n",
      "Epoch 29 -- Batch 214/ 842, training loss 0.3012010157108307\n",
      "Epoch 29 -- Batch 215/ 842, training loss 0.3026931881904602\n",
      "Epoch 29 -- Batch 216/ 842, training loss 0.30496746301651\n",
      "Epoch 29 -- Batch 217/ 842, training loss 0.30788370966911316\n",
      "Epoch 29 -- Batch 218/ 842, training loss 0.3059503138065338\n",
      "Epoch 29 -- Batch 219/ 842, training loss 0.29924455285072327\n",
      "Epoch 29 -- Batch 220/ 842, training loss 0.31040555238723755\n",
      "Epoch 29 -- Batch 221/ 842, training loss 0.3087327778339386\n",
      "Epoch 29 -- Batch 222/ 842, training loss 0.31174135208129883\n",
      "Epoch 29 -- Batch 223/ 842, training loss 0.2994716763496399\n",
      "Epoch 29 -- Batch 224/ 842, training loss 0.3124765157699585\n",
      "Epoch 29 -- Batch 225/ 842, training loss 0.2969505488872528\n",
      "Epoch 29 -- Batch 226/ 842, training loss 0.31464308500289917\n",
      "Epoch 29 -- Batch 227/ 842, training loss 0.30305346846580505\n",
      "Epoch 29 -- Batch 228/ 842, training loss 0.30035677552223206\n",
      "Epoch 29 -- Batch 229/ 842, training loss 0.3214605152606964\n",
      "Epoch 29 -- Batch 230/ 842, training loss 0.29804563522338867\n",
      "Epoch 29 -- Batch 231/ 842, training loss 0.29418739676475525\n",
      "Epoch 29 -- Batch 232/ 842, training loss 0.30303651094436646\n",
      "Epoch 29 -- Batch 233/ 842, training loss 0.3033077120780945\n",
      "Epoch 29 -- Batch 234/ 842, training loss 0.3206068277359009\n",
      "Epoch 29 -- Batch 235/ 842, training loss 0.3058372139930725\n",
      "Epoch 29 -- Batch 236/ 842, training loss 0.3106086254119873\n",
      "Epoch 29 -- Batch 237/ 842, training loss 0.31125909090042114\n",
      "Epoch 29 -- Batch 238/ 842, training loss 0.3018219470977783\n",
      "Epoch 29 -- Batch 239/ 842, training loss 0.31597641110420227\n",
      "Epoch 29 -- Batch 240/ 842, training loss 0.3038470149040222\n",
      "Epoch 29 -- Batch 241/ 842, training loss 0.29444780945777893\n",
      "Epoch 29 -- Batch 242/ 842, training loss 0.2955278158187866\n",
      "Epoch 29 -- Batch 243/ 842, training loss 0.31257227063179016\n",
      "Epoch 29 -- Batch 244/ 842, training loss 0.30229949951171875\n",
      "Epoch 29 -- Batch 245/ 842, training loss 0.3100382387638092\n",
      "Epoch 29 -- Batch 246/ 842, training loss 0.31300023198127747\n",
      "Epoch 29 -- Batch 247/ 842, training loss 0.30901533365249634\n",
      "Epoch 29 -- Batch 248/ 842, training loss 0.30306702852249146\n",
      "Epoch 29 -- Batch 249/ 842, training loss 0.2974497079849243\n",
      "Epoch 29 -- Batch 250/ 842, training loss 0.3073956370353699\n",
      "Epoch 29 -- Batch 251/ 842, training loss 0.3067176043987274\n",
      "Epoch 29 -- Batch 252/ 842, training loss 0.3048097491264343\n",
      "Epoch 29 -- Batch 253/ 842, training loss 0.30346325039863586\n",
      "Epoch 29 -- Batch 254/ 842, training loss 0.29756349325180054\n",
      "Epoch 29 -- Batch 255/ 842, training loss 0.30940738320350647\n",
      "Epoch 29 -- Batch 256/ 842, training loss 0.2962399125099182\n",
      "Epoch 29 -- Batch 257/ 842, training loss 0.30204513669013977\n",
      "Epoch 29 -- Batch 258/ 842, training loss 0.3097839653491974\n",
      "Epoch 29 -- Batch 259/ 842, training loss 0.30710262060165405\n",
      "Epoch 29 -- Batch 260/ 842, training loss 0.30396971106529236\n",
      "Epoch 29 -- Batch 261/ 842, training loss 0.3150855600833893\n",
      "Epoch 29 -- Batch 262/ 842, training loss 0.31460586190223694\n",
      "Epoch 29 -- Batch 263/ 842, training loss 0.30181849002838135\n",
      "Epoch 29 -- Batch 264/ 842, training loss 0.3011568486690521\n",
      "Epoch 29 -- Batch 265/ 842, training loss 0.30924153327941895\n",
      "Epoch 29 -- Batch 266/ 842, training loss 0.3087398409843445\n",
      "Epoch 29 -- Batch 267/ 842, training loss 0.30717337131500244\n",
      "Epoch 29 -- Batch 268/ 842, training loss 0.30086949467658997\n",
      "Epoch 29 -- Batch 269/ 842, training loss 0.30860814452171326\n",
      "Epoch 29 -- Batch 270/ 842, training loss 0.2998051345348358\n",
      "Epoch 29 -- Batch 271/ 842, training loss 0.3087114989757538\n",
      "Epoch 29 -- Batch 272/ 842, training loss 0.30684611201286316\n",
      "Epoch 29 -- Batch 273/ 842, training loss 0.30466267466545105\n",
      "Epoch 29 -- Batch 274/ 842, training loss 0.30360525846481323\n",
      "Epoch 29 -- Batch 275/ 842, training loss 0.3053308427333832\n",
      "Epoch 29 -- Batch 276/ 842, training loss 0.3040451407432556\n",
      "Epoch 29 -- Batch 277/ 842, training loss 0.3089064955711365\n",
      "Epoch 29 -- Batch 278/ 842, training loss 0.305510938167572\n",
      "Epoch 29 -- Batch 279/ 842, training loss 0.307693749666214\n",
      "Epoch 29 -- Batch 280/ 842, training loss 0.3041898310184479\n",
      "Epoch 29 -- Batch 281/ 842, training loss 0.3078034818172455\n",
      "Epoch 29 -- Batch 282/ 842, training loss 0.31608614325523376\n",
      "Epoch 29 -- Batch 283/ 842, training loss 0.3199724853038788\n",
      "Epoch 29 -- Batch 284/ 842, training loss 0.3099590241909027\n",
      "Epoch 29 -- Batch 285/ 842, training loss 0.30619341135025024\n",
      "Epoch 29 -- Batch 286/ 842, training loss 0.30903640389442444\n",
      "Epoch 29 -- Batch 287/ 842, training loss 0.2979625165462494\n",
      "Epoch 29 -- Batch 288/ 842, training loss 0.305866539478302\n",
      "Epoch 29 -- Batch 289/ 842, training loss 0.3097763955593109\n",
      "Epoch 29 -- Batch 290/ 842, training loss 0.3036353290081024\n",
      "Epoch 29 -- Batch 291/ 842, training loss 0.308695524930954\n",
      "Epoch 29 -- Batch 292/ 842, training loss 0.30466943979263306\n",
      "Epoch 29 -- Batch 293/ 842, training loss 0.31810325384140015\n",
      "Epoch 29 -- Batch 294/ 842, training loss 0.3020362854003906\n",
      "Epoch 29 -- Batch 295/ 842, training loss 0.29811805486679077\n",
      "Epoch 29 -- Batch 296/ 842, training loss 0.30461451411247253\n",
      "Epoch 29 -- Batch 297/ 842, training loss 0.2953450381755829\n",
      "Epoch 29 -- Batch 298/ 842, training loss 0.3069404363632202\n",
      "Epoch 29 -- Batch 299/ 842, training loss 0.309068888425827\n",
      "Epoch 29 -- Batch 300/ 842, training loss 0.30366894602775574\n",
      "Epoch 29 -- Batch 301/ 842, training loss 0.30676424503326416\n",
      "Epoch 29 -- Batch 302/ 842, training loss 0.3068000376224518\n",
      "Epoch 29 -- Batch 303/ 842, training loss 0.3021393418312073\n",
      "Epoch 29 -- Batch 304/ 842, training loss 0.30564072728157043\n",
      "Epoch 29 -- Batch 305/ 842, training loss 0.3028450012207031\n",
      "Epoch 29 -- Batch 306/ 842, training loss 0.2962828576564789\n",
      "Epoch 29 -- Batch 307/ 842, training loss 0.30840179324150085\n",
      "Epoch 29 -- Batch 308/ 842, training loss 0.29955434799194336\n",
      "Epoch 29 -- Batch 309/ 842, training loss 0.30991944670677185\n",
      "Epoch 29 -- Batch 310/ 842, training loss 0.3161642253398895\n",
      "Epoch 29 -- Batch 311/ 842, training loss 0.3131902813911438\n",
      "Epoch 29 -- Batch 312/ 842, training loss 0.3045438528060913\n",
      "Epoch 29 -- Batch 313/ 842, training loss 0.3210206627845764\n",
      "Epoch 29 -- Batch 314/ 842, training loss 0.30541786551475525\n",
      "Epoch 29 -- Batch 315/ 842, training loss 0.30559757351875305\n",
      "Epoch 29 -- Batch 316/ 842, training loss 0.3063150644302368\n",
      "Epoch 29 -- Batch 317/ 842, training loss 0.31378746032714844\n",
      "Epoch 29 -- Batch 318/ 842, training loss 0.3091707229614258\n",
      "Epoch 29 -- Batch 319/ 842, training loss 0.30398017168045044\n",
      "Epoch 29 -- Batch 320/ 842, training loss 0.3126257061958313\n",
      "Epoch 29 -- Batch 321/ 842, training loss 0.3053217828273773\n",
      "Epoch 29 -- Batch 322/ 842, training loss 0.3088191747665405\n",
      "Epoch 29 -- Batch 323/ 842, training loss 0.3226841390132904\n",
      "Epoch 29 -- Batch 324/ 842, training loss 0.30715474486351013\n",
      "Epoch 29 -- Batch 325/ 842, training loss 0.3021813631057739\n",
      "Epoch 29 -- Batch 326/ 842, training loss 0.3112669289112091\n",
      "Epoch 29 -- Batch 327/ 842, training loss 0.3077036440372467\n",
      "Epoch 29 -- Batch 328/ 842, training loss 0.3139295279979706\n",
      "Epoch 29 -- Batch 329/ 842, training loss 0.3067561686038971\n",
      "Epoch 29 -- Batch 330/ 842, training loss 0.3116922080516815\n",
      "Epoch 29 -- Batch 331/ 842, training loss 0.3061543107032776\n",
      "Epoch 29 -- Batch 332/ 842, training loss 0.30252406001091003\n",
      "Epoch 29 -- Batch 333/ 842, training loss 0.3105889856815338\n",
      "Epoch 29 -- Batch 334/ 842, training loss 0.31024110317230225\n",
      "Epoch 29 -- Batch 335/ 842, training loss 0.3037543296813965\n",
      "Epoch 29 -- Batch 336/ 842, training loss 0.3096497058868408\n",
      "Epoch 29 -- Batch 337/ 842, training loss 0.30049094557762146\n",
      "Epoch 29 -- Batch 338/ 842, training loss 0.3011402487754822\n",
      "Epoch 29 -- Batch 339/ 842, training loss 0.30953460931777954\n",
      "Epoch 29 -- Batch 340/ 842, training loss 0.30333513021469116\n",
      "Epoch 29 -- Batch 341/ 842, training loss 0.31155920028686523\n",
      "Epoch 29 -- Batch 342/ 842, training loss 0.30813437700271606\n",
      "Epoch 29 -- Batch 343/ 842, training loss 0.31203290820121765\n",
      "Epoch 29 -- Batch 344/ 842, training loss 0.3005162477493286\n",
      "Epoch 29 -- Batch 345/ 842, training loss 0.29550105333328247\n",
      "Epoch 29 -- Batch 346/ 842, training loss 0.320059597492218\n",
      "Epoch 29 -- Batch 347/ 842, training loss 0.30174720287323\n",
      "Epoch 29 -- Batch 348/ 842, training loss 0.30716925859451294\n",
      "Epoch 29 -- Batch 349/ 842, training loss 0.3047841191291809\n",
      "Epoch 29 -- Batch 350/ 842, training loss 0.31092891097068787\n",
      "Epoch 29 -- Batch 351/ 842, training loss 0.3021968603134155\n",
      "Epoch 29 -- Batch 352/ 842, training loss 0.30307334661483765\n",
      "Epoch 29 -- Batch 353/ 842, training loss 0.2992079555988312\n",
      "Epoch 29 -- Batch 354/ 842, training loss 0.31056779623031616\n",
      "Epoch 29 -- Batch 355/ 842, training loss 0.3061519265174866\n",
      "Epoch 29 -- Batch 356/ 842, training loss 0.3225972652435303\n",
      "Epoch 29 -- Batch 357/ 842, training loss 0.29393985867500305\n",
      "Epoch 29 -- Batch 358/ 842, training loss 0.30973219871520996\n",
      "Epoch 29 -- Batch 359/ 842, training loss 0.2996548116207123\n",
      "Epoch 29 -- Batch 360/ 842, training loss 0.30338209867477417\n",
      "Epoch 29 -- Batch 361/ 842, training loss 0.3239908516407013\n",
      "Epoch 29 -- Batch 362/ 842, training loss 0.3083973228931427\n",
      "Epoch 29 -- Batch 363/ 842, training loss 0.30544379353523254\n",
      "Epoch 29 -- Batch 364/ 842, training loss 0.2977825403213501\n",
      "Epoch 29 -- Batch 365/ 842, training loss 0.3053286671638489\n",
      "Epoch 29 -- Batch 366/ 842, training loss 0.3237897455692291\n",
      "Epoch 29 -- Batch 367/ 842, training loss 0.3096381723880768\n",
      "Epoch 29 -- Batch 368/ 842, training loss 0.30749353766441345\n",
      "Epoch 29 -- Batch 369/ 842, training loss 0.30816298723220825\n",
      "Epoch 29 -- Batch 370/ 842, training loss 0.30230948328971863\n",
      "Epoch 29 -- Batch 371/ 842, training loss 0.30482426285743713\n",
      "Epoch 29 -- Batch 372/ 842, training loss 0.31932753324508667\n",
      "Epoch 29 -- Batch 373/ 842, training loss 0.3060466945171356\n",
      "Epoch 29 -- Batch 374/ 842, training loss 0.30873897671699524\n",
      "Epoch 29 -- Batch 375/ 842, training loss 0.3040323853492737\n",
      "Epoch 29 -- Batch 376/ 842, training loss 0.31413745880126953\n",
      "Epoch 29 -- Batch 377/ 842, training loss 0.31380853056907654\n",
      "Epoch 29 -- Batch 378/ 842, training loss 0.3066115081310272\n",
      "Epoch 29 -- Batch 379/ 842, training loss 0.296906054019928\n",
      "Epoch 29 -- Batch 380/ 842, training loss 0.30799004435539246\n",
      "Epoch 29 -- Batch 381/ 842, training loss 0.3088712692260742\n",
      "Epoch 29 -- Batch 382/ 842, training loss 0.3236880898475647\n",
      "Epoch 29 -- Batch 383/ 842, training loss 0.31451401114463806\n",
      "Epoch 29 -- Batch 384/ 842, training loss 0.3100523054599762\n",
      "Epoch 29 -- Batch 385/ 842, training loss 0.3014451861381531\n",
      "Epoch 29 -- Batch 386/ 842, training loss 0.30737587809562683\n",
      "Epoch 29 -- Batch 387/ 842, training loss 0.3155338168144226\n",
      "Epoch 29 -- Batch 388/ 842, training loss 0.3033522963523865\n",
      "Epoch 29 -- Batch 389/ 842, training loss 0.30620360374450684\n",
      "Epoch 29 -- Batch 390/ 842, training loss 0.3130858242511749\n",
      "Epoch 29 -- Batch 391/ 842, training loss 0.293081134557724\n",
      "Epoch 29 -- Batch 392/ 842, training loss 0.30737730860710144\n",
      "Epoch 29 -- Batch 393/ 842, training loss 0.31508442759513855\n",
      "Epoch 29 -- Batch 394/ 842, training loss 0.30553558468818665\n",
      "Epoch 29 -- Batch 395/ 842, training loss 0.30923759937286377\n",
      "Epoch 29 -- Batch 396/ 842, training loss 0.3041931390762329\n",
      "Epoch 29 -- Batch 397/ 842, training loss 0.311816543340683\n",
      "Epoch 29 -- Batch 398/ 842, training loss 0.3169924020767212\n",
      "Epoch 29 -- Batch 399/ 842, training loss 0.3062472343444824\n",
      "Epoch 29 -- Batch 400/ 842, training loss 0.2989874482154846\n",
      "Epoch 29 -- Batch 401/ 842, training loss 0.31094300746917725\n",
      "Epoch 29 -- Batch 402/ 842, training loss 0.30518266558647156\n",
      "Epoch 29 -- Batch 403/ 842, training loss 0.30898192524909973\n",
      "Epoch 29 -- Batch 404/ 842, training loss 0.3087410628795624\n",
      "Epoch 29 -- Batch 405/ 842, training loss 0.30153688788414\n",
      "Epoch 29 -- Batch 406/ 842, training loss 0.30201682448387146\n",
      "Epoch 29 -- Batch 407/ 842, training loss 0.32114043831825256\n",
      "Epoch 29 -- Batch 408/ 842, training loss 0.3087512254714966\n",
      "Epoch 29 -- Batch 409/ 842, training loss 0.3003542125225067\n",
      "Epoch 29 -- Batch 410/ 842, training loss 0.2963889241218567\n",
      "Epoch 29 -- Batch 411/ 842, training loss 0.3070439100265503\n",
      "Epoch 29 -- Batch 412/ 842, training loss 0.3026127219200134\n",
      "Epoch 29 -- Batch 413/ 842, training loss 0.3027094304561615\n",
      "Epoch 29 -- Batch 414/ 842, training loss 0.2995036840438843\n",
      "Epoch 29 -- Batch 415/ 842, training loss 0.3080788552761078\n",
      "Epoch 29 -- Batch 416/ 842, training loss 0.3102353513240814\n",
      "Epoch 29 -- Batch 417/ 842, training loss 0.2964400351047516\n",
      "Epoch 29 -- Batch 418/ 842, training loss 0.31455495953559875\n",
      "Epoch 29 -- Batch 419/ 842, training loss 0.31676456332206726\n",
      "Epoch 29 -- Batch 420/ 842, training loss 0.3048515319824219\n",
      "Epoch 29 -- Batch 421/ 842, training loss 0.31378352642059326\n",
      "Epoch 29 -- Batch 422/ 842, training loss 0.30486440658569336\n",
      "Epoch 29 -- Batch 423/ 842, training loss 0.30375757813453674\n",
      "Epoch 29 -- Batch 424/ 842, training loss 0.30708742141723633\n",
      "Epoch 29 -- Batch 425/ 842, training loss 0.29859259724617004\n",
      "Epoch 29 -- Batch 426/ 842, training loss 0.30052220821380615\n",
      "Epoch 29 -- Batch 427/ 842, training loss 0.30990374088287354\n",
      "Epoch 29 -- Batch 428/ 842, training loss 0.3052009344100952\n",
      "Epoch 29 -- Batch 429/ 842, training loss 0.30989909172058105\n",
      "Epoch 29 -- Batch 430/ 842, training loss 0.3164300322532654\n",
      "Epoch 29 -- Batch 431/ 842, training loss 0.3073212504386902\n",
      "Epoch 29 -- Batch 432/ 842, training loss 0.304347425699234\n",
      "Epoch 29 -- Batch 433/ 842, training loss 0.3070453703403473\n",
      "Epoch 29 -- Batch 434/ 842, training loss 0.3152447044849396\n",
      "Epoch 29 -- Batch 435/ 842, training loss 0.29669830203056335\n",
      "Epoch 29 -- Batch 436/ 842, training loss 0.29849717020988464\n",
      "Epoch 29 -- Batch 437/ 842, training loss 0.31441357731819153\n",
      "Epoch 29 -- Batch 438/ 842, training loss 0.30786845088005066\n",
      "Epoch 29 -- Batch 439/ 842, training loss 0.3015158772468567\n",
      "Epoch 29 -- Batch 440/ 842, training loss 0.3113442063331604\n",
      "Epoch 29 -- Batch 441/ 842, training loss 0.3079957962036133\n",
      "Epoch 29 -- Batch 442/ 842, training loss 0.30654576420783997\n",
      "Epoch 29 -- Batch 443/ 842, training loss 0.3199901282787323\n",
      "Epoch 29 -- Batch 444/ 842, training loss 0.31041911244392395\n",
      "Epoch 29 -- Batch 445/ 842, training loss 0.316003680229187\n",
      "Epoch 29 -- Batch 446/ 842, training loss 0.3058135509490967\n",
      "Epoch 29 -- Batch 447/ 842, training loss 0.2952399253845215\n",
      "Epoch 29 -- Batch 448/ 842, training loss 0.30453163385391235\n",
      "Epoch 29 -- Batch 449/ 842, training loss 0.29381516575813293\n",
      "Epoch 29 -- Batch 450/ 842, training loss 0.3025364875793457\n",
      "Epoch 29 -- Batch 451/ 842, training loss 0.31007230281829834\n",
      "Epoch 29 -- Batch 452/ 842, training loss 0.30165815353393555\n",
      "Epoch 29 -- Batch 453/ 842, training loss 0.3118278980255127\n",
      "Epoch 29 -- Batch 454/ 842, training loss 0.30560362339019775\n",
      "Epoch 29 -- Batch 455/ 842, training loss 0.3112888038158417\n",
      "Epoch 29 -- Batch 456/ 842, training loss 0.3138771951198578\n",
      "Epoch 29 -- Batch 457/ 842, training loss 0.3005596697330475\n",
      "Epoch 29 -- Batch 458/ 842, training loss 0.318837970495224\n",
      "Epoch 29 -- Batch 459/ 842, training loss 0.3049960434436798\n",
      "Epoch 29 -- Batch 460/ 842, training loss 0.3084441125392914\n",
      "Epoch 29 -- Batch 461/ 842, training loss 0.31383591890335083\n",
      "Epoch 29 -- Batch 462/ 842, training loss 0.30356237292289734\n",
      "Epoch 29 -- Batch 463/ 842, training loss 0.3166484236717224\n",
      "Epoch 29 -- Batch 464/ 842, training loss 0.30612561106681824\n",
      "Epoch 29 -- Batch 465/ 842, training loss 0.31107816100120544\n",
      "Epoch 29 -- Batch 466/ 842, training loss 0.3033877909183502\n",
      "Epoch 29 -- Batch 467/ 842, training loss 0.2979847192764282\n",
      "Epoch 29 -- Batch 468/ 842, training loss 0.31728726625442505\n",
      "Epoch 29 -- Batch 469/ 842, training loss 0.3074272572994232\n",
      "Epoch 29 -- Batch 470/ 842, training loss 0.31864142417907715\n",
      "Epoch 29 -- Batch 471/ 842, training loss 0.2985348701477051\n",
      "Epoch 29 -- Batch 472/ 842, training loss 0.31770625710487366\n",
      "Epoch 29 -- Batch 473/ 842, training loss 0.31032794713974\n",
      "Epoch 29 -- Batch 474/ 842, training loss 0.30634862184524536\n",
      "Epoch 29 -- Batch 475/ 842, training loss 0.30882835388183594\n",
      "Epoch 29 -- Batch 476/ 842, training loss 0.31633442640304565\n",
      "Epoch 29 -- Batch 477/ 842, training loss 0.3052991032600403\n",
      "Epoch 29 -- Batch 478/ 842, training loss 0.29930806159973145\n",
      "Epoch 29 -- Batch 479/ 842, training loss 0.30142953991889954\n",
      "Epoch 29 -- Batch 480/ 842, training loss 0.315184623003006\n",
      "Epoch 29 -- Batch 481/ 842, training loss 0.3113446831703186\n",
      "Epoch 29 -- Batch 482/ 842, training loss 0.3121930658817291\n",
      "Epoch 29 -- Batch 483/ 842, training loss 0.3051910400390625\n",
      "Epoch 29 -- Batch 484/ 842, training loss 0.3105847239494324\n",
      "Epoch 29 -- Batch 485/ 842, training loss 0.3025634288787842\n",
      "Epoch 29 -- Batch 486/ 842, training loss 0.2967575788497925\n",
      "Epoch 29 -- Batch 487/ 842, training loss 0.3094060719013214\n",
      "Epoch 29 -- Batch 488/ 842, training loss 0.30122432112693787\n",
      "Epoch 29 -- Batch 489/ 842, training loss 0.3198820948600769\n",
      "Epoch 29 -- Batch 490/ 842, training loss 0.3017701208591461\n",
      "Epoch 29 -- Batch 491/ 842, training loss 0.3128109872341156\n",
      "Epoch 29 -- Batch 492/ 842, training loss 0.3223966956138611\n",
      "Epoch 29 -- Batch 493/ 842, training loss 0.3064965009689331\n",
      "Epoch 29 -- Batch 494/ 842, training loss 0.3048866391181946\n",
      "Epoch 29 -- Batch 495/ 842, training loss 0.3044145405292511\n",
      "Epoch 29 -- Batch 496/ 842, training loss 0.30439499020576477\n",
      "Epoch 29 -- Batch 497/ 842, training loss 0.3066670894622803\n",
      "Epoch 29 -- Batch 498/ 842, training loss 0.31436771154403687\n",
      "Epoch 29 -- Batch 499/ 842, training loss 0.2993481755256653\n",
      "Epoch 29 -- Batch 500/ 842, training loss 0.3076331913471222\n",
      "Epoch 29 -- Batch 501/ 842, training loss 0.30813270807266235\n",
      "Epoch 29 -- Batch 502/ 842, training loss 0.31041064858436584\n",
      "Epoch 29 -- Batch 503/ 842, training loss 0.3055075705051422\n",
      "Epoch 29 -- Batch 504/ 842, training loss 0.3082444667816162\n",
      "Epoch 29 -- Batch 505/ 842, training loss 0.3035601079463959\n",
      "Epoch 29 -- Batch 506/ 842, training loss 0.3161354660987854\n",
      "Epoch 29 -- Batch 507/ 842, training loss 0.32147157192230225\n",
      "Epoch 29 -- Batch 508/ 842, training loss 0.30851370096206665\n",
      "Epoch 29 -- Batch 509/ 842, training loss 0.3185538649559021\n",
      "Epoch 29 -- Batch 510/ 842, training loss 0.30392301082611084\n",
      "Epoch 29 -- Batch 511/ 842, training loss 0.3179554343223572\n",
      "Epoch 29 -- Batch 512/ 842, training loss 0.3030259907245636\n",
      "Epoch 29 -- Batch 513/ 842, training loss 0.30153152346611023\n",
      "Epoch 29 -- Batch 514/ 842, training loss 0.3069244623184204\n",
      "Epoch 29 -- Batch 515/ 842, training loss 0.2997395396232605\n",
      "Epoch 29 -- Batch 516/ 842, training loss 0.30680450797080994\n",
      "Epoch 29 -- Batch 517/ 842, training loss 0.31037265062332153\n",
      "Epoch 29 -- Batch 518/ 842, training loss 0.30197539925575256\n",
      "Epoch 29 -- Batch 519/ 842, training loss 0.29849645495414734\n",
      "Epoch 29 -- Batch 520/ 842, training loss 0.3129187822341919\n",
      "Epoch 29 -- Batch 521/ 842, training loss 0.2966746985912323\n",
      "Epoch 29 -- Batch 522/ 842, training loss 0.31224462389945984\n",
      "Epoch 29 -- Batch 523/ 842, training loss 0.3069207966327667\n",
      "Epoch 29 -- Batch 524/ 842, training loss 0.3003110885620117\n",
      "Epoch 29 -- Batch 525/ 842, training loss 0.3094151020050049\n",
      "Epoch 29 -- Batch 526/ 842, training loss 0.3052143454551697\n",
      "Epoch 29 -- Batch 527/ 842, training loss 0.30446282029151917\n",
      "Epoch 29 -- Batch 528/ 842, training loss 0.31111693382263184\n",
      "Epoch 29 -- Batch 529/ 842, training loss 0.314231812953949\n",
      "Epoch 29 -- Batch 530/ 842, training loss 0.31935548782348633\n",
      "Epoch 29 -- Batch 531/ 842, training loss 0.30849310755729675\n",
      "Epoch 29 -- Batch 532/ 842, training loss 0.30209115147590637\n",
      "Epoch 29 -- Batch 533/ 842, training loss 0.3079020082950592\n",
      "Epoch 29 -- Batch 534/ 842, training loss 0.3020479381084442\n",
      "Epoch 29 -- Batch 535/ 842, training loss 0.3152012825012207\n",
      "Epoch 29 -- Batch 536/ 842, training loss 0.29894715547561646\n",
      "Epoch 29 -- Batch 537/ 842, training loss 0.3069402575492859\n",
      "Epoch 29 -- Batch 538/ 842, training loss 0.3021717667579651\n",
      "Epoch 29 -- Batch 539/ 842, training loss 0.30704575777053833\n",
      "Epoch 29 -- Batch 540/ 842, training loss 0.30081579089164734\n",
      "Epoch 29 -- Batch 541/ 842, training loss 0.3137091398239136\n",
      "Epoch 29 -- Batch 542/ 842, training loss 0.301743745803833\n",
      "Epoch 29 -- Batch 543/ 842, training loss 0.30991330742836\n",
      "Epoch 29 -- Batch 544/ 842, training loss 0.3035303056240082\n",
      "Epoch 29 -- Batch 545/ 842, training loss 0.31474825739860535\n",
      "Epoch 29 -- Batch 546/ 842, training loss 0.2979094982147217\n",
      "Epoch 29 -- Batch 547/ 842, training loss 0.312579482793808\n",
      "Epoch 29 -- Batch 548/ 842, training loss 0.30705636739730835\n",
      "Epoch 29 -- Batch 549/ 842, training loss 0.3085373044013977\n",
      "Epoch 29 -- Batch 550/ 842, training loss 0.30775314569473267\n",
      "Epoch 29 -- Batch 551/ 842, training loss 0.31473419070243835\n",
      "Epoch 29 -- Batch 552/ 842, training loss 0.29748231172561646\n",
      "Epoch 29 -- Batch 553/ 842, training loss 0.30471158027648926\n",
      "Epoch 29 -- Batch 554/ 842, training loss 0.2993093729019165\n",
      "Epoch 29 -- Batch 555/ 842, training loss 0.3096136152744293\n",
      "Epoch 29 -- Batch 556/ 842, training loss 0.31475746631622314\n",
      "Epoch 29 -- Batch 557/ 842, training loss 0.3066120743751526\n",
      "Epoch 29 -- Batch 558/ 842, training loss 0.30429989099502563\n",
      "Epoch 29 -- Batch 559/ 842, training loss 0.30278071761131287\n",
      "Epoch 29 -- Batch 560/ 842, training loss 0.31106674671173096\n",
      "Epoch 29 -- Batch 561/ 842, training loss 0.30946093797683716\n",
      "Epoch 29 -- Batch 562/ 842, training loss 0.31192976236343384\n",
      "Epoch 29 -- Batch 563/ 842, training loss 0.30360648036003113\n",
      "Epoch 29 -- Batch 564/ 842, training loss 0.30495402216911316\n",
      "Epoch 29 -- Batch 565/ 842, training loss 0.31107503175735474\n",
      "Epoch 29 -- Batch 566/ 842, training loss 0.3015245497226715\n",
      "Epoch 29 -- Batch 567/ 842, training loss 0.32296764850616455\n",
      "Epoch 29 -- Batch 568/ 842, training loss 0.3039751946926117\n",
      "Epoch 29 -- Batch 569/ 842, training loss 0.31219103932380676\n",
      "Epoch 29 -- Batch 570/ 842, training loss 0.31667572259902954\n",
      "Epoch 29 -- Batch 571/ 842, training loss 0.30882665514945984\n",
      "Epoch 29 -- Batch 572/ 842, training loss 0.29924270510673523\n",
      "Epoch 29 -- Batch 573/ 842, training loss 0.314126193523407\n",
      "Epoch 29 -- Batch 574/ 842, training loss 0.31232160329818726\n",
      "Epoch 29 -- Batch 575/ 842, training loss 0.3131897747516632\n",
      "Epoch 29 -- Batch 576/ 842, training loss 0.31013748049736023\n",
      "Epoch 29 -- Batch 577/ 842, training loss 0.30422642827033997\n",
      "Epoch 29 -- Batch 578/ 842, training loss 0.3124293088912964\n",
      "Epoch 29 -- Batch 579/ 842, training loss 0.30720946192741394\n",
      "Epoch 29 -- Batch 580/ 842, training loss 0.31499022245407104\n",
      "Epoch 29 -- Batch 581/ 842, training loss 0.30205053091049194\n",
      "Epoch 29 -- Batch 582/ 842, training loss 0.31379973888397217\n",
      "Epoch 29 -- Batch 583/ 842, training loss 0.3124254643917084\n",
      "Epoch 29 -- Batch 584/ 842, training loss 0.3063468039035797\n",
      "Epoch 29 -- Batch 585/ 842, training loss 0.30579084157943726\n",
      "Epoch 29 -- Batch 586/ 842, training loss 0.2996160686016083\n",
      "Epoch 29 -- Batch 587/ 842, training loss 0.3011917471885681\n",
      "Epoch 29 -- Batch 588/ 842, training loss 0.30889931321144104\n",
      "Epoch 29 -- Batch 589/ 842, training loss 0.3066045641899109\n",
      "Epoch 29 -- Batch 590/ 842, training loss 0.31750211119651794\n",
      "Epoch 29 -- Batch 591/ 842, training loss 0.3018603026866913\n",
      "Epoch 29 -- Batch 592/ 842, training loss 0.2995815873146057\n",
      "Epoch 29 -- Batch 593/ 842, training loss 0.3026697635650635\n",
      "Epoch 29 -- Batch 594/ 842, training loss 0.30704987049102783\n",
      "Epoch 29 -- Batch 595/ 842, training loss 0.3231143653392792\n",
      "Epoch 29 -- Batch 596/ 842, training loss 0.3096370995044708\n",
      "Epoch 29 -- Batch 597/ 842, training loss 0.3196433186531067\n",
      "Epoch 29 -- Batch 598/ 842, training loss 0.3077029585838318\n",
      "Epoch 29 -- Batch 599/ 842, training loss 0.30073702335357666\n",
      "Epoch 29 -- Batch 600/ 842, training loss 0.29932305216789246\n",
      "Epoch 29 -- Batch 601/ 842, training loss 0.3038490414619446\n",
      "Epoch 29 -- Batch 602/ 842, training loss 0.3071688413619995\n",
      "Epoch 29 -- Batch 603/ 842, training loss 0.29922398924827576\n",
      "Epoch 29 -- Batch 604/ 842, training loss 0.302531898021698\n",
      "Epoch 29 -- Batch 605/ 842, training loss 0.3059162199497223\n",
      "Epoch 29 -- Batch 606/ 842, training loss 0.3114895820617676\n",
      "Epoch 29 -- Batch 607/ 842, training loss 0.29735445976257324\n",
      "Epoch 29 -- Batch 608/ 842, training loss 0.311209499835968\n",
      "Epoch 29 -- Batch 609/ 842, training loss 0.3013458847999573\n",
      "Epoch 29 -- Batch 610/ 842, training loss 0.32151252031326294\n",
      "Epoch 29 -- Batch 611/ 842, training loss 0.3032899796962738\n",
      "Epoch 29 -- Batch 612/ 842, training loss 0.30919262766838074\n",
      "Epoch 29 -- Batch 613/ 842, training loss 0.30505597591400146\n",
      "Epoch 29 -- Batch 614/ 842, training loss 0.30288439989089966\n",
      "Epoch 29 -- Batch 615/ 842, training loss 0.2981255352497101\n",
      "Epoch 29 -- Batch 616/ 842, training loss 0.30894508957862854\n",
      "Epoch 29 -- Batch 617/ 842, training loss 0.30556827783584595\n",
      "Epoch 29 -- Batch 618/ 842, training loss 0.3045618534088135\n",
      "Epoch 29 -- Batch 619/ 842, training loss 0.3120242655277252\n",
      "Epoch 29 -- Batch 620/ 842, training loss 0.3103480339050293\n",
      "Epoch 29 -- Batch 621/ 842, training loss 0.3084726631641388\n",
      "Epoch 29 -- Batch 622/ 842, training loss 0.31073126196861267\n",
      "Epoch 29 -- Batch 623/ 842, training loss 0.3086414039134979\n",
      "Epoch 29 -- Batch 624/ 842, training loss 0.3053397834300995\n",
      "Epoch 29 -- Batch 625/ 842, training loss 0.3086453974246979\n",
      "Epoch 29 -- Batch 626/ 842, training loss 0.3103761374950409\n",
      "Epoch 29 -- Batch 627/ 842, training loss 0.30283012986183167\n",
      "Epoch 29 -- Batch 628/ 842, training loss 0.298890084028244\n",
      "Epoch 29 -- Batch 629/ 842, training loss 0.31406471133232117\n",
      "Epoch 29 -- Batch 630/ 842, training loss 0.30196675658226013\n",
      "Epoch 29 -- Batch 631/ 842, training loss 0.3067958950996399\n",
      "Epoch 29 -- Batch 632/ 842, training loss 0.3051132559776306\n",
      "Epoch 29 -- Batch 633/ 842, training loss 0.3012091815471649\n",
      "Epoch 29 -- Batch 634/ 842, training loss 0.3091794550418854\n",
      "Epoch 29 -- Batch 635/ 842, training loss 0.30605244636535645\n",
      "Epoch 29 -- Batch 636/ 842, training loss 0.3154435455799103\n",
      "Epoch 29 -- Batch 637/ 842, training loss 0.3001263737678528\n",
      "Epoch 29 -- Batch 638/ 842, training loss 0.32242926955223083\n",
      "Epoch 29 -- Batch 639/ 842, training loss 0.30082571506500244\n",
      "Epoch 29 -- Batch 640/ 842, training loss 0.30050691962242126\n",
      "Epoch 29 -- Batch 641/ 842, training loss 0.3101278245449066\n",
      "Epoch 29 -- Batch 642/ 842, training loss 0.3030362129211426\n",
      "Epoch 29 -- Batch 643/ 842, training loss 0.311057984828949\n",
      "Epoch 29 -- Batch 644/ 842, training loss 0.306844025850296\n",
      "Epoch 29 -- Batch 645/ 842, training loss 0.3185873329639435\n",
      "Epoch 29 -- Batch 646/ 842, training loss 0.3058168590068817\n",
      "Epoch 29 -- Batch 647/ 842, training loss 0.30105718970298767\n",
      "Epoch 29 -- Batch 648/ 842, training loss 0.31388717889785767\n",
      "Epoch 29 -- Batch 649/ 842, training loss 0.3026377558708191\n",
      "Epoch 29 -- Batch 650/ 842, training loss 0.3079712390899658\n",
      "Epoch 29 -- Batch 651/ 842, training loss 0.30901002883911133\n",
      "Epoch 29 -- Batch 652/ 842, training loss 0.30730825662612915\n",
      "Epoch 29 -- Batch 653/ 842, training loss 0.29969921708106995\n",
      "Epoch 29 -- Batch 654/ 842, training loss 0.29483917355537415\n",
      "Epoch 29 -- Batch 655/ 842, training loss 0.30446675419807434\n",
      "Epoch 29 -- Batch 656/ 842, training loss 0.3193243443965912\n",
      "Epoch 29 -- Batch 657/ 842, training loss 0.30576464533805847\n",
      "Epoch 29 -- Batch 658/ 842, training loss 0.31604552268981934\n",
      "Epoch 29 -- Batch 659/ 842, training loss 0.313747376203537\n",
      "Epoch 29 -- Batch 660/ 842, training loss 0.2994704842567444\n",
      "Epoch 29 -- Batch 661/ 842, training loss 0.31037384271621704\n",
      "Epoch 29 -- Batch 662/ 842, training loss 0.31108033657073975\n",
      "Epoch 29 -- Batch 663/ 842, training loss 0.2994093596935272\n",
      "Epoch 29 -- Batch 664/ 842, training loss 0.3161008059978485\n",
      "Epoch 29 -- Batch 665/ 842, training loss 0.3058178126811981\n",
      "Epoch 29 -- Batch 666/ 842, training loss 0.3019198477268219\n",
      "Epoch 29 -- Batch 667/ 842, training loss 0.30358046293258667\n",
      "Epoch 29 -- Batch 668/ 842, training loss 0.3101949989795685\n",
      "Epoch 29 -- Batch 669/ 842, training loss 0.3029017448425293\n",
      "Epoch 29 -- Batch 670/ 842, training loss 0.29942452907562256\n",
      "Epoch 29 -- Batch 671/ 842, training loss 0.3085691034793854\n",
      "Epoch 29 -- Batch 672/ 842, training loss 0.3063390552997589\n",
      "Epoch 29 -- Batch 673/ 842, training loss 0.3106677532196045\n",
      "Epoch 29 -- Batch 674/ 842, training loss 0.30528387427330017\n",
      "Epoch 29 -- Batch 675/ 842, training loss 0.31103864312171936\n",
      "Epoch 29 -- Batch 676/ 842, training loss 0.29942604899406433\n",
      "Epoch 29 -- Batch 677/ 842, training loss 0.3138447701931\n",
      "Epoch 29 -- Batch 678/ 842, training loss 0.30955377221107483\n",
      "Epoch 29 -- Batch 679/ 842, training loss 0.30743375420570374\n",
      "Epoch 29 -- Batch 680/ 842, training loss 0.3060590624809265\n",
      "Epoch 29 -- Batch 681/ 842, training loss 0.3174242675304413\n",
      "Epoch 29 -- Batch 682/ 842, training loss 0.30640512704849243\n",
      "Epoch 29 -- Batch 683/ 842, training loss 0.3220740854740143\n",
      "Epoch 29 -- Batch 684/ 842, training loss 0.30799368023872375\n",
      "Epoch 29 -- Batch 685/ 842, training loss 0.3066510558128357\n",
      "Epoch 29 -- Batch 686/ 842, training loss 0.3008890450000763\n",
      "Epoch 29 -- Batch 687/ 842, training loss 0.31195932626724243\n",
      "Epoch 29 -- Batch 688/ 842, training loss 0.3002925217151642\n",
      "Epoch 29 -- Batch 689/ 842, training loss 0.3053223788738251\n",
      "Epoch 29 -- Batch 690/ 842, training loss 0.31604284048080444\n",
      "Epoch 29 -- Batch 691/ 842, training loss 0.31498095393180847\n",
      "Epoch 29 -- Batch 692/ 842, training loss 0.30915528535842896\n",
      "Epoch 29 -- Batch 693/ 842, training loss 0.30657821893692017\n",
      "Epoch 29 -- Batch 694/ 842, training loss 0.3159613013267517\n",
      "Epoch 29 -- Batch 695/ 842, training loss 0.31616443395614624\n",
      "Epoch 29 -- Batch 696/ 842, training loss 0.3060891330242157\n",
      "Epoch 29 -- Batch 697/ 842, training loss 0.30686241388320923\n",
      "Epoch 29 -- Batch 698/ 842, training loss 0.32261741161346436\n",
      "Epoch 29 -- Batch 699/ 842, training loss 0.3077952265739441\n",
      "Epoch 29 -- Batch 700/ 842, training loss 0.31240057945251465\n",
      "Epoch 29 -- Batch 701/ 842, training loss 0.31614193320274353\n",
      "Epoch 29 -- Batch 702/ 842, training loss 0.310719758272171\n",
      "Epoch 29 -- Batch 703/ 842, training loss 0.29896479845046997\n",
      "Epoch 29 -- Batch 704/ 842, training loss 0.30370450019836426\n",
      "Epoch 29 -- Batch 705/ 842, training loss 0.3002735674381256\n",
      "Epoch 29 -- Batch 706/ 842, training loss 0.3080383837223053\n",
      "Epoch 29 -- Batch 707/ 842, training loss 0.3083394765853882\n",
      "Epoch 29 -- Batch 708/ 842, training loss 0.30498069524765015\n",
      "Epoch 29 -- Batch 709/ 842, training loss 0.3072297275066376\n",
      "Epoch 29 -- Batch 710/ 842, training loss 0.31296828389167786\n",
      "Epoch 29 -- Batch 711/ 842, training loss 0.30297279357910156\n",
      "Epoch 29 -- Batch 712/ 842, training loss 0.31158381700515747\n",
      "Epoch 29 -- Batch 713/ 842, training loss 0.30959343910217285\n",
      "Epoch 29 -- Batch 714/ 842, training loss 0.3090234696865082\n",
      "Epoch 29 -- Batch 715/ 842, training loss 0.3079441487789154\n",
      "Epoch 29 -- Batch 716/ 842, training loss 0.30944159626960754\n",
      "Epoch 29 -- Batch 717/ 842, training loss 0.30808213353157043\n",
      "Epoch 29 -- Batch 718/ 842, training loss 0.30570557713508606\n",
      "Epoch 29 -- Batch 719/ 842, training loss 0.3127548396587372\n",
      "Epoch 29 -- Batch 720/ 842, training loss 0.3072270452976227\n",
      "Epoch 29 -- Batch 721/ 842, training loss 0.30054348707199097\n",
      "Epoch 29 -- Batch 722/ 842, training loss 0.313081830739975\n",
      "Epoch 29 -- Batch 723/ 842, training loss 0.29703962802886963\n",
      "Epoch 29 -- Batch 724/ 842, training loss 0.31224381923675537\n",
      "Epoch 29 -- Batch 725/ 842, training loss 0.30972468852996826\n",
      "Epoch 29 -- Batch 726/ 842, training loss 0.3067820966243744\n",
      "Epoch 29 -- Batch 727/ 842, training loss 0.310821533203125\n",
      "Epoch 29 -- Batch 728/ 842, training loss 0.313642680644989\n",
      "Epoch 29 -- Batch 729/ 842, training loss 0.3140110373497009\n",
      "Epoch 29 -- Batch 730/ 842, training loss 0.30811530351638794\n",
      "Epoch 29 -- Batch 731/ 842, training loss 0.30580148100852966\n",
      "Epoch 29 -- Batch 732/ 842, training loss 0.3130721151828766\n",
      "Epoch 29 -- Batch 733/ 842, training loss 0.31015267968177795\n",
      "Epoch 29 -- Batch 734/ 842, training loss 0.3088051974773407\n",
      "Epoch 29 -- Batch 735/ 842, training loss 0.3085802495479584\n",
      "Epoch 29 -- Batch 736/ 842, training loss 0.31712591648101807\n",
      "Epoch 29 -- Batch 737/ 842, training loss 0.31031686067581177\n",
      "Epoch 29 -- Batch 738/ 842, training loss 0.3002462685108185\n",
      "Epoch 29 -- Batch 739/ 842, training loss 0.32404187321662903\n",
      "Epoch 29 -- Batch 740/ 842, training loss 0.3150806427001953\n",
      "Epoch 29 -- Batch 741/ 842, training loss 0.315192848443985\n",
      "Epoch 29 -- Batch 742/ 842, training loss 0.2904817461967468\n",
      "Epoch 29 -- Batch 743/ 842, training loss 0.29668235778808594\n",
      "Epoch 29 -- Batch 744/ 842, training loss 0.307522714138031\n",
      "Epoch 29 -- Batch 745/ 842, training loss 0.31628912687301636\n",
      "Epoch 29 -- Batch 746/ 842, training loss 0.30543252825737\n",
      "Epoch 29 -- Batch 747/ 842, training loss 0.3153793215751648\n",
      "Epoch 29 -- Batch 748/ 842, training loss 0.3078289031982422\n",
      "Epoch 29 -- Batch 749/ 842, training loss 0.31702619791030884\n",
      "Epoch 29 -- Batch 750/ 842, training loss 0.29948359727859497\n",
      "Epoch 29 -- Batch 751/ 842, training loss 0.3125585913658142\n",
      "Epoch 29 -- Batch 752/ 842, training loss 0.30928319692611694\n",
      "Epoch 29 -- Batch 753/ 842, training loss 0.3088551461696625\n",
      "Epoch 29 -- Batch 754/ 842, training loss 0.302835077047348\n",
      "Epoch 29 -- Batch 755/ 842, training loss 0.30348607897758484\n",
      "Epoch 29 -- Batch 756/ 842, training loss 0.29690757393836975\n",
      "Epoch 29 -- Batch 757/ 842, training loss 0.30336883664131165\n",
      "Epoch 29 -- Batch 758/ 842, training loss 0.30360865592956543\n",
      "Epoch 29 -- Batch 759/ 842, training loss 0.30033162236213684\n",
      "Epoch 29 -- Batch 760/ 842, training loss 0.3172745406627655\n",
      "Epoch 29 -- Batch 761/ 842, training loss 0.31537336111068726\n",
      "Epoch 29 -- Batch 762/ 842, training loss 0.3143884539604187\n",
      "Epoch 29 -- Batch 763/ 842, training loss 0.30946967005729675\n",
      "Epoch 29 -- Batch 764/ 842, training loss 0.3028092384338379\n",
      "Epoch 29 -- Batch 765/ 842, training loss 0.29693230986595154\n",
      "Epoch 29 -- Batch 766/ 842, training loss 0.3057665228843689\n",
      "Epoch 29 -- Batch 767/ 842, training loss 0.2998584508895874\n",
      "Epoch 29 -- Batch 768/ 842, training loss 0.307544469833374\n",
      "Epoch 29 -- Batch 769/ 842, training loss 0.3085610568523407\n",
      "Epoch 29 -- Batch 770/ 842, training loss 0.30890899896621704\n",
      "Epoch 29 -- Batch 771/ 842, training loss 0.3099178373813629\n",
      "Epoch 29 -- Batch 772/ 842, training loss 0.29946890473365784\n",
      "Epoch 29 -- Batch 773/ 842, training loss 0.3161962926387787\n",
      "Epoch 29 -- Batch 774/ 842, training loss 0.297329306602478\n",
      "Epoch 29 -- Batch 775/ 842, training loss 0.3064619302749634\n",
      "Epoch 29 -- Batch 776/ 842, training loss 0.3003612756729126\n",
      "Epoch 29 -- Batch 777/ 842, training loss 0.2973446846008301\n",
      "Epoch 29 -- Batch 778/ 842, training loss 0.30276477336883545\n",
      "Epoch 29 -- Batch 779/ 842, training loss 0.29754725098609924\n",
      "Epoch 29 -- Batch 780/ 842, training loss 0.3262532651424408\n",
      "Epoch 29 -- Batch 781/ 842, training loss 0.299528032541275\n",
      "Epoch 29 -- Batch 782/ 842, training loss 0.3121402859687805\n",
      "Epoch 29 -- Batch 783/ 842, training loss 0.2960563600063324\n",
      "Epoch 29 -- Batch 784/ 842, training loss 0.3058057725429535\n",
      "Epoch 29 -- Batch 785/ 842, training loss 0.314704030752182\n",
      "Epoch 29 -- Batch 786/ 842, training loss 0.31252434849739075\n",
      "Epoch 29 -- Batch 787/ 842, training loss 0.30571305751800537\n",
      "Epoch 29 -- Batch 788/ 842, training loss 0.31044772267341614\n",
      "Epoch 29 -- Batch 789/ 842, training loss 0.3020331859588623\n",
      "Epoch 29 -- Batch 790/ 842, training loss 0.31030377745628357\n",
      "Epoch 29 -- Batch 791/ 842, training loss 0.29454100131988525\n",
      "Epoch 29 -- Batch 792/ 842, training loss 0.31153276562690735\n",
      "Epoch 29 -- Batch 793/ 842, training loss 0.3048422336578369\n",
      "Epoch 29 -- Batch 794/ 842, training loss 0.2941470444202423\n",
      "Epoch 29 -- Batch 795/ 842, training loss 0.3098175525665283\n",
      "Epoch 29 -- Batch 796/ 842, training loss 0.3183346092700958\n",
      "Epoch 29 -- Batch 797/ 842, training loss 0.3083251416683197\n",
      "Epoch 29 -- Batch 798/ 842, training loss 0.31293541193008423\n",
      "Epoch 29 -- Batch 799/ 842, training loss 0.2981407940387726\n",
      "Epoch 29 -- Batch 800/ 842, training loss 0.30986374616622925\n",
      "Epoch 29 -- Batch 801/ 842, training loss 0.31003111600875854\n",
      "Epoch 29 -- Batch 802/ 842, training loss 0.316413551568985\n",
      "Epoch 29 -- Batch 803/ 842, training loss 0.31111255288124084\n",
      "Epoch 29 -- Batch 804/ 842, training loss 0.3015521466732025\n",
      "Epoch 29 -- Batch 805/ 842, training loss 0.31175923347473145\n",
      "Epoch 29 -- Batch 806/ 842, training loss 0.3141725957393646\n",
      "Epoch 29 -- Batch 807/ 842, training loss 0.3036479651927948\n",
      "Epoch 29 -- Batch 808/ 842, training loss 0.3096346855163574\n",
      "Epoch 29 -- Batch 809/ 842, training loss 0.3118128776550293\n",
      "Epoch 29 -- Batch 810/ 842, training loss 0.31079667806625366\n",
      "Epoch 29 -- Batch 811/ 842, training loss 0.3106197118759155\n",
      "Epoch 29 -- Batch 812/ 842, training loss 0.3256448805332184\n",
      "Epoch 29 -- Batch 813/ 842, training loss 0.3122735917568207\n",
      "Epoch 29 -- Batch 814/ 842, training loss 0.31148210167884827\n",
      "Epoch 29 -- Batch 815/ 842, training loss 0.31829699873924255\n",
      "Epoch 29 -- Batch 816/ 842, training loss 0.3026277422904968\n",
      "Epoch 29 -- Batch 817/ 842, training loss 0.3113859295845032\n",
      "Epoch 29 -- Batch 818/ 842, training loss 0.3072197139263153\n",
      "Epoch 29 -- Batch 819/ 842, training loss 0.2947904169559479\n",
      "Epoch 29 -- Batch 820/ 842, training loss 0.3039262890815735\n",
      "Epoch 29 -- Batch 821/ 842, training loss 0.3080889880657196\n",
      "Epoch 29 -- Batch 822/ 842, training loss 0.30684593319892883\n",
      "Epoch 29 -- Batch 823/ 842, training loss 0.30354899168014526\n",
      "Epoch 29 -- Batch 824/ 842, training loss 0.303526371717453\n",
      "Epoch 29 -- Batch 825/ 842, training loss 0.30786609649658203\n",
      "Epoch 29 -- Batch 826/ 842, training loss 0.3149324655532837\n",
      "Epoch 29 -- Batch 827/ 842, training loss 0.30779311060905457\n",
      "Epoch 29 -- Batch 828/ 842, training loss 0.3148936331272125\n",
      "Epoch 29 -- Batch 829/ 842, training loss 0.30658069252967834\n",
      "Epoch 29 -- Batch 830/ 842, training loss 0.3251693844795227\n",
      "Epoch 29 -- Batch 831/ 842, training loss 0.30478209257125854\n",
      "Epoch 29 -- Batch 832/ 842, training loss 0.30835413932800293\n",
      "Epoch 29 -- Batch 833/ 842, training loss 0.3016517758369446\n",
      "Epoch 29 -- Batch 834/ 842, training loss 0.3163076639175415\n",
      "Epoch 29 -- Batch 835/ 842, training loss 0.3141857981681824\n",
      "Epoch 29 -- Batch 836/ 842, training loss 0.3068392872810364\n",
      "Epoch 29 -- Batch 837/ 842, training loss 0.2997692823410034\n",
      "Epoch 29 -- Batch 838/ 842, training loss 0.31021422147750854\n",
      "Epoch 29 -- Batch 839/ 842, training loss 0.31248387694358826\n",
      "Epoch 29 -- Batch 840/ 842, training loss 0.31085020303726196\n",
      "Epoch 29 -- Batch 841/ 842, training loss 0.3160996437072754\n",
      "Epoch 29 -- Batch 842/ 842, training loss 0.31918835639953613\n",
      "----------------------------------------------------------------------\n",
      "Epoch 29 -- Batch 1/ 94, validation loss 0.30024832487106323\n",
      "Epoch 29 -- Batch 2/ 94, validation loss 0.30873918533325195\n",
      "Epoch 29 -- Batch 3/ 94, validation loss 0.30450889468193054\n",
      "Epoch 29 -- Batch 4/ 94, validation loss 0.290749728679657\n",
      "Epoch 29 -- Batch 5/ 94, validation loss 0.29658013582229614\n",
      "Epoch 29 -- Batch 6/ 94, validation loss 0.3000977337360382\n",
      "Epoch 29 -- Batch 7/ 94, validation loss 0.289191871881485\n",
      "Epoch 29 -- Batch 8/ 94, validation loss 0.2938527464866638\n",
      "Epoch 29 -- Batch 9/ 94, validation loss 0.30018332600593567\n",
      "Epoch 29 -- Batch 10/ 94, validation loss 0.297303169965744\n",
      "Epoch 29 -- Batch 11/ 94, validation loss 0.30268213152885437\n",
      "Epoch 29 -- Batch 12/ 94, validation loss 0.29206395149230957\n",
      "Epoch 29 -- Batch 13/ 94, validation loss 0.2895668148994446\n",
      "Epoch 29 -- Batch 14/ 94, validation loss 0.30158039927482605\n",
      "Epoch 29 -- Batch 15/ 94, validation loss 0.3044261634349823\n",
      "Epoch 29 -- Batch 16/ 94, validation loss 0.28537172079086304\n",
      "Epoch 29 -- Batch 17/ 94, validation loss 0.3080052435398102\n",
      "Epoch 29 -- Batch 18/ 94, validation loss 0.3000184893608093\n",
      "Epoch 29 -- Batch 19/ 94, validation loss 0.28100788593292236\n",
      "Epoch 29 -- Batch 20/ 94, validation loss 0.3066807687282562\n",
      "Epoch 29 -- Batch 21/ 94, validation loss 0.2969699203968048\n",
      "Epoch 29 -- Batch 22/ 94, validation loss 0.3096614181995392\n",
      "Epoch 29 -- Batch 23/ 94, validation loss 0.2981267273426056\n",
      "Epoch 29 -- Batch 24/ 94, validation loss 0.3104979693889618\n",
      "Epoch 29 -- Batch 25/ 94, validation loss 0.29631903767585754\n",
      "Epoch 29 -- Batch 26/ 94, validation loss 0.29977500438690186\n",
      "Epoch 29 -- Batch 27/ 94, validation loss 0.28593936562538147\n",
      "Epoch 29 -- Batch 28/ 94, validation loss 0.30187562108039856\n",
      "Epoch 29 -- Batch 29/ 94, validation loss 0.3035486936569214\n",
      "Epoch 29 -- Batch 30/ 94, validation loss 0.3094284236431122\n",
      "Epoch 29 -- Batch 31/ 94, validation loss 0.2992609441280365\n",
      "Epoch 29 -- Batch 32/ 94, validation loss 0.2889384329319\n",
      "Epoch 29 -- Batch 33/ 94, validation loss 0.2950870990753174\n",
      "Epoch 29 -- Batch 34/ 94, validation loss 0.29904159903526306\n",
      "Epoch 29 -- Batch 35/ 94, validation loss 0.3059198260307312\n",
      "Epoch 29 -- Batch 36/ 94, validation loss 0.2814347445964813\n",
      "Epoch 29 -- Batch 37/ 94, validation loss 0.30055925250053406\n",
      "Epoch 29 -- Batch 38/ 94, validation loss 0.29565760493278503\n",
      "Epoch 29 -- Batch 39/ 94, validation loss 0.293781578540802\n",
      "Epoch 29 -- Batch 40/ 94, validation loss 0.3008372485637665\n",
      "Epoch 29 -- Batch 41/ 94, validation loss 0.28778180480003357\n",
      "Epoch 29 -- Batch 42/ 94, validation loss 0.2915794849395752\n",
      "Epoch 29 -- Batch 43/ 94, validation loss 0.29691365361213684\n",
      "Epoch 29 -- Batch 44/ 94, validation loss 0.30069565773010254\n",
      "Epoch 29 -- Batch 45/ 94, validation loss 0.34120067954063416\n",
      "Epoch 29 -- Batch 46/ 94, validation loss 0.30128708481788635\n",
      "Epoch 29 -- Batch 47/ 94, validation loss 0.3053445518016815\n",
      "Epoch 29 -- Batch 48/ 94, validation loss 0.29319441318511963\n",
      "Epoch 29 -- Batch 49/ 94, validation loss 0.28589674830436707\n",
      "Epoch 29 -- Batch 50/ 94, validation loss 0.30543088912963867\n",
      "Epoch 29 -- Batch 51/ 94, validation loss 0.2993195652961731\n",
      "Epoch 29 -- Batch 52/ 94, validation loss 0.2917550802230835\n",
      "Epoch 29 -- Batch 53/ 94, validation loss 0.2897166907787323\n",
      "Epoch 29 -- Batch 54/ 94, validation loss 0.3057081401348114\n",
      "Epoch 29 -- Batch 55/ 94, validation loss 0.29551470279693604\n",
      "Epoch 29 -- Batch 56/ 94, validation loss 0.2958105504512787\n",
      "Epoch 29 -- Batch 57/ 94, validation loss 0.31922534108161926\n",
      "Epoch 29 -- Batch 58/ 94, validation loss 0.2907363772392273\n",
      "Epoch 29 -- Batch 59/ 94, validation loss 0.28964367508888245\n",
      "Epoch 29 -- Batch 60/ 94, validation loss 0.298134446144104\n",
      "Epoch 29 -- Batch 61/ 94, validation loss 0.2871840298175812\n",
      "Epoch 29 -- Batch 62/ 94, validation loss 0.2859216332435608\n",
      "Epoch 29 -- Batch 63/ 94, validation loss 0.29136762022972107\n",
      "Epoch 29 -- Batch 64/ 94, validation loss 0.2932654917240143\n",
      "Epoch 29 -- Batch 65/ 94, validation loss 0.30279722809791565\n",
      "Epoch 29 -- Batch 66/ 94, validation loss 0.3028682470321655\n",
      "Epoch 29 -- Batch 67/ 94, validation loss 0.29835745692253113\n",
      "Epoch 29 -- Batch 68/ 94, validation loss 0.2926662564277649\n",
      "Epoch 29 -- Batch 69/ 94, validation loss 0.2856716513633728\n",
      "Epoch 29 -- Batch 70/ 94, validation loss 0.294636070728302\n",
      "Epoch 29 -- Batch 71/ 94, validation loss 0.2892911732196808\n",
      "Epoch 29 -- Batch 72/ 94, validation loss 0.2992469072341919\n",
      "Epoch 29 -- Batch 73/ 94, validation loss 0.2924073338508606\n",
      "Epoch 29 -- Batch 74/ 94, validation loss 0.29903557896614075\n",
      "Epoch 29 -- Batch 75/ 94, validation loss 0.2973451018333435\n",
      "Epoch 29 -- Batch 76/ 94, validation loss 0.2912290692329407\n",
      "Epoch 29 -- Batch 77/ 94, validation loss 0.28468263149261475\n",
      "Epoch 29 -- Batch 78/ 94, validation loss 0.30077826976776123\n",
      "Epoch 29 -- Batch 79/ 94, validation loss 0.2960919439792633\n",
      "Epoch 29 -- Batch 80/ 94, validation loss 0.292123407125473\n",
      "Epoch 29 -- Batch 81/ 94, validation loss 0.30213773250579834\n",
      "Epoch 29 -- Batch 82/ 94, validation loss 0.3290756344795227\n",
      "Epoch 29 -- Batch 83/ 94, validation loss 0.290472149848938\n",
      "Epoch 29 -- Batch 84/ 94, validation loss 0.3055346608161926\n",
      "Epoch 29 -- Batch 85/ 94, validation loss 0.28872063755989075\n",
      "Epoch 29 -- Batch 86/ 94, validation loss 0.2933686077594757\n",
      "Epoch 29 -- Batch 87/ 94, validation loss 0.3003111183643341\n",
      "Epoch 29 -- Batch 88/ 94, validation loss 0.2968587875366211\n",
      "Epoch 29 -- Batch 89/ 94, validation loss 0.30929240584373474\n",
      "Epoch 29 -- Batch 90/ 94, validation loss 0.29907140135765076\n",
      "Epoch 29 -- Batch 91/ 94, validation loss 0.3037641644477844\n",
      "Epoch 29 -- Batch 92/ 94, validation loss 0.28680503368377686\n",
      "Epoch 29 -- Batch 93/ 94, validation loss 0.3122406005859375\n",
      "Epoch 29 -- Batch 94/ 94, validation loss 0.28031620383262634\n",
      "----------------------------------------------------------------------\n",
      "Epoch 29 loss: Training 0.30648452043533325, Validation 0.28031620383262634\n",
      "----------------------------------------------------------------------\n",
      "Epoch 30/30, training: 842 and validation:94 batches, size 128*1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:08:43] Explicit valence for atom # 11 C, 5, is greater than permitted\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'O=C(O)C1C2CC3C1C(=O)Oc1ccccc1C1c3sc(=O)[nH]c2SC31'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CCN(CC)CC(=O)N1CCc2c(sc3ccccc23)C12CC1'\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)sc1nc(C(C)=O)c(=O)n2C'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 8 9 10 11 12 13 14\n",
      "[19:08:43] SMILES Parse Error: ring closure 2 duplicates bond between atom 14 and atom 15 for input: 'Cc1ccc(CN(C(=O)c2csnn2)C2C2CCCCC2)c(C)c1'\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(C)oc2ccc(N(C(C)=O)S(=O)(=O)c3ccc4c5c(ccc3c3)CCC4)cc12'\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CN1C(C(=O)O)C2C3C=CC(C4)C2C1=O'\n",
      "[19:08:43] SMILES Parse Error: ring closure 4 duplicates bond between atom 15 and atom 16 for input: 'CC1CCCN(C2CCN(C(=O)C[C@H]3C[C@@H]4c4ccccc4C3=O)CC2)C1'\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CC[C@@]1(C(=O)OC)N[C@H](CN(C)C(=O)C2CCCCC2)[C@H](C(=O)O)3C1=O'\n",
      "[19:08:43] Explicit valence for atom # 12 O, 3, is greater than permitted\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 7 8 16 17 30\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@@H]1C[C@H]1[C@@H](NC(=O)c1cccc3ccccc12)CS2'\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'O=C1Cc2nc(SCc3ccccc3Cl)n(Cc3ccccc3)c2C2'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 6 7 9\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 7 8 16 17 18 20 22 23 24\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 13 14 15 16 17 18\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 13 14 16 17\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 9 20\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 21\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[19:08:43] non-ring atom 6 marked aromatic\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 20\n",
      "[19:08:43] SMILES Parse Error: extra open parentheses for input: 'COC(=O)[C@@H](NC(=O)c1ccc(OCC)cc1)C(=O)N(CCC'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 14 15 16\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CCCOC(=O)[C@@H]1C[C@H]1[C@@H](OC)C(=O)N(C)C12CCN(C(=O)Nc1ccc(C(F)(F)F)cc1)CC2'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 15 17 18\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)C1(N(CCc3ccccc3)CC(=O)NCc3cccnc3)CCCC1'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7 9 10\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 1 2 12\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CSc1cc(/C=C2/C=C/c3cccc(C(C)=O)c3)oc2ccccc12'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 20\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'O=c1c2cc(Cl)ccc2nc2n1CC/C=C/c1ccccc1'\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 3 4 6 8 9\n",
      "[19:08:43] Explicit valence for atom # 3 C, 5, is greater than permitted\n",
      "[19:08:43] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 22 24\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'OC(Cc1ccccn1)CN(Cc1cc2ccc(Cc3ccccc3[nH]c3)cc1)C2'\n",
      "[19:08:43] SMILES Parse Error: unclosed ring for input: 'CCCN(Cc1c(-c2ccccc2)nn(C)c1Oc1ccc(F)cc1)CCC2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 -- Batch 1/ 842, training loss 0.2970809042453766\n",
      "Epoch 30 -- Batch 2/ 842, training loss 0.3127555847167969\n",
      "Epoch 30 -- Batch 3/ 842, training loss 0.29363590478897095\n",
      "Epoch 30 -- Batch 4/ 842, training loss 0.301332950592041\n",
      "Epoch 30 -- Batch 5/ 842, training loss 0.3112838566303253\n",
      "Epoch 30 -- Batch 6/ 842, training loss 0.2928890287876129\n",
      "Epoch 30 -- Batch 7/ 842, training loss 0.3063929080963135\n",
      "Epoch 30 -- Batch 8/ 842, training loss 0.3075919449329376\n",
      "Epoch 30 -- Batch 9/ 842, training loss 0.2963351905345917\n",
      "Epoch 30 -- Batch 10/ 842, training loss 0.29250746965408325\n",
      "Epoch 30 -- Batch 11/ 842, training loss 0.29490116238594055\n",
      "Epoch 30 -- Batch 12/ 842, training loss 0.3009416162967682\n",
      "Epoch 30 -- Batch 13/ 842, training loss 0.30367615818977356\n",
      "Epoch 30 -- Batch 14/ 842, training loss 0.3042241632938385\n",
      "Epoch 30 -- Batch 15/ 842, training loss 0.3034258186817169\n",
      "Epoch 30 -- Batch 16/ 842, training loss 0.3045646548271179\n",
      "Epoch 30 -- Batch 17/ 842, training loss 0.29868894815444946\n",
      "Epoch 30 -- Batch 18/ 842, training loss 0.29794833064079285\n",
      "Epoch 30 -- Batch 19/ 842, training loss 0.2980339825153351\n",
      "Epoch 30 -- Batch 20/ 842, training loss 0.2928113639354706\n",
      "Epoch 30 -- Batch 21/ 842, training loss 0.3104063868522644\n",
      "Epoch 30 -- Batch 22/ 842, training loss 0.3028123676776886\n",
      "Epoch 30 -- Batch 23/ 842, training loss 0.31135427951812744\n",
      "Epoch 30 -- Batch 24/ 842, training loss 0.2917431890964508\n",
      "Epoch 30 -- Batch 25/ 842, training loss 0.30857470631599426\n",
      "Epoch 30 -- Batch 26/ 842, training loss 0.312136709690094\n",
      "Epoch 30 -- Batch 27/ 842, training loss 0.29494142532348633\n",
      "Epoch 30 -- Batch 28/ 842, training loss 0.3067838251590729\n",
      "Epoch 30 -- Batch 29/ 842, training loss 0.3098735809326172\n",
      "Epoch 30 -- Batch 30/ 842, training loss 0.2970730662345886\n",
      "Epoch 30 -- Batch 31/ 842, training loss 0.31006941199302673\n",
      "Epoch 30 -- Batch 32/ 842, training loss 0.29754558205604553\n",
      "Epoch 30 -- Batch 33/ 842, training loss 0.2991766631603241\n",
      "Epoch 30 -- Batch 34/ 842, training loss 0.30667343735694885\n",
      "Epoch 30 -- Batch 35/ 842, training loss 0.30299055576324463\n",
      "Epoch 30 -- Batch 36/ 842, training loss 0.30424532294273376\n",
      "Epoch 30 -- Batch 37/ 842, training loss 0.2971577048301697\n",
      "Epoch 30 -- Batch 38/ 842, training loss 0.29939165711402893\n",
      "Epoch 30 -- Batch 39/ 842, training loss 0.2980556786060333\n",
      "Epoch 30 -- Batch 40/ 842, training loss 0.2915097773075104\n",
      "Epoch 30 -- Batch 41/ 842, training loss 0.30902615189552307\n",
      "Epoch 30 -- Batch 42/ 842, training loss 0.3025798201560974\n",
      "Epoch 30 -- Batch 43/ 842, training loss 0.30452924966812134\n",
      "Epoch 30 -- Batch 44/ 842, training loss 0.29326432943344116\n",
      "Epoch 30 -- Batch 45/ 842, training loss 0.29883331060409546\n",
      "Epoch 30 -- Batch 46/ 842, training loss 0.3011154532432556\n",
      "Epoch 30 -- Batch 47/ 842, training loss 0.31113889813423157\n",
      "Epoch 30 -- Batch 48/ 842, training loss 0.3050020635128021\n",
      "Epoch 30 -- Batch 49/ 842, training loss 0.2947026491165161\n",
      "Epoch 30 -- Batch 50/ 842, training loss 0.29876458644866943\n",
      "Epoch 30 -- Batch 51/ 842, training loss 0.3070484399795532\n",
      "Epoch 30 -- Batch 52/ 842, training loss 0.30785471200942993\n",
      "Epoch 30 -- Batch 53/ 842, training loss 0.30317404866218567\n",
      "Epoch 30 -- Batch 54/ 842, training loss 0.29793182015419006\n",
      "Epoch 30 -- Batch 55/ 842, training loss 0.29590240120887756\n",
      "Epoch 30 -- Batch 56/ 842, training loss 0.30965209007263184\n",
      "Epoch 30 -- Batch 57/ 842, training loss 0.312420129776001\n",
      "Epoch 30 -- Batch 58/ 842, training loss 0.29679811000823975\n",
      "Epoch 30 -- Batch 59/ 842, training loss 0.29510384798049927\n",
      "Epoch 30 -- Batch 60/ 842, training loss 0.29842641949653625\n",
      "Epoch 30 -- Batch 61/ 842, training loss 0.3035210072994232\n",
      "Epoch 30 -- Batch 62/ 842, training loss 0.30569392442703247\n",
      "Epoch 30 -- Batch 63/ 842, training loss 0.30862027406692505\n",
      "Epoch 30 -- Batch 64/ 842, training loss 0.3078901171684265\n",
      "Epoch 30 -- Batch 65/ 842, training loss 0.3070999085903168\n",
      "Epoch 30 -- Batch 66/ 842, training loss 0.31136465072631836\n",
      "Epoch 30 -- Batch 67/ 842, training loss 0.2978081703186035\n",
      "Epoch 30 -- Batch 68/ 842, training loss 0.3022785186767578\n",
      "Epoch 30 -- Batch 69/ 842, training loss 0.29755857586860657\n",
      "Epoch 30 -- Batch 70/ 842, training loss 0.2943432331085205\n",
      "Epoch 30 -- Batch 71/ 842, training loss 0.29416945576667786\n",
      "Epoch 30 -- Batch 72/ 842, training loss 0.29526984691619873\n",
      "Epoch 30 -- Batch 73/ 842, training loss 0.3054704964160919\n",
      "Epoch 30 -- Batch 74/ 842, training loss 0.30307242274284363\n",
      "Epoch 30 -- Batch 75/ 842, training loss 0.3154539465904236\n",
      "Epoch 30 -- Batch 76/ 842, training loss 0.30257630348205566\n",
      "Epoch 30 -- Batch 77/ 842, training loss 0.2903309464454651\n",
      "Epoch 30 -- Batch 78/ 842, training loss 0.2913673222064972\n",
      "Epoch 30 -- Batch 79/ 842, training loss 0.2992033362388611\n",
      "Epoch 30 -- Batch 80/ 842, training loss 0.3008190393447876\n",
      "Epoch 30 -- Batch 81/ 842, training loss 0.29951709508895874\n",
      "Epoch 30 -- Batch 82/ 842, training loss 0.29878634214401245\n",
      "Epoch 30 -- Batch 83/ 842, training loss 0.30471354722976685\n",
      "Epoch 30 -- Batch 84/ 842, training loss 0.29222774505615234\n",
      "Epoch 30 -- Batch 85/ 842, training loss 0.28784334659576416\n",
      "Epoch 30 -- Batch 86/ 842, training loss 0.2945498526096344\n",
      "Epoch 30 -- Batch 87/ 842, training loss 0.3019143044948578\n",
      "Epoch 30 -- Batch 88/ 842, training loss 0.29958081245422363\n",
      "Epoch 30 -- Batch 89/ 842, training loss 0.29577282071113586\n",
      "Epoch 30 -- Batch 90/ 842, training loss 0.2965749502182007\n",
      "Epoch 30 -- Batch 91/ 842, training loss 0.3064628541469574\n",
      "Epoch 30 -- Batch 92/ 842, training loss 0.29172754287719727\n",
      "Epoch 30 -- Batch 93/ 842, training loss 0.2947317957878113\n",
      "Epoch 30 -- Batch 94/ 842, training loss 0.2934131920337677\n",
      "Epoch 30 -- Batch 95/ 842, training loss 0.3129037916660309\n",
      "Epoch 30 -- Batch 96/ 842, training loss 0.29953616857528687\n",
      "Epoch 30 -- Batch 97/ 842, training loss 0.30843427777290344\n",
      "Epoch 30 -- Batch 98/ 842, training loss 0.2838844060897827\n",
      "Epoch 30 -- Batch 99/ 842, training loss 0.30935296416282654\n",
      "Epoch 30 -- Batch 100/ 842, training loss 0.30820199847221375\n",
      "Epoch 30 -- Batch 101/ 842, training loss 0.2941528856754303\n",
      "Epoch 30 -- Batch 102/ 842, training loss 0.29062169790267944\n",
      "Epoch 30 -- Batch 103/ 842, training loss 0.30143606662750244\n",
      "Epoch 30 -- Batch 104/ 842, training loss 0.29279711842536926\n",
      "Epoch 30 -- Batch 105/ 842, training loss 0.2863580286502838\n",
      "Epoch 30 -- Batch 106/ 842, training loss 0.3011211156845093\n",
      "Epoch 30 -- Batch 107/ 842, training loss 0.3024531602859497\n",
      "Epoch 30 -- Batch 108/ 842, training loss 0.30541589856147766\n",
      "Epoch 30 -- Batch 109/ 842, training loss 0.294057697057724\n",
      "Epoch 30 -- Batch 110/ 842, training loss 0.3010304570198059\n",
      "Epoch 30 -- Batch 111/ 842, training loss 0.318779319524765\n",
      "Epoch 30 -- Batch 112/ 842, training loss 0.30023467540740967\n",
      "Epoch 30 -- Batch 113/ 842, training loss 0.3097122013568878\n",
      "Epoch 30 -- Batch 114/ 842, training loss 0.3023652732372284\n",
      "Epoch 30 -- Batch 115/ 842, training loss 0.2993740141391754\n",
      "Epoch 30 -- Batch 116/ 842, training loss 0.30403661727905273\n",
      "Epoch 30 -- Batch 117/ 842, training loss 0.2891312539577484\n",
      "Epoch 30 -- Batch 118/ 842, training loss 0.3169390559196472\n",
      "Epoch 30 -- Batch 119/ 842, training loss 0.30213919281959534\n",
      "Epoch 30 -- Batch 120/ 842, training loss 0.308594286441803\n",
      "Epoch 30 -- Batch 121/ 842, training loss 0.2994709610939026\n",
      "Epoch 30 -- Batch 122/ 842, training loss 0.30627721548080444\n",
      "Epoch 30 -- Batch 123/ 842, training loss 0.3188445568084717\n",
      "Epoch 30 -- Batch 124/ 842, training loss 0.3076104521751404\n",
      "Epoch 30 -- Batch 125/ 842, training loss 0.2996569871902466\n",
      "Epoch 30 -- Batch 126/ 842, training loss 0.2972896099090576\n",
      "Epoch 30 -- Batch 127/ 842, training loss 0.2926434874534607\n",
      "Epoch 30 -- Batch 128/ 842, training loss 0.3105390965938568\n",
      "Epoch 30 -- Batch 129/ 842, training loss 0.30063050985336304\n",
      "Epoch 30 -- Batch 130/ 842, training loss 0.30097144842147827\n",
      "Epoch 30 -- Batch 131/ 842, training loss 0.3024231493473053\n",
      "Epoch 30 -- Batch 132/ 842, training loss 0.31117403507232666\n",
      "Epoch 30 -- Batch 133/ 842, training loss 0.2957369089126587\n",
      "Epoch 30 -- Batch 134/ 842, training loss 0.30270659923553467\n",
      "Epoch 30 -- Batch 135/ 842, training loss 0.28611037135124207\n",
      "Epoch 30 -- Batch 136/ 842, training loss 0.29111093282699585\n",
      "Epoch 30 -- Batch 137/ 842, training loss 0.30160558223724365\n",
      "Epoch 30 -- Batch 138/ 842, training loss 0.30392980575561523\n",
      "Epoch 30 -- Batch 139/ 842, training loss 0.3127974271774292\n",
      "Epoch 30 -- Batch 140/ 842, training loss 0.29970189929008484\n",
      "Epoch 30 -- Batch 141/ 842, training loss 0.30946844816207886\n",
      "Epoch 30 -- Batch 142/ 842, training loss 0.30915743112564087\n",
      "Epoch 30 -- Batch 143/ 842, training loss 0.3049885928630829\n",
      "Epoch 30 -- Batch 144/ 842, training loss 0.29733243584632874\n",
      "Epoch 30 -- Batch 145/ 842, training loss 0.3006731867790222\n",
      "Epoch 30 -- Batch 146/ 842, training loss 0.30492091178894043\n",
      "Epoch 30 -- Batch 147/ 842, training loss 0.3012707531452179\n",
      "Epoch 30 -- Batch 148/ 842, training loss 0.2983340322971344\n",
      "Epoch 30 -- Batch 149/ 842, training loss 0.3003658354282379\n",
      "Epoch 30 -- Batch 150/ 842, training loss 0.2985724210739136\n",
      "Epoch 30 -- Batch 151/ 842, training loss 0.29837465286254883\n",
      "Epoch 30 -- Batch 152/ 842, training loss 0.30168381333351135\n",
      "Epoch 30 -- Batch 153/ 842, training loss 0.3065934181213379\n",
      "Epoch 30 -- Batch 154/ 842, training loss 0.30361804366111755\n",
      "Epoch 30 -- Batch 155/ 842, training loss 0.30687275528907776\n",
      "Epoch 30 -- Batch 156/ 842, training loss 0.30784571170806885\n",
      "Epoch 30 -- Batch 157/ 842, training loss 0.30178752541542053\n",
      "Epoch 30 -- Batch 158/ 842, training loss 0.29863059520721436\n",
      "Epoch 30 -- Batch 159/ 842, training loss 0.2963097393512726\n",
      "Epoch 30 -- Batch 160/ 842, training loss 0.304787278175354\n",
      "Epoch 30 -- Batch 161/ 842, training loss 0.3092581629753113\n",
      "Epoch 30 -- Batch 162/ 842, training loss 0.3026358485221863\n",
      "Epoch 30 -- Batch 163/ 842, training loss 0.303191602230072\n",
      "Epoch 30 -- Batch 164/ 842, training loss 0.3102321922779083\n",
      "Epoch 30 -- Batch 165/ 842, training loss 0.3017291724681854\n",
      "Epoch 30 -- Batch 166/ 842, training loss 0.3103584349155426\n",
      "Epoch 30 -- Batch 167/ 842, training loss 0.3025784492492676\n",
      "Epoch 30 -- Batch 168/ 842, training loss 0.2945452928543091\n",
      "Epoch 30 -- Batch 169/ 842, training loss 0.31523558497428894\n",
      "Epoch 30 -- Batch 170/ 842, training loss 0.29847487807273865\n",
      "Epoch 30 -- Batch 171/ 842, training loss 0.30280378460884094\n",
      "Epoch 30 -- Batch 172/ 842, training loss 0.2967112362384796\n",
      "Epoch 30 -- Batch 173/ 842, training loss 0.31280890107154846\n",
      "Epoch 30 -- Batch 174/ 842, training loss 0.28984999656677246\n",
      "Epoch 30 -- Batch 175/ 842, training loss 0.30050668120384216\n",
      "Epoch 30 -- Batch 176/ 842, training loss 0.29748764634132385\n",
      "Epoch 30 -- Batch 177/ 842, training loss 0.29812076687812805\n",
      "Epoch 30 -- Batch 178/ 842, training loss 0.3002485930919647\n",
      "Epoch 30 -- Batch 179/ 842, training loss 0.2949942350387573\n",
      "Epoch 30 -- Batch 180/ 842, training loss 0.2955736219882965\n",
      "Epoch 30 -- Batch 181/ 842, training loss 0.3063879907131195\n",
      "Epoch 30 -- Batch 182/ 842, training loss 0.3101903200149536\n",
      "Epoch 30 -- Batch 183/ 842, training loss 0.30581116676330566\n",
      "Epoch 30 -- Batch 184/ 842, training loss 0.30766963958740234\n",
      "Epoch 30 -- Batch 185/ 842, training loss 0.2939891815185547\n",
      "Epoch 30 -- Batch 186/ 842, training loss 0.3036622703075409\n",
      "Epoch 30 -- Batch 187/ 842, training loss 0.30153754353523254\n",
      "Epoch 30 -- Batch 188/ 842, training loss 0.2917207181453705\n",
      "Epoch 30 -- Batch 189/ 842, training loss 0.2931655943393707\n",
      "Epoch 30 -- Batch 190/ 842, training loss 0.29895710945129395\n",
      "Epoch 30 -- Batch 191/ 842, training loss 0.29320260882377625\n",
      "Epoch 30 -- Batch 192/ 842, training loss 0.2988630533218384\n",
      "Epoch 30 -- Batch 193/ 842, training loss 0.30540764331817627\n",
      "Epoch 30 -- Batch 194/ 842, training loss 0.3069896996021271\n",
      "Epoch 30 -- Batch 195/ 842, training loss 0.30932068824768066\n",
      "Epoch 30 -- Batch 196/ 842, training loss 0.29213207960128784\n",
      "Epoch 30 -- Batch 197/ 842, training loss 0.3172277510166168\n",
      "Epoch 30 -- Batch 198/ 842, training loss 0.3097456991672516\n",
      "Epoch 30 -- Batch 199/ 842, training loss 0.302191823720932\n",
      "Epoch 30 -- Batch 200/ 842, training loss 0.3013609051704407\n",
      "Epoch 30 -- Batch 201/ 842, training loss 0.30605000257492065\n",
      "Epoch 30 -- Batch 202/ 842, training loss 0.30689334869384766\n",
      "Epoch 30 -- Batch 203/ 842, training loss 0.29530778527259827\n",
      "Epoch 30 -- Batch 204/ 842, training loss 0.2900204062461853\n",
      "Epoch 30 -- Batch 205/ 842, training loss 0.2982623279094696\n",
      "Epoch 30 -- Batch 206/ 842, training loss 0.304046094417572\n",
      "Epoch 30 -- Batch 207/ 842, training loss 0.29762837290763855\n",
      "Epoch 30 -- Batch 208/ 842, training loss 0.3055047392845154\n",
      "Epoch 30 -- Batch 209/ 842, training loss 0.30703285336494446\n",
      "Epoch 30 -- Batch 210/ 842, training loss 0.3027085065841675\n",
      "Epoch 30 -- Batch 211/ 842, training loss 0.3095167279243469\n",
      "Epoch 30 -- Batch 212/ 842, training loss 0.3101884424686432\n",
      "Epoch 30 -- Batch 213/ 842, training loss 0.2896307408809662\n",
      "Epoch 30 -- Batch 214/ 842, training loss 0.2948485314846039\n",
      "Epoch 30 -- Batch 215/ 842, training loss 0.30075693130493164\n",
      "Epoch 30 -- Batch 216/ 842, training loss 0.2999781370162964\n",
      "Epoch 30 -- Batch 217/ 842, training loss 0.2990041971206665\n",
      "Epoch 30 -- Batch 218/ 842, training loss 0.31599318981170654\n",
      "Epoch 30 -- Batch 219/ 842, training loss 0.3078322410583496\n",
      "Epoch 30 -- Batch 220/ 842, training loss 0.3143469989299774\n",
      "Epoch 30 -- Batch 221/ 842, training loss 0.31105196475982666\n",
      "Epoch 30 -- Batch 222/ 842, training loss 0.3019295632839203\n",
      "Epoch 30 -- Batch 223/ 842, training loss 0.306525319814682\n",
      "Epoch 30 -- Batch 224/ 842, training loss 0.31113335490226746\n",
      "Epoch 30 -- Batch 225/ 842, training loss 0.30470797419548035\n",
      "Epoch 30 -- Batch 226/ 842, training loss 0.29936039447784424\n",
      "Epoch 30 -- Batch 227/ 842, training loss 0.307163268327713\n",
      "Epoch 30 -- Batch 228/ 842, training loss 0.30898597836494446\n",
      "Epoch 30 -- Batch 229/ 842, training loss 0.3155055344104767\n",
      "Epoch 30 -- Batch 230/ 842, training loss 0.3063487112522125\n",
      "Epoch 30 -- Batch 231/ 842, training loss 0.3036227226257324\n",
      "Epoch 30 -- Batch 232/ 842, training loss 0.29841166734695435\n",
      "Epoch 30 -- Batch 233/ 842, training loss 0.31272247433662415\n",
      "Epoch 30 -- Batch 234/ 842, training loss 0.2988273799419403\n",
      "Epoch 30 -- Batch 235/ 842, training loss 0.2930971384048462\n",
      "Epoch 30 -- Batch 236/ 842, training loss 0.3033005893230438\n",
      "Epoch 30 -- Batch 237/ 842, training loss 0.3020078241825104\n",
      "Epoch 30 -- Batch 238/ 842, training loss 0.29997220635414124\n",
      "Epoch 30 -- Batch 239/ 842, training loss 0.3057158291339874\n",
      "Epoch 30 -- Batch 240/ 842, training loss 0.30867937207221985\n",
      "Epoch 30 -- Batch 241/ 842, training loss 0.29759129881858826\n",
      "Epoch 30 -- Batch 242/ 842, training loss 0.2988293468952179\n",
      "Epoch 30 -- Batch 243/ 842, training loss 0.29622241854667664\n",
      "Epoch 30 -- Batch 244/ 842, training loss 0.31210997700691223\n",
      "Epoch 30 -- Batch 245/ 842, training loss 0.29433563351631165\n",
      "Epoch 30 -- Batch 246/ 842, training loss 0.3051978051662445\n",
      "Epoch 30 -- Batch 247/ 842, training loss 0.30811938643455505\n",
      "Epoch 30 -- Batch 248/ 842, training loss 0.3065124750137329\n",
      "Epoch 30 -- Batch 249/ 842, training loss 0.3050415515899658\n",
      "Epoch 30 -- Batch 250/ 842, training loss 0.30438998341560364\n",
      "Epoch 30 -- Batch 251/ 842, training loss 0.30788254737854004\n",
      "Epoch 30 -- Batch 252/ 842, training loss 0.30899491906166077\n",
      "Epoch 30 -- Batch 253/ 842, training loss 0.3075818717479706\n",
      "Epoch 30 -- Batch 254/ 842, training loss 0.311860591173172\n",
      "Epoch 30 -- Batch 255/ 842, training loss 0.30980631709098816\n",
      "Epoch 30 -- Batch 256/ 842, training loss 0.3019050061702728\n",
      "Epoch 30 -- Batch 257/ 842, training loss 0.2982932925224304\n",
      "Epoch 30 -- Batch 258/ 842, training loss 0.31642889976501465\n",
      "Epoch 30 -- Batch 259/ 842, training loss 0.3106381595134735\n",
      "Epoch 30 -- Batch 260/ 842, training loss 0.3079107403755188\n",
      "Epoch 30 -- Batch 261/ 842, training loss 0.2979632616043091\n",
      "Epoch 30 -- Batch 262/ 842, training loss 0.3083236813545227\n",
      "Epoch 30 -- Batch 263/ 842, training loss 0.3127964735031128\n",
      "Epoch 30 -- Batch 264/ 842, training loss 0.3013142943382263\n",
      "Epoch 30 -- Batch 265/ 842, training loss 0.3202052414417267\n",
      "Epoch 30 -- Batch 266/ 842, training loss 0.3081403374671936\n",
      "Epoch 30 -- Batch 267/ 842, training loss 0.3020835816860199\n",
      "Epoch 30 -- Batch 268/ 842, training loss 0.3039003610610962\n",
      "Epoch 30 -- Batch 269/ 842, training loss 0.30207493901252747\n",
      "Epoch 30 -- Batch 270/ 842, training loss 0.29116347432136536\n",
      "Epoch 30 -- Batch 271/ 842, training loss 0.3049132227897644\n",
      "Epoch 30 -- Batch 272/ 842, training loss 0.3099188506603241\n",
      "Epoch 30 -- Batch 273/ 842, training loss 0.3088747560977936\n",
      "Epoch 30 -- Batch 274/ 842, training loss 0.3046119511127472\n",
      "Epoch 30 -- Batch 275/ 842, training loss 0.3124763071537018\n",
      "Epoch 30 -- Batch 276/ 842, training loss 0.30051711201667786\n",
      "Epoch 30 -- Batch 277/ 842, training loss 0.297288715839386\n",
      "Epoch 30 -- Batch 278/ 842, training loss 0.30495980381965637\n",
      "Epoch 30 -- Batch 279/ 842, training loss 0.30132266879081726\n",
      "Epoch 30 -- Batch 280/ 842, training loss 0.3108309507369995\n",
      "Epoch 30 -- Batch 281/ 842, training loss 0.3116605281829834\n",
      "Epoch 30 -- Batch 282/ 842, training loss 0.29914069175720215\n",
      "Epoch 30 -- Batch 283/ 842, training loss 0.30358049273490906\n",
      "Epoch 30 -- Batch 284/ 842, training loss 0.30598804354667664\n",
      "Epoch 30 -- Batch 285/ 842, training loss 0.3051741421222687\n",
      "Epoch 30 -- Batch 286/ 842, training loss 0.3065620958805084\n",
      "Epoch 30 -- Batch 287/ 842, training loss 0.3073599934577942\n",
      "Epoch 30 -- Batch 288/ 842, training loss 0.30374935269355774\n",
      "Epoch 30 -- Batch 289/ 842, training loss 0.32031384110450745\n",
      "Epoch 30 -- Batch 290/ 842, training loss 0.2999228835105896\n",
      "Epoch 30 -- Batch 291/ 842, training loss 0.29892000555992126\n",
      "Epoch 30 -- Batch 292/ 842, training loss 0.31147029995918274\n",
      "Epoch 30 -- Batch 293/ 842, training loss 0.3190002739429474\n",
      "Epoch 30 -- Batch 294/ 842, training loss 0.29247429966926575\n",
      "Epoch 30 -- Batch 295/ 842, training loss 0.3019965887069702\n",
      "Epoch 30 -- Batch 296/ 842, training loss 0.29812952876091003\n",
      "Epoch 30 -- Batch 297/ 842, training loss 0.3116190433502197\n",
      "Epoch 30 -- Batch 298/ 842, training loss 0.2933724522590637\n",
      "Epoch 30 -- Batch 299/ 842, training loss 0.3122308552265167\n",
      "Epoch 30 -- Batch 300/ 842, training loss 0.3065090775489807\n",
      "Epoch 30 -- Batch 301/ 842, training loss 0.30485042929649353\n",
      "Epoch 30 -- Batch 302/ 842, training loss 0.30440250039100647\n",
      "Epoch 30 -- Batch 303/ 842, training loss 0.3116379678249359\n",
      "Epoch 30 -- Batch 304/ 842, training loss 0.3019813895225525\n",
      "Epoch 30 -- Batch 305/ 842, training loss 0.2866649329662323\n",
      "Epoch 30 -- Batch 306/ 842, training loss 0.2991431951522827\n",
      "Epoch 30 -- Batch 307/ 842, training loss 0.308846652507782\n",
      "Epoch 30 -- Batch 308/ 842, training loss 0.30174675583839417\n",
      "Epoch 30 -- Batch 309/ 842, training loss 0.3074605166912079\n",
      "Epoch 30 -- Batch 310/ 842, training loss 0.3059520721435547\n",
      "Epoch 30 -- Batch 311/ 842, training loss 0.3071891963481903\n",
      "Epoch 30 -- Batch 312/ 842, training loss 0.30406561493873596\n",
      "Epoch 30 -- Batch 313/ 842, training loss 0.30770930647850037\n",
      "Epoch 30 -- Batch 314/ 842, training loss 0.3053116202354431\n",
      "Epoch 30 -- Batch 315/ 842, training loss 0.3126504421234131\n",
      "Epoch 30 -- Batch 316/ 842, training loss 0.298019677400589\n",
      "Epoch 30 -- Batch 317/ 842, training loss 0.3142007887363434\n",
      "Epoch 30 -- Batch 318/ 842, training loss 0.30409619212150574\n",
      "Epoch 30 -- Batch 319/ 842, training loss 0.3039158880710602\n",
      "Epoch 30 -- Batch 320/ 842, training loss 0.30270621180534363\n",
      "Epoch 30 -- Batch 321/ 842, training loss 0.29636096954345703\n",
      "Epoch 30 -- Batch 322/ 842, training loss 0.2986537516117096\n",
      "Epoch 30 -- Batch 323/ 842, training loss 0.30374473333358765\n",
      "Epoch 30 -- Batch 324/ 842, training loss 0.30684953927993774\n",
      "Epoch 30 -- Batch 325/ 842, training loss 0.29824358224868774\n",
      "Epoch 30 -- Batch 326/ 842, training loss 0.299914687871933\n",
      "Epoch 30 -- Batch 327/ 842, training loss 0.2922855615615845\n",
      "Epoch 30 -- Batch 328/ 842, training loss 0.2987140417098999\n",
      "Epoch 30 -- Batch 329/ 842, training loss 0.3075163960456848\n",
      "Epoch 30 -- Batch 330/ 842, training loss 0.3021969795227051\n",
      "Epoch 30 -- Batch 331/ 842, training loss 0.2979458272457123\n",
      "Epoch 30 -- Batch 332/ 842, training loss 0.3060065507888794\n",
      "Epoch 30 -- Batch 333/ 842, training loss 0.29881751537323\n",
      "Epoch 30 -- Batch 334/ 842, training loss 0.30535688996315\n",
      "Epoch 30 -- Batch 335/ 842, training loss 0.31001272797584534\n",
      "Epoch 30 -- Batch 336/ 842, training loss 0.3032126724720001\n",
      "Epoch 30 -- Batch 337/ 842, training loss 0.2995729446411133\n",
      "Epoch 30 -- Batch 338/ 842, training loss 0.31383633613586426\n",
      "Epoch 30 -- Batch 339/ 842, training loss 0.296669602394104\n",
      "Epoch 30 -- Batch 340/ 842, training loss 0.310225248336792\n",
      "Epoch 30 -- Batch 341/ 842, training loss 0.31442779302597046\n",
      "Epoch 30 -- Batch 342/ 842, training loss 0.31355413794517517\n",
      "Epoch 30 -- Batch 343/ 842, training loss 0.31129929423332214\n",
      "Epoch 30 -- Batch 344/ 842, training loss 0.2993827164173126\n",
      "Epoch 30 -- Batch 345/ 842, training loss 0.3047194480895996\n",
      "Epoch 30 -- Batch 346/ 842, training loss 0.3082607090473175\n",
      "Epoch 30 -- Batch 347/ 842, training loss 0.3001209795475006\n",
      "Epoch 30 -- Batch 348/ 842, training loss 0.3014290928840637\n",
      "Epoch 30 -- Batch 349/ 842, training loss 0.29404839873313904\n",
      "Epoch 30 -- Batch 350/ 842, training loss 0.30830565094947815\n",
      "Epoch 30 -- Batch 351/ 842, training loss 0.30893903970718384\n",
      "Epoch 30 -- Batch 352/ 842, training loss 0.3133925199508667\n",
      "Epoch 30 -- Batch 353/ 842, training loss 0.301586389541626\n",
      "Epoch 30 -- Batch 354/ 842, training loss 0.3069889545440674\n",
      "Epoch 30 -- Batch 355/ 842, training loss 0.29910093545913696\n",
      "Epoch 30 -- Batch 356/ 842, training loss 0.2877563238143921\n",
      "Epoch 30 -- Batch 357/ 842, training loss 0.3129270076751709\n",
      "Epoch 30 -- Batch 358/ 842, training loss 0.29364028573036194\n",
      "Epoch 30 -- Batch 359/ 842, training loss 0.3132789433002472\n",
      "Epoch 30 -- Batch 360/ 842, training loss 0.3026830852031708\n",
      "Epoch 30 -- Batch 361/ 842, training loss 0.30824050307273865\n",
      "Epoch 30 -- Batch 362/ 842, training loss 0.3069598972797394\n",
      "Epoch 30 -- Batch 363/ 842, training loss 0.3063177466392517\n",
      "Epoch 30 -- Batch 364/ 842, training loss 0.3007563650608063\n",
      "Epoch 30 -- Batch 365/ 842, training loss 0.30976808071136475\n",
      "Epoch 30 -- Batch 366/ 842, training loss 0.29544270038604736\n",
      "Epoch 30 -- Batch 367/ 842, training loss 0.2982980012893677\n",
      "Epoch 30 -- Batch 368/ 842, training loss 0.3078562617301941\n",
      "Epoch 30 -- Batch 369/ 842, training loss 0.3083590567111969\n",
      "Epoch 30 -- Batch 370/ 842, training loss 0.3092409670352936\n",
      "Epoch 30 -- Batch 371/ 842, training loss 0.28610554337501526\n",
      "Epoch 30 -- Batch 372/ 842, training loss 0.3165439963340759\n",
      "Epoch 30 -- Batch 373/ 842, training loss 0.29470542073249817\n",
      "Epoch 30 -- Batch 374/ 842, training loss 0.30903342366218567\n",
      "Epoch 30 -- Batch 375/ 842, training loss 0.30762943625450134\n",
      "Epoch 30 -- Batch 376/ 842, training loss 0.3125563859939575\n",
      "Epoch 30 -- Batch 377/ 842, training loss 0.30404898524284363\n",
      "Epoch 30 -- Batch 378/ 842, training loss 0.29699352383613586\n",
      "Epoch 30 -- Batch 379/ 842, training loss 0.3118771016597748\n",
      "Epoch 30 -- Batch 380/ 842, training loss 0.3048906624317169\n",
      "Epoch 30 -- Batch 381/ 842, training loss 0.30267250537872314\n",
      "Epoch 30 -- Batch 382/ 842, training loss 0.3099935054779053\n",
      "Epoch 30 -- Batch 383/ 842, training loss 0.30398494005203247\n",
      "Epoch 30 -- Batch 384/ 842, training loss 0.301164835691452\n",
      "Epoch 30 -- Batch 385/ 842, training loss 0.30871546268463135\n",
      "Epoch 30 -- Batch 386/ 842, training loss 0.31218627095222473\n",
      "Epoch 30 -- Batch 387/ 842, training loss 0.30006489157676697\n",
      "Epoch 30 -- Batch 388/ 842, training loss 0.31268957257270813\n",
      "Epoch 30 -- Batch 389/ 842, training loss 0.3063584268093109\n",
      "Epoch 30 -- Batch 390/ 842, training loss 0.30518150329589844\n",
      "Epoch 30 -- Batch 391/ 842, training loss 0.30211111903190613\n",
      "Epoch 30 -- Batch 392/ 842, training loss 0.30288180708885193\n",
      "Epoch 30 -- Batch 393/ 842, training loss 0.29626038670539856\n",
      "Epoch 30 -- Batch 394/ 842, training loss 0.3238492012023926\n",
      "Epoch 30 -- Batch 395/ 842, training loss 0.30519789457321167\n",
      "Epoch 30 -- Batch 396/ 842, training loss 0.30006542801856995\n",
      "Epoch 30 -- Batch 397/ 842, training loss 0.288968563079834\n",
      "Epoch 30 -- Batch 398/ 842, training loss 0.29678842425346375\n",
      "Epoch 30 -- Batch 399/ 842, training loss 0.2980184555053711\n",
      "Epoch 30 -- Batch 400/ 842, training loss 0.3072650730609894\n",
      "Epoch 30 -- Batch 401/ 842, training loss 0.30545616149902344\n",
      "Epoch 30 -- Batch 402/ 842, training loss 0.30188289284706116\n",
      "Epoch 30 -- Batch 403/ 842, training loss 0.29708653688430786\n",
      "Epoch 30 -- Batch 404/ 842, training loss 0.2980084717273712\n",
      "Epoch 30 -- Batch 405/ 842, training loss 0.3006342947483063\n",
      "Epoch 30 -- Batch 406/ 842, training loss 0.30252811312675476\n",
      "Epoch 30 -- Batch 407/ 842, training loss 0.31267744302749634\n",
      "Epoch 30 -- Batch 408/ 842, training loss 0.30422085523605347\n",
      "Epoch 30 -- Batch 409/ 842, training loss 0.296954870223999\n",
      "Epoch 30 -- Batch 410/ 842, training loss 0.30766090750694275\n",
      "Epoch 30 -- Batch 411/ 842, training loss 0.30960574746131897\n",
      "Epoch 30 -- Batch 412/ 842, training loss 0.3068302869796753\n",
      "Epoch 30 -- Batch 413/ 842, training loss 0.30142807960510254\n",
      "Epoch 30 -- Batch 414/ 842, training loss 0.29709216952323914\n",
      "Epoch 30 -- Batch 415/ 842, training loss 0.3032325506210327\n",
      "Epoch 30 -- Batch 416/ 842, training loss 0.3002558648586273\n",
      "Epoch 30 -- Batch 417/ 842, training loss 0.3163122236728668\n",
      "Epoch 30 -- Batch 418/ 842, training loss 0.29449060559272766\n",
      "Epoch 30 -- Batch 419/ 842, training loss 0.3002370297908783\n",
      "Epoch 30 -- Batch 420/ 842, training loss 0.31137213110923767\n",
      "Epoch 30 -- Batch 421/ 842, training loss 0.296235591173172\n",
      "Epoch 30 -- Batch 422/ 842, training loss 0.3137197494506836\n",
      "Epoch 30 -- Batch 423/ 842, training loss 0.3083721995353699\n",
      "Epoch 30 -- Batch 424/ 842, training loss 0.30114102363586426\n",
      "Epoch 30 -- Batch 425/ 842, training loss 0.31362277269363403\n",
      "Epoch 30 -- Batch 426/ 842, training loss 0.29802078008651733\n",
      "Epoch 30 -- Batch 427/ 842, training loss 0.29492446780204773\n",
      "Epoch 30 -- Batch 428/ 842, training loss 0.3010239005088806\n",
      "Epoch 30 -- Batch 429/ 842, training loss 0.3060705363750458\n",
      "Epoch 30 -- Batch 430/ 842, training loss 0.3024316728115082\n",
      "Epoch 30 -- Batch 431/ 842, training loss 0.30327051877975464\n",
      "Epoch 30 -- Batch 432/ 842, training loss 0.29794469475746155\n",
      "Epoch 30 -- Batch 433/ 842, training loss 0.3054220974445343\n",
      "Epoch 30 -- Batch 434/ 842, training loss 0.30453604459762573\n",
      "Epoch 30 -- Batch 435/ 842, training loss 0.2996513247489929\n",
      "Epoch 30 -- Batch 436/ 842, training loss 0.30780670046806335\n",
      "Epoch 30 -- Batch 437/ 842, training loss 0.3011278510093689\n",
      "Epoch 30 -- Batch 438/ 842, training loss 0.3054228723049164\n",
      "Epoch 30 -- Batch 439/ 842, training loss 0.30480149388313293\n",
      "Epoch 30 -- Batch 440/ 842, training loss 0.3016912639141083\n",
      "Epoch 30 -- Batch 441/ 842, training loss 0.30722713470458984\n",
      "Epoch 30 -- Batch 442/ 842, training loss 0.2961619198322296\n",
      "Epoch 30 -- Batch 443/ 842, training loss 0.30925923585891724\n",
      "Epoch 30 -- Batch 444/ 842, training loss 0.29283419251441956\n",
      "Epoch 30 -- Batch 445/ 842, training loss 0.3040113151073456\n",
      "Epoch 30 -- Batch 446/ 842, training loss 0.3140033483505249\n",
      "Epoch 30 -- Batch 447/ 842, training loss 0.2955266833305359\n",
      "Epoch 30 -- Batch 448/ 842, training loss 0.30352669954299927\n",
      "Epoch 30 -- Batch 449/ 842, training loss 0.30869725346565247\n",
      "Epoch 30 -- Batch 450/ 842, training loss 0.30662068724632263\n",
      "Epoch 30 -- Batch 451/ 842, training loss 0.3018687963485718\n",
      "Epoch 30 -- Batch 452/ 842, training loss 0.3035392165184021\n",
      "Epoch 30 -- Batch 453/ 842, training loss 0.3039652109146118\n",
      "Epoch 30 -- Batch 454/ 842, training loss 0.2998230457305908\n",
      "Epoch 30 -- Batch 455/ 842, training loss 0.30339890718460083\n",
      "Epoch 30 -- Batch 456/ 842, training loss 0.3146134614944458\n",
      "Epoch 30 -- Batch 457/ 842, training loss 0.3096587061882019\n",
      "Epoch 30 -- Batch 458/ 842, training loss 0.30111777782440186\n",
      "Epoch 30 -- Batch 459/ 842, training loss 0.3024044930934906\n",
      "Epoch 30 -- Batch 460/ 842, training loss 0.3192373514175415\n",
      "Epoch 30 -- Batch 461/ 842, training loss 0.31355902552604675\n",
      "Epoch 30 -- Batch 462/ 842, training loss 0.3095678687095642\n",
      "Epoch 30 -- Batch 463/ 842, training loss 0.3032798767089844\n",
      "Epoch 30 -- Batch 464/ 842, training loss 0.30210158228874207\n",
      "Epoch 30 -- Batch 465/ 842, training loss 0.3014877736568451\n",
      "Epoch 30 -- Batch 466/ 842, training loss 0.30828526616096497\n",
      "Epoch 30 -- Batch 467/ 842, training loss 0.30719372630119324\n",
      "Epoch 30 -- Batch 468/ 842, training loss 0.3101288676261902\n",
      "Epoch 30 -- Batch 469/ 842, training loss 0.29890701174736023\n",
      "Epoch 30 -- Batch 470/ 842, training loss 0.3133809268474579\n",
      "Epoch 30 -- Batch 471/ 842, training loss 0.2984101474285126\n",
      "Epoch 30 -- Batch 472/ 842, training loss 0.3019297420978546\n",
      "Epoch 30 -- Batch 473/ 842, training loss 0.30182820558547974\n",
      "Epoch 30 -- Batch 474/ 842, training loss 0.32152730226516724\n",
      "Epoch 30 -- Batch 475/ 842, training loss 0.30796492099761963\n",
      "Epoch 30 -- Batch 476/ 842, training loss 0.30399009585380554\n",
      "Epoch 30 -- Batch 477/ 842, training loss 0.2921203076839447\n",
      "Epoch 30 -- Batch 478/ 842, training loss 0.3075310289859772\n",
      "Epoch 30 -- Batch 479/ 842, training loss 0.30609291791915894\n",
      "Epoch 30 -- Batch 480/ 842, training loss 0.30281996726989746\n",
      "Epoch 30 -- Batch 481/ 842, training loss 0.29754653573036194\n",
      "Epoch 30 -- Batch 482/ 842, training loss 0.30695927143096924\n",
      "Epoch 30 -- Batch 483/ 842, training loss 0.3090920150279999\n",
      "Epoch 30 -- Batch 484/ 842, training loss 0.29847055673599243\n",
      "Epoch 30 -- Batch 485/ 842, training loss 0.3090672194957733\n",
      "Epoch 30 -- Batch 486/ 842, training loss 0.29980480670928955\n",
      "Epoch 30 -- Batch 487/ 842, training loss 0.31118056178092957\n",
      "Epoch 30 -- Batch 488/ 842, training loss 0.3012753129005432\n",
      "Epoch 30 -- Batch 489/ 842, training loss 0.3039768636226654\n",
      "Epoch 30 -- Batch 490/ 842, training loss 0.30444419384002686\n",
      "Epoch 30 -- Batch 491/ 842, training loss 0.3080085217952728\n",
      "Epoch 30 -- Batch 492/ 842, training loss 0.3129507601261139\n",
      "Epoch 30 -- Batch 493/ 842, training loss 0.31622573733329773\n",
      "Epoch 30 -- Batch 494/ 842, training loss 0.30037280917167664\n",
      "Epoch 30 -- Batch 495/ 842, training loss 0.3087885081768036\n",
      "Epoch 30 -- Batch 496/ 842, training loss 0.3001273274421692\n",
      "Epoch 30 -- Batch 497/ 842, training loss 0.30066415667533875\n",
      "Epoch 30 -- Batch 498/ 842, training loss 0.29514044523239136\n",
      "Epoch 30 -- Batch 499/ 842, training loss 0.3054431080818176\n",
      "Epoch 30 -- Batch 500/ 842, training loss 0.3098048269748688\n",
      "Epoch 30 -- Batch 501/ 842, training loss 0.3117824196815491\n",
      "Epoch 30 -- Batch 502/ 842, training loss 0.3162720799446106\n",
      "Epoch 30 -- Batch 503/ 842, training loss 0.30590009689331055\n",
      "Epoch 30 -- Batch 504/ 842, training loss 0.296215295791626\n",
      "Epoch 30 -- Batch 505/ 842, training loss 0.3080589771270752\n",
      "Epoch 30 -- Batch 506/ 842, training loss 0.3077240288257599\n",
      "Epoch 30 -- Batch 507/ 842, training loss 0.3068123757839203\n",
      "Epoch 30 -- Batch 508/ 842, training loss 0.30800023674964905\n",
      "Epoch 30 -- Batch 509/ 842, training loss 0.30645957589149475\n",
      "Epoch 30 -- Batch 510/ 842, training loss 0.31057676672935486\n",
      "Epoch 30 -- Batch 511/ 842, training loss 0.29870620369911194\n",
      "Epoch 30 -- Batch 512/ 842, training loss 0.306973934173584\n",
      "Epoch 30 -- Batch 513/ 842, training loss 0.2991357147693634\n",
      "Epoch 30 -- Batch 514/ 842, training loss 0.31016969680786133\n",
      "Epoch 30 -- Batch 515/ 842, training loss 0.3069576025009155\n",
      "Epoch 30 -- Batch 516/ 842, training loss 0.31143853068351746\n",
      "Epoch 30 -- Batch 517/ 842, training loss 0.31048086285591125\n",
      "Epoch 30 -- Batch 518/ 842, training loss 0.31411299109458923\n",
      "Epoch 30 -- Batch 519/ 842, training loss 0.3126402199268341\n",
      "Epoch 30 -- Batch 520/ 842, training loss 0.30469101667404175\n",
      "Epoch 30 -- Batch 521/ 842, training loss 0.3062669038772583\n",
      "Epoch 30 -- Batch 522/ 842, training loss 0.30814695358276367\n",
      "Epoch 30 -- Batch 523/ 842, training loss 0.29062166810035706\n",
      "Epoch 30 -- Batch 524/ 842, training loss 0.3104242980480194\n",
      "Epoch 30 -- Batch 525/ 842, training loss 0.31292998790740967\n",
      "Epoch 30 -- Batch 526/ 842, training loss 0.30482834577560425\n",
      "Epoch 30 -- Batch 527/ 842, training loss 0.3033154606819153\n",
      "Epoch 30 -- Batch 528/ 842, training loss 0.3004937469959259\n",
      "Epoch 30 -- Batch 529/ 842, training loss 0.3053285777568817\n",
      "Epoch 30 -- Batch 530/ 842, training loss 0.30351540446281433\n",
      "Epoch 30 -- Batch 531/ 842, training loss 0.30594301223754883\n",
      "Epoch 30 -- Batch 532/ 842, training loss 0.3130207657814026\n",
      "Epoch 30 -- Batch 533/ 842, training loss 0.2987517714500427\n",
      "Epoch 30 -- Batch 534/ 842, training loss 0.3027021586894989\n",
      "Epoch 30 -- Batch 535/ 842, training loss 0.303009957075119\n",
      "Epoch 30 -- Batch 536/ 842, training loss 0.3062449097633362\n",
      "Epoch 30 -- Batch 537/ 842, training loss 0.30445343255996704\n",
      "Epoch 30 -- Batch 538/ 842, training loss 0.31477341055870056\n",
      "Epoch 30 -- Batch 539/ 842, training loss 0.3107876777648926\n",
      "Epoch 30 -- Batch 540/ 842, training loss 0.32163846492767334\n",
      "Epoch 30 -- Batch 541/ 842, training loss 0.31056493520736694\n",
      "Epoch 30 -- Batch 542/ 842, training loss 0.2998177409172058\n",
      "Epoch 30 -- Batch 543/ 842, training loss 0.2999991774559021\n",
      "Epoch 30 -- Batch 544/ 842, training loss 0.2994159758090973\n",
      "Epoch 30 -- Batch 545/ 842, training loss 0.3006952404975891\n",
      "Epoch 30 -- Batch 546/ 842, training loss 0.31392598152160645\n",
      "Epoch 30 -- Batch 547/ 842, training loss 0.29483383893966675\n",
      "Epoch 30 -- Batch 548/ 842, training loss 0.30617788434028625\n",
      "Epoch 30 -- Batch 549/ 842, training loss 0.3175522983074188\n",
      "Epoch 30 -- Batch 550/ 842, training loss 0.29712074995040894\n",
      "Epoch 30 -- Batch 551/ 842, training loss 0.3070487380027771\n",
      "Epoch 30 -- Batch 552/ 842, training loss 0.30084919929504395\n",
      "Epoch 30 -- Batch 553/ 842, training loss 0.29542869329452515\n",
      "Epoch 30 -- Batch 554/ 842, training loss 0.308234840631485\n",
      "Epoch 30 -- Batch 555/ 842, training loss 0.3062480092048645\n",
      "Epoch 30 -- Batch 556/ 842, training loss 0.3134744465351105\n",
      "Epoch 30 -- Batch 557/ 842, training loss 0.32352468371391296\n",
      "Epoch 30 -- Batch 558/ 842, training loss 0.3064136207103729\n",
      "Epoch 30 -- Batch 559/ 842, training loss 0.30364108085632324\n",
      "Epoch 30 -- Batch 560/ 842, training loss 0.3109839856624603\n",
      "Epoch 30 -- Batch 561/ 842, training loss 0.30257928371429443\n",
      "Epoch 30 -- Batch 562/ 842, training loss 0.28565990924835205\n",
      "Epoch 30 -- Batch 563/ 842, training loss 0.31171733140945435\n",
      "Epoch 30 -- Batch 564/ 842, training loss 0.3094545900821686\n",
      "Epoch 30 -- Batch 565/ 842, training loss 0.3063805401325226\n",
      "Epoch 30 -- Batch 566/ 842, training loss 0.3080277144908905\n",
      "Epoch 30 -- Batch 567/ 842, training loss 0.3033236861228943\n",
      "Epoch 30 -- Batch 568/ 842, training loss 0.31061476469039917\n",
      "Epoch 30 -- Batch 569/ 842, training loss 0.29879140853881836\n",
      "Epoch 30 -- Batch 570/ 842, training loss 0.317545086145401\n",
      "Epoch 30 -- Batch 571/ 842, training loss 0.3095676302909851\n",
      "Epoch 30 -- Batch 572/ 842, training loss 0.3130943477153778\n",
      "Epoch 30 -- Batch 573/ 842, training loss 0.30849504470825195\n",
      "Epoch 30 -- Batch 574/ 842, training loss 0.30893588066101074\n",
      "Epoch 30 -- Batch 575/ 842, training loss 0.3080867528915405\n",
      "Epoch 30 -- Batch 576/ 842, training loss 0.3020631670951843\n",
      "Epoch 30 -- Batch 577/ 842, training loss 0.3067336082458496\n",
      "Epoch 30 -- Batch 578/ 842, training loss 0.3107309341430664\n",
      "Epoch 30 -- Batch 579/ 842, training loss 0.3029175102710724\n",
      "Epoch 30 -- Batch 580/ 842, training loss 0.3069579005241394\n",
      "Epoch 30 -- Batch 581/ 842, training loss 0.3028601109981537\n",
      "Epoch 30 -- Batch 582/ 842, training loss 0.3131382167339325\n",
      "Epoch 30 -- Batch 583/ 842, training loss 0.2995184659957886\n",
      "Epoch 30 -- Batch 584/ 842, training loss 0.30666932463645935\n",
      "Epoch 30 -- Batch 585/ 842, training loss 0.3149299621582031\n",
      "Epoch 30 -- Batch 586/ 842, training loss 0.3088603913784027\n",
      "Epoch 30 -- Batch 587/ 842, training loss 0.30536574125289917\n",
      "Epoch 30 -- Batch 588/ 842, training loss 0.2981293201446533\n",
      "Epoch 30 -- Batch 589/ 842, training loss 0.30736640095710754\n",
      "Epoch 30 -- Batch 590/ 842, training loss 0.30460795760154724\n",
      "Epoch 30 -- Batch 591/ 842, training loss 0.29905301332473755\n",
      "Epoch 30 -- Batch 592/ 842, training loss 0.30004799365997314\n",
      "Epoch 30 -- Batch 593/ 842, training loss 0.3142232596874237\n",
      "Epoch 30 -- Batch 594/ 842, training loss 0.31547677516937256\n",
      "Epoch 30 -- Batch 595/ 842, training loss 0.30365684628486633\n",
      "Epoch 30 -- Batch 596/ 842, training loss 0.31108787655830383\n",
      "Epoch 30 -- Batch 597/ 842, training loss 0.31295639276504517\n",
      "Epoch 30 -- Batch 598/ 842, training loss 0.3052407503128052\n",
      "Epoch 30 -- Batch 599/ 842, training loss 0.3053198754787445\n",
      "Epoch 30 -- Batch 600/ 842, training loss 0.303212970495224\n",
      "Epoch 30 -- Batch 601/ 842, training loss 0.3093754053115845\n",
      "Epoch 30 -- Batch 602/ 842, training loss 0.30579647421836853\n",
      "Epoch 30 -- Batch 603/ 842, training loss 0.3031233847141266\n",
      "Epoch 30 -- Batch 604/ 842, training loss 0.3186770975589752\n",
      "Epoch 30 -- Batch 605/ 842, training loss 0.30388128757476807\n",
      "Epoch 30 -- Batch 606/ 842, training loss 0.2944280207157135\n",
      "Epoch 30 -- Batch 607/ 842, training loss 0.3027503788471222\n",
      "Epoch 30 -- Batch 608/ 842, training loss 0.293220579624176\n",
      "Epoch 30 -- Batch 609/ 842, training loss 0.3106106221675873\n",
      "Epoch 30 -- Batch 610/ 842, training loss 0.3077693581581116\n",
      "Epoch 30 -- Batch 611/ 842, training loss 0.2943142354488373\n",
      "Epoch 30 -- Batch 612/ 842, training loss 0.3060528337955475\n",
      "Epoch 30 -- Batch 613/ 842, training loss 0.30254849791526794\n",
      "Epoch 30 -- Batch 614/ 842, training loss 0.3042107820510864\n",
      "Epoch 30 -- Batch 615/ 842, training loss 0.3132874667644501\n",
      "Epoch 30 -- Batch 616/ 842, training loss 0.3082752227783203\n",
      "Epoch 30 -- Batch 617/ 842, training loss 0.3189927935600281\n",
      "Epoch 30 -- Batch 618/ 842, training loss 0.3009503483772278\n",
      "Epoch 30 -- Batch 619/ 842, training loss 0.30002620816230774\n",
      "Epoch 30 -- Batch 620/ 842, training loss 0.3038347065448761\n",
      "Epoch 30 -- Batch 621/ 842, training loss 0.3101889193058014\n",
      "Epoch 30 -- Batch 622/ 842, training loss 0.31763148307800293\n",
      "Epoch 30 -- Batch 623/ 842, training loss 0.3172386884689331\n",
      "Epoch 30 -- Batch 624/ 842, training loss 0.3177095055580139\n",
      "Epoch 30 -- Batch 625/ 842, training loss 0.2972787916660309\n",
      "Epoch 30 -- Batch 626/ 842, training loss 0.3085708022117615\n",
      "Epoch 30 -- Batch 627/ 842, training loss 0.30709636211395264\n",
      "Epoch 30 -- Batch 628/ 842, training loss 0.2946264147758484\n",
      "Epoch 30 -- Batch 629/ 842, training loss 0.30990278720855713\n",
      "Epoch 30 -- Batch 630/ 842, training loss 0.30792465806007385\n",
      "Epoch 30 -- Batch 631/ 842, training loss 0.31756195425987244\n",
      "Epoch 30 -- Batch 632/ 842, training loss 0.30171287059783936\n",
      "Epoch 30 -- Batch 633/ 842, training loss 0.3084542453289032\n",
      "Epoch 30 -- Batch 634/ 842, training loss 0.3030870854854584\n",
      "Epoch 30 -- Batch 635/ 842, training loss 0.30805081129074097\n",
      "Epoch 30 -- Batch 636/ 842, training loss 0.30589109659194946\n",
      "Epoch 30 -- Batch 637/ 842, training loss 0.30688053369522095\n",
      "Epoch 30 -- Batch 638/ 842, training loss 0.314089298248291\n",
      "Epoch 30 -- Batch 639/ 842, training loss 0.31325003504753113\n",
      "Epoch 30 -- Batch 640/ 842, training loss 0.2951773405075073\n",
      "Epoch 30 -- Batch 641/ 842, training loss 0.3000658452510834\n",
      "Epoch 30 -- Batch 642/ 842, training loss 0.29502612352371216\n",
      "Epoch 30 -- Batch 643/ 842, training loss 0.3149058520793915\n",
      "Epoch 30 -- Batch 644/ 842, training loss 0.3038840591907501\n",
      "Epoch 30 -- Batch 645/ 842, training loss 0.3024314045906067\n",
      "Epoch 30 -- Batch 646/ 842, training loss 0.3044557273387909\n",
      "Epoch 30 -- Batch 647/ 842, training loss 0.30614393949508667\n",
      "Epoch 30 -- Batch 648/ 842, training loss 0.29785144329071045\n",
      "Epoch 30 -- Batch 649/ 842, training loss 0.315047949552536\n",
      "Epoch 30 -- Batch 650/ 842, training loss 0.30736422538757324\n",
      "Epoch 30 -- Batch 651/ 842, training loss 0.3003300726413727\n",
      "Epoch 30 -- Batch 652/ 842, training loss 0.3022952377796173\n",
      "Epoch 30 -- Batch 653/ 842, training loss 0.30873751640319824\n",
      "Epoch 30 -- Batch 654/ 842, training loss 0.3162972033023834\n",
      "Epoch 30 -- Batch 655/ 842, training loss 0.3072255551815033\n",
      "Epoch 30 -- Batch 656/ 842, training loss 0.30098140239715576\n",
      "Epoch 30 -- Batch 657/ 842, training loss 0.29986390471458435\n",
      "Epoch 30 -- Batch 658/ 842, training loss 0.3041285574436188\n",
      "Epoch 30 -- Batch 659/ 842, training loss 0.302334725856781\n",
      "Epoch 30 -- Batch 660/ 842, training loss 0.3127272427082062\n",
      "Epoch 30 -- Batch 661/ 842, training loss 0.316625714302063\n",
      "Epoch 30 -- Batch 662/ 842, training loss 0.3000490665435791\n",
      "Epoch 30 -- Batch 663/ 842, training loss 0.310025155544281\n",
      "Epoch 30 -- Batch 664/ 842, training loss 0.30182865262031555\n",
      "Epoch 30 -- Batch 665/ 842, training loss 0.29489099979400635\n",
      "Epoch 30 -- Batch 666/ 842, training loss 0.3055952191352844\n",
      "Epoch 30 -- Batch 667/ 842, training loss 0.2953345775604248\n",
      "Epoch 30 -- Batch 668/ 842, training loss 0.3088022768497467\n",
      "Epoch 30 -- Batch 669/ 842, training loss 0.30655866861343384\n",
      "Epoch 30 -- Batch 670/ 842, training loss 0.30965471267700195\n",
      "Epoch 30 -- Batch 671/ 842, training loss 0.2993704080581665\n",
      "Epoch 30 -- Batch 672/ 842, training loss 0.30537286400794983\n",
      "Epoch 30 -- Batch 673/ 842, training loss 0.30770185589790344\n",
      "Epoch 30 -- Batch 674/ 842, training loss 0.2996178865432739\n",
      "Epoch 30 -- Batch 675/ 842, training loss 0.31364285945892334\n",
      "Epoch 30 -- Batch 676/ 842, training loss 0.3153374195098877\n",
      "Epoch 30 -- Batch 677/ 842, training loss 0.30032259225845337\n",
      "Epoch 30 -- Batch 678/ 842, training loss 0.30990540981292725\n",
      "Epoch 30 -- Batch 679/ 842, training loss 0.30117207765579224\n",
      "Epoch 30 -- Batch 680/ 842, training loss 0.30451512336730957\n",
      "Epoch 30 -- Batch 681/ 842, training loss 0.3052334487438202\n",
      "Epoch 30 -- Batch 682/ 842, training loss 0.30314207077026367\n",
      "Epoch 30 -- Batch 683/ 842, training loss 0.30371132493019104\n",
      "Epoch 30 -- Batch 684/ 842, training loss 0.31458166241645813\n",
      "Epoch 30 -- Batch 685/ 842, training loss 0.31203073263168335\n",
      "Epoch 30 -- Batch 686/ 842, training loss 0.30053314566612244\n",
      "Epoch 30 -- Batch 687/ 842, training loss 0.28911104798316956\n",
      "Epoch 30 -- Batch 688/ 842, training loss 0.30774638056755066\n",
      "Epoch 30 -- Batch 689/ 842, training loss 0.29985567927360535\n",
      "Epoch 30 -- Batch 690/ 842, training loss 0.31530213356018066\n",
      "Epoch 30 -- Batch 691/ 842, training loss 0.30561351776123047\n",
      "Epoch 30 -- Batch 692/ 842, training loss 0.29863643646240234\n",
      "Epoch 30 -- Batch 693/ 842, training loss 0.30488309264183044\n",
      "Epoch 30 -- Batch 694/ 842, training loss 0.32153114676475525\n",
      "Epoch 30 -- Batch 695/ 842, training loss 0.3096994161605835\n",
      "Epoch 30 -- Batch 696/ 842, training loss 0.30212950706481934\n",
      "Epoch 30 -- Batch 697/ 842, training loss 0.310737669467926\n",
      "Epoch 30 -- Batch 698/ 842, training loss 0.31700149178504944\n",
      "Epoch 30 -- Batch 699/ 842, training loss 0.31104958057403564\n",
      "Epoch 30 -- Batch 700/ 842, training loss 0.30489787459373474\n",
      "Epoch 30 -- Batch 701/ 842, training loss 0.30167409777641296\n",
      "Epoch 30 -- Batch 702/ 842, training loss 0.32304519414901733\n",
      "Epoch 30 -- Batch 703/ 842, training loss 0.306299090385437\n",
      "Epoch 30 -- Batch 704/ 842, training loss 0.30849483609199524\n",
      "Epoch 30 -- Batch 705/ 842, training loss 0.29370251297950745\n",
      "Epoch 30 -- Batch 706/ 842, training loss 0.316536009311676\n",
      "Epoch 30 -- Batch 707/ 842, training loss 0.30902254581451416\n",
      "Epoch 30 -- Batch 708/ 842, training loss 0.30747130513191223\n",
      "Epoch 30 -- Batch 709/ 842, training loss 0.29433009028434753\n",
      "Epoch 30 -- Batch 710/ 842, training loss 0.31654608249664307\n",
      "Epoch 30 -- Batch 711/ 842, training loss 0.29970455169677734\n",
      "Epoch 30 -- Batch 712/ 842, training loss 0.33001622557640076\n",
      "Epoch 30 -- Batch 713/ 842, training loss 0.30714452266693115\n",
      "Epoch 30 -- Batch 714/ 842, training loss 0.3069659471511841\n",
      "Epoch 30 -- Batch 715/ 842, training loss 0.3030020594596863\n",
      "Epoch 30 -- Batch 716/ 842, training loss 0.2986772954463959\n",
      "Epoch 30 -- Batch 717/ 842, training loss 0.30515846610069275\n",
      "Epoch 30 -- Batch 718/ 842, training loss 0.3062717318534851\n",
      "Epoch 30 -- Batch 719/ 842, training loss 0.30045080184936523\n",
      "Epoch 30 -- Batch 720/ 842, training loss 0.30773335695266724\n",
      "Epoch 30 -- Batch 721/ 842, training loss 0.3081449568271637\n",
      "Epoch 30 -- Batch 722/ 842, training loss 0.3130119740962982\n",
      "Epoch 30 -- Batch 723/ 842, training loss 0.3039546310901642\n",
      "Epoch 30 -- Batch 724/ 842, training loss 0.30407461524009705\n",
      "Epoch 30 -- Batch 725/ 842, training loss 0.30641111731529236\n",
      "Epoch 30 -- Batch 726/ 842, training loss 0.31668734550476074\n",
      "Epoch 30 -- Batch 727/ 842, training loss 0.3138379156589508\n",
      "Epoch 30 -- Batch 728/ 842, training loss 0.29654017090797424\n",
      "Epoch 30 -- Batch 729/ 842, training loss 0.30786848068237305\n",
      "Epoch 30 -- Batch 730/ 842, training loss 0.30954739451408386\n",
      "Epoch 30 -- Batch 731/ 842, training loss 0.3121199607849121\n",
      "Epoch 30 -- Batch 732/ 842, training loss 0.30996933579444885\n",
      "Epoch 30 -- Batch 733/ 842, training loss 0.2960518002510071\n",
      "Epoch 30 -- Batch 734/ 842, training loss 0.3089156150817871\n",
      "Epoch 30 -- Batch 735/ 842, training loss 0.2925616204738617\n",
      "Epoch 30 -- Batch 736/ 842, training loss 0.30649447441101074\n",
      "Epoch 30 -- Batch 737/ 842, training loss 0.3093757927417755\n",
      "Epoch 30 -- Batch 738/ 842, training loss 0.3088279664516449\n",
      "Epoch 30 -- Batch 739/ 842, training loss 0.3087387681007385\n",
      "Epoch 30 -- Batch 740/ 842, training loss 0.30694541335105896\n",
      "Epoch 30 -- Batch 741/ 842, training loss 0.3065967559814453\n",
      "Epoch 30 -- Batch 742/ 842, training loss 0.30862975120544434\n",
      "Epoch 30 -- Batch 743/ 842, training loss 0.3045857548713684\n",
      "Epoch 30 -- Batch 744/ 842, training loss 0.2919391989707947\n",
      "Epoch 30 -- Batch 745/ 842, training loss 0.31264710426330566\n",
      "Epoch 30 -- Batch 746/ 842, training loss 0.3042600452899933\n",
      "Epoch 30 -- Batch 747/ 842, training loss 0.3090687096118927\n",
      "Epoch 30 -- Batch 748/ 842, training loss 0.3109288215637207\n",
      "Epoch 30 -- Batch 749/ 842, training loss 0.30970463156700134\n",
      "Epoch 30 -- Batch 750/ 842, training loss 0.3036535978317261\n",
      "Epoch 30 -- Batch 751/ 842, training loss 0.3017568588256836\n",
      "Epoch 30 -- Batch 752/ 842, training loss 0.30613815784454346\n",
      "Epoch 30 -- Batch 753/ 842, training loss 0.303219199180603\n",
      "Epoch 30 -- Batch 754/ 842, training loss 0.30307337641716003\n",
      "Epoch 30 -- Batch 755/ 842, training loss 0.305107444524765\n",
      "Epoch 30 -- Batch 756/ 842, training loss 0.3090958297252655\n",
      "Epoch 30 -- Batch 757/ 842, training loss 0.30637165904045105\n",
      "Epoch 30 -- Batch 758/ 842, training loss 0.3069358468055725\n",
      "Epoch 30 -- Batch 759/ 842, training loss 0.3265421688556671\n",
      "Epoch 30 -- Batch 760/ 842, training loss 0.30150341987609863\n",
      "Epoch 30 -- Batch 761/ 842, training loss 0.30302050709724426\n",
      "Epoch 30 -- Batch 762/ 842, training loss 0.3066406846046448\n",
      "Epoch 30 -- Batch 763/ 842, training loss 0.2983430325984955\n",
      "Epoch 30 -- Batch 764/ 842, training loss 0.30108821392059326\n",
      "Epoch 30 -- Batch 765/ 842, training loss 0.3040807247161865\n",
      "Epoch 30 -- Batch 766/ 842, training loss 0.30615663528442383\n",
      "Epoch 30 -- Batch 767/ 842, training loss 0.3155660331249237\n",
      "Epoch 30 -- Batch 768/ 842, training loss 0.30559641122817993\n",
      "Epoch 30 -- Batch 769/ 842, training loss 0.30868831276893616\n",
      "Epoch 30 -- Batch 770/ 842, training loss 0.3024131655693054\n",
      "Epoch 30 -- Batch 771/ 842, training loss 0.2944393754005432\n",
      "Epoch 30 -- Batch 772/ 842, training loss 0.31138119101524353\n",
      "Epoch 30 -- Batch 773/ 842, training loss 0.3077000081539154\n",
      "Epoch 30 -- Batch 774/ 842, training loss 0.3050246238708496\n",
      "Epoch 30 -- Batch 775/ 842, training loss 0.31475308537483215\n",
      "Epoch 30 -- Batch 776/ 842, training loss 0.30193203687667847\n",
      "Epoch 30 -- Batch 777/ 842, training loss 0.3033933639526367\n",
      "Epoch 30 -- Batch 778/ 842, training loss 0.3082297444343567\n",
      "Epoch 30 -- Batch 779/ 842, training loss 0.3122067451477051\n",
      "Epoch 30 -- Batch 780/ 842, training loss 0.3054996430873871\n",
      "Epoch 30 -- Batch 781/ 842, training loss 0.3017555773258209\n",
      "Epoch 30 -- Batch 782/ 842, training loss 0.30339115858078003\n",
      "Epoch 30 -- Batch 783/ 842, training loss 0.3018147051334381\n",
      "Epoch 30 -- Batch 784/ 842, training loss 0.3022274672985077\n",
      "Epoch 30 -- Batch 785/ 842, training loss 0.3041704297065735\n",
      "Epoch 30 -- Batch 786/ 842, training loss 0.29981908202171326\n",
      "Epoch 30 -- Batch 787/ 842, training loss 0.30132389068603516\n",
      "Epoch 30 -- Batch 788/ 842, training loss 0.30713900923728943\n",
      "Epoch 30 -- Batch 789/ 842, training loss 0.3076719641685486\n",
      "Epoch 30 -- Batch 790/ 842, training loss 0.3003602921962738\n",
      "Epoch 30 -- Batch 791/ 842, training loss 0.3171440362930298\n",
      "Epoch 30 -- Batch 792/ 842, training loss 0.3080933392047882\n",
      "Epoch 30 -- Batch 793/ 842, training loss 0.3047062158584595\n",
      "Epoch 30 -- Batch 794/ 842, training loss 0.3031257092952728\n",
      "Epoch 30 -- Batch 795/ 842, training loss 0.3046206831932068\n",
      "Epoch 30 -- Batch 796/ 842, training loss 0.30869460105895996\n",
      "Epoch 30 -- Batch 797/ 842, training loss 0.29901406168937683\n",
      "Epoch 30 -- Batch 798/ 842, training loss 0.3094375729560852\n",
      "Epoch 30 -- Batch 799/ 842, training loss 0.30612441897392273\n",
      "Epoch 30 -- Batch 800/ 842, training loss 0.30481618642807007\n",
      "Epoch 30 -- Batch 801/ 842, training loss 0.3117847144603729\n",
      "Epoch 30 -- Batch 802/ 842, training loss 0.3039540648460388\n",
      "Epoch 30 -- Batch 803/ 842, training loss 0.30235061049461365\n",
      "Epoch 30 -- Batch 804/ 842, training loss 0.31346920132637024\n",
      "Epoch 30 -- Batch 805/ 842, training loss 0.3009498417377472\n",
      "Epoch 30 -- Batch 806/ 842, training loss 0.29959729313850403\n",
      "Epoch 30 -- Batch 807/ 842, training loss 0.3093569576740265\n",
      "Epoch 30 -- Batch 808/ 842, training loss 0.3008166253566742\n",
      "Epoch 30 -- Batch 809/ 842, training loss 0.3088839054107666\n",
      "Epoch 30 -- Batch 810/ 842, training loss 0.2966488003730774\n",
      "Epoch 30 -- Batch 811/ 842, training loss 0.2984721064567566\n",
      "Epoch 30 -- Batch 812/ 842, training loss 0.30906182527542114\n",
      "Epoch 30 -- Batch 813/ 842, training loss 0.3044005334377289\n",
      "Epoch 30 -- Batch 814/ 842, training loss 0.30347803235054016\n",
      "Epoch 30 -- Batch 815/ 842, training loss 0.29501602053642273\n",
      "Epoch 30 -- Batch 816/ 842, training loss 0.3057008683681488\n",
      "Epoch 30 -- Batch 817/ 842, training loss 0.30754023790359497\n",
      "Epoch 30 -- Batch 818/ 842, training loss 0.30731070041656494\n",
      "Epoch 30 -- Batch 819/ 842, training loss 0.30463236570358276\n",
      "Epoch 30 -- Batch 820/ 842, training loss 0.31648388504981995\n",
      "Epoch 30 -- Batch 821/ 842, training loss 0.30281105637550354\n",
      "Epoch 30 -- Batch 822/ 842, training loss 0.304118275642395\n",
      "Epoch 30 -- Batch 823/ 842, training loss 0.3204330503940582\n",
      "Epoch 30 -- Batch 824/ 842, training loss 0.30872485041618347\n",
      "Epoch 30 -- Batch 825/ 842, training loss 0.30300506949424744\n",
      "Epoch 30 -- Batch 826/ 842, training loss 0.3074803650379181\n",
      "Epoch 30 -- Batch 827/ 842, training loss 0.30385321378707886\n",
      "Epoch 30 -- Batch 828/ 842, training loss 0.30986011028289795\n",
      "Epoch 30 -- Batch 829/ 842, training loss 0.30675551295280457\n",
      "Epoch 30 -- Batch 830/ 842, training loss 0.3128724694252014\n",
      "Epoch 30 -- Batch 831/ 842, training loss 0.31510448455810547\n",
      "Epoch 30 -- Batch 832/ 842, training loss 0.31150496006011963\n",
      "Epoch 30 -- Batch 833/ 842, training loss 0.2993285357952118\n",
      "Epoch 30 -- Batch 834/ 842, training loss 0.30674484372138977\n",
      "Epoch 30 -- Batch 835/ 842, training loss 0.29920127987861633\n",
      "Epoch 30 -- Batch 836/ 842, training loss 0.3002510368824005\n",
      "Epoch 30 -- Batch 837/ 842, training loss 0.3115229904651642\n",
      "Epoch 30 -- Batch 838/ 842, training loss 0.30197957158088684\n",
      "Epoch 30 -- Batch 839/ 842, training loss 0.31122151017189026\n",
      "Epoch 30 -- Batch 840/ 842, training loss 0.3107397258281708\n",
      "Epoch 30 -- Batch 841/ 842, training loss 0.31126168370246887\n",
      "Epoch 30 -- Batch 842/ 842, training loss 0.3081241250038147\n",
      "----------------------------------------------------------------------\n",
      "Epoch 30 -- Batch 1/ 94, validation loss 0.30487748980522156\n",
      "Epoch 30 -- Batch 2/ 94, validation loss 0.30699506402015686\n",
      "Epoch 30 -- Batch 3/ 94, validation loss 0.2962297797203064\n",
      "Epoch 30 -- Batch 4/ 94, validation loss 0.2890172004699707\n",
      "Epoch 30 -- Batch 5/ 94, validation loss 0.29151663184165955\n",
      "Epoch 30 -- Batch 6/ 94, validation loss 0.2940796911716461\n",
      "Epoch 30 -- Batch 7/ 94, validation loss 0.2912445068359375\n",
      "Epoch 30 -- Batch 8/ 94, validation loss 0.29909172654151917\n",
      "Epoch 30 -- Batch 9/ 94, validation loss 0.29901495575904846\n",
      "Epoch 30 -- Batch 10/ 94, validation loss 0.2943897247314453\n",
      "Epoch 30 -- Batch 11/ 94, validation loss 0.2946179509162903\n",
      "Epoch 30 -- Batch 12/ 94, validation loss 0.30049142241477966\n",
      "Epoch 30 -- Batch 13/ 94, validation loss 0.2902294397354126\n",
      "Epoch 30 -- Batch 14/ 94, validation loss 0.29184141755104065\n",
      "Epoch 30 -- Batch 15/ 94, validation loss 0.3056170344352722\n",
      "Epoch 30 -- Batch 16/ 94, validation loss 0.2911803424358368\n",
      "Epoch 30 -- Batch 17/ 94, validation loss 0.2941379249095917\n",
      "Epoch 30 -- Batch 18/ 94, validation loss 0.2926847040653229\n",
      "Epoch 30 -- Batch 19/ 94, validation loss 0.2987157106399536\n",
      "Epoch 30 -- Batch 20/ 94, validation loss 0.2934589087963104\n",
      "Epoch 30 -- Batch 21/ 94, validation loss 0.3054657578468323\n",
      "Epoch 30 -- Batch 22/ 94, validation loss 0.29457587003707886\n",
      "Epoch 30 -- Batch 23/ 94, validation loss 0.30023637413978577\n",
      "Epoch 30 -- Batch 24/ 94, validation loss 0.28726232051849365\n",
      "Epoch 30 -- Batch 25/ 94, validation loss 0.2979927659034729\n",
      "Epoch 30 -- Batch 26/ 94, validation loss 0.2962105870246887\n",
      "Epoch 30 -- Batch 27/ 94, validation loss 0.29115721583366394\n",
      "Epoch 30 -- Batch 28/ 94, validation loss 0.29913708567619324\n",
      "Epoch 30 -- Batch 29/ 94, validation loss 0.2923550307750702\n",
      "Epoch 30 -- Batch 30/ 94, validation loss 0.2887547016143799\n",
      "Epoch 30 -- Batch 31/ 94, validation loss 0.2915830612182617\n",
      "Epoch 30 -- Batch 32/ 94, validation loss 0.2989243268966675\n",
      "Epoch 30 -- Batch 33/ 94, validation loss 0.2947654724121094\n",
      "Epoch 30 -- Batch 34/ 94, validation loss 0.2923002243041992\n",
      "Epoch 30 -- Batch 35/ 94, validation loss 0.33396995067596436\n",
      "Epoch 30 -- Batch 36/ 94, validation loss 0.3030305802822113\n",
      "Epoch 30 -- Batch 37/ 94, validation loss 0.29104313254356384\n",
      "Epoch 30 -- Batch 38/ 94, validation loss 0.30098965764045715\n",
      "Epoch 30 -- Batch 39/ 94, validation loss 0.29636478424072266\n",
      "Epoch 30 -- Batch 40/ 94, validation loss 0.28786909580230713\n",
      "Epoch 30 -- Batch 41/ 94, validation loss 0.3346135914325714\n",
      "Epoch 30 -- Batch 42/ 94, validation loss 0.2999681532382965\n",
      "Epoch 30 -- Batch 43/ 94, validation loss 0.31084924936294556\n",
      "Epoch 30 -- Batch 44/ 94, validation loss 0.29522261023521423\n",
      "Epoch 30 -- Batch 45/ 94, validation loss 0.2836422920227051\n",
      "Epoch 30 -- Batch 46/ 94, validation loss 0.29649803042411804\n",
      "Epoch 30 -- Batch 47/ 94, validation loss 0.3089313507080078\n",
      "Epoch 30 -- Batch 48/ 94, validation loss 0.2919099032878876\n",
      "Epoch 30 -- Batch 49/ 94, validation loss 0.30060243606567383\n",
      "Epoch 30 -- Batch 50/ 94, validation loss 0.29067009687423706\n",
      "Epoch 30 -- Batch 51/ 94, validation loss 0.2908127009868622\n",
      "Epoch 30 -- Batch 52/ 94, validation loss 0.2972753942012787\n",
      "Epoch 30 -- Batch 53/ 94, validation loss 0.29384416341781616\n",
      "Epoch 30 -- Batch 54/ 94, validation loss 0.2959353029727936\n",
      "Epoch 30 -- Batch 55/ 94, validation loss 0.2949300706386566\n",
      "Epoch 30 -- Batch 56/ 94, validation loss 0.28795796632766724\n",
      "Epoch 30 -- Batch 57/ 94, validation loss 0.27889227867126465\n",
      "Epoch 30 -- Batch 58/ 94, validation loss 0.2881013751029968\n",
      "Epoch 30 -- Batch 59/ 94, validation loss 0.2818714380264282\n",
      "Epoch 30 -- Batch 60/ 94, validation loss 0.30207720398902893\n",
      "Epoch 30 -- Batch 61/ 94, validation loss 0.28476646542549133\n",
      "Epoch 30 -- Batch 62/ 94, validation loss 0.29003769159317017\n",
      "Epoch 30 -- Batch 63/ 94, validation loss 0.2871972620487213\n",
      "Epoch 30 -- Batch 64/ 94, validation loss 0.2942448854446411\n",
      "Epoch 30 -- Batch 65/ 94, validation loss 0.30929604172706604\n",
      "Epoch 30 -- Batch 66/ 94, validation loss 0.29415902495384216\n",
      "Epoch 30 -- Batch 67/ 94, validation loss 0.296497642993927\n",
      "Epoch 30 -- Batch 68/ 94, validation loss 0.2911335825920105\n",
      "Epoch 30 -- Batch 69/ 94, validation loss 0.33650335669517517\n",
      "Epoch 30 -- Batch 70/ 94, validation loss 0.2998213768005371\n",
      "Epoch 30 -- Batch 71/ 94, validation loss 0.28942978382110596\n",
      "Epoch 30 -- Batch 72/ 94, validation loss 0.2922990918159485\n",
      "Epoch 30 -- Batch 73/ 94, validation loss 0.28943637013435364\n",
      "Epoch 30 -- Batch 74/ 94, validation loss 0.2947762906551361\n",
      "Epoch 30 -- Batch 75/ 94, validation loss 0.2842625379562378\n",
      "Epoch 30 -- Batch 76/ 94, validation loss 0.3097319006919861\n",
      "Epoch 30 -- Batch 77/ 94, validation loss 0.2868751585483551\n",
      "Epoch 30 -- Batch 78/ 94, validation loss 0.2872749865055084\n",
      "Epoch 30 -- Batch 79/ 94, validation loss 0.2893662452697754\n",
      "Epoch 30 -- Batch 80/ 94, validation loss 0.29892921447753906\n",
      "Epoch 30 -- Batch 81/ 94, validation loss 0.2997712790966034\n",
      "Epoch 30 -- Batch 82/ 94, validation loss 0.2932426631450653\n",
      "Epoch 30 -- Batch 83/ 94, validation loss 0.3042140305042267\n",
      "Epoch 30 -- Batch 84/ 94, validation loss 0.2929582893848419\n",
      "Epoch 30 -- Batch 85/ 94, validation loss 0.29433444142341614\n",
      "Epoch 30 -- Batch 86/ 94, validation loss 0.29652178287506104\n",
      "Epoch 30 -- Batch 87/ 94, validation loss 0.2961471378803253\n",
      "Epoch 30 -- Batch 88/ 94, validation loss 0.2891392707824707\n",
      "Epoch 30 -- Batch 89/ 94, validation loss 0.29742616415023804\n",
      "Epoch 30 -- Batch 90/ 94, validation loss 0.2991161346435547\n",
      "Epoch 30 -- Batch 91/ 94, validation loss 0.3029870390892029\n",
      "Epoch 30 -- Batch 92/ 94, validation loss 0.2867824137210846\n",
      "Epoch 30 -- Batch 93/ 94, validation loss 0.29330113530158997\n",
      "Epoch 30 -- Batch 94/ 94, validation loss 0.2891078293323517\n",
      "----------------------------------------------------------------------\n",
      "Epoch 30 loss: Training 0.3045324981212616, Validation 0.2891078293323517\n",
      "----------------------------------------------------------------------\n",
      "Done training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 12 13 21\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 22\n",
      "[19:08:54] SMILES Parse Error: extra close parentheses while parsing: Cc1ncn(-c2ccc(Nc3cc(-c4nocc4C)ccn3)cc2)n1)c1\n",
      "[19:08:54] SMILES Parse Error: Failed parsing SMILES 'Cc1ncn(-c2ccc(Nc3cc(-c4nocc4C)ccn3)cc2)n1)c1' for input: 'Cc1ncn(-c2ccc(Nc3cc(-c4nocc4C)ccn3)cc2)n1)c1'\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'O=C1NC(=O)c2c1c1ccccc1c2ccccc12'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 2 3 9 11 26\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 17 18 19\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'CS(=O)(=O)N1CCc2nc(C3COc3ccccc3CNCc3ccco3)CC2C1'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 15 16 17 18\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)Nc2c3c(nn2-c2nc(C(C)(C)C)no2)CS(=O)(=O)C2)cc1OC'\n",
      "[19:08:54] SMILES Parse Error: extra close parentheses while parsing: Cc1nc2sc(C(=O)O)c3c(=O)[nH]c(=O)n23)c(C)c1\n",
      "[19:08:54] SMILES Parse Error: Failed parsing SMILES 'Cc1nc2sc(C(=O)O)c3c(=O)[nH]c(=O)n23)c(C)c1' for input: 'Cc1nc2sc(C(=O)O)c3c(=O)[nH]c(=O)n23)c(C)c1'\n",
      "[19:08:54] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 16\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'OCCNCCn1cc(-c2ccccc2)c2n3c(nc4ccccc24)CC2'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 12 13 14 28 29 31 32\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'CC(C)C(NC(=O)c1cccc2ccccc12)c1nncn1C1'\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'C[C@]12CC3CCC1CCC1CC[C@]24CCCN4CC[C@@H]3CC[C@]24CC'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c3ncnc4cccc4c2c2'\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'CCCNC(=O)C[C@@H]1CC[C@H]2NC[C@@H](O)COC[C@H]2O[C@@H](CC(=O)O)CC[C@@H]21'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1NC(=O)C(=O)Nc1c2c3n(c(=O)c3c1C)CCCC3'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 7 8 10 11 12 13 14 15 16\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 1 2 16 17 18 19 20 21 22\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'COc1ccc2nc(N3CCC(NC4C5CC6CC4CC(O)(C6)C4)CC3)nc(C)c2c1'\n",
      "[19:08:54] SMILES Parse Error: unclosed ring for input: 'Cn(c(N)c(C(=O)COC(=O)CCC(=O)NC(C)(C)C)c2ccccc2)cc1=O'\n",
      "[19:08:54] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 29 30 31\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../src/model/')\n",
    "\n",
    "for dataset in dataset_list:\n",
    "\tdataset_dir = f'{PRETRAIN_RESULTS}/{dataset}_results/dataset4lstm'\n",
    "\n",
    "\tappname  \t= 'generative_models.apps.BuildModel.TrainerApp'\n",
    "\tnworkers \t= 20\n",
    "\tdata \t   \t= f'{dataset_dir}/passed_mols.tsv'\n",
    "\tvocab \t\t= f'{dataset_dir}/vocab.pickle'\n",
    "\toutfd \t\t= f'{PRETRAIN_RESULTS}/{dataset}_results/vanilalstm'\n",
    "\tsmicol     \t= 'rdkit_smiles'\n",
    "\tvalratio \t= 0.1\n",
    "\tespatience \t= 4\n",
    "\tepochs \t\t= 30\n",
    "\tnsamples \t= 1000\n",
    "\tbatch_size  = 128\n",
    "\tsave_snapshot = '10,20,30'\n",
    "\t# model parameters \n",
    "\tmodel \t\t= 'lstm'\n",
    "\tlr \t\t\t= 0.0005\n",
    "\tembed\t\t= 256\n",
    "\tnlayers \t= 4\n",
    "\thidden  \t= 512\n",
    "\tdropout \t= 0.2\n",
    "\tlayernorm \t= True\n",
    "\n",
    "\n",
    "\tgeneral_params = [\n",
    "\t\t\t\tf'--num-workers={nworkers}',\n",
    "\t\t\t\t'--override-folder=1',\n",
    "\t\t\t\tf'--epochs={epochs}', \n",
    "\t\t\t\tf'--validation-ratio={valratio}',\n",
    "\t\t\t\t'--exclude-pad-loss=1',\n",
    "\t\t\t\tf'--early-stopping-patience={espatience}',\n",
    "\t\t\t\tf'--data={data}',\n",
    "\t\t\t\tf'--smi-colname={smicol}',\n",
    "\t\t\t\tf'--vocab={vocab}',\n",
    "\t\t\t\tf'--outdir={outfd}',\n",
    "\t\t\t\tf'--sampling-epoch={nsamples}',\n",
    "\t\t\t\tf'--batch-size={batch_size}',\n",
    "\t\t\t\tf'--save-snapshot-models={save_snapshot}'\n",
    "\t\t\t\t]\n",
    "\n",
    "\tmodel_params = [\n",
    "\t\tf'--model={model}',\n",
    "\t\tf'--lr={lr}',\n",
    "\t\tf'--embed-dim={embed}',\n",
    "\t\tf'--dropout-ratio={dropout}',\n",
    "\t\tf'--nlayers={nlayers}',\n",
    "\t\tf'--hidden-dim={hidden}',\n",
    "\t\t'--layernorm' if layernorm else '--no-layernorm'\n",
    "\t]\n",
    "\tparams = general_params + model_params\n",
    "\trun(appname, *params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilize the trained LSTM model\n",
    "Since training the lstm model has been completed, the trained model has been used to generate new smiles as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:34:08] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 19 20 21 22\n",
      "[22:34:08] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 9 15 16 17 18 19 20 25\n",
      "[22:34:08] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21\n",
      "[22:34:08] Can't kekulize mol.  Unkekulized atoms: 4 5 6 14 15\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C)cc1N1C(=O)[C@H](c2ccccc2)C(c2n(c3ccc(Cl)cc2)c2ccccc2Cl)=NN1c1ccccc1'\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 20\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'c1cc2occ(CCSc3ncnc4sccc35)n2c2c1'\n",
      "[22:34:09] SMILES Parse Error: syntax error while parsing: CC[C@@H](CCc1ccccc1)NC(=O)NC[C@]1(c2cccc()C2)CC(C)C)c1\n",
      "[22:34:09] SMILES Parse Error: Failed parsing SMILES 'CC[C@@H](CCc1ccccc1)NC(=O)NC[C@]1(c2cccc()C2)CC(C)C)c1' for input: 'CC[C@@H](CCc1ccccc1)NC(=O)NC[C@]1(c2cccc()C2)CC(C)C)c1'\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@@H]2CCCN2c2nc3ccc4c(c3)OCCO4)cc1'\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 4 5 6 8 12 13 21\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 8 9 10 15 16\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1cc2c(cc1NC(=O)[C@H]1CC(=O)N(c2ccc(C)cc3C)C1)OCO2'\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 7 8 11 23 24 25 26\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 21 30 31\n",
      "[22:34:09] SMILES Parse Error: extra close parentheses while parsing: C[C@@H](NC(=O)N1C[C@@H]2CCC1)[C@@](=O)n2C)c1cccc(F)c1\n",
      "[22:34:09] SMILES Parse Error: Failed parsing SMILES 'C[C@@H](NC(=O)N1C[C@@H]2CCC1)[C@@](=O)n2C)c1cccc(F)c1' for input: 'C[C@@H](NC(=O)N1C[C@@H]2CCC1)[C@@](=O)n2C)c1cccc(F)c1'\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 11 12 13\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'CN1C[C@H]2COC[C@@H]1CN(S(=O)(=O)c1ccc3c(c1)CCCC4)C2'\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 7 8 9 16 17 18 19 20 21\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 11 12 20\n",
      "[22:34:09] Can't kekulize mol.  Unkekulized atoms: 3 4 5 18 19\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'C[C@H]1COc2ccccc2CCCCC2(CCN(C(=O)C3CCC3)CC2)C(=O)N2'\n",
      "[22:34:09] SMILES Parse Error: unclosed ring for input: 'O=C1C[C@H](c2ccc3c(c2)OCO3)c2c(O)nc3nc4nc4ccccc4n3c2N1'\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 12 13 14 27\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 18\n",
      "[22:34:10] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2oc(Cn3c(=O)cnc3nc(C4CC4)nnc32)c1=O'\n",
      "[22:34:10] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)c2ccc(Cn3cnnc2)cc2)cc1'\n",
      "[22:34:10] SMILES Parse Error: unclosed ring for input: 'CC1=CC(C)(C)n2c(=O)oc(=O)c3cc(C)ccc31'\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 4 17 18 19 20 21 22 23 24\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 5 13 14 15 16 17 18\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 33\n",
      "[22:34:10] SMILES Parse Error: ring closure 4 duplicates bond between atom 14 and atom 18 for input: 'Cc1cc(C)n([C@@H]2CCN(C(=O)c3cn4(C(C)C)c4c(c3)CCO4)C2)n1'\n",
      "[22:34:10] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN(C)C(=O)c2ccc(CN(c3ccccc3F)S(=O)(=O)N3C)cc2)cc1'\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 7 9 10 12 24\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 2 3 13 14 15 16 18 19 20\n",
      "[22:34:10] SMILES Parse Error: unclosed ring for input: 'CCn1ncc2c3c(NC(=O)c4ccc(OC)c(OC)c4)nnn3c3ccccc21'\n",
      "[22:34:10] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(CNC(=O)[C@@H](C)Oc2ccc(Cl)c(C)c2'\n",
      "[22:34:10] Can't kekulize mol.  Unkekulized atoms: 22 23 24\n",
      "[22:34:11] SMILES Parse Error: unclosed ring for input: 'O=C(c1cc(N2CCC(NCC[C@H]3C[C@H]4C=O)CC3)ccc2)nc2ccccc12'\n",
      "[22:34:11] Can't kekulize mol.  Unkekulized atoms: 1 2 16 17 18 20 21 22 23\n",
      "[22:34:11] SMILES Parse Error: unclosed ring for input: 'CC1(C)[C@H](O)CC[C@@]2(C)[C@H]1CC[C@]13CC[C@@]4(CC(=O)CC[C@]4(C)CC[C@]13C)[C@@H]1C[C@@H](O)C2(C)C'\n",
      "[22:34:11] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[22:34:11] Can't kekulize mol.  Unkekulized atoms: 14 15 16 25 26\n",
      "[22:34:11] SMILES Parse Error: unclosed ring for input: 'COCCOC(=O)c1c2c(n(-c3ccc(Cl)cc3)c(=O)c2nc4ccccc4n22)CCCC3'\n",
      "[22:34:11] Can't kekulize mol.  Unkekulized atoms: 11 12 13 24 25\n",
      "[22:34:11] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 19 20 21 22\n",
      "[22:34:12] Explicit valence for atom # 2 C, 5, is greater than permitted\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 2 3 11\n",
      "[22:34:12] SMILES Parse Error: unclosed ring for input: 'C[C@H](O)[C@]1(CN)CCCN(C)c1ccccc1'\n",
      "[22:34:12] SMILES Parse Error: extra open parentheses for input: 'Fc1ccc(-c2noc(CSc3nc(N)nc(N)n3)c2'\n",
      "[22:34:12] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNC[C@H]2C(=O)O[C@@H]3C[C@@]4(C)CCC[C@H]([C@H]5C)[C@@H]32)cc1OC'\n",
      "[22:34:12] SMILES Parse Error: unclosed ring for input: 'CN(C)CCCn1c2nc(Cc3ccccc3)c(S(=O)(=O)c3ccc(Cl)cc3)c3cc(Cl)ccc21'\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 29\n",
      "[22:34:12] SMILES Parse Error: unclosed ring for input: 'CC1CCN(c2cc(S(=O)(=O)N3CCCc3ccccc32)ccn2)CC1'\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 17 19 20 21 29\n",
      "[22:34:12] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)[C@@H](C(=O)N1CCC[C@H]1C(=O)N1)CCC2'\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 3 4 18 19 20\n",
      "[22:34:12] SMILES Parse Error: unclosed ring for input: 'O=c1cc(CN(Cc2c(F)cccc2Cl)CCCC2)nc2c(F)cccc12'\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 5 6 8\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 3 4 6 14 15\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 17 18 19 20 21 22 23\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 1 2 5\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 28\n",
      "[22:34:12] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 17\n",
      "[22:34:13] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 18 19\n",
      "[22:34:13] SMILES Parse Error: unclosed ring for input: 'O[C@H]1[C@H]2[C@H]3[C@@H]2[C@@H]2[C@H]5[C@@H]4[C@@H]41'\n",
      "[22:34:13] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(CNS(=O)(=O)c2cccc(Cl)c2'\n",
      "[22:34:13] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10\n",
      "[22:34:13] SMILES Parse Error: unclosed ring for input: 'CCCc1cc(C(=O)N2CCC[C@@H](N(CCCO)CC2CC2)C2CCCC2)cn1C'\n",
      "[22:34:13] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 15 16 17 19 20\n",
      "[22:34:13] SMILES Parse Error: unclosed ring for input: 'COc1ccc(N2C(=O)[C@@H]3[C@@H](C2=O)[C@]2(C)C(=O)[C@@](C)(OC(C)=O)[C@@]2(O)C(F)(F)F)cc1'\n",
      "[22:34:13] SMILES Parse Error: extra close parentheses while parsing: Cn1c(SCC(=O)N[C@@H]2CCCC[C@H]2C)nnc1-c1cccs1)NC(=O)c1ccccc1\n",
      "[22:34:13] SMILES Parse Error: Failed parsing SMILES 'Cn1c(SCC(=O)N[C@@H]2CCCC[C@H]2C)nnc1-c1cccs1)NC(=O)c1ccccc1' for input: 'Cn1c(SCC(=O)N[C@@H]2CCCC[C@H]2C)nnc1-c1cccs1)NC(=O)c1ccccc1'\n",
      "[22:34:13] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 19 23 24\n",
      "[22:34:13] Can't kekulize mol.  Unkekulized atoms: 2 3 7\n",
      "[22:34:13] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 24 27\n",
      "[22:34:13] SMILES Parse Error: unclosed ring for input: 'Cn1c(CO)nnc1[C@@H]1CCCN1c1ncnc3ccsc12'\n",
      "[22:34:14] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[22:34:14] SMILES Parse Error: unclosed ring for input: 'O=C1O[C@H](c2ccc3c(c2)OCO3)[C@@H]2C(=O)O1'\n",
      "[22:34:14] Can't kekulize mol.  Unkekulized atoms: 12 13 14 28 29\n",
      "[22:34:14] SMILES Parse Error: extra close parentheses while parsing: Cc1nc2ccc(NC(=O)N[C@@H]3C[C@H]4C(=O)N(C)C(=O)OC4)cc3s2)cc1C\n",
      "[22:34:14] SMILES Parse Error: Failed parsing SMILES 'Cc1nc2ccc(NC(=O)N[C@@H]3C[C@H]4C(=O)N(C)C(=O)OC4)cc3s2)cc1C' for input: 'Cc1nc2ccc(NC(=O)N[C@@H]3C[C@H]4C(=O)N(C)C(=O)OC4)cc3s2)cc1C'\n",
      "[22:34:14] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(CN2CCn3c(Cn4cccn3)C[C@H]2CO)cc1OC'\n",
      "[22:34:14] SMILES Parse Error: ring closure 1 duplicates bond between atom 10 and atom 11 for input: 'O=C(NC1CCCC1)NC(C1C1CC1)C1CCNCC1'\n",
      "[22:34:14] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 22\n",
      "[22:34:14] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6\n",
      "[22:34:14] Can't kekulize mol.  Unkekulized atoms: 4 5 7 22 23\n",
      "[22:34:14] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1[C@H](N)C[C@H]2N1'\n",
      "[22:34:14] SMILES Parse Error: unclosed ring for input: 'CC1(C)C[C@H]2[C@H]1CO[C@]1(C)C(=O)N(C)C[C@@H]1c1ccc3c(c1)OCO3'\n",
      "[22:34:14] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[22:34:14] SMILES Parse Error: unclosed ring for input: 'COc1cc(CN2CCc3nnc([C@@H]3CCCN(C(C)=O)C3)C2)ccc1F'\n",
      "[22:34:14] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 13 22 24\n",
      "[22:34:14] SMILES Parse Error: extra open parentheses for input: 'O=c1c(Cl)c(Cl)sn1Clc1=N\\[C@H](O'\n",
      "[22:34:14] non-ring atom 21 marked aromatic\n",
      "[22:34:15] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 17 18 20 21 23 24\n",
      "[22:34:15] SMILES Parse Error: ring closure 2 duplicates bond between atom 15 and atom 16 for input: 'O=C(Cn1ccc2ccccc21)N[C@@H]1C[C@H]2c2ccccc2OC1'\n",
      "[22:34:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 17\n",
      "[22:34:15] SMILES Parse Error: unclosed ring for input: 'O[C@H]1CC[C@H]2[C@H](C1)c1c(-c3ccccc3)cnc(O)c2C(=O)O2'\n",
      "[22:34:15] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[22:34:15] Can't kekulize mol.  Unkekulized atoms: 7 8 19 20 21 22 23\n",
      "[22:34:15] Can't kekulize mol.  Unkekulized atoms: 10 11 20 21 22\n",
      "[22:34:15] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 19 20 21\n",
      "[22:34:15] SMILES Parse Error: extra close parentheses while parsing: COCCN1[C@H]2CO[C@H](COC[C@H]2CCCO)C1)CC=C1CCCC1\n",
      "[22:34:15] SMILES Parse Error: Failed parsing SMILES 'COCCN1[C@H]2CO[C@H](COC[C@H]2CCCO)C1)CC=C1CCCC1' for input: 'COCCN1[C@H]2CO[C@H](COC[C@H]2CCCO)C1)CC=C1CCCC1'\n",
      "[22:34:15] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1C[C@H]1CCCN1C(=O)c1cnc2c(c1)C(C)(C)CO'\n",
      "[22:34:16] SMILES Parse Error: ring closure 3 duplicates bond between atom 16 and atom 17 for input: 'OCC[C@H]1SC2=C(CCN3Cc3ccccc3N3)CCC[C@H]12'\n",
      "[22:34:16] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(C)c(CC(=O)NCCCn2ccnc2)c2ccccc2)c1OC\n",
      "[22:34:16] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C)c(CC(=O)NCCCn2ccnc2)c2ccccc2)c1OC' for input: 'Cc1cc(C)c(CC(=O)NCCCn2ccnc2)c2ccccc2)c1OC'\n",
      "[22:34:16] Explicit valence for atom # 19 C, 5, is greater than permitted\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'O=C(O)C[C@@]1(c2ccccc2)CC(=O)N(C)2'\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'COc1cc([C@@H]2Oc3ccccc3[C@H]3[C@H](C(=O)OCC)[C@H]4CC[C@]3(C)[C@H]2O)cc1OC'\n",
      "[22:34:16] Can't kekulize mol.  Unkekulized atoms: 5 6 9\n",
      "[22:34:16] Can't kekulize mol.  Unkekulized atoms: 11 12 21\n",
      "[22:34:16] SMILES Parse Error: extra close parentheses while parsing: O=C(c1cc2nc[nH]n2)c(O)n1)N(Cc1ccncc1)Cc1ccc3c(c1)OCO2\n",
      "[22:34:16] SMILES Parse Error: Failed parsing SMILES 'O=C(c1cc2nc[nH]n2)c(O)n1)N(Cc1ccncc1)Cc1ccc3c(c1)OCO2' for input: 'O=C(c1cc2nc[nH]n2)c(O)n1)N(Cc1ccncc1)Cc1ccc3c(c1)OCO2'\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'COc1cc(CN2CCc3nc([C@@H]3CCCNC3)ncc2C(F)(F)C2)ccc1F'\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'O=C(O)[C@@]12CCN(C(=O)OCC3CCCCC3)[C@@H]1CCN(C3CC1)C2'\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CN1CCN(OC2CC3)CC[C@@]2(C)C1'\n",
      "[22:34:16] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 20\n",
      "[22:34:16] Can't kekulize mol.  Unkekulized atoms: 4 19 20 21 22 23 24 25 26\n",
      "[22:34:16] Can't kekulize mol.  Unkekulized atoms: 6 7 22\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'CSc1nn2c(=O)n(C3CCCC3)c(=O)c3cc(F)ccc2[C@@H]1N'\n",
      "[22:34:16] Can't kekulize mol.  Unkekulized atoms: 2 3 23 26 27 28 29 30 31\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'COc1ccnc(N2C[C@@H]3[C@H](C2)OCc3ccccc32)n1'\n",
      "[22:34:16] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1nc(-c2ccc3[nH]c(Br)cc24)ns1)c1ccc(I)cc1'\n",
      "[22:34:17] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1[C@@H](NC(=O)c2ccccc2Cl)CC(=O)N2CCC1=O'\n",
      "[22:34:17] Can't kekulize mol.  Unkekulized atoms: 3 4 13\n",
      "[22:34:17] Can't kekulize mol.  Unkekulized atoms: 4 5 21\n",
      "[22:34:17] Can't kekulize mol.  Unkekulized atoms: 3 4 18\n",
      "[22:34:17] Explicit valence for atom # 25 O, 3, is greater than permitted\n",
      "[22:34:17] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C(=O)N[C@H]2C[C@@]3(C[C@H]3CN(C)C(=O)O4)CCC(=O)N2C)c1'\n",
      "[22:34:17] SMILES Parse Error: unclosed ring for input: 'CC[C@]1(C(=O)/N=C(/C)NC(=O)N(CC)C2=O)CCN(C(=O)C2CCC2)C1'\n",
      "[22:34:17] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1ccc(NC(=O)c2nc([C@H]3CCS(=O)(=O)c3ccccc3O)sc2)cc1'\n",
      "[22:34:17] SMILES Parse Error: unclosed ring for input: 'CC(C)c1n[nH]c([C@H]2CN(C(=O)[C@@H]3CCc3ccc(F)cc3O)CCO2)n1'\n",
      "[22:34:17] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 24\n",
      "[22:34:17] Can't kekulize mol.  Unkekulized atoms: 11 12 14 16 17 18 19\n",
      "[22:34:17] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(Cl)cc1N(CCCC(=O)NCCN1C(=O)S/C(=C\\c2ccccc2Cl)C1=O'\n",
      "[22:34:17] SMILES Parse Error: unclosed ring for input: 'O=C(O)[C@@H]1[C@@H](O)[C@H]2C3[C@@H]4[C@@H]2[C@@H]5[C@@H]3[C@@H]41'\n",
      "[22:34:18] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2[C@@H](C(=O)N3C[C@@H]4C[C@H](O)C[C@H]3C3CC3)CCC(=O)N2C)cc1'\n",
      "[22:34:18] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)C[C@@H]2[C@H]3CC[C@H]2CC(=O)N3'\n",
      "[22:34:18] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[22:34:18] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 29\n",
      "[22:34:18] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 22 23 24 25 26 27 28 30\n",
      "[22:34:18] Can't kekulize mol.  Unkekulized atoms: 1 2 3 15\n",
      "[22:34:18] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1[C@@H]1CN1CCN(C(=O)CCn2cccn2)CC1'\n",
      "[22:34:18] Can't kekulize mol.  Unkekulized atoms: 10 11 18 19 20 21 22 23 24\n",
      "[22:34:19] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)CC[C@@H]2[C@H](CCCN2Cc2ccco3)CC[C@H]1C'\n",
      "[22:34:19] SMILES Parse Error: unclosed ring for input: 'CCN1C[C@H]2C3(CCN(C(=O)c4ccccn4)CC3)CC[C@@]2(C(=O)N3CCCC2)C1'\n",
      "[22:34:19] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 17\n",
      "[22:34:19] Explicit valence for atom # 17 C, 5, is greater than permitted\n",
      "[22:34:19] SMILES Parse Error: unclosed ring for input: 'O=C(c1cnc(-c2ncccn2)s1)N1C2CC3C[C@@H]1CCCN2c1cccc(F)c1'\n",
      "[22:34:19] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)CSc2nnc(Nc3ccccc3C)c2-c3ccccc3)[nH]c2c1'\n",
      "[22:34:19] Can't kekulize mol.  Unkekulized atoms: 1 3\n",
      "[22:34:19] Can't kekulize mol.  Unkekulized atoms: 2 3 21 22 24 25 26\n",
      "[22:34:19] Can't kekulize mol.  Unkekulized atoms: 3 7 8\n",
      "[22:34:19] SMILES Parse Error: unclosed ring for input: 'Cc1nnc(SCC[C@@H]2[C@H]3CC[C@@H]4[C@H]2CC[C@]33C[C@@H](O)C2)n1Cc1ccccc1'\n",
      "[22:34:19] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9\n",
      "[22:34:19] Can't kekulize mol.  Unkekulized atoms: 6 7 18\n",
      "[22:34:19] SMILES Parse Error: extra close parentheses while parsing: O=C(Cc1ccc2c(c1)CCCC2)N[C@H]1CCCc2[nH]1)c1ccccc1O\n",
      "[22:34:19] SMILES Parse Error: Failed parsing SMILES 'O=C(Cc1ccc2c(c1)CCCC2)N[C@H]1CCCc2[nH]1)c1ccccc1O' for input: 'O=C(Cc1ccc2c(c1)CCCC2)N[C@H]1CCCc2[nH]1)c1ccccc1O'\n",
      "[22:34:19] SMILES Parse Error: extra close parentheses while parsing: CC12C[C@H]3C[C@@H](C1)CC(C(=O)N[C@@H]1CC(=O)N(c4ccc(Cl)cc4)C1)C3)C2\n",
      "[22:34:19] SMILES Parse Error: Failed parsing SMILES 'CC12C[C@H]3C[C@@H](C1)CC(C(=O)N[C@@H]1CC(=O)N(c4ccc(Cl)cc4)C1)C3)C2' for input: 'CC12C[C@H]3C[C@@H](C1)CC(C(=O)N[C@@H]1CC(=O)N(c4ccc(Cl)cc4)C1)C3)C2'\n",
      "[22:34:19] SMILES Parse Error: unclosed ring for input: 'CCCNC(=O)c1cc2n(c(=O)c1cnccn2C)n1Cc1ccccc1F'\n",
      "[22:34:20] SMILES Parse Error: unclosed ring for input: 'CN(C)S(=O)(=O)c1cccc2cc(C(=O)N3CCN4CCS4(=O)=O)cnc2c1'\n",
      "[22:34:20] SMILES Parse Error: extra close parentheses while parsing: Cc1nc(C)c(C(=O)N[C@@H]2CCCc3c2F)cnn3C1C(C)C)C1\n",
      "[22:34:20] SMILES Parse Error: Failed parsing SMILES 'Cc1nc(C)c(C(=O)N[C@@H]2CCCc3c2F)cnn3C1C(C)C)C1' for input: 'Cc1nc(C)c(C(=O)N[C@@H]2CCCc3c2F)cnn3C1C(C)C)C1'\n",
      "[22:34:20] SMILES Parse Error: unclosed ring for input: 'O=C(Oc1ccc(Cl)c2ccccc12)N1CCC(C(=O)Nc2ccccc2OC(F)(F)C2)CC1'\n",
      "[22:34:20] SMILES Parse Error: unclosed ring for input: 'O=Cc1c(Sc2ncc(-c3nnc(-c4ccccc4)o3)n3C2CCCCC2)nc2ccccn12'\n",
      "[22:34:20] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 16\n",
      "[22:34:20] Can't kekulize mol.  Unkekulized atoms: 1 2 3 19 20\n",
      "[22:34:20] Can't kekulize mol.  Unkekulized atoms: 2 4 5 6 20 21 22\n",
      "[22:34:20] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[22:34:20] SMILES Parse Error: unclosed ring for input: 'COc1cc2nc3c(c2cc1NC(=O)CC1)NC(=O)CO3'\n",
      "[22:34:20] Can't kekulize mol.  Unkekulized atoms: 7 8 10 11 12 16 17\n",
      "[22:34:20] SMILES Parse Error: unclosed ring for input: 'Cn1cc(-c2ccnc(OCC(=O)NC3CC4)c2)cn1'\n",
      "[22:34:20] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 19 23 24\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'CN1C2C(=O)[C@]2(NC(=O)c3ccccc3NC(=O)c3ccc(Cl)cc3)[C@H](C)C(=O)N2C1=O'\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'CCN(C(=O)COC(=O)COc1ccc(Cl)cc11)[C@H]1CCS(=O)(=O)C1'\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'CCn1c(C)cc(O)c(C(c2ccc(OC)c(OC)c2)c2=O)c1'\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccc[nH]1)N1CCN2[C@@H](COC[C@H]3C2CC2)CN1'\n",
      "[22:34:21] SMILES Parse Error: syntax error while parsing: O=C(c1cscc1C(F)(F)F)N1CC[C@@H](c2n[nH]cc2-)o1\n",
      "[22:34:21] SMILES Parse Error: Failed parsing SMILES 'O=C(c1cscc1C(F)(F)F)N1CC[C@@H](c2n[nH]cc2-)o1' for input: 'O=C(c1cscc1C(F)(F)F)N1CC[C@@H](c2n[nH]cc2-)o1'\n",
      "[22:34:21] Can't kekulize mol.  Unkekulized atoms: 8 9 11\n",
      "[22:34:21] Can't kekulize mol.  Unkekulized atoms: 11 12 13\n",
      "[22:34:21] Can't kekulize mol.  Unkekulized atoms: 11 12 25 26 27\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C(=O)N2C[C@H]3C[C@@H](C2)[C@H](CCC(C)C)N2C(=O)OCCc2ccccc23)n1'\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'O=C1CCOc2cc3cc(C(=O)O)ccc3c2cc1N1CCCC[C@@H]12'\n",
      "[22:34:21] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CN=c2ccnc3[nH]cnc23'\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'COc1ccc(F)cc1-c1ccccc1C[C@@H]1CC(=O)Nc2[nH]nc3c(N)c(C(=O)O)cc13'\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'Fc1ccc(N2CCN(Cc3ccc4c(c3)CN([C@@H]3CCSC3)C[C@@H]44)C2=O)cc1'\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)C[C@@H]2C[C@@]13CS(=O)(=O)c3ccccc3N21'\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2noc(C2=Cc4c(c(Br)c5c(c4OC)OCO5)CCN3C)n2)cc1'\n",
      "[22:34:22] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[22:34:22] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 13 16 17\n",
      "[22:34:22] Can't kekulize mol.  Unkekulized atoms: 14 15 22 23 24 25 26 27 28\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCNC(=O)c2cc3[n+](nc3cnc4c(C)nnn4c3C)ccc2OC)cc1'\n",
      "[22:34:22] Can't kekulize mol.  Unkekulized atoms: 2 12 13 14 15\n",
      "[22:34:22] SMILES Parse Error: extra close parentheses while parsing: CC(C)NC(=O)CN1C(=O)N[C@](C)(c2ccc3c(c2)OCCCO4)C1=O)c1ccccc1\n",
      "[22:34:22] SMILES Parse Error: Failed parsing SMILES 'CC(C)NC(=O)CN1C(=O)N[C@](C)(c2ccc3c(c2)OCCCO4)C1=O)c1ccccc1' for input: 'CC(C)NC(=O)CN1C(=O)N[C@](C)(c2ccc3c(c2)OCCCO4)C1=O)c1ccccc1'\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'COc1ccc(S(=O)(=O)C2CN(C(=O)[C@@H]3CSCN3C(=O)c2ccc(Cl)cc2)CC3)cc1'\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@@H]1C(=O)Nc2nc3nc(c4ccccc4)nn2C1)c1cccc(O)c1'\n",
      "[22:34:22] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 16 18 19\n",
      "[22:34:22] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@H]1CC[C@@H]2OCCCN(C(=O)c3ccc(OC)c(OC)c3)C2'\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'CCCCNC(=O)N1CCC[C@H]2CCc3cnc(C(=O)NCC)nc31'\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)N1CCC(Nc2nc3c(c(C(F)(F)F)n3)CCC3)CC1'\n",
      "[22:34:23] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23 24 25 26 27\n",
      "[22:34:23] SMILES Parse Error: syntax error while parsing: Cc1sc2ncn(C3CCN(C(=O)c4csc5c(C(F)(F)F)ccc(5)c55)CC4)c(=O)c2c1C\n",
      "[22:34:23] SMILES Parse Error: Failed parsing SMILES 'Cc1sc2ncn(C3CCN(C(=O)c4csc5c(C(F)(F)F)ccc(5)c55)CC4)c(=O)c2c1C' for input: 'Cc1sc2ncn(C3CCN(C(=O)c4csc5c(C(F)(F)F)ccc(5)c55)CC4)c(=O)c2c1C'\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2CCC[C@]3(CCc4c(-c5ccccc5)CCN(C(C)C)(C4CC3)C3)C2)cc1'\n",
      "[22:34:23] Can't kekulize mol.  Unkekulized atoms: 4 5 14\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(CSc2ccccn2)N(C)C(=O)CC(C)(C)C'\n",
      "[22:34:23] Can't kekulize mol.  Unkekulized atoms: 5 6 7 12 13\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'Cc1nn(-c2ccccc2)cc1C(=O)N[C@H](C)C12CC4CC(CC(C3)C1)C2'\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(Cl)c1F)c1cccc2c3c(ccc12)CC2'\n",
      "[22:34:23] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 8 9 24 25 26\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'COc1cc(C)c(S(=O)(=O)Nc2ccc3c(c2)NC(=O)C(C)(C)O4)c(C)c1C'\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'CN(C)c1nccc([C@@H]2C[C@@H]3CCN2Cc3cnc[nH]3)nc2n1'\n",
      "[22:34:23] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[22:34:23] Can't kekulize mol.  Unkekulized atoms: 19 20 23\n",
      "[22:34:23] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)n(CCc2nc3c4c(s2)C[C@@H]2CN([C@@H](C)C(=O)O)CC4)CO[C@@H]21'\n",
      "[22:34:24] SMILES Parse Error: unclosed ring for input: 'CCc1nnc([C@H]2CN(C(=O)c2ccc4c(c3)CCC4)C[C@H]2C)o1'\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 17\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 6 7 9 10 25 28 29\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 16 17\n",
      "[22:34:24] SMILES Parse Error: extra open parentheses for input: 'Cc1sc(NC(=O)COC(=O)c2c(C)noc2N)c(C(=O)O'\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 18\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 7 8 19 20 21 22 23\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 6 7 18\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 4 5 6 11 12\n",
      "[22:34:24] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2[C@H](C(=O)N3C[C@@H]4C[C@H](O)C3)CC(C)=C2C(=O)OC2CCCCC2)cc1OC'\n",
      "[22:34:24] SMILES Parse Error: extra close parentheses while parsing: O=C1/[C@@H](Cc2ccccc2)C(=O)N(CCc2ccccc2)C1=O)NCCCSCc1ccccc1\n",
      "[22:34:24] SMILES Parse Error: Failed parsing SMILES 'O=C1/[C@@H](Cc2ccccc2)C(=O)N(CCc2ccccc2)C1=O)NCCCSCc1ccccc1' for input: 'O=C1/[C@@H](Cc2ccccc2)C(=O)N(CCc2ccccc2)C1=O)NCCCSCc1ccccc1'\n",
      "[22:34:24] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1Cc2cc(C(=O)N3CC[C@@H](c4nc5c(s(=O)N4CCN(C)CC5)C4)ccc3C2)ccc1O'\n",
      "[22:34:24] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCN2C3=NCC(CC(C)C)C2)cc1OC'\n",
      "[22:34:24] SMILES Parse Error: unclosed ring for input: 'c1ccc(CN2CC[C@@]3(COCC4CCN(Cc4cccnc4)C3)C2)cc1'\n",
      "[22:34:24] SMILES Parse Error: unclosed ring for input: 'Cc1nn(-c2ncnc3[nH]cnc23)c2c1[C@H](c1c(F)ncn2C)n2C'\n",
      "[22:34:24] Can't kekulize mol.  Unkekulized atoms: 1 2 3 18 19\n",
      "[22:34:25] SMILES Parse Error: extra close parentheses while parsing: O=C(NCC1(O)CSCCSC1)C(=O)O)c1cc(Br)ccc1F\n",
      "[22:34:25] SMILES Parse Error: Failed parsing SMILES 'O=C(NCC1(O)CSCCSC1)C(=O)O)c1cc(Br)ccc1F' for input: 'O=C(NCC1(O)CSCCSC1)C(=O)O)c1cc(Br)ccc1F'\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'CO[C@H]1C[C@H](c2nnc(C)o2)N(C(=O)[C@H]2SCC(=O)OC23CCCCC2)C1'\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)C(=O)NC[C@H]1CN(Cc2cccc(F)c2)C2(CCN(C3CCS(C)(=O)=O)C3)CO1'\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)C[C@H]2[C@H](C1)N(C(=O)c1ccccn1)C[C@H]1OCC2'\n",
      "[22:34:25] Explicit valence for atom # 6 Cl, 2, is greater than permitted\n",
      "[22:34:25] Can't kekulize mol.  Unkekulized atoms: 1 2 12\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C(C)C)nc(N2CCC[C@@H]3[C@@H](NC(=O)c5ccccn4)CO[C@@H]32)n1'\n",
      "[22:34:25] SMILES Parse Error: extra close parentheses while parsing: O=C1[C@@H]2[C@@H]3C[C@@H]4[C@@H]2S[C@@H]2[C@@H]13)[C@H]1[C@H]2[C@H]3C41O1CCO2\n",
      "[22:34:25] SMILES Parse Error: Failed parsing SMILES 'O=C1[C@@H]2[C@@H]3C[C@@H]4[C@@H]2S[C@@H]2[C@@H]13)[C@H]1[C@H]2[C@H]3C41O1CCO2' for input: 'O=C1[C@@H]2[C@@H]3C[C@@H]4[C@@H]2S[C@@H]2[C@@H]13)[C@H]1[C@H]2[C@H]3C41O1CCO2'\n",
      "[22:34:25] Can't kekulize mol.  Unkekulized atoms: 6 7 8 10 26\n",
      "[22:34:25] Can't kekulize mol.  Unkekulized atoms: 8 9 10\n",
      "[22:34:25] non-ring atom 10 marked aromatic\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'Cc1c(CNC(=O)N[C@@H]2CCCC[C@@H]3C[C@H]22)nnn1C'\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CC(=O)N=[C@@H]2S[C@@H](C)C(=O)N1C'\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'CCc1nnc(N2C(=O)[C@@H]3CC=C(C4=Nc5ccccc5)nn3[C@H]2C)s1'\n",
      "[22:34:25] SMILES Parse Error: ring closure 2 duplicates bond between atom 13 and atom 14 for input: 'O=C1C[C@H](C(=O)N2CCSCC2)C[C@@H]2c2ccccc1F'\n",
      "[22:34:25] SMILES Parse Error: unclosed ring for input: 'C[C@@]1(C(=O)O[C@H](C)C(=O)N2C[C@H]2CCC[C@@H]2C)CCCC[C@@H]1O'\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1ncc2c(C=O)n(Cc3ccccc3)nc1-c1ccc(Cl)cc1'\n",
      "[22:34:26] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'NC(=O)c1ccc(Cn2cc(N=c3[nH]c4ccccc4[nH]3)CC2CCCC2)cc1'\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 15 16 17 20 21 22 23 24 25\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 1 2 14\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'COCCn1c(C(C)C)cc2(C)c3c(sc2C1CCN(C(C)=O)CC1)O2'\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'O=C(CN1[C@@H]2C[C@@H]3C[C@@H]2C1)c1ccccc1O2'\n",
      "[22:34:26] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc(-n2ncc3c(O)nc(C4CCN(C(=O)Cc5ccc(F)cc5)nn44)c3CCCC4)n2)cc1\n",
      "[22:34:26] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(-n2ncc3c(O)nc(C4CCN(C(=O)Cc5ccc(F)cc5)nn44)c3CCCC4)n2)cc1' for input: 'Cc1ccc(-n2ncc3c(O)nc(C4CCN(C(=O)Cc5ccc(F)cc5)nn44)c3CCCC4)n2)cc1'\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 2 8 24\n",
      "[22:34:26] Explicit valence for atom # 4 S, 7, is greater than permitted\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 28 29 30\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'CC1=C(C)C[C@@H](C(=O)Nc2ccc(-n3cnnn3)cc2)S(C)(=O)=O'\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'Cc1nnc2n1C[C@H](NCc1ccccc1OCC(=O)N(C)C1)CC2'\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'CC1=NN(c2ccccc2)[C@@H]2O[C@@]3(C)[C@@H]4[C@@](C)(O[C@@H]5OC(C)=O)[C@@]5(C)C(C)=C[C@H]3[C@H]21'\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 11 14 15\n",
      "[22:34:26] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 11 12 13 16 25 26 32\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 22 23 24\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 11\n",
      "[22:34:26] SMILES Parse Error: extra close parentheses while parsing: CCn1cc(COc2cccc(C)c2)c(=O)n2CCC(=O)NCc2ccc(OC)cc2)c1\n",
      "[22:34:26] SMILES Parse Error: Failed parsing SMILES 'CCn1cc(COc2cccc(C)c2)c(=O)n2CCC(=O)NCc2ccc(OC)cc2)c1' for input: 'CCn1cc(COc2cccc(C)c2)c(=O)n2CCC(=O)NCc2ccc(OC)cc2)c1'\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)COc2ccc(F)c3c2C(=O)C[C@@H]3c2nc2ccccc3s2)cc1'\n",
      "[22:34:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 35\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'O=C(c1cnccn1)N1C[C@H]2C[C@@H](C1)N(Cc1cscn1)Cc2ccccc12'\n",
      "[22:34:26] SMILES Parse Error: unclosed ring for input: 'C[C@H](O)C1(CNc2ncnc3nc(C(F)(F)F)cn2C2)CCOCC1'\n",
      "[22:34:27] SMILES Parse Error: unclosed ring for input: 'NC(=O)c1cccc2c3c(ccc13)CCN(Cc1ccc(C(=O)O)cc1)C3'\n",
      "[22:34:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 22 23 24 25\n",
      "[22:34:27] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@@H]1O[C@H]2[C@@H](OC(=O)c3ccccc3)Oc2ccccc2O1)[C@@H]1Cc2ccccc2O1'\n",
      "[22:34:27] SMILES Parse Error: unclosed ring for input: 'Cc1cc(NC(=O)N2CC2(Cc3ccccc3OC2)CC(C)(C)C)on1'\n",
      "[22:34:27] SMILES Parse Error: unclosed ring for input: 'C[C@]12COc3ccccc3[C@@H]2[C@H]3CC(c4ccc(Br)cc4)=CN[C@@H]3[C@@H]21'\n",
      "[22:34:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 8 17 18\n",
      "[22:34:27] SMILES Parse Error: extra close parentheses while parsing: C=CC(=O)N[C@@H]1[C@H]2[C@@H]3CC[C@@H]4[C@H]5CC[C@H](CC(C)C)CC[C@]45C)[C@@H]3C[C@H]2C[C@]14C\n",
      "[22:34:27] SMILES Parse Error: Failed parsing SMILES 'C=CC(=O)N[C@@H]1[C@H]2[C@@H]3CC[C@@H]4[C@H]5CC[C@H](CC(C)C)CC[C@]45C)[C@@H]3C[C@H]2C[C@]14C' for input: 'C=CC(=O)N[C@@H]1[C@H]2[C@@H]3CC[C@@H]4[C@H]5CC[C@H](CC(C)C)CC[C@]45C)[C@@H]3C[C@H]2C[C@]14C'\n",
      "[22:34:27] Can't kekulize mol.  Unkekulized atoms: 6 7 10\n",
      "[22:34:28] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 9 23\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'Cc1cc(N2CCN(C[C@@H](C)[C@H](O)COc3ccccc2)CC2)ncn1'\n",
      "[22:34:28] non-ring atom 8 marked aromatic\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'C[C@H]1Oc2ccc(CN(C)c3nc(C(F)(F)F)nc(C(F)(F)F)c3C)c2c2Oc3ccccc3Oc3cc(O)ccc12'\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'Cc1c(C)c2nc3c(C)c(C)c(=O)oc3c2n2C=C(C)C[C@H]2O'\n",
      "[22:34:28] SMILES Parse Error: duplicated ring closure 2 bonds atom 7 to itself for input: 'O=C(NC[C@@H]1CC[C@@]22CCCN(CCc3ccccc3)C2)C(=O)N1Cc1ccccc1'\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'Cn1c(CN2CCOc3ccc(F)cc32)nc2c1c(=O)n(Cc1ccc2c(c1)OCO3)c(=O)n2C'\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'CSCc1nc(CNC(=O)[C@@H]2CC[C@]3(CCc4ccccc4C(=O)O)C2)cs1'\n",
      "[22:34:28] SMILES Parse Error: syntax error while parsing: O=C(O)c1ccc(S(=O)(=O)c2ccc3c(c2)C(=O)N(CCc2ccccc2)C==O)cc1\n",
      "[22:34:28] SMILES Parse Error: Failed parsing SMILES 'O=C(O)c1ccc(S(=O)(=O)c2ccc3c(c2)C(=O)N(CCc2ccccc2)C==O)cc1' for input: 'O=C(O)c1ccc(S(=O)(=O)c2ccc3c(c2)C(=O)N(CCc2ccccc2)C==O)cc1'\n",
      "[22:34:28] Explicit valence for atom # 15 C, 5, is greater than permitted\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1N1C[C@H]2C[C@@H](C1CC1)OCC2'\n",
      "[22:34:28] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 2 for input: 'O=C1[C@@H]1CN(C(=O)c2ccc3nc[nH]c3c2)CCN1'\n",
      "[22:34:28] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H](CNC(=O)c2cccc(-n3)cn3)c2ccccc2)cc1'\n",
      "[22:34:28] Can't kekulize mol.  Unkekulized atoms: 1 4 26\n",
      "[22:34:28] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 17\n",
      "[22:34:28] Can't kekulize mol.  Unkekulized atoms: 15 16 17 20 21 22 23 24 25\n",
      "[22:34:28] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 19 20 21\n",
      "[22:34:28] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[22:34:28] Explicit valence for atom # 11 O, 3, is greater than permitted\n",
      "[22:34:29] SMILES Parse Error: unclosed ring for input: 'C1=C2[C@@H]3[C@@H]4[C@H]5[C@@H]2[C@@H]1[C@@H]2[C@@H]4[C@@H]4C2(O)[C@@H]1[C@@H]5Cl'\n",
      "[22:34:29] SMILES Parse Error: unclosed ring for input: 'O=C(CS1(=O)=O)C(=O)Nc1ccc(F)c(Cl)c1'\n",
      "[22:34:29] Can't kekulize mol.  Unkekulized atoms: 1 2 11\n",
      "[22:34:29] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[22:34:29] SMILES Parse Error: unclosed ring for input: 'CC1=C(C)[C@@]2(C)[C@@H]3C=C[C@@H]3[C@H]2[C@H]2O1'\n",
      "[22:34:29] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17\n",
      "[22:34:29] Explicit valence for atom # 21 O, 3, is greater than permitted\n",
      "[22:34:29] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 2 for input: 'O=C1C1=CC[C@@H](CNCCN2CCCCC2)c2ccccc21'\n",
      "[22:34:29] SMILES Parse Error: unclosed ring for input: 'CC(C)n1cc([C@]2(O)CCN(Cc3c(sc4c(C)ccc(C)c35)CC2)CO2)cc1'\n",
      "[22:34:29] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 24 25 27\n",
      "[22:34:29] SMILES Parse Error: extra close parentheses while parsing: CC(=O)c1c(C)[nH]c(C(=O)NS2(=O)=O)C2(C)C)c1C\n",
      "[22:34:29] SMILES Parse Error: Failed parsing SMILES 'CC(=O)c1c(C)[nH]c(C(=O)NS2(=O)=O)C2(C)C)c1C' for input: 'CC(=O)c1c(C)[nH]c(C(=O)NS2(=O)=O)C2(C)C)c1C'\n",
      "[22:34:29] SMILES Parse Error: unclosed ring for input: 'O=C(C1CCC2)N1Cc2ccnn2C[C@@H](CNc2ccccn2)C1'\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc2nscc2c1)c1cn([C@@H]3CCCN(Cc4ccccc4)C2)nn12'\n",
      "[22:34:30] SMILES Parse Error: syntax error while parsing: O=C1CC2=C(N=C2N=C2c3ccccc3NC2=O)NC==\n",
      "[22:34:30] SMILES Parse Error: Failed parsing SMILES 'O=C1CC2=C(N=C2N=C2c3ccccc3NC2=O)NC==' for input: 'O=C1CC2=C(N=C2N=C2c3ccccc3NC2=O)NC=='\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'CC1(C)[C@H](O)CC[C@@]2CCC[C@H]2[C@H]1CCCO1'\n",
      "[22:34:30] Can't kekulize mol.  Unkekulized atoms: 4 5 6 14 15\n",
      "[22:34:30] SMILES Parse Error: extra open parentheses for input: 'CCCOc1ccc([C@H]2c3c(oc4ccc(F)cc4c3=O)C(=O)N2CCN2CCN(C)CC2'\n",
      "[22:34:30] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'O=C(Nn1c(SCc2ccccc2)c2cc(=O)[nH]c3c(O)nc(O)cc21)C12CC3CC(CC(C3)C1)C2'\n",
      "[22:34:30] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1[C@H]1CC1(=O)c2cccc(OCC(=O)Nc3ccc(C)cc3)c2O)cc1\n",
      "[22:34:30] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1[C@H]1CC1(=O)c2cccc(OCC(=O)Nc3ccc(C)cc3)c2O)cc1' for input: 'COc1ccccc1[C@H]1CC1(=O)c2cccc(OCC(=O)Nc3ccc(C)cc3)c2O)cc1'\n",
      "[22:34:30] SMILES Parse Error: ring closure 2 duplicates bond between atom 6 and atom 7 for input: 'CN(C(=O)c1nc2c2c(F)cccc2s1)C1CCN([C@@H]2CCOC2)CC1'\n",
      "[22:34:30] SMILES Parse Error: extra close parentheses while parsing: O=c1onc(SCC(=O)N2CCN(S(=O)(=O)c3ccccc3F)CC2)n1)N1CCCCCC1\n",
      "[22:34:30] SMILES Parse Error: Failed parsing SMILES 'O=c1onc(SCC(=O)N2CCN(S(=O)(=O)c3ccccc3F)CC2)n1)N1CCCCCC1' for input: 'O=c1onc(SCC(=O)N2CCN(S(=O)(=O)c3ccccc3F)CC2)n1)N1CCCCCC1'\n",
      "[22:34:30] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'OC[C@@H]1O[C@@H](Oc2ccc(Cl)cc2)[C@@H](OC(c2ccccc2)=N[C@@H]2O)C1'\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'COc1cccc(Nc2noc3c2CN(C(=O)[C@@H](C)NC2=O)CCC3)C1'\n",
      "[22:34:30] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 23 24\n",
      "[22:34:30] Can't kekulize mol.  Unkekulized atoms: 17 18 19 21 22 23 24 25 26\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'Cc1nc2sc3oc([C@H](c3ccccc4)N3C[C@@H](C)O[C@@H](C)C3)c2cc1OC'\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc2c(c1)CN(C(=O)N[C@H]1CCOC3(C)C1)CC2'\n",
      "[22:34:30] SMILES Parse Error: unclosed ring for input: 'Cc1nnc([C@@H](Nc2cccc(C)c2)n2CCc2ccccc2)[nH]1'\n",
      "[22:34:31] SMILES Parse Error: ring closure 2 duplicates bond between atom 13 and atom 14 for input: 'CCN(CCO)C(=O)NC[C@@H]1CC[C@H]2[C@@H]2CC[C@H]2C[C@@H]1C3'\n",
      "[22:34:31] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 21\n",
      "[22:34:31] SMILES Parse Error: syntax error while parsing: COC(=O)c1ccc(CN2C(=O)N/C(=C\\/C(=C\\c3ccccc3)c3ccccc3)C2=O)cc1\n",
      "[22:34:31] SMILES Parse Error: Failed parsing SMILES 'COC(=O)c1ccc(CN2C(=O)N/C(=C\\/C(=C\\c3ccccc3)c3ccccc3)C2=O)cc1' for input: 'COC(=O)c1ccc(CN2C(=O)N/C(=C\\/C(=C\\c3ccccc3)c3ccccc3)C2=O)cc1'\n",
      "[22:34:31] Can't kekulize mol.  Unkekulized atoms: 1 2 17\n",
      "[22:34:31] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@H]1[C@H]2C[C@@H](O1)[C@@H]1C(=O)N1CCCc3ccccc31)c1ccco1'\n",
      "[22:34:31] Can't kekulize mol.  Unkekulized atoms: 21 22 23 28 29\n",
      "[22:34:31] SMILES Parse Error: unclosed ring for input: 'Cc1ncsc1C(=O)N1CC[C@]2(C(=O)NCCc3c(O)nc4n(C)c(=O)nc4C2)CO1'\n",
      "[22:34:31] SMILES Parse Error: unclosed ring for input: 'CCN1CCN(C(=O)C1CCC2)C[C@@H](Cc2cccc(-c3cccnc3)c2)C1=O'\n",
      "[22:34:31] SMILES Parse Error: ring closure 3 duplicates bond between atom 29 and atom 30 for input: 'Cc1cc(C)c(C)c(O[C@@H](C)C(=O)Nc2ccc3c(c2)N(C)C(=O)c3ccccc3O3)c1'\n",
      "[22:34:31] SMILES Parse Error: unclosed ring for input: 'C[C@@]12CCC(=O)N1[C@@H]1CCCc3ccccc31'\n",
      "[22:34:31] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(c3c1C[C@H](c1nccc4nc[nH]c13)CCC3)OCCO2'\n",
      "[22:34:31] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1ccc(CSc2nnc(NC(=O)c3cc4c(cc3Cl)n(C)c(=O)n3C)o2)cc1'\n",
      "[22:34:31] Can't kekulize mol.  Unkekulized atoms: 10 11 15 16 18\n",
      "[22:34:32] SMILES Parse Error: unclosed ring for input: 'C[n+](CCN)csc1Cl'\n",
      "[22:34:32] Can't kekulize mol.  Unkekulized atoms: 12 13 21 26\n",
      "[22:34:32] Can't kekulize mol.  Unkekulized atoms: 14 16 17 18 19 20 21\n",
      "[22:34:32] Can't kekulize mol.  Unkekulized atoms: 1 2 30\n",
      "[22:34:32] SMILES Parse Error: unclosed ring for input: 'CCS(=O)(=O)Nc1cc(C(=O)N2CCN(c3cccc(C)c3C)CC2)c2cc3c(cc1nn1C)OCO3'\n",
      "[22:34:32] Can't kekulize mol.  Unkekulized atoms: 13 14 27\n",
      "[22:34:32] Explicit valence for atom # 12 N, 5, is greater than permitted\n",
      "[22:34:32] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 13 17 18\n",
      "[22:34:32] Explicit valence for atom # 26 O, 4, is greater than permitted\n",
      "[22:34:32] Can't kekulize mol.  Unkekulized atoms: 6 7 14 15 22\n",
      "[22:34:32] SMILES Parse Error: extra close parentheses while parsing: CCC(=O)N(Cc1ccco1)Cc1cc2cc4c(cc2[nH]c1=O)OCO3)CC\n",
      "[22:34:32] SMILES Parse Error: Failed parsing SMILES 'CCC(=O)N(Cc1ccco1)Cc1cc2cc4c(cc2[nH]c1=O)OCO3)CC' for input: 'CCC(=O)N(Cc1ccco1)Cc1cc2cc4c(cc2[nH]c1=O)OCO3)CC'\n",
      "[22:34:33] SMILES Parse Error: unclosed ring for input: 'CC1(C)CN2C[C@H]1NC(=O)c1ccccc1OC1CCCC1'\n",
      "[22:34:33] SMILES Parse Error: unclosed ring for input: 'CC1CCN(c2ccc(CNC(=O)c3ccc4c(c3)S(=O)(=O)c3ccccc3OC3)cc2)CC1'\n",
      "[22:34:33] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN(C(=O)Cc2cn3c4c(sc2=O)CCC(=O)NC4)C[C@H](C)O1'\n",
      "[22:34:33] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[22:34:33] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(NC(=O)[C@H](C)OC(=O)/C=C/c2ccc(O)c(OC)c2)sc2c1'\n",
      "[22:34:33] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CCCN(S(=O)(=O)c2cccs2)CC1)N1C[C@H]2C[C@@H]2C[C@@H]2C1'\n",
      "[22:34:33] Can't kekulize mol.  Unkekulized atoms: 10 11 21\n",
      "[22:34:33] SMILES Parse Error: ring closure 2 duplicates bond between atom 21 and atom 22 for input: 'CN(C)[C@@H]1CN(CC2CCCCC2)C[C@H]1C1CCN(C(=O)c2c2cccn2)CC1'\n",
      "[22:34:33] SMILES Parse Error: unclosed ring for input: 'CCc1ccc([C@H]2CC3=C(N4)c4nc(SC(C)=O)c(C)c(C)c4N3CC(=O)Nc2ccccc24)cc1'\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 2 3 4 9 10 11 14 15 21 22 24\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 14 15 19\n",
      "[22:34:34] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C(=O)CCN1C(=O)N[C@@H](Cc2c[nH]c3ccccc23)C(=O)OC(C)(C)C2)cc1'\n",
      "[22:34:34] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C=C2\\CCC[C@H]3[C@H]2C(=O)N(CC(=O)Nc2ccc(C)cc2C)C(=O)N2)cc1'\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 8 25 26\n",
      "[22:34:34] SMILES Parse Error: unclosed ring for input: 'CSc1ccc([C@@H]2Oc3ccc(C)cc3[C@@H]3CC(c4cccc(NS(C)(=O)=O)c4=O)=NN24)cc1'\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 7 8 23\n",
      "[22:34:34] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)CO[C@@H](C(=O)NCc2nnc3n2CCSC3)c1ccccc1'\n",
      "[22:34:34] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)[C@H]1CC[C@@H](n2ccnc2-c2cc3c(n2)OCCO4)C1'\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 11 12 13 23 24 25 26\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14 15 23\n",
      "[22:34:34] Can't kekulize mol.  Unkekulized atoms: 2 3 7\n",
      "[22:34:34] SMILES Parse Error: extra close parentheses while parsing: O=C(C1CCOCC1)N1CC[C@@]2(CO)COCC(C)(C)C2)c1CO\n",
      "[22:34:34] SMILES Parse Error: Failed parsing SMILES 'O=C(C1CCOCC1)N1CC[C@@]2(CO)COCC(C)(C)C2)c1CO' for input: 'O=C(C1CCOCC1)N1CC[C@@]2(CO)COCC(C)(C)C2)c1CO'\n",
      "[22:34:35] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 10\n",
      "[22:34:35] SMILES Parse Error: unclosed ring for input: 'O=C(O)CCC(=O)O[C@@H]1C=C[C@@H]2[C@H]1[C@@]2(Cl)C(Cl)=C(Cl)[C@@]2(Cl)C1(Cl)Cl'\n",
      "[22:34:35] SMILES Parse Error: unclosed ring for input: 'O=c1[nH]n(-c2ccc(Cl)cc2)c(=O)c2c1nc1n2-c1ccccc1OC2'\n",
      "[22:34:35] Can't kekulize mol.  Unkekulized atoms: 13 14 16 17 18 19 29\n",
      "[22:34:35] SMILES Parse Error: unclosed ring for input: 'Cn1cnnc1SCC(=O)N1CC[C@@]23OC[C@@H](NC(=O)c3cnccn3)[C@H]2C1'\n",
      "[22:34:35] Can't kekulize mol.  Unkekulized atoms: 4 5 6 16\n",
      "[22:34:35] Explicit valence for atom # 16 C, 5, is greater than permitted\n",
      "[22:34:35] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[22:34:36] non-ring atom 17 marked aromatic\n",
      "[22:34:36] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 12 13 14\n",
      "[22:34:36] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-c1cc(C)nc(N[C@@]2(C(F)(F)F)C(=O)Nc2ccc(F)cc32)n1'\n",
      "[22:34:36] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[22:34:36] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCNC(=O)[C@H]2O[C@H]3OC(C)(C)O[C@@H]4[C@@H]3O[C@H]3c2ccc(OC(C)C)cc23)cc1'\n",
      "[22:34:36] Can't kekulize mol.  Unkekulized atoms: 2 3 5 12 23 25\n",
      "[22:34:36] SMILES Parse Error: syntax error while parsing: Clc1nc2nccn(2)c2c1CCNC2\n",
      "[22:34:36] SMILES Parse Error: Failed parsing SMILES 'Clc1nc2nccn(2)c2c1CCNC2' for input: 'Clc1nc2nccn(2)c2c1CCNC2'\n",
      "[22:34:36] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17 18 19\n",
      "[22:34:36] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C(=O)N(C)C2CCC(C(=O)NCc3ccccc3Cl)CC1)S1C'\n",
      "[22:34:36] SMILES Parse Error: extra close parentheses while parsing: CCc1ccc(O[C@H]2CCN(C(=O)Nc3ccc(C4SCCO)c4)=C3)C2)cc1\n",
      "[22:34:36] SMILES Parse Error: Failed parsing SMILES 'CCc1ccc(O[C@H]2CCN(C(=O)Nc3ccc(C4SCCO)c4)=C3)C2)cc1' for input: 'CCc1ccc(O[C@H]2CCN(C(=O)Nc3ccc(C4SCCO)c4)=C3)C2)cc1'\n",
      "[22:34:36] SMILES Parse Error: unclosed ring for input: 'O=C(O)[C@H]1CC[C@@H](C(=O)N2CC2(C(F)(F)F)CC2)CN1CCc1ccccc1'\n",
      "[22:34:36] Can't kekulize mol.  Unkekulized atoms: 22 23 26 27 28\n",
      "[22:34:37] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[22:34:37] SMILES Parse Error: extra close parentheses while parsing: COC1=C(C)[C@H]2C=C[C@@H]3[C@@]1(Cl)C(Cl)(Cl)[C@@]5(Cl)C(Cl)(Cl)Cl)[C@]21O\n",
      "[22:34:37] SMILES Parse Error: Failed parsing SMILES 'COC1=C(C)[C@H]2C=C[C@@H]3[C@@]1(Cl)C(Cl)(Cl)[C@@]5(Cl)C(Cl)(Cl)Cl)[C@]21O' for input: 'COC1=C(C)[C@H]2C=C[C@@H]3[C@@]1(Cl)C(Cl)(Cl)[C@@]5(Cl)C(Cl)(Cl)Cl)[C@]21O'\n",
      "[22:34:46] SMILES Parse Error: unclosed ring for input: 'O=C(O)CN(C[C@@H]1CCCO1)c1nccnc12'\n",
      "[22:34:46] Explicit valence for atom # 16 O, 3, is greater than permitted\n",
      "[22:34:47] SMILES Parse Error: unclosed ring for input: 'O=C1CCN2C[C@H]3CC[C@@H](C1)N3C(=O)N1CCN(Cc3ccco3)CC1'\n",
      "[22:34:47] SMILES Parse Error: unclosed ring for input: 'O=C(Cn1nccn1)N1CCOC2(CCC(=O)c3ccccc3)CC1'\n",
      "[22:34:47] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 21 23\n",
      "[22:34:47] SMILES Parse Error: unclosed ring for input: 'COCCN1C[C@H]2OCCN(C(=O)C3(OC)CCCCC2)CC1'\n",
      "[22:34:47] Can't kekulize mol.  Unkekulized atoms: 7 8 20\n",
      "[22:34:47] SMILES Parse Error: unclosed ring for input: 'C[C@H](O[C@H](C)C(=O)N1CCC[C@@H]1c1cccn2Cc1ccncc1)C(=O)O'\n",
      "[22:34:47] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[22:34:47] Can't kekulize mol.  Unkekulized atoms: 2 3 4 27 28\n",
      "[22:34:47] Can't kekulize mol.  Unkekulized atoms: 1 2 3 10 22\n",
      "[22:34:47] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(NS(=O)(=O)c2ccc(OCC(=O)NC3=O)c(C)c2)cc1'\n",
      "[22:34:47] Explicit valence for atom # 7 F, 3, is greater than permitted\n",
      "[22:34:47] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 23 for input: 'CC(C(=O)N1CCC[C@@]2(CCc3cnc(-c4ccccc4Cl)n3)C2)COC1=O'\n",
      "[22:34:48] SMILES Parse Error: unclosed ring for input: 'O=C1N[C@H](c2ccc(Cl)cc2Cl)[C@@H](C(=O)C(F)(F)F)[C@@]1(O)c2ccccc21'\n",
      "[22:34:48] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 17 18 19 20 21 22 23\n",
      "[22:34:48] Can't kekulize mol.  Unkekulized atoms: 13 14 16 17 18 20 23\n",
      "[22:34:48] Can't kekulize mol.  Unkekulized atoms: 2 3 10 11 13 14 15\n",
      "[22:34:48] SMILES Parse Error: extra close parentheses while parsing: O=C1OC2(CCCC2)C1)N1CCC[C@H](O)C1\n",
      "[22:34:48] SMILES Parse Error: Failed parsing SMILES 'O=C1OC2(CCCC2)C1)N1CCC[C@H](O)C1' for input: 'O=C1OC2(CCCC2)C1)N1CCC[C@H](O)C1'\n",
      "[22:34:48] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)CC[C@@](O)(C1CCCCC1)C1(O)CC1'\n",
      "[22:34:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 8 25 26 27\n",
      "[22:34:48] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14\n",
      "[22:34:48] SMILES Parse Error: extra close parentheses while parsing: NC1(c2ccc(Cl)c(Cl)c2)C/C1=C\\c2ccccc2)CC1\n",
      "[22:34:48] SMILES Parse Error: Failed parsing SMILES 'NC1(c2ccc(Cl)c(Cl)c2)C/C1=C\\c2ccccc2)CC1' for input: 'NC1(c2ccc(Cl)c(Cl)c2)C/C1=C\\c2ccccc2)CC1'\n",
      "[22:34:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 24 26 27 28\n",
      "[22:34:48] SMILES Parse Error: unclosed ring for input: 'O=C1NC2=C(CCc3ccccc31)N1'\n",
      "[22:34:49] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 15\n",
      "[22:34:49] SMILES Parse Error: unclosed ring for input: 'CC(C)CCOc1ccccc1NC(=O)N[C@H]1[C@H]2C[C@@H]3[C@@H]1O2'\n",
      "[22:34:49] SMILES Parse Error: extra close parentheses while parsing: Cn1cc(C)c2nc(N)[nH]c2S(=O)(=O)N2CCCCC2)c1\n",
      "[22:34:49] SMILES Parse Error: Failed parsing SMILES 'Cn1cc(C)c2nc(N)[nH]c2S(=O)(=O)N2CCCCC2)c1' for input: 'Cn1cc(C)c2nc(N)[nH]c2S(=O)(=O)N2CCCCC2)c1'\n",
      "[22:34:49] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 18\n",
      "[22:34:49] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 22 23\n",
      "[22:34:49] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 25\n",
      "[22:34:49] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1cn([C@H]2C[C@H]2CN(C(=O)c2cccc(N)c2)CCO2)cn1'\n",
      "[22:34:49] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 13 14\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'CC(C)CN(C)C[C@@H](C)n1c2ccccc2n2c(=O)sc2ccccc21'\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'CC(=O)O[C@H]1CC[C@]2(C)CC[C@]3(C)[C@@H]4CC[C@@H]4C[C@H](OC(C)=O)CC[C@]4(C)[C@H]3CC[C@]12C'\n",
      "[22:34:50] SMILES Parse Error: extra close parentheses while parsing: C[C@@H]1C[C@H]1C)NCc1cccc(OCC(F)(F)F)c1\n",
      "[22:34:50] SMILES Parse Error: Failed parsing SMILES 'C[C@@H]1C[C@H]1C)NCc1cccc(OCC(F)(F)F)c1' for input: 'C[C@@H]1C[C@H]1C)NCc1cccc(OCC(F)(F)F)c1'\n",
      "[22:34:50] SMILES Parse Error: extra open parentheses for input: 'Fc1cccc(N2N=C([N+]n(Cn3cncn3)CCO2)c1ccccc1'\n",
      "[22:34:50] Can't kekulize mol.  Unkekulized atoms: 9 10 13 14 19 20 31\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'CN1C[C@H]2C3C[C@@H]1CN(C(=O)N=C1CCc3ccccc31)C2'\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'CC[C@@H](C)NC(=O)c1cn(CC2CC2)c3c(C)c(C)sc3c(=O)n12'\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NCCN2C(=O)c3ccc(C(=O)c5ccc(F)cc4)cc3C2=O)cc1'\n",
      "[22:34:50] Can't kekulize mol.  Unkekulized atoms: 15 16 18 19 21\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'CCc1nnc2ccc(N3C[C@@H]4CN(C(=O)NC[C@@H]5CCCO5)C[C@]4(C)[C@H]4C)nn12'\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2ncc(C(=O)N3C[C@H]4CN(C[S@]6(C)=O)CC4)c(O)c3c2c1'\n",
      "[22:34:50] SMILES Parse Error: unclosed ring for input: 'Nc1cc(C(=O)c2cc([N+](=O)[O-])ccc2N1CCOCC2)ccc1O'\n",
      "[22:34:50] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 21 22 24\n",
      "[22:34:50] SMILES Parse Error: syntax error while parsing: COc1ccc(-c2/c(=N-(C(=O)c3ccccc3)[C@H](c3ccccc3)c3ccccc3)o2)cc1\n",
      "[22:34:50] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(-c2/c(=N-(C(=O)c3ccccc3)[C@H](c3ccccc3)c3ccccc3)o2)cc1' for input: 'COc1ccc(-c2/c(=N-(C(=O)c3ccccc3)[C@H](c3ccccc3)c3ccccc3)o2)cc1'\n",
      "[22:34:50] Explicit valence for atom # 6 C, 5, is greater than permitted\n",
      "[22:34:51] Can't kekulize mol.  Unkekulized atoms: 2 3 23\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'CCCc1cc(=O)oc2c(C)c(O[C@H](C)C(=O)N[C@H](Cc3c[nH]c4ccccc35)C(=O)O)ccc12'\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'CC(=O)O[C@@H]1C[C@@H]2[C@](C)(CC[C@H]3[C@@]2(C)CC[C@]2(C)C4(C)C)OC1'\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'CC(=O)C[C@@H]1[C@@H](O)CCC[C@@H]2C(=O)N1CCc1sccc1C'\n",
      "[22:34:51] Can't kekulize mol.  Unkekulized atoms: 1 2 3 24 25 26 27 28 29\n",
      "[22:34:51] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 27 28 29\n",
      "[22:34:51] SMILES Parse Error: extra close parentheses while parsing: O=c1c2cc3c(nc2Cl)CCCC3)on1\n",
      "[22:34:51] SMILES Parse Error: Failed parsing SMILES 'O=c1c2cc3c(nc2Cl)CCCC3)on1' for input: 'O=c1c2cc3c(nc2Cl)CCCC3)on1'\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'CC1(C)C[C@@H]2C[C@@](C)(c3ccc4c(-c4ccc(F)cc4)c(=O)n(C)c32)CN1'\n",
      "[22:34:51] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 15\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(NC(=O)COc2c(Cl)cc(Cl)c3c=C(=O)c2ccccc2)cc1OC'\n",
      "[22:34:51] Can't kekulize mol.  Unkekulized atoms: 1 2 3 15 16 17 18 19 20\n",
      "[22:34:51] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'O=C(c1cc[nH]c1)N1CC[C@@]23OC[C@@H]2c2ccccc21'\n",
      "[22:34:51] SMILES Parse Error: extra close parentheses while parsing: CNC(=O)c1cccc(NC(=O)c2cccc3c2Cl)[C@@H](O)[C@@H]2O)c1\n",
      "[22:34:51] SMILES Parse Error: Failed parsing SMILES 'CNC(=O)c1cccc(NC(=O)c2cccc3c2Cl)[C@@H](O)[C@@H]2O)c1' for input: 'CNC(=O)c1cccc(NC(=O)c2cccc3c2Cl)[C@@H](O)[C@@H]2O)c1'\n",
      "[22:34:51] SMILES Parse Error: unclosed ring for input: 'Cn1cc(N2CC[C@H](N3CCc4c(Cl)c(Cl)c(Cl)cc4Cl)C2)CC1=O'\n",
      "[22:34:52] Can't kekulize mol.  Unkekulized atoms: 1 2 3 19 20\n",
      "[22:34:52] SMILES Parse Error: extra open parentheses for input: 'O=C(N1CC[C@@]2(C1)Cn1c(nnc2-c2ccncc2)C1'\n",
      "[22:34:52] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 8 9 10\n",
      "[22:34:52] SMILES Parse Error: extra close parentheses while parsing: COc1cccc(/C=C/C(=O)OCC(=O)c2[nH]c(C)c(C(C)=O)c2C)c1OC)c1\n",
      "[22:34:52] SMILES Parse Error: Failed parsing SMILES 'COc1cccc(/C=C/C(=O)OCC(=O)c2[nH]c(C)c(C(C)=O)c2C)c1OC)c1' for input: 'COc1cccc(/C=C/C(=O)OCC(=O)c2[nH]c(C)c(C(C)=O)c2C)c1OC)c1'\n",
      "[22:34:52] SMILES Parse Error: syntax error while parsing: CCOc1ccccc1[C@H]1C(C(NC)=O)=NC(c2ccc(OCC#=O)OC)cc2Cl)=N1\n",
      "[22:34:52] SMILES Parse Error: Failed parsing SMILES 'CCOc1ccccc1[C@H]1C(C(NC)=O)=NC(c2ccc(OCC#=O)OC)cc2Cl)=N1' for input: 'CCOc1ccccc1[C@H]1C(C(NC)=O)=NC(c2ccc(OCC#=O)OC)cc2Cl)=N1'\n",
      "[22:34:52] Can't kekulize mol.  Unkekulized atoms: 10 11 13\n",
      "[22:34:52] Can't kekulize mol.  Unkekulized atoms: 17 18 19 22 23\n",
      "[22:34:52] SMILES Parse Error: unclosed ring for input: 'COCC1(CC(=O)N2CCC(c3nnc(CN3CCCCC4)n3C)CC2)CCC1'\n",
      "[22:34:52] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 16 22 23 25 26\n",
      "[22:34:52] SMILES Parse Error: unclosed ring for input: 'CCS(=O)(=O)c1ccc(NC(=O)CN2[C@@H]3C[C@H]4CC[C@@H]2C4)cc1'\n",
      "[22:34:52] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccccc1N2CCOCC1)N1CCC[C@@H](c2nncn2C2CC2)C1'\n",
      "[22:34:53] Can't kekulize mol.  Unkekulized atoms: 10 11 12 22 23\n",
      "[22:34:53] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2c(C)nn3c(=O)c4c([nH]c23)CCCC5)cc1'\n",
      "[22:34:53] SMILES Parse Error: unclosed ring for input: 'Cc1c(C)c(C)c2c([C@@H]1CC(=O)N(c3ccc(C(=O)O)cc3)C2)C(=O)OC1'\n",
      "[22:34:53] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 12\n",
      "[22:34:53] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 10 12 13 14 23 24\n",
      "[22:34:53] Can't kekulize mol.  Unkekulized atoms: 21 22 23 24 25 26 27 28 29\n",
      "[22:34:53] SMILES Parse Error: ring closure 2 duplicates bond between atom 5 and atom 6 for input: 'Cn1c(S/C(=C2c2ccccc3C(=O)O)c(=O)n(C)c2=O)nc2ccccc21'\n",
      "[22:34:53] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1C[C@H]1C(=O)Nc1cc2c3c(c1)CCN3C(=O)CCc1ccc2c(c1)OCCO2'\n",
      "[22:34:53] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C)cc1[C@H]1[C@H]2C(=O)N(c4ccc(Br)cc4)C(=O)[C@H]3ON2c1ccccc1'\n",
      "[22:34:53] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13 14 21\n",
      "[22:34:53] SMILES Parse Error: unclosed ring for input: 'O=C(Cn1cnnn1)N1CCC(O)(C1CCCCC2)CC1'\n",
      "[22:34:53] SMILES Parse Error: extra close parentheses while parsing: CCOC[C@H](O)CN1CCN(C(=O)[C@@H]2C[C@@]3(C)C2(C)C)CCO3)cn1\n",
      "[22:34:53] SMILES Parse Error: Failed parsing SMILES 'CCOC[C@H](O)CN1CCN(C(=O)[C@@H]2C[C@@]3(C)C2(C)C)CCO3)cn1' for input: 'CCOC[C@H](O)CN1CCN(C(=O)[C@@H]2C[C@@]3(C)C2(C)C)CCO3)cn1'\n",
      "[22:34:53] Can't kekulize mol.  Unkekulized atoms: 6 7 9 23 24 25 26\n",
      "[22:34:53] SMILES Parse Error: extra close parentheses while parsing: C[C@@H]1CCC[C@@H]2CCCCN2C(=O)CN2CCC[C@H]2Cc2c(C)noc2C)C1\n",
      "[22:34:53] SMILES Parse Error: Failed parsing SMILES 'C[C@@H]1CCC[C@@H]2CCCCN2C(=O)CN2CCC[C@H]2Cc2c(C)noc2C)C1' for input: 'C[C@@H]1CCC[C@@H]2CCCCN2C(=O)CN2CCC[C@H]2Cc2c(C)noc2C)C1'\n",
      "[22:34:54] SMILES Parse Error: unclosed ring for input: 'O=C1CCC2(CCN(C(=O)c3ccco3)CC2)N1CC1CCC2'\n",
      "[22:34:54] Can't kekulize mol.  Unkekulized atoms: 9 25 26 27 28 29 30\n",
      "[22:34:54] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 7\n",
      "[22:34:54] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C(=O)N2C[C@@H]3C[C@@H]2[C@H]2C(=O)NCC2CC2)no1'\n",
      "[22:34:54] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 9 20 21 22 23 24 25 28 29 30\n",
      "[22:34:54] SMILES Parse Error: unclosed ring for input: 'C[C@@H](O)CNS(=O)(=O)[C@H]1[C@H]2[C@H]1C=C[C@H]2C2'\n",
      "[22:34:54] Can't kekulize mol.  Unkekulized atoms: 9 10 18 19 21\n",
      "[22:34:54] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1C(=O)N(c2cccc3c2CCN([C@@H]2CCC(=O)NC2=O)C3)C(=O)O'\n",
      "[22:34:54] Can't kekulize mol.  Unkekulized atoms: 1 2 3 11 12\n",
      "[22:34:54] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2ncnc3c2nnn2Cc2ccccc2Cl)cc1'\n",
      "[22:34:54] SMILES Parse Error: unclosed ring for input: 'CCc1nc(NC(=O)[C@@H]2CC(=O)N(C)C2)sc1c1'\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)NCC(=O)N1CCC2(CC1)C(=O)N(CCN1CCOCC1)CCc2c2[nH]c2ccccc31'\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 22 23 24 25\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 10 11 12 17 19\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'CC1CCC12'\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'CC(C)N(C(=O)c1cnc2ccccc2c1c1cnn2C)C(=O)c1ccccc1'\n",
      "[22:34:55] SMILES Parse Error: ring closure 2 duplicates bond between atom 20 and atom 21 for input: 'O=C1COc2ccccc2C/C=C/C/C=C/CCCC[C@H]2[C@H]2CCCC[C@]2(C)OC1'\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 14\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)OC(=O)CCOCCOCCOCCOCCOCCN1C(=O)C=CC2=C1=O'\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 7 8 22 23 24\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'CC1CCN(S(=O)(=O)c2ccc(CC(=O)Nc3ccc(c4nc4ccccc5o4)cc3)cc2)CC1'\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 3 4 19\n",
      "[22:34:55] Explicit valence for atom # 15 O, 3, is greater than permitted\n",
      "[22:34:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 17 19 20\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'O=C(C1CCN(S(=O)(=O)Cc2ccccc2)CC1)N1CCN(C2CC3)CC1'\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'O=C1CSc2ccc(C(=O)N3CCc4cc(-c5cncn5)nn4c4)cc3N1'\n",
      "[22:34:55] SMILES Parse Error: unclosed ring for input: 'O=C(/C=C/c1cccs1)N1N=C2\\C(=CC2)N(c2ccccc2)C(=O)[C@@H]2C1=O'\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)n1nnc([C@H]2CCCN(Cc3cccc4c3[nH]C3)C2)n1'\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'C[C@@H](OC(=O)CC12C[C@H]3C[C@@H](CC(N)(C)C1)C2)C(=O)c1c[nH]c2ccccc12'\n",
      "[22:34:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 15 16 17 18\n",
      "[22:34:56] SMILES Parse Error: extra close parentheses while parsing: CCN(CC)S(=O)(=O)c1ccc(C)c(NCC(=O)Nc2ccc(N3CCCC3=O)cc2)S(C)(=O)=O)c1\n",
      "[22:34:56] SMILES Parse Error: Failed parsing SMILES 'CCN(CC)S(=O)(=O)c1ccc(C)c(NCC(=O)Nc2ccc(N3CCCC3=O)cc2)S(C)(=O)=O)c1' for input: 'CCN(CC)S(=O)(=O)c1ccc(C)c(NCC(=O)Nc2ccc(N3CCCC3=O)cc2)S(C)(=O)=O)c1'\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'Cn1nc([C@H]2CCCN(C(=O)c3cnn(-c4ccccc4)n3)C2)n2c2CCCC2'\n",
      "[22:34:56] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CNc2nc3n(n2)[C@H](c2ccccc2)C2=C(N=c4ccccc4CC2)O4)cc1'\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'O=C(NC(=S)Nc1ccc2oc(-c3ccc(Cl)cc3)nc2c1)cs1'\n",
      "[22:34:56] Explicit valence for atom # 15 S, 7, is greater than permitted\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'CC1(C)OC(=O)C=C[C@@]2(C)[C@H]1CC(=O)[C@]1(C)[C@H]2CC[C@@]2(C)[C@H](c3ccoc3)OC(=O)[C@H]3O[C@]221'\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'Cn1ccc2c1c(C(=O)N1CCOC3)n(C)c(=O)c2C1'\n",
      "[22:34:56] Can't kekulize mol.  Unkekulized atoms: 18 20 21\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'CS(=O)(=O)c1ccccc1C(=O)N1CC2C[C@@H](CCO)C1'\n",
      "[22:34:56] Can't kekulize mol.  Unkekulized atoms: 14 16 17 18 19 20 21\n",
      "[22:34:56] SMILES Parse Error: unclosed ring for input: 'Cc1cc(CN2C[C@H]2CN(C)C[C@]3(C)C2)c(O)cc1F'\n",
      "[22:34:57] Can't kekulize mol.  Unkekulized atoms: 11 12 14 22\n",
      "[22:34:57] SMILES Parse Error: unclosed ring for input: 'O=c1nc(O)ccn1[C@H]1C[C@@H]ICC[C@@H]1C2'\n",
      "[22:34:57] Can't kekulize mol.  Unkekulized atoms: 3 11 12\n",
      "[22:34:57] Can't kekulize mol.  Unkekulized atoms: 5 6 7 19 20\n",
      "[22:34:57] SMILES Parse Error: unclosed ring for input: 'O=C(O)[C@@H](Cc1c[nH]c2ccccc12)NC(=O)[C@@H]1[C@H]2OC3c3c(cccc32)C(=O)N2C'\n",
      "[22:34:57] SMILES Parse Error: unclosed ring for input: 'C1=Cn2CCC[C@H]3CN(CC(=O)Nc4ccccn4)C[C@@H]1c2ccccc12'\n",
      "[22:34:57] SMILES Parse Error: syntax error while parsing: CSc1ccc([C@@H](CC(=O)O)NC(=S)Nc2c(\n",
      "[22:34:57] SMILES Parse Error: Failed parsing SMILES 'CSc1ccc([C@@H](CC(=O)O)NC(=S)Nc2c(' for input: 'CSc1ccc([C@@H](CC(=O)O)NC(=S)Nc2c('\n",
      "[22:34:57] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13 14 15\n",
      "[22:34:57] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[22:34:57] SMILES Parse Error: unclosed ring for input: 'C[C@H](Sc1nn(CCC(N)=O)c(=O)c3ccccc23)C(=O)N1CCOCC1'\n",
      "[22:34:57] Can't kekulize mol.  Unkekulized atoms: 2 3 14 22\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'O=C1OC2(CCN(C(=O)[C@@H]3CCC[C@H]3C(F)(F)F)CC2)Oc2c1ccccc2-2'\n",
      "[22:34:58] Can't kekulize mol.  Unkekulized atoms: 7 8 10 19 20 21 22\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'O=C(O)C[C@@]1(c2ccccc2)CC(=O)N(C)2'\n",
      "[22:34:58] Can't kekulize mol.  Unkekulized atoms: 17 18 20 21 22 23 24\n",
      "[22:34:58] SMILES Parse Error: ring closure 4 duplicates bond between atom 12 and atom 13 for input: 'COC(=O)c1cccc(N2C[C@H]3C4c4ccccc4C(C)(C)C[C@@H]4C[C@@H]2C3)c1'\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'O=C(CCCOc1ccc(Cl)cc1Cl)Nc1nc2c3c3c(sc2ncn2n1)CCCC4'\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'COc1cc(CCC(=O)OCC(=O)c2ccc(CNC(=O)C(C)(C)Cs3)cc2)cc(OC)c1OC'\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'Brc1ccccc1-c1nc2ccccc2n1CCCN2CCCCC1'\n",
      "[22:34:58] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 28\n",
      "[22:34:58] Can't kekulize mol.  Unkekulized atoms: 15 16 24\n",
      "[22:34:58] Can't kekulize mol.  Unkekulized atoms: 2 3 14 15 16\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'C[C@@]12CN=C3CC[C@H]4OC(=O)[C@H]1[C@@H](c1ccccc1)N3'\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'Cc1nc2ccccc2c(C)c1C(=O)OCC(=O)NC12CC4CC(CC(C3)C1)C2'\n",
      "[22:34:58] Can't kekulize mol.  Unkekulized atoms: 2 3 4 15 16 18 19\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C(=O)N(Cc2ccc(Cl)cc2)C2CC2)c2nn[nH]n2'\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'Cc1nc(N)sc1C(=O)NCCc1ncc2c(n1CCOC(C)(C)C2)CC2'\n",
      "[22:34:58] SMILES Parse Error: unclosed ring for input: 'CCCc1cc(=O)oc2c(C)c(O[C@@H](C)C(=O)N[C@H](Cc3c[nH]c4ccccc35)C(=O)O)ccc12'\n",
      "[22:34:59] SMILES Parse Error: ring closure 2 duplicates bond between atom 15 and atom 16 for input: 'O=C1C[C@H](S(=O)(=O)c2ccc(Br)cc2Cl)C2(c2ccc(Cl)cc2)CCCN1'\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'C[C@@H](C(=O)N[C@H]1CCOC2(CCN(C)CC2)O2)c1ccc(F)cc1'\n",
      "[22:34:59] Can't kekulize mol.  Unkekulized atoms: 4 5 18 19 20 21 22 23 24\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'CC[C@@]1(C)NC(=O)N(NC(=O)CSc2nnc3n2CCOC(C)C2)C1=O'\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'Cc1cn2c(CN3C[C@H]4CN(CC5CC4)C[C@H]4C3)cc(=O)oc2c1C'\n",
      "[22:34:59] Can't kekulize mol.  Unkekulized atoms: 12 13 20\n",
      "[22:34:59] Can't kekulize mol.  Unkekulized atoms: 8 9 10\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)[C@@H](O)[C@H]2CCN1C=C2CCCC1'\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'O=S(=O)(N[C@H]1C[C@H]2C[C@@H](C2)[C@@H]1C2CC1)N1CCOCC1'\n",
      "[22:34:59] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 8 27 28\n",
      "[22:34:59] SMILES Parse Error: extra close parentheses while parsing: CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1\n",
      "[22:34:59] SMILES Parse Error: Failed parsing SMILES 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1' for input: 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1'\n",
      "[22:34:59] SMILES Parse Error: ring closure 3 duplicates bond between atom 13 and atom 14 for input: 'O=C(O)c1csc(S(=O)(=O)Nc2cc3c3c(c2)Cc2ccccc2-3)c1'\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(Br)cc1)C([C@H]1C(=O)c2ccccc2)N=C2c1ccccc1'\n",
      "[22:34:59] Can't kekulize mol.  Unkekulized atoms: 4 5 6 25 26\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1nc1c(C)cnn2[C@H](C)C(=O)N1CC[C@](O)(C(F)(F)F)C1'\n",
      "[22:34:59] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'Cc1cnn(CCC(=O)N2CC[C@@H](c3nc4c(s2)CCCC4)C2)c1'\n",
      "[22:34:59] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@@H]2C3=C(N=c4s/c(=C\\c5ccc(OC(=O)c5ccc(C)cc5)c(=O)c4cccc5c4=O)n3C)C(=O)O2)c1'\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'CCN(Cc1ccccc1)[C@H](c1cc2cc(C)c(C)cc2[nH]c1=O)c1nnnn2C1CCCCC1'\n",
      "[22:34:59] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1CC[C@@H]1NC[C@@H]2N'\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'O=c1cc2c(nc3n1-c1ccccc1O)CCC[C@H]2[C@H]1CCCN(c2cnc3ccccc3n2)C1'\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'Nc1nc2ccc(NC(=O)C[C@@H]3C[C@H]3CC[C@H]3C4)cc2[nH]1'\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 7 8 9 11 18 20\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(NC(=O)[C@H]2[C@@H]3CC[C@@H]4[C@H]3C(=O)N(c4ccccc4)[C@@H]22)cc1'\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 26 30 31\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 7 8 19 20 21 22 23\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'Cc1nc2ccccc2c(=O)n1-c1c2c(C)c(C)n(C(C)C)c1C'\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'CC(C)Cc1nc(CN2CCC[C@@]23CCCN(C2CC2)C3CC2)no1'\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 17 18 19 21 22\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 8 9 23\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1cccc(NC(=O)[C@@H]2[C@@H]3CC[C@@]4(CC(=O)c4ccc(F)cc44)=C2[C@H]2N)c1'\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 3 26 27\n",
      "[22:35:00] SMILES Parse Error: unclosed ring for input: 'Cn1nccc1C(=O)N1CC[C@@]2(CCn3nnc(C(=O)NC3CC3)c2=O)CC1'\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 19\n",
      "[22:35:00] Can't kekulize mol.  Unkekulized atoms: 10 18 19\n",
      "[22:35:00] SMILES Parse Error: syntax error while parsing: Cc1ccccc1[C@H](C1=NN()[C@@H](C)S(=O)(=O)N2)C1\n",
      "[22:35:00] SMILES Parse Error: Failed parsing SMILES 'Cc1ccccc1[C@H](C1=NN()[C@@H](C)S(=O)(=O)N2)C1' for input: 'Cc1ccccc1[C@H](C1=NN()[C@@H](C)S(=O)(=O)N2)C1'\n",
      "[22:35:00] Explicit valence for atom # 1 S, 7, is greater than permitted\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'COCCN1CCC(CN(C/C=C/c2ccc(Cl)cc2)CC1CCN(C)CC2)C1'\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'Cc1cc2scc(CCC(=O)N3C[C@@H](O)[C@@H](n4cnc5c(N)nc(N)nc44)c3)c2c1'\n",
      "[22:35:01] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 21\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'CN1CCO[C@H]2CCN(C(=O)Nc3c4c(cn4CCO)cnc4C3)CS2(=O)=N1'\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Nc2nc(CNc3ccc(F)cc3)c3c4n(c2)COC(C)(C)C4)cc1'\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'CN1CC2(CC1)C[C@H](NC(=O)CN(C)Cc1cccnc1)C1=O'\n",
      "[22:35:01] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 15\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'C[C@]1(C(=O)N[C@H]2CCOC23CCCC2)CCC(=O)NC1=O'\n",
      "[22:35:01] Can't kekulize mol.  Unkekulized atoms: 2 3 4 17 18 19 20\n",
      "[22:35:01] Can't kekulize mol.  Unkekulized atoms: 6 7 8 11 12 13 14 15 16\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'C=CC(=O)NC[C@@]1(c2ccccc2)C[C@@H](NC(=O)c2cncc(C)c2)C2'\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'O=C(/C=C/c1ccccc1)Nc1nc2nc3c(s2)C1(CCCC1)CCN(Cc1ccccc1)C3'\n",
      "[22:35:01] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 18 21 22\n",
      "[22:35:01] SMILES Parse Error: unclosed ring for input: 'O=C(c1cnc2cccnn12)N1CCN2CC(F)(F)C1'\n",
      "[22:35:01] SMILES Parse Error: extra open parentheses for input: 'O=C([C@H]1CN(c2ccccc2)CCN1C(=O)c1ccc(Cn2cncn2)cc1'\n",
      "[22:35:01] SMILES Parse Error: duplicated ring closure 2 bonds atom 7 to itself for input: 'O=C(NC[C@H]1CC[C@@]22CCCN(CCc3ccccc3)C2)C(=O)N1Cc1ccccc1'\n",
      "[22:35:01] Explicit valence for atom # 4 C, 5, is greater than permitted\n",
      "[22:35:02] SMILES Parse Error: extra open parentheses for input: 'COC(=O)c1cc(NC(=O)CN(c2ccc(OC)cc2OS(C)(=O)=O)cc2ccccc23'\n",
      "[22:35:02] SMILES Parse Error: extra close parentheses while parsing: O=C(N1CC[C@@H](N2CCc3ccccc3C2)C1)C(=O)O)c1ccccc1\n",
      "[22:35:02] SMILES Parse Error: Failed parsing SMILES 'O=C(N1CC[C@@H](N2CCc3ccccc3C2)C1)C(=O)O)c1ccccc1' for input: 'O=C(N1CC[C@@H](N2CCc3ccccc3C2)C1)C(=O)O)c1ccccc1'\n",
      "[22:35:02] SMILES Parse Error: unclosed ring for input: 'Cc1nc(C)c(C)c(N2CCC(c3nc(O)c4c4c(ncn3n3)CCN(C)C4=O)CC2)n1'\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 3 4 5 9 26 27 28\n",
      "[22:35:02] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNC(=O)CN(c2ccc(Oc4ccccc4)cc3)S(C)(=O)=O)cc1'\n",
      "[22:35:02] SMILES Parse Error: extra close parentheses while parsing: CCOc1cncc(C(=O)N2CCCC[C@@H](C)c2)c2ccc(OC)cc2)c1\n",
      "[22:35:02] SMILES Parse Error: Failed parsing SMILES 'CCOc1cncc(C(=O)N2CCCC[C@@H](C)c2)c2ccc(OC)cc2)c1' for input: 'CCOc1cncc(C(=O)N2CCCC[C@@H](C)c2)c2ccc(OC)cc2)c1'\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 10\n",
      "[22:35:02] SMILES Parse Error: unclosed ring for input: 'COc1cccc(OC)c1C(=O)N[C@H]1C=C[C@H](C1)N1C(=O)OC(C)(C)C'\n",
      "[22:35:02] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 9 for input: 'COCCOCc1nc2n2n(n1)[C@H](C(F)F)C[C@@H](c1ccc(Cl)cc1)N2'\n",
      "[22:35:02] SMILES Parse Error: unclosed ring for input: 'CC1=NN(c2ccc(C(=O)N3C[C@@H](N)[C@@H](c4ncc(C)n4C)cc3)C2)cc1C1'\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 21 22 23 24\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 24 27 28\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[22:35:02] SMILES Parse Error: unclosed ring for input: 'CC(C)n1c(CN2CCOC[C@H]2c2nc(C2CC3)no2)nc2ccccc21'\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 2 3 18 19 20\n",
      "[22:35:02] SMILES Parse Error: unclosed ring for input: 'CC1=C[C@@]23C(c4ccccc41)[C@H]2c2ccccc2C(=O)N1CC(=O)c1ccccc1'\n",
      "[22:35:02] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 15 25 26\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccc(F)cc1F)N1CC[C@H]2CCN(Cc3cccnc3)C2(C2)C1'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N2CCN(C(=O)c3ncc4c(-c4ccc(F)cc4)csc3=O)CC2)c1C'\n",
      "[22:35:03] SMILES Parse Error: ring closure 2 duplicates bond between atom 21 and atom 22 for input: 'Oc1c([C@H](c2nc(-c3cccnc3)nc3ccccn23)nn2-c2ccccc2)c2ccccc21'\n",
      "[22:35:03] Can't kekulize mol.  Unkekulized atoms: 6\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'CN1CC[C@@H](N2CC3(CC1)Oc2ccccc2O[C@H]2c2cccnc2)C1'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'O=C(OCC1c2ccccc2-c2ccccc21)N1CC(O)[C@H]1CCCC[C@@H]1C(=O)O'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC(=O)c2sc3c(c2c1[C@@H]1CCCC[C@@H]3C(=O)N1CCC(=O)O)CO1'\n",
      "[22:35:03] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CCN(Cc1cccs1)c1ccc3nnnn3c2N1CCOCC1'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'CN(Cc1ccco1)C(=O)CC12CCN(Cc2cnn(C)c2)CC2'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'COC(=O)Nc1ccc(NC(=O)[C@H]2CC2(c2ccccc2)CC2)cc1'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CCCS[C@@H]1CCO)c1ccccc12'\n",
      "[22:35:03] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc(C2=NNC(=O)C(C)(C)CN3C)cs2)cc1'\n",
      "[22:35:04] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 15 16 19\n",
      "[22:35:04] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 34\n",
      "[22:35:04] Can't kekulize mol.  Unkekulized atoms: 7 8 26 27 28 29 30 31 32\n",
      "[22:35:04] Explicit valence for atom # 8 C, 5, is greater than permitted\n",
      "[22:35:04] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "[22:35:04] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CCC[C@H](C)c1c1cnc3ccccc3c1-2'\n",
      "[22:35:04] SMILES Parse Error: unclosed ring for input: 'CC(C)n1nccc1C(=O)N1CC[C@@]2(C1)Cn2c(nnc2C(F)(F)F)C1'\n",
      "[22:35:04] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(F)cc1C(=O)N1C[C@@H]2[C@H](C1)[C@H]2CC[C@H]2O'\n",
      "[22:35:05] SMILES Parse Error: extra close parentheses while parsing: CCOc1c(Br)cc([C@H]2O[C@@H](c3ccc(C)cc3))=COC2=N2CCN(C)CC2)c1OC\n",
      "[22:35:05] SMILES Parse Error: Failed parsing SMILES 'CCOc1c(Br)cc([C@H]2O[C@@H](c3ccc(C)cc3))=COC2=N2CCN(C)CC2)c1OC' for input: 'CCOc1c(Br)cc([C@H]2O[C@@H](c3ccc(C)cc3))=COC2=N2CCN(C)CC2)c1OC'\n",
      "[22:35:05] SMILES Parse Error: unclosed ring for input: 'C[C@H](O)C12CC3CC(CC(C1)C1)C2'\n",
      "[22:35:05] SMILES Parse Error: unclosed ring for input: 'COc1cc2ccc(=O)oc2cc1O[C@H]1O[C@@H](O[C@@H]2O[C@H]([C@H](O)[C@H](O)[C@H](O)[C@H](O)[C@H](O)[C@@H]2O)[C@@H](O)[C@@H](O)[C@@H]2O)C(=O)[C@@H](O)[C@@H]1O'\n",
      "[22:35:05] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[22:35:05] SMILES Parse Error: ring closure 1 duplicates bond between atom 5 and atom 6 for input: 'O=C(NCCN1c1ccc(Cl)cc1)C1(c2ccc(Cl)cc2)CCCC1'\n",
      "[22:35:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C)c(N2SC(NC(=O)CN2C(=O)c3ccccc3C2=O)C(=O)N(C)C)c1'\n",
      "[22:35:05] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)N2CCO[C@@H]3[C@@H]2CCCc3ccccc32)cn1'\n",
      "[22:35:05] Can't kekulize mol.  Unkekulized atoms: 3 4 22 23 24\n",
      "[22:35:05] SMILES Parse Error: unclosed ring for input: 'CCC(=O)N1CC[C@H]2[C@H](C1)c1c(-c3ccc(C)cc3)nn(C)c2=O'\n",
      "[22:35:05] Can't kekulize mol.  Unkekulized atoms: 5 6 14\n",
      "[22:35:06] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[22:35:06] SMILES Parse Error: ring closure 1 duplicates bond between atom 2 and atom 3 for input: 'O=C(C1C1CCOCC1)N1CC[C@H](COc2ccccn2)C1'\n",
      "[22:35:06] SMILES Parse Error: unclosed ring for input: 'Cc1nnnn1C[C@H]1CCCN(C(=O)c2cc3n(n2)CC[C@H](N)C2)C1'\n",
      "[22:35:06] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 19\n",
      "[22:35:06] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 22\n",
      "[22:35:06] SMILES Parse Error: unclosed ring for input: 'O[C@](c1nc(N2CCCC2)ccn1)C(N1CCOCC1)C1=O'\n",
      "[22:35:06] Can't kekulize mol.  Unkekulized atoms: 21 22 33 35 36\n",
      "[22:35:06] SMILES Parse Error: unclosed ring for input: 'CCc1c[nH]c2ncnc(N3C[C@@H]4[C@@H](COCc5ccccc5)C[C@@H]4C)c12'\n",
      "[22:35:06] SMILES Parse Error: unclosed ring for input: 'COc1c2c(c(N3)[C@@H](c3ccccc3)c1c(O)c1ccccc1c1=O)OC(C)(C)C[C@H]2C'\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)CN(C(=O)[C@@H]1C[C@@H](O)CN1)c2ccccc12'\n",
      "[22:35:07] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[22:35:07] SMILES Parse Error: ring closure 1 duplicates bond between atom 18 and atom 19 for input: 'CCCCC[C@H](O)/C=C/[C@H]1O[C@@H](O)C[C@H](O)[C@@H]1Cs1c1CCC(C)(C)CC1=N2'\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'CC1(C)CCC[C@]2(C)[C@@H]1[C@@H](OC(=O)CCC(=O)O)C=C1CO[C@@H](C)C1'\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1ccc(CNc2nc3c(cnn23)CCCC3)cc1'\n",
      "[22:35:07] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(C2(CNC(=O)C3CCOCC3)CCN(CCC(=O)NC[C@@H]2CCOC2)CC1'\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 27\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'CCc1nnc2c(c1C)OC(=N[C@H]1N2C[C@@H](c3cccs4)N=C12)N2N'\n",
      "[22:35:07] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[22:35:07] SMILES Parse Error: extra close parentheses while parsing: CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1\n",
      "[22:35:07] SMILES Parse Error: Failed parsing SMILES 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1' for input: 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1'\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 19 20 21 22\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'CO[C@H]1C[C@@H](c2nnc(C)o2)N(C(=O)[C@H]2S[C@]3(C(=O)Nc4ccc(F)cc43)CCC[C@H]23)C1'\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 19 21\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 2 3 17 18 19 20 21 22 23\n",
      "[22:35:07] SMILES Parse Error: extra close parentheses while parsing: O=c1c(-c2ccc(OC(F)(F)F)cc2)cc2nn1-c1ccccc1)C[C@@H]2C1CC1\n",
      "[22:35:07] SMILES Parse Error: Failed parsing SMILES 'O=c1c(-c2ccc(OC(F)(F)F)cc2)cc2nn1-c1ccccc1)C[C@@H]2C1CC1' for input: 'O=c1c(-c2ccc(OC(F)(F)F)cc2)cc2nn1-c1ccccc1)C[C@@H]2C1CC1'\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 3 13 14 17 18 19 20 21 22\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'C[C@]12CO[C@@H](c3ccc(Cl)cc3)N(C(=O)CCCO)[C@@H]1O'\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 3\n",
      "[22:35:07] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CCNC(=O)CN(c2ccc(CC)cc1)S(=O)(=O)c2ccc(C)cc2)cc1'\n",
      "[22:35:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'Cc1ccnc([C@@H](NC(=O)N[C@@H](C)[C@@H]2CC(F)(F)F)C2CC2)n1'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'CCCSc1nc2sc3sc(-c4c(NC)c(OC)c(OC)c4)nn3c2c2c1C'\n",
      "[22:35:08] Can't kekulize mol.  Unkekulized atoms: 3 4 18\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'CCCOc1ccc(S(=O)(=O)N(CC)CC)c2ccccc1C(=O)NCc1ccc(Cl)cc1'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1nc2c(s1)-c1cccc3cccc(c33)[C@@H]12'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(Cl)c1F)/C1=C2CCCCC1'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'O=c1oc2c(cnc2nc3ccccc3[nH]1)c1ccccc12'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'O=C(/C=C/c1ccccc1)N[C@@H]1CCC[C@H](C2)N1Cc1ccccc1'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1CC[C@H](O)[C@@]2(CCc3ccccc3OC)C2'\n",
      "[22:35:08] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2[nH]cc(CCNC(=O)N[C@@H]3CCCCNS(=O)(=O)CO)c3c2)cc1\n",
      "[22:35:08] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2[nH]cc(CCNC(=O)N[C@@H]3CCCCNS(=O)(=O)CO)c3c2)cc1' for input: 'COc1ccc2[nH]cc(CCNC(=O)N[C@@H]3CCCCNS(=O)(=O)CO)c3c2)cc1'\n",
      "[22:35:08] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 22\n",
      "[22:35:08] Can't kekulize mol.  Unkekulized atoms: 7 8 18 19 20\n",
      "[22:35:08] SMILES Parse Error: extra close parentheses while parsing: O=C(Nc1nnc(SCc2ccccc2)s1)[C@H]1CCCN2S(=O)(=O)c2cccs2)c1\n",
      "[22:35:08] SMILES Parse Error: Failed parsing SMILES 'O=C(Nc1nnc(SCc2ccccc2)s1)[C@H]1CCCN2S(=O)(=O)c2cccs2)c1' for input: 'O=C(Nc1nnc(SCc2ccccc2)s1)[C@H]1CCCN2S(=O)(=O)c2cccs2)c1'\n",
      "[22:35:08] Explicit valence for atom # 27 O, 5, is greater than permitted\n",
      "[22:35:08] Can't kekulize mol.  Unkekulized atoms: 5 15 16 17 18 19 20 21 22 23 24\n",
      "[22:35:08] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'CC(C)Oc1ccc(-c2noc(CN3N=N[C@@H]4C(=O)N(c5ccccc5)C(=O)C5)n32)cc1'\n",
      "[22:35:08] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@@H]3C[C@@H]([C@@H]4SC(=O)N#C4)[C@H]2C(=O)N1c1cccc2ccccc12'\n",
      "[22:35:09] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(cc1OC)CN(c1ncnc3sc4nc(C(C)C)nc(C)c3c13)CC2'\n",
      "[22:35:09] SMILES Parse Error: unclosed ring for input: 'CC[C@H](CSc1ccccc1)NC(=O)N[C@H]1CCc2c1ccc(Cl)cc1Cl'\n",
      "[22:35:09] Can't kekulize mol.  Unkekulized atoms: 3 4 6 8 16\n",
      "[22:35:09] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)[C@@H]1N(S(=O)(=O)c2ccc(C)cc2)c2ccccc2O[C@@]1(C)Oc1ccccc12'\n",
      "[22:35:09] Can't kekulize mol.  Unkekulized atoms: 14 16 17 18 19 20 21\n",
      "[22:35:09] Explicit valence for atom # 21 C, 5, is greater than permitted\n",
      "[22:35:09] SMILES Parse Error: unclosed ring for input: 'O=C(COc1cccc(/C=C2/SC(=O)N(-c4ccccc4)N32)c1)c1ccc(F)cc1F'\n",
      "[22:35:09] Can't kekulize mol.  Unkekulized atoms: 2 3 4 33 34\n",
      "[22:35:09] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 14 15 16 17 27\n",
      "[22:35:09] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 8 24\n",
      "[22:35:09] SMILES Parse Error: unclosed ring for input: 'CCCc1cc(=O)oc2c(C)c(O[C@H](C)C(=O)N[C@H](Cc3c[nH]c4ccccc35)C(=O)O)ccc12'\n",
      "[22:35:09] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1OCC(=O)N[C@H](c1cc(Cl)ccc1OC)C1CC1)c1ccccc1F\n",
      "[22:35:09] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1OCC(=O)N[C@H](c1cc(Cl)ccc1OC)C1CC1)c1ccccc1F' for input: 'COc1ccccc1OCC(=O)N[C@H](c1cc(Cl)ccc1OC)C1CC1)c1ccccc1F'\n",
      "[22:35:09] SMILES Parse Error: unclosed ring for input: 'CN(Cc1ccccc1)S(=O)(=O)N1C[C@H]1CN(c2ccc(C(F)(F)F)cn2)CC1'\n",
      "[22:35:09] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 21\n",
      "[22:35:10] Can't kekulize mol.  Unkekulized atoms: 8 9 13 14 32 34\n",
      "[22:35:10] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[22:35:10] Can't kekulize mol.  Unkekulized atoms: 2 4 5 6 22 23 24\n",
      "[22:35:10] SMILES Parse Error: unclosed ring for input: 'O[C@H]1CCN(C2CC2(COc3ccccc3)CC2)C1'\n",
      "[22:35:10] SMILES Parse Error: extra open parentheses for input: 'COc1ccc2c(c1O[C@]1(C)O[C@H](O)C[C@@H](C)[C@@H](C)[C@H]1C(C)=C[C@@H]3CC[C@H]22'\n",
      "[22:35:10] Can't kekulize mol.  Unkekulized atoms: 6 7 8 17 18 19 20 21 22\n",
      "[22:35:10] Can't kekulize mol.  Unkekulized atoms: 16 17 23\n",
      "[22:35:10] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CCC[C@@H](c2nnc3n2CCNCC3)c1C(N3CCCC1)=O)Cc1ccccc1'\n",
      "[22:35:10] SMILES Parse Error: unclosed ring for input: 'Cc1cnc(N)c(N2CC3(C[C@H]4CC[C@@H](C4)N4C)C2)n1'\n",
      "[22:35:10] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 23 24 25\n",
      "[22:35:10] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[22:35:10] SMILES Parse Error: extra close parentheses while parsing: C[C@]12COC3(CCN(C(=O)c4ccn(C)c(=O)c4)CC3)CO1)[C@H](C)O2\n",
      "[22:35:10] SMILES Parse Error: Failed parsing SMILES 'C[C@]12COC3(CCN(C(=O)c4ccn(C)c(=O)c4)CC3)CO1)[C@H](C)O2' for input: 'C[C@]12COC3(CCN(C(=O)c4ccn(C)c(=O)c4)CC3)CO1)[C@H](C)O2'\n",
      "[22:35:11] SMILES Parse Error: unclosed ring for input: 'O=C(OCc1nc(N2CCOCC2)nc2scc(-c3ccccc3)c12)c1ccccc1OC2CCCC1'\n",
      "[22:35:11] Can't kekulize mol.  Unkekulized atoms: 2 3 19\n",
      "[22:35:11] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[22:35:11] SMILES Parse Error: unclosed ring for input: 'S=C(NCCC12CC4CC(CC(C3)C1)C2)c1ccccc1'\n",
      "[22:35:11] Can't kekulize mol.  Unkekulized atoms: 2 15 16 23 24\n",
      "[22:35:11] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@@]1(C(C)C)O[C@H]3CN(C(=O)CC(C)C)C[C@H](O[C@@H]3CCC[C@H]3O)[C@@H]2O1'\n",
      "[22:35:11] SMILES Parse Error: unclosed ring for input: 'C[C@H](Sc1nc2sc3c(c2c(-c2ccccc1))CCCC3)C(=O)O'\n",
      "[22:35:11] Can't kekulize mol.  Unkekulized atoms: 10 11 13 26 27 28 29\n",
      "[22:35:11] Can't kekulize mol.  Unkekulized atoms: 9 10 14 15 16 17 18 19 20\n",
      "[22:35:11] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(OCCn2c3ccccc3ccccc34)c2=O)c1Cc1ccccc1'\n",
      "[22:35:11] SMILES Parse Error: extra open parentheses for input: 'O=C(Cc1nn2c(n1)[C@@H](c1ccc(Br)cc1)C[C@@H](c1ccccc1)N2'\n",
      "[22:35:12] SMILES Parse Error: unclosed ring for input: 'CCN1C[C@@H](c2nc3c(c2C)nc(C)[nH]c2=O)O[C@@H](C2CC2)C1'\n",
      "[22:35:12] SMILES Parse Error: unclosed ring for input: 'Cn1c(=O)c2c(nc3n(-c4ccc(OCc5ccccc5)c(N)c4)c(=O)n23)c1ccccc12'\n",
      "[22:35:12] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@@H]1[C@H](n2cnc3c2c(=O)n(C)c(=O)n2C)N(C)C(=O)N[C@@H]1c1ccccc1'\n",
      "[22:35:12] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 7 8 19 20 21 22\n",
      "[22:35:12] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)NC2N3CCC[C@H]2CC(c2ccccc2)=O)cc1F'\n",
      "[22:35:12] Can't kekulize mol.  Unkekulized atoms: 10\n",
      "[22:35:12] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(C(=O)NCc2nnc3cc(C(=O)NC[C@@H](C)s)n2C)cc1OC)cc1\n",
      "[22:35:12] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(C(=O)NCc2nnc3cc(C(=O)NC[C@@H](C)s)n2C)cc1OC)cc1' for input: 'COc1ccc(C(=O)NCc2nnc3cc(C(=O)NC[C@@H](C)s)n2C)cc1OC)cc1'\n",
      "[22:35:12] Can't kekulize mol.  Unkekulized atoms: 6 7 9\n",
      "[22:35:12] Can't kekulize mol.  Unkekulized atoms: 11 12 24\n",
      "[22:35:12] Explicit valence for atom # 11 S, 7, is greater than permitted\n",
      "[22:35:12] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[22:35:12] Can't kekulize mol.  Unkekulized atoms: 1 2 4\n",
      "[22:35:13] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 19\n",
      "[22:35:13] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 15\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'O=C1C[C@@H]2NS3C(=O)N(CCc4ccccc4)C(=O)[C@H]3[C@H]2C=CN12'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'Fc1cc(-c2ccccc2)cc2c1O[C@H]1O[C@H]1[C@H]2[C@H]3OC[C@@H]([C@H]3O)[C@H]21'\n",
      "[22:35:13] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 19\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'Cc1cc(-c2cnc3n(C2CCCCC3)cc2CO)no1'\n",
      "[22:35:13] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14\n",
      "[22:35:13] SMILES Parse Error: extra close parentheses while parsing: CN1CC2(C(=O)Nc3ccc(N4CCCC4=O)cc3)CC2)C1=O\n",
      "[22:35:13] SMILES Parse Error: Failed parsing SMILES 'CN1CC2(C(=O)Nc3ccc(N4CCCC4=O)cc3)CC2)C1=O' for input: 'CN1CC2(C(=O)Nc3ccc(N4CCCC4=O)cc3)CC2)C1=O'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)Nc2nnc3n2CCS2)cc1'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(NC(=O)[C@@H]2[C@@H]3C[C@@H]4[C@@H]2C(=O)N3Cc2ccccc2)c1)c1ccccc1'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(CC(=O)N1C[C@H](NC(=O)OC(C)(C)C)CCN(C(=O)c3ccccc3)CC2)cc1'\n",
      "[22:35:13] SMILES Parse Error: ring closure 1 duplicates bond between atom 9 and atom 10 for input: 'Fc1c(I)ccc2c1CN1c1ccccc1F'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'COc1cccc(C(=O)COC(=O)[C@H](C)N2C(=O)[C@@H]3[C@H]5C=C[C@@H]([C@@H]5C[C@H]45)[C@H]3C2=O)c1'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'CCO[C@@H]1C[C@@H](NC[C@@H]2Cc3ccccc3O3)C12CCOCC2'\n",
      "[22:35:13] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)[C@@H]1Cc2c(ncn2C)N(C(=O)CCCOc2ccccc2Cl)C1'\n",
      "[22:35:14] SMILES Parse Error: unclosed ring for input: 'COc1cc(CNC[C@@]2(O)COC3(F)F)cc(OC)c2O1'\n",
      "[22:35:14] SMILES Parse Error: unclosed ring for input: 'Cc1cc2cc(CNc3ccc(-c4cnc6ccccc5c4)cc3)cc(=O)n2c1'\n",
      "[22:35:14] Can't kekulize mol.  Unkekulized atoms: 13 14 15 17 18\n",
      "[22:35:14] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN(C(=O)c2cc(F)cc(S(N)(=O)=O)c2Br)C[C@H]2NC[C@H]1C'\n",
      "[22:35:14] Can't kekulize mol.  Unkekulized atoms: 6 7 8 10\n",
      "[22:35:14] Can't kekulize mol.  Unkekulized atoms: 5 6 7 16 17\n",
      "[22:35:14] Can't kekulize mol.  Unkekulized atoms: 4 5 20 21 22\n",
      "[22:35:14] Can't kekulize mol.  Unkekulized atoms: 10 11 12\n",
      "[22:35:14] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)C2CCN(c3nc4c(C(F)(F)F)ccc(C(F)(F)F)n3n3)CC2)nc1'\n",
      "[22:35:14] SMILES Parse Error: duplicated ring closure 1 bonds atom 12 to itself for input: 'O=C(c1ccccc1)C12C3[C@@H]4[C@H]1C11[C@H]2C4=C2[C@H]5c4sc(Br)cc4[C@H](c3ccccc3)[C@H]31'\n",
      "[22:35:14] SMILES Parse Error: unclosed ring for input: 'C[C@H](O)c1cccc(N2CCC(NCc3cccn3nc3ccccc34)CC2)n1'\n",
      "[22:35:14] Explicit valence for atom # 8 N, 4, is greater than permitted\n",
      "[22:35:14] SMILES Parse Error: unclosed ring for input: 'O=S(=O)(O)[C@@H]1C[C@H]2C[C@@H]1[C@H]2C2(CCCC2)O1'\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 29\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 1 2 10 11 20 21 22\n",
      "[22:35:15] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1ccccc1)[C@@H]1CS[C@@H]2C[C@H]3C[C@H]2[C@H]24'\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 1 2 20 21 22\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 1 2 19\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 5 6 16\n",
      "[22:35:15] SMILES Parse Error: unclosed ring for input: 'COc1cc2ccc(=O)oc2cc1O[C@H]1O[C@@](O)([C@H]2O)[C@@H](O)[C@H](O)[C@@H]1O'\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 2 25 26\n",
      "[22:35:15] SMILES Parse Error: unclosed ring for input: 'Cn1cccc1[C@@H]1CNCCN1C(=O)CNc1c(C2CC3)cnc2ccccc12'\n",
      "[22:35:15] SMILES Parse Error: unclosed ring for input: 'CN(C)S(=O)(=O)N1C[C@H]2C[C@H]2C[C@H]1[C@@H]1OCC[C@H]2O1'\n",
      "[22:35:15] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[22:35:15] SMILES Parse Error: ring closure 3 duplicates bond between atom 15 and atom 16 for input: 'CC(C)c1cc([C@@H]2CCCN2C(=O)c2cc3n3c(n2)CCCN(C)C3)no1'\n",
      "[22:35:16] SMILES Parse Error: unclosed ring for input: 'CC(C)(O)[C@@H]1CC[C@]2(C)[C@H]3CC[C@]4(O)C[C@H](OC(C)=O)[C@](CO)(CO)[C@@H]41'\n",
      "[22:35:16] Explicit valence for atom # 11 O, 3, is greater than permitted\n",
      "[22:35:16] SMILES Parse Error: unclosed ring for input: 'COCCN1C(=O)[C@@H]2C[C@@H](O)CN2C2=N1'\n",
      "[22:35:16] SMILES Parse Error: unclosed ring for input: 'FC(F)c1nn(CCN2CCOCC2)c2c1CCC3'\n",
      "[22:35:16] SMILES Parse Error: ring closure 2 duplicates bond between atom 21 and atom 22 for input: 'FC(F)(F)c1cccc2c1NCC[C@H]2NC[C@@H]1[C@H]3CCC[C@H]2[C@H]2O1'\n",
      "[22:35:16] Can't kekulize mol.  Unkekulized atoms: 18 19 20 22 23\n",
      "[22:35:16] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[22:35:16] Can't kekulize mol.  Unkekulized atoms: 10 11 18\n",
      "[22:35:16] Can't kekulize mol.  Unkekulized atoms: 7 8 25 26 28\n",
      "[22:35:16] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13 14 15\n",
      "[22:35:16] SMILES Parse Error: unclosed ring for input: 'CC(=O)N1CCN(c2c(C)c3ccc(C)cc3nc2Cl)c(CC)nc2C1'\n",
      "[22:35:16] Can't kekulize mol.  Unkekulized atoms: 2 15 16 21 22 29 30\n",
      "[22:35:16] SMILES Parse Error: unclosed ring for input: 'O=C(c1cc[nH]c1)N1CC[C@@]23OC[C@@H](c4ccccc4)N2C(=O)C[C@@H]14'\n",
      "[22:35:16] SMILES Parse Error: unclosed ring for input: 'CN(CCOc1ccccc1)Cc1cnc(C2CCOCC2)n2C'\n",
      "[22:35:16] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 11 12 20 21 22\n",
      "[22:35:16] non-ring atom 17 marked aromatic\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 17 18 19 22 23\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 14 15 19\n",
      "[22:35:17] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)N2CCC[C@@H](c3ccc4c(n3)CCN(C)C3)C2)c2ccccc12'\n",
      "[22:35:17] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc(NC(=O)CSc2nc(O)c3c(n2)c(=O)n(C)c2=O)n2Cc2ccccc2)cc1C\n",
      "[22:35:17] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(NC(=O)CSc2nc(O)c3c(n2)c(=O)n(C)c2=O)n2Cc2ccccc2)cc1C' for input: 'Cc1ccc(NC(=O)CSc2nc(O)c3c(n2)c(=O)n(C)c2=O)n2Cc2ccccc2)cc1C'\n",
      "[22:35:17] SMILES Parse Error: unclosed ring for input: 'FC(F)(F)c1cccc2cnn(Cc3ccc(Cl)c4c3OC[C@@H]3CO)c12'\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 25\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 17 18 19\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 3 5 6 7 23\n",
      "[22:35:17] Can't kekulize mol.  Unkekulized atoms: 9 10 19 20 21 22 23\n",
      "[22:35:17] SMILES Parse Error: extra close parentheses while parsing: Cc1ncc[nH]1)c1ccc(NC(=O)N(C)Cc2noc(C)n2)cc1\n",
      "[22:35:17] SMILES Parse Error: Failed parsing SMILES 'Cc1ncc[nH]1)c1ccc(NC(=O)N(C)Cc2noc(C)n2)cc1' for input: 'Cc1ncc[nH]1)c1ccc(NC(=O)N(C)Cc2noc(C)n2)cc1'\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 2 3 11 12 14 23 25\n",
      "[22:35:18] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N2C(=O)[C@@H]3[C@H]4C[C@H]5[C@H](O[C@]2(C)[C@@H]53)[C@@H]43)c1'\n",
      "[22:35:18] Explicit valence for atom # 10 C, 5, is greater than permitted\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 8 9 16 17 22\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 29 30 31\n",
      "[22:35:18] SMILES Parse Error: unclosed ring for input: 'O=C1c2cccc(Br)c2[C@H]2C[C@H]1[C@@H]2C(=O)NC1=O'\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 9\n",
      "[22:35:18] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CCCCN1CCCC(=O)N1CC2[C@@H](CCc2ccccc2)[C@@H]1O'\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 7 24 25 27 28\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7 8 9 10 11\n",
      "[22:35:18] SMILES Parse Error: ring closure 4 duplicates bond between atom 14 and atom 15 for input: 'Cc1ccc2[nH]c(C3CCN(C(=O)CC4(c4ccc(F)cc4)CC3)nn2)cc1'\n",
      "[22:35:18] Can't kekulize mol.  Unkekulized atoms: 9 10 22\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@@]12CC[C@@H](C[C@@H]1C2(C)C)C1)c1cnn2cccnc12'\n",
      "[22:35:19] SMILES Parse Error: duplicated ring closure 2 bonds atom 13 to itself for input: 'Cc1cc(C)c(C(=O)[C@@H](C)OC(=O)C22CC3C[C@@]3(CC(=O)CC2)C4)n1'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'CC(C)c1ccc(C(=O)Oc2cccc(S(=O)(=O)N[C@@H]3CCC[C@H]4C3=O)c2)cc1'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'Cc1nn(C)c(NS(=O)(=O)c2ccc3c(c2)CCC(C)(C)C3)c(=O)n12'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'NC(=O)c1ccc(Cn2c(NCc3cccs3)nc3c5c(ccc3n2)OCO4)c1'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)CS[C@@H](N2CCN(c3cccc4nccc3c3)CC2)C1=O'\n",
      "[22:35:19] Can't kekulize mol.  Unkekulized atoms: 8 9 19\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccccc1CSc1ncnc3sccc22)N1C[C@@H]2COC[C@@H](C1)O2'\n",
      "[22:35:19] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 15 16 24\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N2CCN(C(=O)[C@@H]3CC(c4ccc(F)cc3)=NO3)CC2)c1C'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C2=NO[C@@H]3[C@H]4C[C@@H]([C@H]23)[C@@H]2C(=O)N(c3ccc(Oc4ccccc4)cc3)C2=O)cc1'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1sc(N2C(=O)[C@H]3C[C@H](c3ccccc3)C2=C)nc1C'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'COc1ccc(N2C[C@H](C(=O)N3CCN4C(=O)[C@H]4CC=CC[C@@H]4C3=O)CC2=O)c(OC)c1'\n",
      "[22:35:19] SMILES Parse Error: ring closure 2 duplicates bond between atom 6 and atom 7 for input: 'COc1ccc([C@H]2C2=C(C)Nc3nc4ccccc4n32)cc1OC'\n",
      "[22:35:19] SMILES Parse Error: extra close parentheses while parsing: O=C(O)CCOCCOCCOCCO)CNC(=O)CCCc1nc2ccccc2s1\n",
      "[22:35:19] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CCOCCOCCOCCO)CNC(=O)CCCc1nc2ccccc2s1' for input: 'O=C(O)CCOCCOCCOCCO)CNC(=O)CCCc1nc2ccccc2s1'\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1c(CNC2CC2(C2)CCC3)nn(CC(F)(F)F)c1C'\n",
      "[22:35:19] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 22 23 24\n",
      "[22:35:19] SMILES Parse Error: unclosed ring for input: 'O=C(Oc1ccccc1)c1cccc(N2C(=O)[C@@H]3[C@H]4C=C[C@@H]([C@@H]5C[C@H]55)[C@@H]3C2=O)c1'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)CC(=O)N1CCC[C@H]1C(=O)N[C@@H]1CC1(C(N)=O)Cc2ccccc21'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'CC(C)c1ccc(C(=O)Cn2ccc3c(sc2s)c(=O)n(C)c2=O)cc1'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'Cc1c(NC(=O)[C@@H](C)Sc2nc(N)c3c4c(sc3n2)CCCCC3)c(C)nc1C'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'FC(F)(F)c1[nH]nc2cc(Cl3nn([C@H]4CCS(=O)(=O)C4)c(=O)[nH]c32)cc12'\n",
      "[22:35:20] Can't kekulize mol.  Unkekulized atoms: 13 14 15 17 18\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'CCNS(=O)(=O)c1ccc(NC(=S)NCCN2CCc4ccccc3C2)cc1'\n",
      "[22:35:20] Can't kekulize mol.  Unkekulized atoms: 1 2 3 21 22\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)c1c(NC(=O)c2cc(-c3ccc(C)s3)nc3ccccc23)sc2c1CC[C@H](CCC2CCCC1)C2'\n",
      "[22:35:20] Can't kekulize mol.  Unkekulized atoms: 16 17 26\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'c1csc(-c2ccnc3nc(CCCCc4c[nH]c4ccccc25)nn23)c1'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'CN(C)CCN1C[C@@H]2C[C@H]3C[C@@H](C(=O)N3CCCC3)[C@@H]2C1'\n",
      "[22:35:20] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 18 22 23 24 25\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'Cn1ncc(C(=O)N[C@@H]2COc3c2c(Cl)cccc2Cl)c1C(F)(F)F'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'C[C@]12C[C@H]1c1cnc3c(c1-c1cccnc1)CCCS3'\n",
      "[22:35:20] SMILES Parse Error: unclosed ring for input: 'O=C(C1CCCCC1)N1CC[C@@H](NC(=O)N2C[C@H]3C[C@@H]2C2)C[C@@H]1O'\n",
      "[22:35:20] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23 24 25 26 27\n",
      "[22:35:20] Can't kekulize mol.  Unkekulized atoms: 2 4 5\n",
      "[22:35:20] SMILES Parse Error: extra open parentheses for input: 'O=C(C[C@@H](Nc1cc(-n2cnnn2)ccc1=O)C(=O)N1CCOCC1'\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'COc1cccc(CNc2nc3n(n3)[C@H](C(C)C)C[C@@H](c2ccc(C)cc2)N3)c1'\n",
      "[22:35:21] SMILES Parse Error: ring closure 1 duplicates bond between atom 3 and atom 4 for input: 'O=C(NC1C1CCCCC1)NC1(CN2CCOCC2)CCCCC1'\n",
      "[22:35:21] Explicit valence for atom # 19 O, 3, is greater than permitted\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'CCCCCCc1cc2cc(C(N)=O)/c(=N/c3cccc(-c4nc5ccccc5s4)c3o2)ccc1N1'\n",
      "[22:35:21] Explicit valence for atom # 24 O, 3, is greater than permitted\n",
      "[22:35:21] Explicit valence for atom # 14 C, 5, is greater than permitted\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'COc1cc(NC(=O)CN2N=N(c4cccc(Cl)c3)C(=O)[C@@H]2C)cc(I)c1OCc1ccc(Cl)cc1'\n",
      "[22:35:21] SMILES Parse Error: ring closure 2 duplicates bond between atom 1 and atom 20 for input: 'C[C@@]12CC[C@@]3(C)[C@@H](CC[C@@H]4[C@@H](C)C(=O)O)C[C@@]4(C)CC[C@@H]12'\n",
      "[22:35:21] non-ring atom 3 marked aromatic\n",
      "[22:35:21] SMILES Parse Error: extra close parentheses while parsing: N[C@@H](CC1=CCCC1)C(=O)O)N\n",
      "[22:35:21] SMILES Parse Error: Failed parsing SMILES 'N[C@@H](CC1=CCCC1)C(=O)O)N' for input: 'N[C@@H](CC1=CCCC1)C(=O)O)N'\n",
      "[22:35:21] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11\n",
      "[22:35:21] SMILES Parse Error: extra close parentheses while parsing: Cc1c(C)c2ccc3c(c2)OCCC3)c(OCC)cc1=O\n",
      "[22:35:21] SMILES Parse Error: Failed parsing SMILES 'Cc1c(C)c2ccc3c(c2)OCCC3)c(OCC)cc1=O' for input: 'Cc1c(C)c2ccc3c(c2)OCCC3)c(OCC)cc1=O'\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'CC[C@@H]1CCc2nc3sc4c(-c4ccccc5)c(-c5ccccc5)nc4c3c2C1'\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'CCOc1cc(=O)oc2c(C)c(O[C@@H](C)C(=O)N[C@H](Cc3c[nH]c4ccccc35)C(=O)O)ccc12'\n",
      "[22:35:21] Can't kekulize mol.  Unkekulized atoms: 10 11 12\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'Cc1ccc([C@@H](C)N(C)Cc2cc3c(cc2OC)OC)c(OC)c1'\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'NC(=O)C(Cl)(NC(=O)c1ccc2c(c1)C(=O)N1)c1ccccc1O2'\n",
      "[22:35:21] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 16 17 18\n",
      "[22:35:21] SMILES Parse Error: unclosed ring for input: 'Cc1csc(N2CCN(c3ncnc4c3[nH]c3cccc(F)c3)C[C@@H]23)n1'\n",
      "[22:35:21] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 21\n",
      "[22:35:22] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 24\n",
      "[22:35:22] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 15 16 17 18 19\n",
      "[22:35:22] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 10 17 18 19 20 21 22\n",
      "[22:35:22] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 22\n",
      "[22:35:22] SMILES Parse Error: extra close parentheses while parsing: COc1cc(-c2c(C)oc3cc(OCC(=O)NCCCCO)ccc32)ccOOBr)cc1O\n",
      "[22:35:22] SMILES Parse Error: Failed parsing SMILES 'COc1cc(-c2c(C)oc3cc(OCC(=O)NCCCCO)ccc32)ccOOBr)cc1O' for input: 'COc1cc(-c2c(C)oc3cc(OCC(=O)NCCCCO)ccc32)ccOOBr)cc1O'\n",
      "[22:35:22] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)N2CCN([C@H]3C[C@@H](C(=O)NCc4ccccc4)Oc3ccccc34)CC2)cc1'\n",
      "[22:35:22] Can't kekulize mol.  Unkekulized atoms: 14 15 17\n",
      "[22:35:22] SMILES Parse Error: unclosed ring for input: 'Cc1cc(-n2ncc3c(N)n[nH]c3[C@@H]23)n(C)c1C'\n",
      "[22:35:22] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[22:35:22] SMILES Parse Error: unclosed ring for input: 'CC[C@H](Oc1ccccc1)C(=O)Nc1ccc(C(c2(C)C)c2ccccc2)cc1'\n",
      "[22:35:22] SMILES Parse Error: extra close parentheses while parsing: O=C(CCn1c(=O)oc2ccccc21)N[C@H]1O[C@@H](C(=O)O)C[C@@H]2OCCS2)n1\n",
      "[22:35:22] SMILES Parse Error: Failed parsing SMILES 'O=C(CCn1c(=O)oc2ccccc21)N[C@H]1O[C@@H](C(=O)O)C[C@@H]2OCCS2)n1' for input: 'O=C(CCn1c(=O)oc2ccccc21)N[C@H]1O[C@@H](C(=O)O)C[C@@H]2OCCS2)n1'\n",
      "[22:35:22] Explicit valence for atom # 12 F, 2, is greater than permitted\n",
      "[22:35:22] SMILES Parse Error: unclosed ring for input: 'Cc1nc(-c2ccc(CNC(=O)c3nccn3c(C)ccc23)o2)cs1'\n",
      "[22:35:23] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20\n",
      "[22:35:23] Can't kekulize mol.  Unkekulized atoms: 25 26 27\n",
      "[22:35:23] SMILES Parse Error: unclosed ring for input: 'CC1=C2[C@@H]3C4CC(C)(C4)[C@H]1[C@@H]2[C@@H]2C(=O)N(c4ccccc4O)C(=O)[C@@H]2[C@@H]31'\n",
      "[22:35:23] SMILES Parse Error: unclosed ring for input: 'Cc1cc2oc3c(c(=O)c2CCc2ccccc2)c(=O)n(C)c2o'\n",
      "[22:35:23] SMILES Parse Error: unclosed ring for input: 'Cc1noc2ncnc(N[C@H](c3nnnn3C3CC3)c2ccccc2)c1=O'\n",
      "[22:35:23] SMILES Parse Error: unclosed ring for input: 'Cc1ccccc1N1c2c(c3ccccc3[C@@H]2c3ccc(Br)cc31)C(=O)N(Cc1ccco1)C2=O'\n",
      "[22:35:23] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 9\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 2 10 12\n",
      "[22:35:24] SMILES Parse Error: extra close parentheses while parsing: CN(C)S(=O)(=O)c1ccc(CNC(=O)[C@@H]2CC=O)CC2)cc1\n",
      "[22:35:24] SMILES Parse Error: Failed parsing SMILES 'CN(C)S(=O)(=O)c1ccc(CNC(=O)[C@@H]2CC=O)CC2)cc1' for input: 'CN(C)S(=O)(=O)c1ccc(CNC(=O)[C@@H]2CC=O)CC2)cc1'\n",
      "[22:35:24] Explicit valence for atom # 13 Cl, 2, is greater than permitted\n",
      "[22:35:24] SMILES Parse Error: ring closure 2 duplicates bond between atom 4 and atom 20 for input: 'CN1CC[C@]2(CC(=O)NCc3c(C)n4ccnc4n3C)[C@H]2CCCC[C@@H]21'\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7 8 9 22 24\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 2 23 24 25 26 27 28\n",
      "[22:35:24] SMILES Parse Error: unclosed ring for input: 'Cc1cc(CN2C[C@@H]2CN(Cc3ccc(F)cc3)C2)nc(N)n1'\n",
      "[22:35:24] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(Cn2nc3n(c(=O)c2=O)C[C@H]2NCc2ccccc2C4)cc1'\n",
      "[22:35:24] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)N(C2CCCCC2)[C@H]2O[C@H]([C@H]3O[C@@H]4[C@H]([C@H]4O)O[C@]4(C)[C@@H](C(C)C)[C@H]32)C1'\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 27\n",
      "[22:35:24] SMILES Parse Error: unclosed ring for input: 'COC(=O)c1cc(-c2ccc(Cl)cc2)s/c(=N\\c3ccc(Cc4ccccc4)cc3)n(C)c12'\n",
      "[22:35:24] SMILES Parse Error: unclosed ring for input: 'Cc1ccc([C@H](CNC(=O)c2cc(-c3cn(C)nn3)no2)C2)cc1'\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 13 14 22 23 24\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:35:24] SMILES Parse Error: unclosed ring for input: 'COc1cccc(OCC(=O)Nc2cccc(CN2CCCCC3)c2)c1'\n",
      "[22:35:24] Can't kekulize mol.  Unkekulized atoms: 2 4 5\n",
      "[22:35:24] SMILES Parse Error: extra close parentheses while parsing: O=C(C[C@@H]1SCn2cccn2)NC1=O)NCc1ccc2c(c1)OCO2\n",
      "[22:35:24] SMILES Parse Error: Failed parsing SMILES 'O=C(C[C@@H]1SCn2cccn2)NC1=O)NCc1ccc2c(c1)OCO2' for input: 'O=C(C[C@@H]1SCn2cccn2)NC1=O)NCc1ccc2c(c1)OCO2'\n",
      "[22:35:25] Can't kekulize mol.  Unkekulized atoms: 6 7 21 22 23 24 25\n",
      "[22:35:25] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 18 19 20 21 22 23 24 25 26\n",
      "[22:35:25] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[22:35:25] SMILES Parse Error: extra close parentheses while parsing: CCOc1ccc2c(c1)O[C@]1(C=C2)CN(c2ncccn2)C1)C(=O)O2\n",
      "[22:35:25] SMILES Parse Error: Failed parsing SMILES 'CCOc1ccc2c(c1)O[C@]1(C=C2)CN(c2ncccn2)C1)C(=O)O2' for input: 'CCOc1ccc2c(c1)O[C@]1(C=C2)CN(c2ncccn2)C1)C(=O)O2'\n",
      "[22:35:25] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1cc(-c2cn([C@H]2CCN(S(C)(=O)=O)C2)n(-c2ccccc2)n1)cn1'\n",
      "[22:35:25] SMILES Parse Error: unclosed ring for input: 'Cc1nccn1[C@@H]1CC[C@H]2NC(=O)[C@H]3[C@@H](O)CCC[C@]12C'\n",
      "[22:35:25] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 17 18 19 20 21\n",
      "[22:35:25] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C(F)(F)F)nn1C[C@@H](C)c1nc2c3c4scnc3nc(-c4ccccc4)nc3c2c2ccccc12'\n",
      "[22:35:25] Can't kekulize mol.  Unkekulized atoms: 14 15 16 18 19 20\n",
      "[22:35:25] SMILES Parse Error: unclosed ring for input: 'O=C1N[C@@H](Cc2cccc(O)c2)C(=O)N2C[C@@H](c3ccccc3)Oc3ccccc31'\n",
      "[22:35:25] Can't kekulize mol.  Unkekulized atoms: 2 3 13\n",
      "[22:35:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 29\n",
      "[22:35:26] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 23\n",
      "[22:35:26] Can't kekulize mol.  Unkekulized atoms: 13 14 16 17 19 21\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(/C=C2/C[C@@H]3C[C@@H]2[C@H]2C(=O)O)C3(CC(=O)O)C1'\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'C[C@]1(C(=O)NC23CC4CC(CC(C4)C2)C3)CC[C@H]2Oc2ccc(F)cc2'\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'CN(C[C@@H]1C[C@@](Br)(c2cccOc3ccccc34)nn1)C(=O)O'\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'Cc1cc(F)c(NS(=O)(=O)c2ccc3c(c2)C(=O)NCCc2ccccc2)c(C)c1C'\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1csc(NC(=O)[C@@H]2CC(=O)N(C)c2ccccc23)n1'\n",
      "[22:35:26] Explicit valence for atom # 1 C, 6, is greater than permitted\n",
      "[22:35:26] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14\n",
      "[22:35:26] Can't kekulize mol.  Unkekulized atoms: 21 22 23 24 25 26 33 35\n",
      "[22:35:26] SMILES Parse Error: extra close parentheses while parsing: O=C(NCc1cc(C2CC2)no1)[C@]12CCCN(CCc3c[nH]c4ccccc34)C2)CN1\n",
      "[22:35:26] SMILES Parse Error: Failed parsing SMILES 'O=C(NCc1cc(C2CC2)no1)[C@]12CCCN(CCc3c[nH]c4ccccc34)C2)CN1' for input: 'O=C(NCc1cc(C2CC2)no1)[C@]12CCCN(CCc3c[nH]c4ccccc34)C2)CN1'\n",
      "[22:35:26] Can't kekulize mol.  Unkekulized atoms: 7 8 9 23 24\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(N2C(=O)CN(Cc3nc4ccccc4[nH]3)CC23CCCCC2)c(OC)c1'\n",
      "[22:35:26] SMILES Parse Error: unclosed ring for input: 'O=[N+]([O-])[C@@]12C[C@@H]1C[C@@H]2CC[C@H]2O1'\n",
      "[22:35:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 7 22 23\n",
      "[22:35:27] SMILES Parse Error: unclosed ring for input: 'CC(=O)Oc1ccc(C(=O)N[C@H]2C3CCCCCC2(O)c2ccccc2)cc1'\n",
      "[22:35:27] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19\n",
      "[22:35:27] SMILES Parse Error: unclosed ring for input: 'Cn1cc([C@H]2N(c3ccccc3)OC(c3ccccc3)=Nc2ccc(F)cc21)c1ccccc1'\n",
      "[22:35:27] SMILES Parse Error: unclosed ring for input: 'COC(=O)CN1C(=S)N(c2ccc(Cl)cc2)C(=O)/C1=C/c1c(O)c2c(cc1ccccc12)c1ccc(Cl)cc1'\n",
      "[22:35:27] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 22 23\n",
      "[22:35:27] Can't kekulize mol.  Unkekulized atoms: 2 3 4 33 34\n",
      "[22:35:27] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(N2C(=O)C[C@H](N(CCc3ccccc3)C(=O)CCC(=O)N3)C2=O)cc1'\n",
      "[22:35:28] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 20\n",
      "[22:35:28] Can't kekulize mol.  Unkekulized atoms: 2 3 4 12\n",
      "[22:35:28] SMILES Parse Error: unclosed ring for input: 'Cc1nnc([C@H]2CN(C(=O)c2cnc4[nH]ncc3c3)CCO2)o1'\n",
      "[22:35:28] Can't kekulize mol.  Unkekulized atoms: 3 4 5 26 27\n",
      "[22:35:28] SMILES Parse Error: unclosed ring for input: 'c1ccoc1-c1nc(CN2CCSCCO)ns1'\n",
      "[22:35:28] SMILES Parse Error: extra open parentheses for input: 'COc1ccc2cc(-n3ncc(C(=O)[C@@H]4CC(=O)N(c5ccc(C)cc4)C3)nc2c2'\n",
      "[22:35:28] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      "[22:35:28] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@H]3[C@H]4[C@@H]2[C@H]2[C@H]1[C@@H]3[C@@H]3C21OCCO1'\n",
      "[22:35:28] SMILES Parse Error: extra open parentheses for input: 'C[C@@H]1OC(=O)c2ccc3c(c2c2[C@@]1(C(=O)Nc2ccc(F)cc1)OC(=O)C[C@@H](c1ccco1)C3'\n",
      "[22:35:29] Can't kekulize mol.  Unkekulized atoms: 4 5 6 10 11 12 13 14 15\n",
      "[22:35:29] SMILES Parse Error: ring closure 3 duplicates bond between atom 15 and atom 16 for input: 'COc1ccc([C@H]2[C@@H](C(=O)N3C[C@@H]4C[C@H]3[C@H]3C3CC4)CCC(=O)N2C)cc1'\n",
      "[22:35:29] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CCCn2c1sc1c2c(=O)n(Cc3cccs3)c(=O)n(C)c2C1'\n",
      "[22:35:29] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19 20 22\n",
      "[22:35:29] SMILES Parse Error: unclosed ring for input: 'S=C1[C@@H](CCc2ccccc2)N[C@H]2CCN1c1ccc(F)cc1'\n",
      "[22:35:29] SMILES Parse Error: unclosed ring for input: 'CCN(CC)S(=O)(=O)c1ccc(=O)n(CC(=O)OC)c(=O)c2c1'\n",
      "[22:35:29] Explicit valence for atom # 5 N, 5, is greater than permitted\n",
      "[22:35:29] Can't kekulize mol.  Unkekulized atoms: 2 3 20 22\n",
      "[22:35:29] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 13 14 15 16\n",
      "[22:35:29] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 23 24\n",
      "[22:35:29] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 24 25 26\n",
      "[22:35:29] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1ccc(Br)cc1)C([C@H]1C(=O)OCCN(C(=O)c2cccc3ccccc23)C1'\n",
      "[22:35:29] SMILES Parse Error: unclosed ring for input: 'O=C(COc1ccccc1)Nc1ncnc2ncn2c1OCCO2'\n",
      "[22:35:30] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN2CCN(CC3=CC4)CCOCC2)s1'\n",
      "[22:35:30] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)c1cc(NC(=O)NC[C@H](NC(=O)N[C@H]2CCOc3ccccc32)CC3)no1'\n",
      "[22:35:30] Can't kekulize mol.  Unkekulized atoms: 2 3 4 22 24\n",
      "[22:35:30] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1ccccc1)c1cccc(-n2c3ccccc3n3c(nnc3c3ccccc3)=NN2'\n",
      "[22:35:30] SMILES Parse Error: ring closure 2 duplicates bond between atom 10 and atom 11 for input: 'Cc1ccc(C(=O)CSC(N2C2CC2)C(=O)N(Cc2ccccc2)C(=O)NC(C)C)cc1'\n",
      "[22:35:30] SMILES Parse Error: unclosed ring for input: 'O=C(O)[C@@H]1C[C@@H]2COC[C@H](NC[C@H]3CCCO3)C2'\n",
      "[22:35:30] SMILES Parse Error: unclosed ring for input: 'Cc1ccc([C@@]23C[C@H]3CC(O)C[C@H](C(C)C)NC3=O)c(C)n1'\n",
      "[22:35:30] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 8 30 31\n",
      "[22:35:30] Can't kekulize mol.  Unkekulized atoms: 13 14 25\n",
      "[22:35:30] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c3c(ccc2c1)[C@@H]1[C@@H]2CCCC[C@H]2S1(=O)=O'\n",
      "[22:35:30] Can't kekulize mol.  Unkekulized atoms: 16 17 18 21 22\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@H]3C[C@@H]4[C@@H]2C(=O)[C@H]1[C@H]4[C@@H]2[C@@H]4[C@@H]41'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)Nc2sc3c(c2C(N)=O)CC(C)(C)NC(C)=O)cc1OC'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C)cc1NC(=O)c1ccc(Cl)c(S(=O)(=O)N2C[C@H]3C[C@H](C2)c2cccc(=O)[nH]2)C3'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@@H]1CC(=O)N(CCc2ccccc2)C1)c1ccccc12'\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 19 20\n",
      "[22:35:31] Explicit valence for atom # 14 Br, 2, is greater than permitted\n",
      "[22:35:31] Explicit valence for atom # 2 O, 4, is greater than permitted\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 24 25 26 28 29 30 31\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 7 8 9 20 21 23 24 25 26 27 28\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 3 4 6 7 12\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'CCn1c(C)cc([C@H]2C[C@@H](c3ccc(C)cc3)N3NC(=S)N[C@H]2c2ccc(Cl)cc2)c1C'\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 11 12 20 21 22 23 24 25 26\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 1 2 3 20 21\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'CC(C)N1CCN(CC(=O)Nc2ccc(N3CCOCC2)cc2)C[C@@H]1CCO'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c([C@H](C)NC(=O)[C@@H]2CCCN(S(C)(=O)=O)C2)c2ccccc2'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)n1c(=O)n(C2CCCCC1)c(=O)n2CC(=O)N1CCC(C(N)=O)CC1'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'COc1cc([C@@H]2nnc3c2c(C)nn(C)c2O)cc(OC)c1O'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'C=CCc1ccc(C(=O)N2CCC[C@@H]2c2nn3c(nc2=O)SC[C@@H]2C)cc1'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNC(=S)Nc(ccccc2C)c2ccccc2)cc1OC'\n",
      "[22:35:31] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 10 13 14 15 16\n",
      "[22:35:31] SMILES Parse Error: duplicated ring closure 2 bonds atom 7 to itself for input: 'O=C(NC[C@@H]1CC[C@]22CCCCC[C@]2(O)CCc2ccccc2)C1'\n",
      "[22:35:31] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@@H]1CCCN(C1Cc2ccccc2)C1)c1cccc(F)c1'\n",
      "[22:35:32] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 23\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)OC(=O)N[C@@]12CC[C@H]2[C@@H]1CC(=O)O2'\n",
      "[22:35:32] SMILES Parse Error: extra close parentheses while parsing: CC(C)S(=O)(=O)N(C)C)c1ccc(S(=O)(=O)NCc2ccco2)cc1\n",
      "[22:35:32] SMILES Parse Error: Failed parsing SMILES 'CC(C)S(=O)(=O)N(C)C)c1ccc(S(=O)(=O)NCc2ccco2)cc1' for input: 'CC(C)S(=O)(=O)N(C)C)c1ccc(S(=O)(=O)NCc2ccco2)cc1'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CC(C)N1C(=O)c2cccc3c(N)c(S(=O)(=O)O)cc(c2c2)C1=O'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CCc1nc(O)c2c(n1)CN(CCN3CCCC1)CC2'\n",
      "[22:35:32] Can't kekulize mol.  Unkekulized atoms: 14 15 22 23 24 25 26\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CN(Cc1nnc2n1-c3ccccc3C[C@@H]1CCC3)S(=O)(=O)c1ccc(Cl)cc1'\n",
      "[22:35:32] Can't kekulize mol.  Unkekulized atoms: 11 12 13 15 16\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'NCCCCN(C(=O)c1ccnn1C)C(=O)c2ccccc1'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'Cc1occc1C(=O)N(C)Cc1nnc2n1-cncc1C2'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CC1=NS(=O)(=O)N(Cc2ccccc2Cl)C(=O)/C1=C\\c1cc(Br)c(OCc2ccc3c(c2)OCO4)c(I)c1'\n",
      "[22:35:32] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 19 20 21 23 24\n",
      "[22:35:32] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 20\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1-c1nc2oc(=O)c(CC(=O)NCc3ccccc3C)c(=O)c3cccc13'\n",
      "[22:35:32] non-ring atom 4 marked aromatic\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'Cc1cc2cc(C(=O)N3CCC([C@@H]4C[C@H](C(F)(F)F)n5ncnc5N5CCCC[C@H]55)[C@@H]3C3)ccc(=O)o2n1'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@@H]3[C@H]4[C@@H]2[C@H]2[C@H]1[C@@H]3[C@@H]4C[C@@H]1[C@@H]2[C@@H]52'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CN(C(=O)[C@@]12C=CC(=O)N(c2ccccc2)C1=O)[C@@H]1CCNC1=O'\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CC[C@]1(C)nOc(SCC(=O)Nc2ccc(C)cc2)n2c(=O)oc3ccccc31'\n",
      "[22:35:32] Explicit valence for atom # 20 N, 5, is greater than permitted\n",
      "[22:35:32] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 17 19\n",
      "[22:35:32] SMILES Parse Error: unclosed ring for input: 'CCC[C@H]1N=C2c3ccccc3N=C(S[C@H](C)C(=O)Nc4ccccc4F)[C@@H]31'\n",
      "[22:35:32] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(Oc2ccc(NC(=O)CC[C@@H]3CC[C@@H]4[C@H]3CC[C@H]5C[C@@H]42)cc1C'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'O=c1oc2ccccc2c2cc(-c3nnc(SCC(=O)Nc4cccc(CN4CCOCC5)c4)s3)cccc12'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CO[C@H]2CSC3(C2)O[C@@H]3CN(C)C[C@H](O)[C@H]32)c1'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@@H]1CC(=O)N(c2ccc3oc4c(=O)cc(C)cc4c3)C2'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1[C@]1(c2ccccc2)NC(=O)N(CC(=O)Nc2cc(C)ccc2OCC(=O)NC2=O)C1=O'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'O=C(c1n[nH]c2c1CCCC2)N1CCC2(CC1)C[C@H]2CN(Cc1ccsc1)C1=O'\n",
      "[22:35:33] Can't kekulize mol.  Unkekulized atoms: 1\n",
      "[22:35:33] SMILES Parse Error: syntax error while parsing: O=C(NCC1CCN(c2ccc3cccn(3)c2)CC1)c1cscn1\n",
      "[22:35:33] SMILES Parse Error: Failed parsing SMILES 'O=C(NCC1CCN(c2ccc3cccn(3)c2)CC1)c1cscn1' for input: 'O=C(NCC1CCN(c2ccc3cccn(3)c2)CC1)c1cscn1'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'Clc1cccc([C@@H]2Oc3c(Cl)cc(Cl)cc3[C@H]3CC(c4ccc5ccccc5c4)=NN22)c1'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'O=c1c2c3c(sc2ncn1Cc1ccccn1)C[C@H](NCc1cccn2Cc1ccccc1)CC3'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C(=O)NC2=C(C(=O)O)C[C@@H]2C(=O)Nc3ccc(F)c(F)c32)c1'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'CN1CC[C@]23C(=O)NC(=C/O)CC[C@]2(C(=O)NCc2ccccc2)C1'\n",
      "[22:35:33] SMILES Parse Error: unclosed ring for input: 'CO[C@@H]1CO[C@@]23CCN(C(=O)Cc4ccccc4)C[C@]3(CO)C1'\n",
      "[22:35:33] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 21\n",
      "[22:35:34] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 23\n",
      "[22:35:34] Can't kekulize mol.  Unkekulized atoms: 2 3 15\n",
      "[22:35:34] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 18 20\n",
      "[22:35:34] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 21 22\n",
      "[22:35:34] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 15 16 17 18 19\n",
      "[22:35:34] Can't kekulize mol.  Unkekulized atoms: 8 18 19 20 21 22 23 24 25 26 27\n",
      "[22:35:34] SMILES Parse Error: unclosed ring for input: 'Cc1snnc1CC(=O)N(Cc1cccc2c1cc2CCCC2)c1cccc(F)c1'\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@H]2C[C@@H]3CN2CC3=C2C(=O)N(CC(C)C)C(=O)N3)c1'\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'O=C1[C@H]2C=CC=C2CCC=C1CCC2=O'\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'CC(C)NC(=O)c1nnn2c1CC[C@@H]1CN(C(=O)CCCN1CCCC1=O)C2'\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)c1ccc2c(c1)[C@@]13CCCCC[C@H]11CCC[C@]1(c1ccccc1)N2'\n",
      "[22:35:35] Explicit valence for atom # 4 Br, 3, is greater than permitted\n",
      "[22:35:35] Can't kekulize mol.  Unkekulized atoms: 17 18 19 20 21 23 25 26 27\n",
      "[22:35:35] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 10 11 12\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'Cc1nccn1[C@@H]1CCC[C@H]1NC(=O)N=C2c3ccccc3C3c3ccccc3C1=O'\n",
      "[22:35:35] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 11\n",
      "[22:35:35] Can't kekulize mol.  Unkekulized atoms: 1 2 3 25 26\n",
      "[22:35:35] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 8 16 17\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'CS(=O)(=O)Nc1cccc2c1ccn2CC(=O)NCC1CCC2'\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@@H]2c3nc(-c4cc(C)ncc4)nn3C3=N2)c1OC'\n",
      "[22:35:35] SMILES Parse Error: unclosed ring for input: 'C=CC(=O)N1CC(O)(CNC[C@H](c2ccco2)N(C)C)c1ccccn1'\n",
      "[22:35:35] SMILES Parse Error: extra close parentheses while parsing: C[C@H](N1C(=O)COC2=C/C=C/C=C/C)c2C)c1c(=O)[nH]c(=O)n(C)c1=O\n",
      "[22:35:35] SMILES Parse Error: Failed parsing SMILES 'C[C@H](N1C(=O)COC2=C/C=C/C=C/C)c2C)c1c(=O)[nH]c(=O)n(C)c1=O' for input: 'C[C@H](N1C(=O)COC2=C/C=C/C=C/C)c2C)c1c(=O)[nH]c(=O)n(C)c1=O'\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6\n",
      "[22:35:36] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c(CNC[C@@H]3CN4CCC[C@@H]4C3)ncnc2c12'\n",
      "[22:35:36] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[22:35:36] SMILES Parse Error: ring closure 2 duplicates bond between atom 6 and atom 7 for input: 'COc1ccc([C@H]2[C@H]2C(=O)N(c4ccc(Cl)c(C)c4)C(=O)[C@@H]3ON2c2ccccc2)cc1'\n",
      "[22:35:36] SMILES Parse Error: unclosed ring for input: 'O=C(C[C@]1(COc2ccc(Cl)cc2)CN(Cc2cn(CCO)nc22)CCOC1)c1ccccc1'\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 5 7\n",
      "[22:35:36] SMILES Parse Error: extra open parentheses for input: 'Cc1cccc(OC(C)(C)C(=O)N2CCC([C@H]3Nc3ccccc3-n3nccc32)C(C)C1'\n",
      "[22:35:36] SMILES Parse Error: extra close parentheses while parsing: CCOC(=O)[C@H]1O[C@]12CCOC(C)(C)I)C2\n",
      "[22:35:36] SMILES Parse Error: Failed parsing SMILES 'CCOC(=O)[C@H]1O[C@]12CCOC(C)(C)I)C2' for input: 'CCOC(=O)[C@H]1O[C@]12CCOC(C)(C)I)C2'\n",
      "[22:35:36] SMILES Parse Error: unclosed ring for input: 'CC(C)C(=O)Oc1ccc2c(c1)C(=O)N1CCN(C(=O)Cn2ccc4ccccc43)C1'\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 9 10 11\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 7 20\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 1 2 4 23 24 26\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 8 15 16 17 18\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 23\n",
      "[22:35:36] Can't kekulize mol.  Unkekulized atoms: 3 5 6 7 28\n",
      "[22:35:37] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 28\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'COc1ccsc1CN(C)C(=O)[C@H]1C[C@@]23CCN(CC[C@@]1(C)C)C[C@H]2C(C)=O'\n",
      "[22:35:37] Explicit valence for atom # 8 O, 3, is greater than permitted\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)cc([C@H]2Cc3ccccc3N3C(=O)c3ccc(Br)cc32)c1'\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'CCO[C@@H]1C[C@]1(NC(=O)c1ccc3c(c1)CCC(=O)N3)CC(C)(C)C2'\n",
      "[22:35:37] Can't kekulize mol.  Unkekulized atoms: 5 6 7 16 18\n",
      "[22:35:37] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 10\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'c1cc(CNC[C@@H]2Cc3nnc4nccn3c2)c2ccccc2n1'\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'COc1ccc2cc(CN(C)C(=O)Cn(c3ccc(Cl)cc3)c2ccccc2)cc(=O)c1O'\n",
      "[22:35:37] Can't kekulize mol.  Unkekulized atoms: 14 15 17 27 28 29\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'O=C1[C@H]2[C@@H]3C[C@@H]([C@@H]4[C@@H]3C(=O)N1[C@@H]1[C@H]3[C@H]4C[C@H]2[C@H]5[C@@H]43)[C@@H]1[C@H]2[C@H]4CC[C@@H](O3)[C@H]2[C@@H]14'\n",
      "[22:35:37] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 18 19\n",
      "[22:35:37] SMILES Parse Error: syntax error while parsing: COCCOc1ccc(-c2cccc()c2CN)cc1\n",
      "[22:35:37] SMILES Parse Error: Failed parsing SMILES 'COCCOc1ccc(-c2cccc()c2CN)cc1' for input: 'COCCOc1ccc(-c2cccc()c2CN)cc1'\n",
      "[22:35:37] SMILES Parse Error: unclosed ring for input: 'Cc1cn2cc(CNC(=O)c3ccc(F)cc3)c3c2ncn(C(=O)Nc4ccccc4Cl)n12'\n",
      "[22:35:37] Can't kekulize mol.  Unkekulized atoms: 2 3 4 21 22 23 24\n",
      "[22:35:38] Can't kekulize mol.  Unkekulized atoms: 8\n",
      "[22:35:38] Can't kekulize mol.  Unkekulized atoms: 1 2 9 20 21\n",
      "[22:35:38] Can't kekulize mol.  Unkekulized atoms: 2 3 4 7 9 16 17 19 20\n",
      "[22:35:38] SMILES Parse Error: unclosed ring for input: 'CCOc1ncc(CN2CCOC[C@]2(CC(=O)O)C2)cn1'\n",
      "[22:35:38] Explicit valence for atom # 6 C, 5, is greater than permitted\n",
      "[22:35:39] Can't kekulize mol.  Unkekulized atoms: 9 11 22 23 24\n",
      "[22:35:39] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C[C@H]2Nc3ccccc3SC2=N2CC(=O)Nc2cc(C)ccc23)cc1'\n",
      "[22:35:39] SMILES Parse Error: extra open parentheses for input: 'CCc1ccc(-c2ccc(/C=C/C(=O)N2CC[C@@H](c3[nH]ncc3C)C2)o1'\n",
      "[22:35:39] SMILES Parse Error: extra close parentheses while parsing: CN(C)[C@@H](c1ccc(OCC(=O)O)cc1)C(=O)O)c1ccccc1\n",
      "[22:35:39] SMILES Parse Error: Failed parsing SMILES 'CN(C)[C@@H](c1ccc(OCC(=O)O)cc1)C(=O)O)c1ccccc1' for input: 'CN(C)[C@@H](c1ccc(OCC(=O)O)cc1)C(=O)O)c1ccccc1'\n",
      "[22:35:39] SMILES Parse Error: unclosed ring for input: 'CO[C@@]12C=CC=C[C@@H]3C=C[C@H]1NC(=O)[C@@H]22'\n",
      "[22:35:39] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 21 22\n",
      "[22:35:39] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2C[C@@H]3CC[C@H]2C[C@@H](O)C2)cc1OC[C@H](O)CN1CCCNCC1'\n",
      "[22:35:39] SMILES Parse Error: unclosed ring for input: 'Cc1nc(Nc2ccc(F)cc2)sc1C(=O)c1css2'\n",
      "[22:35:39] SMILES Parse Error: unclosed ring for input: 'Cn1c(=O)c2[nH]c(N(CC(=O)N3CC[C@H](N4CCOCC4)Cc3ccccc3)n2)n(C)c1=O'\n",
      "[22:35:39] Can't kekulize mol.  Unkekulized atoms: 3 4 5 7 8 10 11 12 15 16 17 19 21\n",
      "[22:35:39] SMILES Parse Error: unclosed ring for input: 'Cc1c(CN2CC[C@@]3(C2)Cn2c(nnc2C(=O)NC(C)C)n3c2ncnc3[nH]c(=O)n(C)c2=O)n(C)c1C'\n",
      "[22:35:40] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[22:35:40] SMILES Parse Error: unclosed ring for input: 'Fc1ccc(-c2nccn2-c2ccc3c(c2)CCC4)cc1'\n",
      "[22:35:40] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2[nH]cc(CCN3C(=O)O[C@]4(C)C(=O)CC[C@](C)(CCC=C)C)[nH]33)cc1=O\n",
      "[22:35:40] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2[nH]cc(CCN3C(=O)O[C@]4(C)C(=O)CC[C@](C)(CCC=C)C)[nH]33)cc1=O' for input: 'COc1ccc2[nH]cc(CCN3C(=O)O[C@]4(C)C(=O)CC[C@](C)(CCC=C)C)[nH]33)cc1=O'\n",
      "[22:35:40] SMILES Parse Error: extra close parentheses while parsing: O=C(O)Cn1nnc2c(Nc3cccc(C(F)(F)F)c3)nnc2-c2ccccc2)c1\n",
      "[22:35:40] SMILES Parse Error: Failed parsing SMILES 'O=C(O)Cn1nnc2c(Nc3cccc(C(F)(F)F)c3)nnc2-c2ccccc2)c1' for input: 'O=C(O)Cn1nnc2c(Nc3cccc(C(F)(F)F)c3)nnc2-c2ccccc2)c1'\n",
      "[22:35:40] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[22:35:40] Can't kekulize mol.  Unkekulized atoms: 5 20 21 22 32 33 34\n",
      "[22:35:40] SMILES Parse Error: unclosed ring for input: 'OC1=C2C[C@H]3C[C@@H]([C@@H]2C(=O)N1c1nn(Cc3ccccc3)c(=O)c1-c1ccccc1)CC2'\n",
      "[22:35:40] SMILES Parse Error: unclosed ring for input: 'CC(C)c1nnc([C@H]2CN(Cc2ccn(C(C)C)n3)C2)n1C'\n",
      "[22:35:40] Can't kekulize mol.  Unkekulized atoms: 4 19 20 21 22 23 24\n",
      "[22:35:40] SMILES Parse Error: unclosed ring for input: 'CC(C)c1noc([C@H]2CCCN2C(=O)C2=NNC(=O)CC3)n1'\n",
      "[22:35:40] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)Nc2ccc(Cl)c(-c3nnc4n2CCCCC3)c2)cc1'\n",
      "[22:35:40] Can't kekulize mol.  Unkekulized atoms: 18 19 21\n",
      "[22:35:40] Can't kekulize mol.  Unkekulized atoms: 1 2 5\n",
      "[22:35:40] SMILES Parse Error: ring closure 2 duplicates bond between atom 21 and atom 22 for input: 'O=C1[C@H]2[C@@H]3C[C@H]([C@@H]4[C@@H]5CC[C@H](C5)[C@@H]34)[C@H]2[C@H]2[C@@H]3[C@@H]4CC[C@@H]([C@H]12)[C@H]2[C@@H]2C(=O)[C@@H]2[C@H]1[C@@H]3[C@@H]43C1(C)O[C@@H]2[C@@H]1[C@@H]4C3(C)CO[C@H]3[C@H]21'\n",
      "[22:35:40] Can't kekulize mol.  Unkekulized atoms: 3 5 24\n",
      "[22:35:40] SMILES Parse Error: unclosed ring for input: 'Nc1nc(-c2ccc(Cl)cc2)c2sc3c(c2c1n1)CCCC3'\n",
      "[22:35:40] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[22:35:41] SMILES Parse Error: unclosed ring for input: 'O=C1CC[C@@H](C(F)(F)F)C(F)(F)I'\n",
      "[22:35:41] SMILES Parse Error: unclosed ring for input: 'O=C(NC1CC2)[C@@H](Cc1ccccc1)N1CCN(c2ccc(O)cc2)CC1'\n",
      "[22:35:41] Can't kekulize mol.  Unkekulized atoms: 1 2 3 7 13 14 15\n",
      "[22:35:41] SMILES Parse Error: unclosed ring for input: 'O=C(N1CCC(O)(c2nn[nH]n2)CC1)C1(CCCCC2)CCOCC1'\n",
      "[22:35:41] Explicit valence for atom # 14 C, 5, is greater than permitted\n",
      "[22:35:41] Can't kekulize mol.  Unkekulized atoms: 6 7 8 15 16\n",
      "[22:35:41] SMILES Parse Error: extra open parentheses for input: 'O=C(Nc1nc(CC(=O)N2CC[C@@]3(C2)Cc2ccc(Cl)cc2CC3)cc(=O)n1-c1ccccc1Cl'\n",
      "[22:35:41] SMILES Parse Error: unclosed ring for input: 'C[C@]12CC[C@H]3[C@@H]1[C@@](C)(CO)[C@H]2C'\n",
      "[22:35:41] SMILES Parse Error: extra open parentheses for input: 'Cc1c(C(N)=O)sc2ncn(CC(=O)c3ccc(N4CCOCC4)c(C(N)=O)c3c1=O'\n",
      "[22:35:42] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 12\n",
      "[22:35:42] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23 24 25 26 27\n",
      "[22:35:42] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 16 17 19\n",
      "[22:35:42] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 18 19 20 27 28\n",
      "[22:35:42] SMILES Parse Error: unclosed ring for input: 'O[C@H]1CCN(c2cc(N3CCN(Cc4c(F)cccc3Cl)CC3)ccn2)C1'\n",
      "[22:35:42] SMILES Parse Error: unclosed ring for input: 'CO[C@H]1[C@H](CNC(=O)Cc2ccc3c[nH]cc2n2)[C@H](c2ccccc2)O[C@H]1c1ccccc1'\n",
      "[22:35:42] SMILES Parse Error: duplicated ring closure 2 bonds atom 19 to itself for input: 'O=C(Nc1ccc(S(=O)(=O)N2CCCc2ccccc22)cc1)c1ccc[nH]c1Cl'\n",
      "[22:35:42] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 20\n",
      "[22:35:42] SMILES Parse Error: unclosed ring for input: 'CC12C[C@@]3(CCC[C@@H]3C(N)=O)CCC(O)CC1'\n",
      "[22:35:42] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 9 for input: 'CC(=O)[C@@H]1C(=O)C(=O)N2c2cc(C(=O)O)ccc2S[C@@H]1c1ccc(Br)cc1'\n",
      "[22:35:42] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 22 23 24 25 26\n",
      "[22:35:43] Explicit valence for atom # 10 C, 5, is greater than permitted\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 3 4 27 28 30\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 3 4 5 7 18 20 21\n",
      "[22:35:43] SMILES Parse Error: syntax error while parsing: O=C(COC(=O)CS==)c1ccc2c(c1)OCCO2\n",
      "[22:35:43] SMILES Parse Error: Failed parsing SMILES 'O=C(COC(=O)CS==)c1ccc2c(c1)OCCO2' for input: 'O=C(COC(=O)CS==)c1ccc2c(c1)OCCO2'\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 3 4 5 16 17 30 31\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 24 25 27\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 1 2 19\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7\n",
      "[22:35:43] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@H](c1c2ccccc2)C1CCC2)N1CCC(O)CC1'\n",
      "[22:35:43] Can't kekulize mol.  Unkekulized atoms: 5 6 7 21 22 23 24 25 26\n",
      "[22:35:44] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'CSc1ccc(S(=O)(=O)Nc2cc3c(c2)-c2nc4ccccc3o2)cc1'\n",
      "[22:35:44] Can't kekulize mol.  Unkekulized atoms: 1 2 12 13 14 15 16 17 18\n",
      "[22:35:44] Can't kekulize mol.  Unkekulized atoms: 12 13 19\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'N=c1nc2c(cnn2Cc1ccccc1)c1nnn2ccccc12'\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'C[C@H](O)[C@H](NS(=O)(=O)c1ccc2c(c1)oc(=O)n2CCc1ccccc1)C2'\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'CCCN1C[C@H]2C3CCN(C(=O)[C@H](C)C3CCN(C)CC3)[C@H]2C1'\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(S(=O)(=O)N[C@H](CC)C(=O)N[C@@H](CC)cccc1C)cc1C'\n",
      "[22:35:44] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 21 22\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'Cn1cc(C(=O)N2CCC(c3nc(O)nc(-c4ccccn3)n3)CC2)cn1'\n",
      "[22:35:44] SMILES Parse Error: duplicated ring closure 2 bonds atom 32 to itself for input: 'c1ccc2c(c1)-c1ccccc1[C@@H]1[C@@H](c3cc4c4ccccc5[nH]c4=O)N1c2ccccc22'\n",
      "[22:35:44] SMILES Parse Error: unclosed ring for input: 'Cc1cc(NC(=O)[C@H](NC(=O)c2ccc(=O)[nH]c2)ccc1Cl)c2ccccc2n1'\n",
      "[22:35:45] SMILES Parse Error: extra open parentheses for input: 'C/C(=C\\Cl'\n",
      "[22:35:45] SMILES Parse Error: unclosed ring for input: 'COP(=O)(OC)C1(c2ccccc2)[C@@H]2[C@@H]3CC[C@H](C2)[C@H]21'\n",
      "[22:35:45] SMILES Parse Error: extra open parentheses for input: 'COc1cc(OC)cc([C@H](C)NC(=O)c1ccc(C)c(S(=O)(=O)Nc2ccc(C)cc2)c1'\n",
      "[22:35:45] SMILES Parse Error: unclosed ring for input: 'O=C(c1ccnn1C1CCCCC1)N1CCC2(CC1)C[C@]2(COc3ccccc3-c3ccccc3)CC[C@@H]12'\n",
      "[22:35:45] Can't kekulize mol.  Unkekulized atoms: 6 7 18\n",
      "[22:35:45] Can't kekulize mol.  Unkekulized atoms: 5 6 19 21 27\n",
      "[22:35:45] SMILES Parse Error: unclosed ring for input: 'Cc1nn(C)c2nc(C3CC3)cc(C(=O)NCc3nc4c(s4)CCCC4)c12'\n",
      "[22:35:45] SMILES Parse Error: unclosed ring for input: 'CC(=O)O[C@H]1CC[C@]2(C)[C@@H](CC[C@H]3[C@H]2CC[C@@]2(C)[C@H]3C[C@@H](C)[C@]3(Br)C(C)=O)CC1'\n",
      "[22:35:45] Can't kekulize mol.  Unkekulized atoms: 2 3 22 23 27\n",
      "[22:35:45] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 21\n",
      "[22:35:45] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 9 12 13 14 16 18 20 22 24 25 26 27 28 29\n",
      "[22:35:46] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 27\n",
      "[22:35:46] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@H]1CC22CCN(CC1CCOCC1)CC2)c1cnccn1'\n",
      "[22:35:46] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 20 21 23 24\n",
      "[22:35:46] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)cc(CSc2nnc(NC(=O)C3CC3)c2c2CCCC3)c1'\n",
      "[22:35:46] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CS(=O)(=O)c2cccc2c2C[C@H](C(=O)O)N(C)C3)cc1'\n",
      "[22:35:46] SMILES Parse Error: unclosed ring for input: 'C[C@H](NC(=O)CNC(=O)N1CCC[C@@]2(C)C1(C)C)C(=O)c1ccccc1F'\n",
      "[22:35:46] SMILES Parse Error: extra close parentheses while parsing: COCC[S@](=O)Cc1cccc(NC(=O)/c2c3cccc(C)c3C)c2)c1\n",
      "[22:35:46] SMILES Parse Error: Failed parsing SMILES 'COCC[S@](=O)Cc1cccc(NC(=O)/c2c3cccc(C)c3C)c2)c1' for input: 'COCC[S@](=O)Cc1cccc(NC(=O)/c2c3cccc(C)c3C)c2)c1'\n",
      "[22:35:46] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 19\n",
      "[22:35:46] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13\n",
      "[22:35:46] SMILES Parse Error: unclosed ring for input: 'CC1CC1(CCNC(=O)Nc2ncccn2)CCOC1'\n",
      "[22:35:46] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[22:35:46] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN2COC3=C(O)C(=O)[C@@H](Cc3ccccc3)C2)cc1'\n",
      "[22:35:47] SMILES Parse Error: extra close parentheses while parsing: CC(C)(C)OC(=O)N1CCN2CC(c3ccccc3)=NO2)C1\n",
      "[22:35:47] SMILES Parse Error: Failed parsing SMILES 'CC(C)(C)OC(=O)N1CCN2CC(c3ccccc3)=NO2)C1' for input: 'CC(C)(C)OC(=O)N1CCN2CC(c3ccccc3)=NO2)C1'\n",
      "[22:35:47] SMILES Parse Error: unclosed ring for input: 'CC(=O)N[C@@H]1[C@@H]2[C@@H]3C[C@@H]4[C@@H]2[C@H]2OC1=O'\n",
      "[22:35:47] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[22:35:47] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 23 25 26 27 28 30 31\n",
      "[22:35:47] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 7 21 22\n",
      "[22:35:47] SMILES Parse Error: unclosed ring for input: 'CCCCn1cnc2c(c1=N)[C@@H](c1ccc(OC)c(OC)c1)Cc1c2ccc3ccccc3c2n1'\n",
      "[22:35:47] SMILES Parse Error: unclosed ring for input: 'O=C(NCCc1ccc2c(c1)CCO2)N[C@@H]1CCC[C@]12CN(C1CCC2)CO2'\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 2 3 5\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 2 26 27 28 29 30 31\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 20 25 28 29\n",
      "[22:35:48] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@H]1CC=CCC1)N1CCC[C@]2(CCC(=O)N=C2)C2'\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 21 24 25\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 25\n",
      "[22:35:48] SMILES Parse Error: syntax error while parsing: C[C@H](ONC(=O)C12C[C@H]3C[C@@H](C1)CC(c1ccccc1)(3)C2)C(=O)O\n",
      "[22:35:48] SMILES Parse Error: Failed parsing SMILES 'C[C@H](ONC(=O)C12C[C@H]3C[C@@H](C1)CC(c1ccccc1)(3)C2)C(=O)O' for input: 'C[C@H](ONC(=O)C12C[C@H]3C[C@@H](C1)CC(c1ccccc1)(3)C2)C(=O)O'\n",
      "[22:35:48] SMILES Parse Error: extra close parentheses while parsing: CC(=O)Nc1sc(C)c(C)c1C(=O)O[C@H](C(=O)NC1CC1)c1ccccc1)c1ccccc1\n",
      "[22:35:48] SMILES Parse Error: Failed parsing SMILES 'CC(=O)Nc1sc(C)c(C)c1C(=O)O[C@H](C(=O)NC1CC1)c1ccccc1)c1ccccc1' for input: 'CC(=O)Nc1sc(C)c(C)c1C(=O)O[C@H](C(=O)NC1CC1)c1ccccc1)c1ccccc1'\n",
      "[22:35:48] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@@H]3C[C@H]([C@H]4[C@@H]5CC[C@H](C5)[C@@H]34)[C@@H]2[C@H]2[C@@H]3CC[C@@H]([C@H]12)[C@H]2[C@@H]5Cl1'\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 9 10 17 18 19 20 21 22 23\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 9 21 22 31 32\n",
      "[22:35:48] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 8\n",
      "[22:35:48] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1C(=O)/N=c1nc2cnc3cc[nH]c2n1'\n",
      "[22:35:49] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(Cn2c(=O)c3c(nc4n(CCCC(N)=O)c(C)c4c3C)n2C)c2c1OCOC'\n",
      "[22:35:49] SMILES Parse Error: unclosed ring for input: 'Cc1noc([C@@H]2[C@@H](N3CCC(=O)NC3=O)Cc2ccccc23)n1'\n",
      "[22:35:49] SMILES Parse Error: ring closure 1 duplicates bond between atom 14 and atom 15 for input: 'CNC(=O)C(C)(C)CN[C@@H]1C[C@H]2C[C@H]1[C@H]1[C@H]1CC[C@H]2O1'\n",
      "[22:35:49] SMILES Parse Error: unclosed ring for input: 'CN(CCC12CC4CC(CC(C3)C1)C2)CN1CCN(c2ccc(F)cc2)CC1'\n",
      "[22:35:49] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7\n",
      "[22:35:49] SMILES Parse Error: unclosed ring for input: 'Cc1nncn1CC(=O)N1C[C@H]2C[C@@H](c3c(C)ncn32)C2'\n",
      "[22:35:49] SMILES Parse Error: extra open parentheses for input: 'CC(=O)N1CC[C@H](c2ncc3c(n2)CCN(C2CCN(C(C)C)CC2)CC3'\n",
      "[22:35:49] Can't kekulize mol.  Unkekulized atoms: 1 2 3 34 35\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 14 15 21 22 23 24 25 26 27\n",
      "[22:35:50] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(Cl)cc1N(CC(=O)NC(=O)NC(C)(C)C)CC)S(=O)(=O)c1ccc2c(c1)OCCO2\n",
      "[22:35:50] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(Cl)cc1N(CC(=O)NC(=O)NC(C)(C)C)CC)S(=O)(=O)c1ccc2c(c1)OCCO2' for input: 'COc1ccc(Cl)cc1N(CC(=O)NC(=O)NC(C)(C)C)CC)S(=O)(=O)c1ccc2c(c1)OCCO2'\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 22\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 16 17 18\n",
      "[22:35:50] SMILES Parse Error: unclosed ring for input: 'Clc1ncnc2[nH]1'\n",
      "[22:35:50] SMILES Parse Error: extra close parentheses while parsing: O=C(O)[C@@H]1Cc2noc(CNC(=O)[C@@H]3C[C@H]3c3ccc(F)cc3Cl)n2)C1\n",
      "[22:35:50] SMILES Parse Error: Failed parsing SMILES 'O=C(O)[C@@H]1Cc2noc(CNC(=O)[C@@H]3C[C@H]3c3ccc(F)cc3Cl)n2)C1' for input: 'O=C(O)[C@@H]1Cc2noc(CNC(=O)[C@@H]3C[C@H]3c3ccc(F)cc3Cl)n2)C1'\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 1 2 4 19\n",
      "[22:35:50] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 19 22\n",
      "[22:35:50] SMILES Parse Error: unclosed ring for input: 'Cn1c(C(=O)N2C[C@@H]3COC[C@](Cc3ccccc3)(C(=O)O)C2)cc2cc(F)ccc21'\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 19\n",
      "[22:35:50] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1NS(=O)(=O)c1ccc(SC)CC(=O)Nc2cccc(NC(C)=O)c2)cc1C\n",
      "[22:35:50] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1NS(=O)(=O)c1ccc(SC)CC(=O)Nc2cccc(NC(C)=O)c2)cc1C' for input: 'COc1ccccc1NS(=O)(=O)c1ccc(SC)CC(=O)Nc2cccc(NC(C)=O)c2)cc1C'\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 7 8 29 30 32\n",
      "[22:35:50] SMILES Parse Error: ring closure 2 duplicates bond between atom 15 and atom 16 for input: 'O=C1[C@@H]2[C@@H]3C[C@@H]([C@@H]4[C@@H]5CC[C@H](Br)[C@@H]5[C@H]55)[C@@H]2[C@H]2[C@H]2[C@H]3OC3=C12'\n",
      "[22:35:50] Can't kekulize mol.  Unkekulized atoms: 5 6 13 14 15\n",
      "[22:35:51] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2sc(NC(=O)c3ccc4c(=O)n4ccccc4n3)c(C)c2c1'\n",
      "[22:35:51] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "[22:35:51] SMILES Parse Error: extra open parentheses for input: 'O=C(c1cnc(O)nc1'\n",
      "[22:35:51] Can't kekulize mol.  Unkekulized atoms: 3 5 6 9 10\n",
      "[22:35:51] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 23 24 25 26\n",
      "[22:35:51] Can't kekulize mol.  Unkekulized atoms: 1 2 17\n",
      "[22:35:51] SMILES Parse Error: unclosed ring for input: 'Cc1n[nH]c(C)c1NC(=O)[C@@H]1[C@H]2C=C[C@]3(C2)C1(C)C'\n",
      "[22:35:51] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[22:35:51] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@H]1COCc2nc3ccc(CO)cc3C1)c1cccc(=O)n1C'\n",
      "[22:35:51] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17 18 19\n",
      "[22:35:51] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19 20 21 22 23\n",
      "[22:35:51] SMILES Parse Error: unclosed ring for input: 'CCc1cc2cc(S(=O)(=O)NCCOC)cnc3o1'\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 22\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 22 23 24 26 28\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 9 11\n",
      "[22:35:52] Explicit valence for atom # 8 O, 3, is greater than permitted\n",
      "[22:35:52] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)C1CCN(S(=O)(=O)c3cn(C(C)C)cn3)CC2)nc1C'\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 22 23 24 25 26\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 5 6 8 10 11 12 20 21 22 24 25\n",
      "[22:35:52] SMILES Parse Error: unclosed ring for input: 'CC(C)CCCCCCCCC[C@H]1CC[C@H]2[C@@H]3C[C@H](C4)[C@H]2C(=O)C[C@@H]13'\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 2 5 6\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[22:35:52] SMILES Parse Error: duplicated ring closure 2 bonds atom 6 to itself for input: 'Cc1c(C(=O)N[C@@]22CCC[C@@H]2NC(=O)OC(C)(C)C)cnn1C'\n",
      "[22:35:52] SMILES Parse Error: unclosed ring for input: 'Cc1noc2nc(C3CC3)cc(C(=O)N3CSCC[C@@H]4C)c3c12'\n",
      "[22:35:52] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 24 25 27 28\n",
      "[22:35:52] SMILES Parse Error: unclosed ring for input: 'CCCCNC(=O)CSc1nc2cc(NC(=O)c3cc(OC)c(OC)c(OC)c3)ncn1'\n",
      "[22:35:52] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN([C@H]2[C@@H]3CCO3)CCN1C(=O)c1ccc2nccnc2c1'\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 13 14 15 17 19 21 22\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 14 15 16 17 18 21 22\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 24\n",
      "[22:35:53] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1C(=O)N1CCn2c1nc1c(C)nn(-c3ccccc3)c1=O'\n",
      "[22:35:53] Explicit valence for atom # 24 N, 5, is greater than permitted\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 17 18 26\n",
      "[22:35:53] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c([C@H](C)NC(=O)[C@@H](C)N(c2ccc(F)c(F)c2)S(C)(=O)=O)c2ccccc1'\n",
      "[22:35:53] SMILES Parse Error: extra close parentheses while parsing: CC(C)(C)NC(=O)N[C@@H](c1nnc2n1CCC2)C(F)(F)F)c1ccccc1\n",
      "[22:35:53] SMILES Parse Error: Failed parsing SMILES 'CC(C)(C)NC(=O)N[C@@H](c1nnc2n1CCC2)C(F)(F)F)c1ccccc1' for input: 'CC(C)(C)NC(=O)N[C@@H](c1nnc2n1CCC2)C(F)(F)F)c1ccccc1'\n",
      "[22:35:53] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)OC(C)(C)[C@H]1[C@@H]3C(=O)N(CC(=O)NC4CCCC4)C(=O)[C@@H]3[C@@H]3CC[C@@H]21'\n",
      "[22:35:53] Can't kekulize mol.  Unkekulized atoms: 7 8 9\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(cc1NC(=S)NC(=O)c1cc(OC)c(OC)c(OC)c1)oc1ccc3ccccc13'\n",
      "[22:35:54] Can't kekulize mol.  Unkekulized atoms: 0 1 2 17 18 19 20 21 22\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2noc(CCNC(=O)[C@@H]3C[C@H]3CC(C)C)n23)cc1'\n",
      "[22:35:54] non-ring atom 13 marked aromatic\n",
      "[22:35:54] SMILES Parse Error: extra open parentheses for input: 'O=C(CSc1nn2n(n1)[C@H](c1cccc(F)c1)C1=C(N=2)c2ccccc2CC1'\n",
      "[22:35:54] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'O=C(c1n[nH]c2c1CCCC2)N1CCC2(CC1)COCCN(CC3CCC1)C2'\n",
      "[22:35:54] Can't kekulize mol.  Unkekulized atoms: 7 8 9 11 12\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)c(C(=O)NC[C@@H]2CC3c4ccccc4C3c4ccccc4n32)c1'\n",
      "[22:35:54] Can't kekulize mol.  Unkekulized atoms: 2 3 5\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2[C@H](C(=O)N3C[C@@H]4C[C@H](O)C[C@H]3C3=O)COc3ccccc32)cc1'\n",
      "[22:35:54] Can't kekulize mol.  Unkekulized atoms: 1 2 10 11 12 13 14 15 16\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'O=C(COC(=O)C12C[C@H]3C[C@@H](CC(Br)(C4)C1)C2)Nc1ccc(I)cc1'\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'O=C(NCC1C[C@H]2CCC[C@]2(CO1)C1)c1cccc(N2CCCC2=O)c1'\n",
      "[22:35:54] SMILES Parse Error: ring closure 1 duplicates bond between atom 9 and atom 10 for input: 'Nc1nc(N)c2c(n1)O[C@H]1c1c(ccc3ccccc13)C2'\n",
      "[22:35:54] SMILES Parse Error: unclosed ring for input: 'O=c1c2c3n(ccn3c2=S)CCC=CC3=C1CC[C@H](NC(=O)c2cccc(Cl)c2)C1'\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'COCc1ccc(S(=O)(=O)N2CC[C@@H](O)C23CCC4)cc2F'\n",
      "[22:35:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 2 for input: 'O=C1c1ccccc1OCC(F)(F)C(F)(F)F'\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CC=C2C[C@H]3CC[C@@H](C(=O)[C@H]4[C@@H]5CCC[C@]55C)[C@@H]3CC[C@]21C'\n",
      "[22:35:55] Can't kekulize mol.  Unkekulized atoms: 11 12 13 23 24\n",
      "[22:35:55] Explicit valence for atom # 23 C, 5, is greater than permitted\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc2cc2c(C)c(C(=O)N[C@@H](CC(C)C)C(=O)O)nn2c1'\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CNC(=O)NC2(CC(=O)O)CCCCC1'\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'Cc1nc(C)c(CNC(=O)N2CCC[C@@]3(CC(C)(C)O)C3)s1'\n",
      "[22:35:55] Can't kekulize mol.  Unkekulized atoms: 7 8 10 24 25 26 27\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'O=C(Cn1nc2n(-c1ccc(Cl)cc1)CCC2)Nc1ccc(Cl)cc1'\n",
      "[22:35:55] Can't kekulize mol.  Unkekulized atoms: 3 4 17 18 19 20 21 23 24\n",
      "[22:35:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(C[C@@H](C)N2CCC(n3nccc3NC(=O)C(C)(C)C3)CC2)n1'\n",
      "[22:35:55] Can't kekulize mol.  Unkekulized atoms: 12 13 22\n",
      "[22:35:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 19\n",
      "[22:35:56] SMILES Parse Error: unclosed ring for input: 'Cc1nnc2n1C[C@@H](NC(=O)Cn1c(C)cc3ccccc32)CC2'\n",
      "[22:35:56] Can't kekulize mol.  Unkekulized atoms: 12 13 14 21 28\n",
      "[22:35:56] SMILES Parse Error: unclosed ring for input: 'Cc1ncn(C2CN(C(=O)[C@H]3CCCCn3n3)C2)c1C'\n",
      "[22:35:56] SMILES Parse Error: unclosed ring for input: 'CC[C@@]1(C(=O)c2ccccc2)[C@H]2c3c(ccc4ccccc34)ON1C(=O)Nc1ccc(Cl)cc1'\n",
      "[22:35:56] SMILES Parse Error: extra open parentheses for input: 'CCc1cccc(N(CC(=O)N(Cc2ccc(OC)cc2)[C@H](Cc2ccccc2)C(=O)NC)c1'\n",
      "[22:35:56] SMILES Parse Error: extra close parentheses while parsing: CCOCCCN1C(=O)c2cccc3cc(C)c(=O)o23)C1\n",
      "[22:35:56] SMILES Parse Error: Failed parsing SMILES 'CCOCCCN1C(=O)c2cccc3cc(C)c(=O)o23)C1' for input: 'CCOCCCN1C(=O)c2cccc3cc(C)c(=O)o23)C1'\n",
      "[22:35:56] Can't kekulize mol.  Unkekulized atoms: 15 17\n",
      "[22:35:56] Can't kekulize mol.  Unkekulized atoms: 7 8 9 15 16\n",
      "[22:35:56] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(I)c(NC(=O)c2c3c(C(F)(F)F)cnn3C)[nH]c2=O)cc1F\n",
      "[22:35:56] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(I)c(NC(=O)c2c3c(C(F)(F)F)cnn3C)[nH]c2=O)cc1F' for input: 'Cc1cc(I)c(NC(=O)c2c3c(C(F)(F)F)cnn3C)[nH]c2=O)cc1F'\n",
      "[22:35:57] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 19 20 29\n",
      "[22:35:57] SMILES Parse Error: unclosed ring for input: 'CN(C(=O)c1nn[nH]c1N2CCC[C@H](CO)C2)c1C'\n",
      "[22:35:57] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 26\n",
      "[22:35:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[22:35:57] SMILES Parse Error: unclosed ring for input: 'Cc1nc(CSc2ccc(C(=O)Nc3nc4ccc(C(F)(F)F)cc3s3)cc2)cs1'\n",
      "[22:35:57] SMILES Parse Error: syntax error while parsing: O=C(Nc1ccc(S(=O)(=O)C(F)F)c()c1)c1ccc(F)cc1F\n",
      "[22:35:57] SMILES Parse Error: Failed parsing SMILES 'O=C(Nc1ccc(S(=O)(=O)C(F)F)c()c1)c1ccc(F)cc1F' for input: 'O=C(Nc1ccc(S(=O)(=O)C(F)F)c()c1)c1ccc(F)cc1F'\n",
      "[22:35:57] SMILES Parse Error: unclosed ring for input: 'CN1C(=O)N(C2CCCCC2)[C@H]2O[C@H]([C@H]3O[C@@H]4[C@H]([C@H]43)O[C@]3(C)[C@H](C(C)(C)C)O2)C(C)(C)O[C@H]1C'\n",
      "[22:35:57] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19\n",
      "[22:35:57] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 15\n",
      "[22:35:57] Explicit valence for atom # 22 Cl, 3, is greater than permitted\n",
      "[22:35:57] SMILES Parse Error: syntax error while parsing: O=[N+]1Oc2ccccc2Oc2ccc((Cl)cc21)[C@H]1CN(Cc2cccnc2)C[C@H]1O\n",
      "[22:35:57] SMILES Parse Error: Failed parsing SMILES 'O=[N+]1Oc2ccccc2Oc2ccc((Cl)cc21)[C@H]1CN(Cc2cccnc2)C[C@H]1O' for input: 'O=[N+]1Oc2ccccc2Oc2ccc((Cl)cc21)[C@H]1CN(Cc2cccnc2)C[C@H]1O'\n",
      "[22:35:57] SMILES Parse Error: extra open parentheses for input: 'COCc1ccc(C(=O)N2CC(=O)N(Cc3ccc(OC)cc3)C[C@H](OCc3ccncc3'\n",
      "[22:35:57] Can't kekulize mol.  Unkekulized atoms: 15 16 17 19 20 21 22\n",
      "[22:35:57] SMILES Parse Error: unclosed ring for input: 'O=C(Cc1ccc(F)cc1)N1CC[C@]2(CCC1=O)C[C@@H](NCc1ccccn1)C12CCCCC2'\n",
      "[22:35:57] SMILES Parse Error: extra close parentheses while parsing: Cc1ccccc1CN1Cc2ccc(NC(=O)[C@@H]3CCCO3)cc2)CC1\n",
      "[22:35:57] SMILES Parse Error: Failed parsing SMILES 'Cc1ccccc1CN1Cc2ccc(NC(=O)[C@@H]3CCCO3)cc2)CC1' for input: 'Cc1ccccc1CN1Cc2ccc(NC(=O)[C@@H]3CCCO3)cc2)CC1'\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'Cc1n[nH]c([C@@H]2C[C@@H]3CCN(C(=O)[C@H]4Cc4ccccc4O3)C[C@H]2O)n1'\n",
      "[22:35:58] Can't kekulize mol.  Unkekulized atoms: 11 12 21\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC2=Nc3cccc4c(C)c(C)cc2c3C(C)=C(C)N1'\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(Nc2cnc(NC(=O)c3cc4c(S(=O)(=O)N4CCOCC4)cccc3o2)C2CC2)c1'\n",
      "[22:35:58] Can't kekulize mol.  Unkekulized atoms: 2 3 5\n",
      "[22:35:58] SMILES Parse Error: syntax error while parsing: Cc1nc(CNC(=O)c2cc(C3CC3)on2)c(-2c(C)cc(C)n2)n1\n",
      "[22:35:58] SMILES Parse Error: Failed parsing SMILES 'Cc1nc(CNC(=O)c2cc(C3CC3)on2)c(-2c(C)cc(C)n2)n1' for input: 'Cc1nc(CNC(=O)c2cc(C3CC3)on2)c(-2c(C)cc(C)n2)n1'\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'CCN(CC)C[C@@H]1CCN2C[C@H]3C[C@H](C(=O)N3CCCC3)[C@@H]2C1'\n",
      "[22:35:58] Explicit valence for atom # 9 C, 5, is greater than permitted\n",
      "[22:35:58] Can't kekulize mol.  Unkekulized atoms: 8 9 16 17 19\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'CC(C)C1=NN2CCC[C@H]1CC(N)[C@@H]2N2'\n",
      "[22:35:58] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'COc1cccc2c1O[C@H](c1ccc(F)c(F)c1)N1N=C1[C@@H](c3ccc(Cl)cc3)OC[C@H]12'\n",
      "[22:35:58] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)[C@@H]1CCCN(C(=O)CN2CCC[C@H](c3cc(O)n4nc(-c4ccc(F)cc4)cn3)C2)C1'\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'Cc1cc([C@H](C)n2cccn2)N(CCn2cccn2)C(C)(C)C'\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@@H]1CSc2c1nc1ccccc1c1C(=O)O)c1ccccc1'\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 8 9 10 15 17\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 21 22 23 24\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'CC(C)Oc1cccc(NC(=O)[C@@H]2[C@@H]3CC[C@]4(CC(=O)c5ccc(F)cc54)=C[C@H]24)c1'\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'CC(C)COC(=O)N[C@H]([C@@H]1COC(C)(C)O[C@H]2C3CC1)C(=O)O'\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(N2C(=O)[C@@H]3[C@@H](c4ccccc4Cl)[C@H](c4ccco4)N3CCC[C@@]2(C)O3)cc1'\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 16\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'CCn1c(C2CC[C@H]3[C@@H](CC2)C4(F)F)nnc1N(C)Cc1cc(C)on1'\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)C(=O)Nc1ccc3c(c1)[C@]1(C(=O)Nc2ccc(F)cc1)N(C)C(=O)CO2'\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 5 7 8\n",
      "[22:35:59] SMILES Parse Error: unclosed ring for input: 'BrC1(Br)[C@@H]2[C@@H]3C[C@@H]4[C@@H]5C[C@H](C1)[C@@H]2[C@H]1[C@@H]3[C@@H]41'\n",
      "[22:35:59] SMILES Parse Error: syntax error while parsing: COc1ccccc1Oc1c(F)c(F)c(/=C/C#N)c2cc(F)ccc2F)c(=O)[nH]1\n",
      "[22:35:59] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1Oc1c(F)c(F)c(/=C/C#N)c2cc(F)ccc2F)c(=O)[nH]1' for input: 'COc1ccccc1Oc1c(F)c(F)c(/=C/C#N)c2cc(F)ccc2F)c(=O)[nH]1'\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 12 13 25 26 27 28 29 30 31\n",
      "[22:35:59] SMILES Parse Error: extra close parentheses while parsing: CN1C(=O)N(C2CCCCC2)[C@H]2O[C@H]([C@H]3O[C@@H]4[C@H]([C@H]43)O[C@]3(C(C)(C)C)[C@H](O)[C@]2(C)[C@@H](C)[C@H](C(C)=O)O4)C1)CC(C)C\n",
      "[22:35:59] SMILES Parse Error: Failed parsing SMILES 'CN1C(=O)N(C2CCCCC2)[C@H]2O[C@H]([C@H]3O[C@@H]4[C@H]([C@H]43)O[C@]3(C(C)(C)C)[C@H](O)[C@]2(C)[C@@H](C)[C@H](C(C)=O)O4)C1)CC(C)C' for input: 'CN1C(=O)N(C2CCCCC2)[C@H]2O[C@H]([C@H]3O[C@@H]4[C@H]([C@H]43)O[C@]3(C(C)(C)C)[C@H](O)[C@]2(C)[C@@H](C)[C@H](C(C)=O)O4)C1)CC(C)C'\n",
      "[22:35:59] non-ring atom 20 marked aromatic\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 11 12 15 16 19\n",
      "[22:35:59] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 16 17 20\n",
      "[22:36:00] SMILES Parse Error: unclosed ring for input: 'O=c1n(-c2ccccc2)c(=O)n2n1[C@@H]1[C@@H]3[C@H]4[C@H]5[C@@]33[C@@]12CC=CC[C@]53[C@@H]42'\n",
      "[22:36:00] SMILES Parse Error: unclosed ring for input: 'CNC(=O)c1ccc(N2C[C@@H]3COC[C@@H](C(O)=N4)C2)cc1'\n",
      "[22:36:00] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@@H]1C[C@]2(CCc3ccccc3OC3)CN1C(=O)OCc1ccccc1)c1ccccc1'\n",
      "[22:36:00] Can't kekulize mol.  Unkekulized atoms: 3 5 14\n",
      "[22:36:00] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 12\n",
      "[22:36:00] SMILES Parse Error: duplicated ring closure 2 bonds atom 19 to itself for input: 'CN1CCN(C(=O)N[C@H]2CCCN(C)c2ccccc22)CC1'\n",
      "[22:36:00] Can't kekulize mol.  Unkekulized atoms: 1 2 29\n",
      "[22:36:00] SMILES Parse Error: extra close parentheses while parsing: CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1\n",
      "[22:36:00] SMILES Parse Error: Failed parsing SMILES 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1' for input: 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1'\n",
      "[22:36:00] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(N2CCN(C(=O)c3cnc4c(-c4ccccc5)cnn34)CC2)cc1'\n",
      "[22:36:00] SMILES Parse Error: unclosed ring for input: 'O=C([C@H]1CC[C@H](NC2CC3)CC1)N1CCc2ccccc2C1'\n",
      "[22:36:00] SMILES Parse Error: extra open parentheses for input: 'O=C1CC[C@@H](C(=O)N2CCOC3(CCN(c4nc(-c5ccncc4)nc4ccccc34)CC2)O1'\n",
      "[22:36:01] SMILES Parse Error: extra open parentheses for input: 'CCOC(=O)c1c(C)oc2nc(CSc3nnc(-c4ccco4)n3c3ccccc3[n+](C)c(C)n12'\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 21 22 23\n",
      "[22:36:01] SMILES Parse Error: unclosed ring for input: 'COCCn1c(CN2CC[C@@]3(CC2)CNC(=O)c2Occccc2=O)Cc2ccccc21'\n",
      "[22:36:01] SMILES Parse Error: extra close parentheses while parsing: N[C@@H](COc1c(F)cc(Cl)cc1Cl)C(=O)O)NC(=O)OC\n",
      "[22:36:01] SMILES Parse Error: Failed parsing SMILES 'N[C@@H](COc1c(F)cc(Cl)cc1Cl)C(=O)O)NC(=O)OC' for input: 'N[C@@H](COc1c(F)cc(Cl)cc1Cl)C(=O)O)NC(=O)OC'\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14\n",
      "[22:36:01] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 13 14 15\n",
      "[22:36:01] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)Nc2cc3c(c2)CC(=O)Nc3ccc(C)cc23)cc1'\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 7\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 29\n",
      "[22:36:01] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S[C@@H]2C[C@H]3C[C@@H]2[C@H]2O)cc1'\n",
      "[22:36:01] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)N[C@H](Cc2ccccc2)C(=O)Oc2ccc3c4c(c(=O)[nH]c3c2)CCCCC3)cc1'\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 11 12 14 15 16\n",
      "[22:36:01] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:36:02] SMILES Parse Error: unclosed ring for input: 'O=C(c1nn2c(c1Cl)N[C@@H](c1ccc(Br)cc1)C[C@@H]2C(F)(F)F)N1CCC2'\n",
      "[22:36:02] SMILES Parse Error: extra close parentheses while parsing: CC(C)[C@@H]1NC(=O)N(CN(C)S(=O)(=O)c3ccc(Cl)cc3Cl)C2=O)C1\n",
      "[22:36:02] SMILES Parse Error: Failed parsing SMILES 'CC(C)[C@@H]1NC(=O)N(CN(C)S(=O)(=O)c3ccc(Cl)cc3Cl)C2=O)C1' for input: 'CC(C)[C@@H]1NC(=O)N(CN(C)S(=O)(=O)c3ccc(Cl)cc3Cl)C2=O)C1'\n",
      "[22:36:02] non-ring atom 7 marked aromatic\n",
      "[22:36:02] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2NN=c3c(oc4ccccc44)[C@@H]2c2ccc3c(c2)OCCO3)cc1'\n",
      "[22:36:02] SMILES Parse Error: unclosed ring for input: 'CN1CCc2cc(-c3csc(NC(=O)C4CCN(C(=O)OC(C)(C)C)CC3)n3)ccc21'\n",
      "[22:36:02] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 18 19\n",
      "[22:36:02] Can't kekulize mol.  Unkekulized atoms: 2 3 10 11 26\n",
      "[22:36:02] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 15 16 17 18\n",
      "[22:36:02] SMILES Parse Error: unclosed ring for input: 'CC(C)Cc1cc(-c2nn3c(CC(C)C)nnc2s2)n[nH]1'\n",
      "[22:36:02] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 17 18 19 30 31 32 33 34 35\n",
      "[22:36:03] SMILES Parse Error: unclosed ring for input: 'Brc1ccccc1-c1nn2c(CSc3ncnc3sccc24)nn12'\n",
      "[22:36:03] SMILES Parse Error: unclosed ring for input: 'O=C(NC1CCCCCC1)c1nnn2c1[nH]c(=O)c(=O)[nH]c1=O'\n",
      "[22:36:03] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CO[C@@H]2C[C@@H](O)C23CCC2)cc1OC'\n",
      "[22:36:03] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 12 16\n",
      "[22:36:03] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 10\n",
      "[22:36:03] SMILES Parse Error: extra close parentheses while parsing: CC(C)NC(=O)N1C[C@H]2Cc3c(nc(-c4cccnc4)nc3CO)C2)CC1\n",
      "[22:36:03] SMILES Parse Error: Failed parsing SMILES 'CC(C)NC(=O)N1C[C@H]2Cc3c(nc(-c4cccnc4)nc3CO)C2)CC1' for input: 'CC(C)NC(=O)N1C[C@H]2Cc3c(nc(-c4cccnc4)nc3CO)C2)CC1'\n",
      "[22:36:03] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C2(C(=O)N3CC[C@]4(CC(=O)c4ccccc5O4)CC3)CCCC2)cc1'\n",
      "[22:36:03] SMILES Parse Error: unclosed ring for input: 'CCc1cccc(CC)c1N(Cc1ccc(C(=O)Nc2ccc3c4c(cccc25)CC3)cc1)S(C)(=O)=O'\n",
      "[22:36:04] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:36:04] SMILES Parse Error: ring closure 2 duplicates bond between atom 22 and atom 23 for input: 'OC1=N[C@H]2[C@H]3C[C@@H]([C@@H]22)[C@@H]1[C@@H]2C(=O)N(c4ccc(Br)cc3)C(=O)[C@@H]2[C@@H]2C'\n",
      "[22:36:04] SMILES Parse Error: extra close parentheses while parsing: C[C@@]1(Cc2cccc(F)c2)C)nnc1NCc1csc(CN(C)C)n1\n",
      "[22:36:04] SMILES Parse Error: Failed parsing SMILES 'C[C@@]1(Cc2cccc(F)c2)C)nnc1NCc1csc(CN(C)C)n1' for input: 'C[C@@]1(Cc2cccc(F)c2)C)nnc1NCc1csc(CN(C)C)n1'\n",
      "[22:36:04] SMILES Parse Error: syntax error while parsing: CCOC(=O)COC(=O)c1c2c(nc3ccccc3c2Br)-(c2ccc(F)cc2)C(=O)C1\n",
      "[22:36:04] SMILES Parse Error: Failed parsing SMILES 'CCOC(=O)COC(=O)c1c2c(nc3ccccc3c2Br)-(c2ccc(F)cc2)C(=O)C1' for input: 'CCOC(=O)COC(=O)c1c2c(nc3ccccc3c2Br)-(c2ccc(F)cc2)C(=O)C1'\n",
      "[22:36:04] SMILES Parse Error: unclosed ring for input: 'CNc1cc(C)c([C@@H](C)NC(=O)Nc2ccc(F)cc2)c2cccnc1'\n",
      "[22:36:04] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 26\n",
      "[22:36:04] Can't kekulize mol.  Unkekulized atoms: 6 7 18\n",
      "[22:36:04] SMILES Parse Error: unclosed ring for input: 'Nc1ccc(C2CCC(NC(=O)N[C@H]3C[C@H]4C(=O)NCC3)CC2)cc1'\n",
      "[22:36:04] SMILES Parse Error: unclosed ring for input: 'Cc1nc2nnc3c(-c4ccccc4)c4c(n22)CCCC4'\n",
      "[22:36:04] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccc(Br)cc1)N12CCC(c3ccccc3)CC1'\n",
      "[22:36:04] Can't kekulize mol.  Unkekulized atoms: 2 3 18 19 20\n",
      "[22:36:04] non-ring atom 14 marked aromatic\n",
      "[22:36:04] Can't kekulize mol.  Unkekulized atoms: 3 27 28 29 30 31 32\n",
      "[22:36:04] SMILES Parse Error: unclosed ring for input: 'CCCN1Cc2c(nc3cc(C)nn3c3Cc3ccccc3)C[C@@H]2C1=O'\n",
      "[22:36:05] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(c1)C[C@H]1[C@H]3CC(=O)N[C@H]3[C@H]1C'\n",
      "[22:36:05] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 19 20\n",
      "[22:36:05] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)c1cc2c(nc1SCC(=O)Nc1cccc3c1O)=OC(=O)n2C'\n",
      "[22:36:05] Can't kekulize mol.  Unkekulized atoms: 12 13 21 24\n",
      "[22:36:05] Can't kekulize mol.  Unkekulized atoms: 5 6 16 17 18 19 29\n",
      "[22:36:05] SMILES Parse Error: extra close parentheses while parsing: O=Sc1nc2nc[nH]n2)c(I)n1\n",
      "[22:36:05] SMILES Parse Error: Failed parsing SMILES 'O=Sc1nc2nc[nH]n2)c(I)n1' for input: 'O=Sc1nc2nc[nH]n2)c(I)n1'\n",
      "[22:36:05] SMILES Parse Error: unclosed ring for input: 'CC1=NN(C(=O)[C@@H](C)n2Cc2ccc(Cl)cc2)C[C@@]1(O)C(F)(F)F'\n",
      "[22:36:05] Can't kekulize mol.  Unkekulized atoms: 12 13 18\n",
      "[22:36:05] Can't kekulize mol.  Unkekulized atoms: 16 21 22 23 24\n",
      "[22:36:05] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 22\n",
      "[22:36:05] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(OC)c([C@@H](CNC(=O)N[C@@H]2CCC(C)(C)OC2)C1'\n",
      "[22:36:06] SMILES Parse Error: unclosed ring for input: 'CC1(C)C[C@@H](O)[C@]2(C)CC[C@]3(C(=O)O)CC[C@]4(O)C[C@H](O)CC[C@]3(C)[C@H]1C(O)C2'\n",
      "[22:36:06] Can't kekulize mol.  Unkekulized atoms: 4 9 10 11 12 31 32\n",
      "[22:36:06] SMILES Parse Error: unclosed ring for input: 'c1nc(N2CCC(c3nc(-c4ccncc4)nc3CCCO3)CC2)cs1'\n",
      "[22:36:06] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 22\n",
      "[22:36:06] SMILES Parse Error: unclosed ring for input: 'Cc1cnc2c(cnn2CC(=O)N2CCC[C@H]3C[C@@H](CO)C[C@@H]42)c1'\n",
      "[22:36:06] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 18 27\n",
      "[22:36:06] SMILES Parse Error: unclosed ring for input: 'CC[C@@H](Sc1nnc(SCc2csc(-c3ccc(C)cc3)n2)C1CC1)C(=O)NC(N)=O'\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'C[C@@]1(C)Cc2c(sc3nc(SC(C)C)n(Cc4ccc5c(c4)C(=O)N([C@@H](C)Cn5cccn5)C4=O)n23)CO1'\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@@H]2[C@H](C(=O)NCCCO)CN3CCCN2C2=O)c1'\n",
      "[22:36:07] SMILES Parse Error: extra close parentheses while parsing: O=C1COc2ccccc2C/C=C/C[C@@]23CC[C@H]2O3)C1\n",
      "[22:36:07] SMILES Parse Error: Failed parsing SMILES 'O=C1COc2ccccc2C/C=C/C[C@@]23CC[C@H]2O3)C1' for input: 'O=C1COc2ccccc2C/C=C/C[C@@]23CC[C@H]2O3)C1'\n",
      "[22:36:07] Can't kekulize mol.  Unkekulized atoms: 12 13 21 22 23 24 27 30 31\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'O=C(CN1C(=O)N[C@](C)2CCc3ccccc32)(c1ccccc1)c1ccccc1'\n",
      "[22:36:07] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 8\n",
      "[22:36:07] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 22\n",
      "[22:36:07] Can't kekulize mol.  Unkekulized atoms: 2 3 5 6 7 8 9\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'CCCCN1CCCC[C@H]1C(=O)N1CCC2(CC1)Cc1c(c(O)nc(C)n12)CCN2'\n",
      "[22:36:07] Explicit valence for atom # 1 O, 3, is greater than permitted\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'CN(C)C(=O)Cn1cc(C(=O)N2CCC3(CC2)c2nccn3C2CC2)cn1'\n",
      "[22:36:07] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(-c2nnc([C@@H](C)Sc3nnc(N4CCOCC4)n2-c2ccccc2)o1'\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'CN(C)[C@@H]1CN(c2ncc(C(F)(F)F)cc2Cl)C[C@@H]2O[C@@H](CO)CO1'\n",
      "[22:36:07] SMILES Parse Error: ring closure 1 duplicates bond between atom 7 and atom 8 for input: 'Cc1ccccc1[C@@H]1[C@@H]1CC(=O)Nc2c1nc12'\n",
      "[22:36:07] Can't kekulize mol.  Unkekulized atoms: 6 7 15 16 17 18 19 20 21\n",
      "[22:36:07] SMILES Parse Error: unclosed ring for input: 'O=C1CCc2cc(C(=O)N3CCC(N4CCN([C@H]4CCOC4)CC3)CC2)ccc21'\n",
      "[22:36:07] Can't kekulize mol.  Unkekulized atoms: 9 10 11 14 15 16 18 20 22\n",
      "[22:36:08] Can't kekulize mol.  Unkekulized atoms: 1 2 9 10 11 12 14 16 18\n",
      "[22:36:08] SMILES Parse Error: extra close parentheses while parsing: COCCC(=O)N1CC2(C1)[C@H](COCC1CC1)CCS2(=O)=O)C1CCOCC1\n",
      "[22:36:08] SMILES Parse Error: Failed parsing SMILES 'COCCC(=O)N1CC2(C1)[C@H](COCC1CC1)CCS2(=O)=O)C1CCOCC1' for input: 'COCCC(=O)N1CC2(C1)[C@H](COCC1CC1)CCS2(=O)=O)C1CCOCC1'\n",
      "[22:36:08] Can't kekulize mol.  Unkekulized atoms: 13 14 15\n",
      "[22:36:08] Can't kekulize mol.  Unkekulized atoms: 22 23 26 27 28\n",
      "[22:36:08] Can't kekulize mol.  Unkekulized atoms: 10 11 12 14 17 20\n",
      "[22:36:08] SMILES Parse Error: unclosed ring for input: 'Cc1noc(C)c1CCC(=O)N1CC2(CCC2)c1ccccc12'\n",
      "[22:36:08] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 16\n",
      "[22:36:08] SMILES Parse Error: unclosed ring for input: 'CN(CCC(=O)NC1CCCCC1)Cc1nc(O)c2c(n1)C1(CC2)CCO2'\n",
      "[22:36:08] SMILES Parse Error: unclosed ring for input: 'CC(C)[C@H]1CC(=O)NCCN1C(=O)c1cc(N2CCNCC1)nn1C'\n",
      "[22:36:08] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(C(C)=O)cc1N1C(=O)[C@@H]2[C@H](ON(c3cccc4)[C@H]2c2ccc(Cl)cc2)[C@H]1c1cccc(Cl)c1'\n",
      "[22:36:09] Can't kekulize mol.  Unkekulized atoms: 1 2 3 6 7 26 27\n",
      "[22:36:09] Can't kekulize mol.  Unkekulized atoms: 2 10 11 12 13 14 26 27 28\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'C[C@]12C[C@@]13CO[C@@]1(CNC[C@]3(C)C1)CNCC3'\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'Cc1cc(N2CC[C@@H](NC(=O)COc3ccc(Cl)c(C)c2)C2=O)nc(N)n1'\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2cc(CC3=C4[C@@H]([C@@H]4C[C@@H]43)[C@H]3CC[C@H](O)C4)n(C)n2)cc1'\n",
      "[22:36:09] Can't kekulize mol.  Unkekulized atoms: 11 12 14 25 26\n",
      "[22:36:09] Can't kekulize mol.  Unkekulized atoms: 5 6 8 9 10 11 12 13 14\n",
      "[22:36:09] SMILES Parse Error: extra close parentheses while parsing: CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1\n",
      "[22:36:09] SMILES Parse Error: Failed parsing SMILES 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1' for input: 'CN(CC(=O)NC1CC1)C(=O)CNc1ccccc1N1CCCCC1)c1ccccc1'\n",
      "[22:36:09] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 13 14 15 31\n",
      "[22:36:09] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(C(=O)[C@H]2[C@@H]3C(=O)N(c4ccc(OC)cc4)C(=O)[C@@H]3[C@H]3c3ccc(Cl)cc3Cl)cc2OC)c1\n",
      "[22:36:09] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(C(=O)[C@H]2[C@@H]3C(=O)N(c4ccc(OC)cc4)C(=O)[C@@H]3[C@H]3c3ccc(Cl)cc3Cl)cc2OC)c1' for input: 'COc1ccc(C(=O)[C@H]2[C@@H]3C(=O)N(c4ccc(OC)cc4)C(=O)[C@@H]3[C@H]3c3ccc(Cl)cc3Cl)cc2OC)c1'\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'CCN1C(=O)C(Cl)=C(Cl)CN1C1=N'\n",
      "[22:36:09] Can't kekulize mol.  Unkekulized atoms: 9 10 11 13 14 15 16\n",
      "[22:36:09] Explicit valence for atom # 12 C, 5, is greater than permitted\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'CC[S@@](=O)[C@@H]1CCC[C@H]2NC(=O)N[C@H]2c2ccc(C)cc2OC'\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'CCCc1cc(=O)oc2c(C)c(O[C@H](C)C(=O)N[C@H](Cc3c[nH]c4ccccc35)C(=O)O)ccc12'\n",
      "[22:36:09] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccc(OCCCC(=O)Nc2cc(S(=O)(=O)N3CCCCC2)ccc2OC)cc1'\n",
      "[22:36:10] SMILES Parse Error: unclosed ring for input: 'CCCc1sc(-n2ccnc2)nc1C1CC=Br'\n",
      "[22:36:10] SMILES Parse Error: extra close parentheses while parsing: O=C(NCc1ccccc1)[C@]12CCCN(CC3CC3)C1)CCO2\n",
      "[22:36:10] SMILES Parse Error: Failed parsing SMILES 'O=C(NCc1ccccc1)[C@]12CCCN(CC3CC3)C1)CCO2' for input: 'O=C(NCc1ccccc1)[C@]12CCCN(CC3CC3)C1)CCO2'\n",
      "[22:36:10] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C1CCN(Cn2c3c(oc3ccccc34)C[C@H](C)O2)CC1'\n",
      "[22:36:10] Can't kekulize mol.  Unkekulized atoms: 1 2 3 12 13 14 21 22 29\n",
      "[22:36:10] Can't kekulize mol.  Unkekulized atoms: 14 15 16 22 23\n",
      "[22:36:10] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1ccc2c(c1)OCCO2)Nc1nc(-c2ccc3[nH]c(C(F)(F)F)cc2s2)cs1'\n",
      "[22:36:10] SMILES Parse Error: unclosed ring for input: 'O=C(CN1C(=O)NC=C2c3ccccc3C2(CC2)C1)N1CCCCC1'\n",
      "[22:36:10] Explicit valence for atom # 1 O, 3, is greater than permitted\n",
      "[22:36:10] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 22\n",
      "[22:36:10] Can't kekulize mol.  Unkekulized atoms: 13 14 16\n",
      "[22:36:10] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[22:36:10] SMILES Parse Error: extra close parentheses while parsing: CCCC(=O)NCc1cccnc1)c1nc2cccc(F)c2[nH]1\n",
      "[22:36:10] SMILES Parse Error: Failed parsing SMILES 'CCCC(=O)NCc1cccnc1)c1nc2cccc(F)c2[nH]1' for input: 'CCCC(=O)NCc1cccnc1)c1nc2cccc(F)c2[nH]1'\n",
      "[22:36:11] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9\n",
      "[22:36:11] SMILES Parse Error: unclosed ring for input: 'O=C(Cn1ccccc1=O)N[C@H]1CCOC22CCC(C(F)(F)F)CC2'\n",
      "[22:36:11] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(N2C(=O)[C@@H]3[C@@H]4C[C@H]5[C@H]5[C@]4(C(=O)O[C@@H]5CCC[C@@]5(C)[C@@H]42)[C@@]2(C)O[C@@H]4[C@]2(C)O[C@H](C)[C@H]42)C1'\n",
      "[22:36:11] SMILES Parse Error: unclosed ring for input: 'Cn1ncc2c(C(=O)N3CCC(C(=O)N3CCCCC3)CC2)cc(Cl)cc11'\n",
      "[22:36:11] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[22:36:11] SMILES Parse Error: extra open parentheses for input: 'O=C([C@H](Sc1nnc(-c2cccnc2)n1C[C@@H]1CCCO1)N1CCCC1'\n",
      "[22:36:11] SMILES Parse Error: extra close parentheses while parsing: Cn1c(O)nc2nnc([C@H](C)NC(=O)NC3c4CCCC4)CC(C)(C)C3)c2n1\n",
      "[22:36:11] SMILES Parse Error: Failed parsing SMILES 'Cn1c(O)nc2nnc([C@H](C)NC(=O)NC3c4CCCC4)CC(C)(C)C3)c2n1' for input: 'Cn1c(O)nc2nnc([C@H](C)NC(=O)NC3c4CCCC4)CC(C)(C)C3)c2n1'\n",
      "[22:36:11] SMILES Parse Error: unclosed ring for input: 'O[C@]12[C@@H]3C[C@@H]4[C@@H]5[C@H]3[C@H]3C[C@H]2[C@@H]4[C@@H]4[C@@H]41'\n",
      "[22:36:11] SMILES Parse Error: extra close parentheses while parsing: O=C1c2ccccc2I)N[C@H]1c1ccc(F)cc1\n",
      "[22:36:11] SMILES Parse Error: Failed parsing SMILES 'O=C1c2ccccc2I)N[C@H]1c1ccc(F)cc1' for input: 'O=C1c2ccccc2I)N[C@H]1c1ccc(F)cc1'\n",
      "[22:36:11] Can't kekulize mol.  Unkekulized atoms: 4 16 18\n",
      "[22:36:11] SMILES Parse Error: unclosed ring for input: 'Cc1nc(COc2ccc(C(=O)Nc3cccc([C@]4(C)NS(C)(=O)=O)c3)cc2)cs1'\n",
      "[22:36:12] SMILES Parse Error: unclosed ring for input: 'Cc1ncc(-c2ccc(F)cc2)c([C@H]2CCCN(S(=O)(=O)c3cccc4c3OCO3)C2)n1'\n",
      "[22:36:12] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 26 27 28\n",
      "[22:36:12] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 19\n",
      "[22:36:12] SMILES Parse Error: extra close parentheses while parsing: CCCc1cc(C(=O)N(CCc2ccccc2)C)[C@H]2CCS(=O)(=O)C2)nc1C\n",
      "[22:36:12] SMILES Parse Error: Failed parsing SMILES 'CCCc1cc(C(=O)N(CCc2ccccc2)C)[C@H]2CCS(=O)(=O)C2)nc1C' for input: 'CCCc1cc(C(=O)N(CCc2ccccc2)C)[C@H]2CCS(=O)(=O)C2)nc1C'\n",
      "[22:36:12] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(Br)cc1NC(=O)c1cc(=O)c2cccc3c1O[C@H]1C(=O)O[C@@H](C)C1'\n",
      "[22:36:12] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 24\n",
      "[22:36:12] Explicit valence for atom # 3 F, 2, is greater than permitted\n",
      "[22:36:12] SMILES Parse Error: extra open parentheses for input: 'O=C(c1cccc(OCc2c(CO)n(O)c3ccccc3[n+]2=O)N1CCc2ccccc2C1'\n",
      "[22:36:12] SMILES Parse Error: unclosed ring for input: 'Clc1ccc2c(c1)[C@@H]1CC2(CCN(C(=O)c3ccncc3)CC2)CN(Cc1cccc(OCC(F)F)c1)CC2'\n",
      "[22:36:12] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(Cl)cc1S(=O)(=O)N[C@@H](C)C(=O)Nc1c(C)cccc1C)c1ccccc1\n",
      "[22:36:12] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(Cl)cc1S(=O)(=O)N[C@@H](C)C(=O)Nc1c(C)cccc1C)c1ccccc1' for input: 'COc1ccc(Cl)cc1S(=O)(=O)N[C@@H](C)C(=O)Nc1c(C)cccc1C)c1ccccc1'\n",
      "[22:36:13] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:36:13] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 19\n",
      "[22:36:13] Can't kekulize mol.  Unkekulized atoms: 2 3 4 10\n",
      "[22:36:13] SMILES Parse Error: unclosed ring for input: 'Cc1c2c(CNC(=O)[C@@H](C)Oc3ccccc3)c(=O)oc2cc3oc(O)nc21'\n",
      "[22:36:13] SMILES Parse Error: unclosed ring for input: 'O[C@@H]1CC2(CCN(C[C@H](O)c3cccc4oc4ccccc35)CC2)O1'\n",
      "[22:36:13] SMILES Parse Error: unclosed ring for input: 'Cc1c(C(=O)O[C@H](C)[C@@H](C)[C@@H]2C[C@@H]3C[C@@H]2[C@H]2C)c(=O)[nH]c2ccccc12'\n",
      "[22:36:13] SMILES Parse Error: unclosed ring for input: 'Cc1cnc(Cn2nc(C)nc2C1CC2)o1'\n",
      "[22:36:13] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[22:36:13] Can't kekulize mol.  Unkekulized atoms: 2 22 23 24 25 26 27\n",
      "[22:36:13] Can't kekulize mol.  Unkekulized atoms: 2 3 11\n",
      "[22:36:14] SMILES Parse Error: unclosed ring for input: 'CC1CCN(CC(=O)N2CC[C@H](Cc3ccc4c(c3)OCO4)[C@H](O)[C@@H]2C2)CC1'\n",
      "[22:36:14] Can't kekulize mol.  Unkekulized atoms: 9 10 23\n",
      "[22:36:14] SMILES Parse Error: unclosed ring for input: 'CCCCNC(=O)[C@H](c1ccc(OC)cc1)N(c2cccc(OC)c1)S(C)(=O)=O'\n",
      "[22:36:14] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[22:36:14] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 9 for input: 'FC(F)(F)OCc1nc2c2ncn(C[C@@H]3CCCO3)c2cc1C'\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 22\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 19\n",
      "[22:36:15] SMILES Parse Error: extra close parentheses while parsing: Cn1cccc1C(=O)N1C[C@H]2C[C@@H](C1)[C@H](Cc1cccc(F)c1)N1C(=O)CNC2=O)C1\n",
      "[22:36:15] SMILES Parse Error: Failed parsing SMILES 'Cn1cccc1C(=O)N1C[C@H]2C[C@@H](C1)[C@H](Cc1cccc(F)c1)N1C(=O)CNC2=O)C1' for input: 'Cn1cccc1C(=O)N1C[C@H]2C[C@@H](C1)[C@H](Cc1cccc(F)c1)N1C(=O)CNC2=O)C1'\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17 20 21\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 2 3 16 17 18 19 20\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 16\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 7 30 32 33\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 1 2 5 23\n",
      "[22:36:15] Explicit valence for atom # 6 Br, 2, is greater than permitted\n",
      "[22:36:15] Can't kekulize mol.  Unkekulized atoms: 14 15 19\n",
      "[22:36:15] Explicit valence for atom # 2 N, 5, is greater than permitted\n",
      "[22:36:16] SMILES Parse Error: unclosed ring for input: 'O=C([C@@H]1CC=CCC1)N1[C@H]2CC[C@H]1CC(n1cc(C3(OCC4)CCC3)nn1)C2'\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 5 6 7 16 17 18 19 20 21\n",
      "[22:36:16] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)n(-c2ccc(NC(=O)C3CCN(C(=O)Cc4c[nH]c4ccccc35)CC3)nn2)n1'\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 15\n",
      "[22:36:16] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-n2c(S[C@@H](C)C(=O)N3CC(C)(C)C)nn2)nnc1N1CCOCC1'\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 8 9 10 11\n",
      "[22:36:16] SMILES Parse Error: ring closure 2 duplicates bond between atom 15 and atom 16 for input: 'C[C@@H]1CN(C2CC2)CCN1C(=O)[C@@H]1CS[C@@]2(c2ccccc2)CCC(=O)N1'\n",
      "[22:36:16] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@H]3C[C@H]([C@@H]4[C@@H]5CC[C@H](C5)[C@@H]34)[C@H]2[C@H]2[C@@H]3[C@@H]4C[C@H]1[C@H]52'\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 15 18 20\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[22:36:16] Can't kekulize mol.  Unkekulized atoms: 4 5 14\n",
      "[22:36:16] SMILES Parse Error: unclosed ring for input: 'Clc1nnc2n1CCN(Cc1cc(Cl)c3c4c(c(=O)oc2c1)CCC3)C2'\n",
      "[22:36:17] Can't kekulize mol.  Unkekulized atoms: 2 3 5 20 21 22 23 24 25 26\n",
      "[22:36:17] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(N2C[C@@H](C(=O)Nc3ccc4nc[nH]c(=O)c3c3)CC2=O)c(C)c1'\n",
      "[22:36:17] SMILES Parse Error: ring closure 1 duplicates bond between atom 7 and atom 8 for input: 'O=C1[C@@H]2[C@@H]3C[C@]([C@@H]41N1c1ccccc1)[C@H]1[C@H]3C4=C[C@H]5C=C[C@H]4[C@H]([C@@H]23)[C@H]21'\n",
      "[22:36:17] SMILES Parse Error: syntax error while parsing: COc1ccc([C@@H]2C3=C(Nc4nc5nc(C(=O)OC(C)=O)c(C)n5nn4nn3=)N[C@H]2I)cc1\n",
      "[22:36:17] SMILES Parse Error: Failed parsing SMILES 'COc1ccc([C@@H]2C3=C(Nc4nc5nc(C(=O)OC(C)=O)c(C)n5nn4nn3=)N[C@H]2I)cc1' for input: 'COc1ccc([C@@H]2C3=C(Nc4nc5nc(C(=O)OC(C)=O)c(C)n5nn4nn3=)N[C@H]2I)cc1'\n",
      "[22:36:17] SMILES Parse Error: ring closure 2 duplicates bond between atom 5 and atom 6 for input: 'CCOC(=O)[C@]12[C@@H]2C(=O)N(Cc3ccccc3Cl)C(=O)[C@H]2[C@@H]2CCCN21'\n",
      "[22:36:17] Can't kekulize mol.  Unkekulized atoms: 29\n",
      "[22:36:17] SMILES Parse Error: unclosed ring for input: 'c1ccc(Cc2cn3c4c(cccc4c2C2)C[C@@H]2C[C@@H](C(F)(F)F)O2)cc1'\n",
      "[22:36:17] SMILES Parse Error: extra close parentheses while parsing: COc1cccc([C@@H]2C[C@@H](O)CN2C(=O)CCN2C(C)=O)C2)c1\n",
      "[22:36:17] SMILES Parse Error: Failed parsing SMILES 'COc1cccc([C@@H]2C[C@@H](O)CN2C(=O)CCN2C(C)=O)C2)c1' for input: 'COc1cccc([C@@H]2C[C@@H](O)CN2C(=O)CCN2C(C)=O)C2)c1'\n",
      "[22:36:17] Explicit valence for atom # 6 O, 3, is greater than permitted\n",
      "[22:36:17] SMILES Parse Error: extra close parentheses while parsing: C[C@]1(CC[C@H]2C[C@@H]2CNC[C@]3(C)C2)O)cccc1O\n",
      "[22:36:17] SMILES Parse Error: Failed parsing SMILES 'C[C@]1(CC[C@H]2C[C@@H]2CNC[C@]3(C)C2)O)cccc1O' for input: 'C[C@]1(CC[C@H]2C[C@@H]2CNC[C@]3(C)C2)O)cccc1O'\n",
      "[22:36:17] Can't kekulize mol.  Unkekulized atoms: 19 20 21 25 26\n",
      "[22:36:17] Can't kekulize mol.  Unkekulized atoms: 0 1 2 18 19 20 22\n",
      "[22:36:17] Can't kekulize mol.  Unkekulized atoms: 22 23 24 25 26\n",
      "[22:36:18] SMILES Parse Error: extra close parentheses while parsing: CC(=O)[C@H]1CC2CCC(C(=O)Nc3ccc(OCC)cc3)CC2)CN1\n",
      "[22:36:18] SMILES Parse Error: Failed parsing SMILES 'CC(=O)[C@H]1CC2CCC(C(=O)Nc3ccc(OCC)cc3)CC2)CN1' for input: 'CC(=O)[C@H]1CC2CCC(C(=O)Nc3ccc(OCC)cc3)CC2)CN1'\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 0 2 3 4 11 12 26\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 16 18 22\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 3 4 9 10 11 12 17 18 19\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 2 3 5 21 22 23 26 29 30\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 11 12 14 23 24 25 26\n",
      "[22:36:18] SMILES Parse Error: extra open parentheses for input: 'O=C(c1cn(C2CCCCC2)nn1)N1CCC(OCC1CC1'\n",
      "[22:36:18] SMILES Parse Error: unclosed ring for input: 'C[C@H](OC(=O)CC12C[C@H]3C[C@@H](CC(Cl)(C3)C1)C2)C(=O)c1c[nH]c3ccccc12'\n",
      "[22:36:18] SMILES Parse Error: duplicated ring closure 3 bonds atom 28 to itself for input: 'CC(C)C(=O)N1CCN(c2cc(N3CCN(C(=O)OCC)c(=O)c3ccccc33)CC2)ncn1'\n",
      "[22:36:18] SMILES Parse Error: extra close parentheses while parsing: C[C@H]1CCC[C@H]1N)C(=O)COC(=O)Cc1ccc2c(c1)CCC2\n",
      "[22:36:18] SMILES Parse Error: Failed parsing SMILES 'C[C@H]1CCC[C@H]1N)C(=O)COC(=O)Cc1ccc2c(c1)CCC2' for input: 'C[C@H]1CCC[C@H]1N)C(=O)COC(=O)Cc1ccc2c(c1)CCC2'\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 7 8 9 24 27\n",
      "[22:36:18] SMILES Parse Error: ring closure 1 duplicates bond between atom 3 and atom 4 for input: 'C[C@H](N[C@H]1[C@@H]1CCC[C@]12CCN(C(=O)CC1CCCC1)C2)c1ccncc1'\n",
      "[22:36:18] SMILES Parse Error: unclosed ring for input: 'COC(=O)C12C[C@@H]3C[C@H](CC(NC(=O)c4cc(OC)c5c(c4)OCCO4)C1)CC2'\n",
      "[22:36:18] Can't kekulize mol.  Unkekulized atoms: 10 11 22\n",
      "[22:36:19] SMILES Parse Error: unclosed ring for input: 'O=C1N[C@@H](Oc2ccccc2Cl)[C@@H]2O[C@H]3[C@@H](O1)c1cc(O)ccc1O2'\n",
      "[22:36:19] Can't kekulize mol.  Unkekulized atoms: 2 3 4 21 22\n",
      "[22:36:19] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2c(c1)C(=O)N1CCN(S(=O)(=O)c3cnn(C)c3C)CC2)[C@H]1Cc1ccc(Cl)cc1\n",
      "[22:36:19] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2c(c1)C(=O)N1CCN(S(=O)(=O)c3cnn(C)c3C)CC2)[C@H]1Cc1ccc(Cl)cc1' for input: 'Cc1ccc2c(c1)C(=O)N1CCN(S(=O)(=O)c3cnn(C)c3C)CC2)[C@H]1Cc1ccc(Cl)cc1'\n",
      "[22:36:19] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 19\n",
      "[22:36:19] SMILES Parse Error: unclosed ring for input: 'Cn1c(N2C[C@H]3CN(S(C)(=O)=O)CCO3)nc2ccccc21'\n",
      "[22:36:19] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 26\n",
      "[22:36:19] Can't kekulize mol.  Unkekulized atoms: 2 3 24\n",
      "[22:36:19] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 27\n",
      "[22:36:19] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8\n",
      "[22:36:20] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 18 19 20 21\n",
      "[22:36:20] SMILES Parse Error: unclosed ring for input: 'COc1ccc(N2C[C@](O)(c3cc(-c4cccs4)nc3ccc(C)cc34)CC2=O)cc1'\n",
      "[22:36:20] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 19 20 21\n",
      "[22:36:20] SMILES Parse Error: unclosed ring for input: 'CC(=O)Oc1c(I)cc(/C=C2/N=C(c3cccc4Cl)OC3=O)c(OC)c1C'\n",
      "[22:36:20] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 3 for input: 'O=S1(=O)N1CC[C@H](Nc2nccn3cnnc23)CC1'\n",
      "[22:36:20] SMILES Parse Error: extra close parentheses while parsing: CC(=O)Nc1ccc([C@@H]2C[C@H]3CN(Cc4ccccc4OC(=O)c4cccnc4)S3)C2)cc1\n",
      "[22:36:20] SMILES Parse Error: Failed parsing SMILES 'CC(=O)Nc1ccc([C@@H]2C[C@H]3CN(Cc4ccccc4OC(=O)c4cccnc4)S3)C2)cc1' for input: 'CC(=O)Nc1ccc([C@@H]2C[C@H]3CN(Cc4ccccc4OC(=O)c4cccnc4)S3)C2)cc1'\n",
      "[22:36:20] SMILES Parse Error: extra close parentheses while parsing: N[C@H](Cc1cccc(CC(=O)O)c1)C(=O)O)c1ccc(Cl)cc1\n",
      "[22:36:20] SMILES Parse Error: Failed parsing SMILES 'N[C@H](Cc1cccc(CC(=O)O)c1)C(=O)O)c1ccc(Cl)cc1' for input: 'N[C@H](Cc1cccc(CC(=O)O)c1)C(=O)O)c1ccc(Cl)cc1'\n",
      "[22:36:20] SMILES Parse Error: unclosed ring for input: 'CC1=C(C)N2C(=O)[C@@]3(N[C@@H](C(=O)Nc4ccccc4)[C@H]4C=C[C@@]2(O4)[C@@H]2OC(C)(C)C[C@H]21)C(C)(C)C'\n",
      "[22:36:20] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 14 15 19 21\n",
      "[22:36:21] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14\n",
      "[22:36:21] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7\n",
      "[22:36:22] SMILES Parse Error: unclosed ring for input: 'NC(=O)c1ccc(C(=O)N2CCC2(CC2)COCCc2ccccc2NC2=CS(=O)(=O)CC2)cc1'\n",
      "[22:36:22] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OCCOC(=O)c2ccc3[nH]c4c(c2c2)CCCCC4)cc1'\n",
      "[22:36:22] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2COc3c(Cl)c4c(c(=O)oc4c3C2)CCC4)c(OC)c1'\n",
      "[22:36:22] Can't kekulize mol.  Unkekulized atoms: 9 10 11 19 20\n",
      "[22:36:22] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(OCC(=O)Nc2cccc(-c3nc4c(ccc5ccccc55)o3)c2)c1'\n",
      "[22:36:22] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 28 29 30\n",
      "[22:36:22] Explicit valence for atom # 6 C, 5, is greater than permitted\n",
      "[22:36:22] Can't kekulize mol.  Unkekulized atoms: 8 9 11 12 13\n",
      "[22:36:22] SMILES Parse Error: unclosed ring for input: 'Cc1cc(NC(=O)c2ccc(OCc3ccc(S(=O)(=O)N(C)Co4)cc3)cc2)no1'\n",
      "[22:36:22] SMILES Parse Error: ring closure 1 duplicates bond between atom 1 and atom 2 for input: 'O=C1C1(CN2CCOCC2)CC1'\n",
      "[22:36:22] SMILES Parse Error: extra close parentheses while parsing: O=C(NC[C@@H]1CC=2)CC1)c1ccccn1\n",
      "[22:36:22] SMILES Parse Error: Failed parsing SMILES 'O=C(NC[C@@H]1CC=2)CC1)c1ccccn1' for input: 'O=C(NC[C@@H]1CC=2)CC1)c1ccccn1'\n",
      "[22:36:22] Explicit valence for atom # 9 O, 3, is greater than permitted\n",
      "[22:36:23] Can't kekulize mol.  Unkekulized atoms: 4 19 20 21 22 23 24\n",
      "[22:36:23] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c(NC(=O)CN(C)C(=O)CNN2C(=O)c4ccccc3C2=O)c(C)c1'\n",
      "[22:36:23] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Br)c(CN2CCCn3c2nc2c3c(=O)n(C)c(=O)n3C)c1'\n",
      "[22:36:23] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2NC3=NCN(CCCO)CN3c3nnc4ncnn32)cc1OC'\n",
      "[22:36:23] SMILES Parse Error: unclosed ring for input: 'Cc1nccn1CC12CCN(C(=O)[C@@H]2CC[C@@H]2S)CC1'\n",
      "[22:36:23] SMILES Parse Error: unclosed ring for input: 'Cc1ccn2ncc(C(=O)Nc3cnn4c(C(F)(F)F)cc(-c5ccccc4)c(=O)c3c2)c1'\n",
      "[22:36:23] Can't kekulize mol.  Unkekulized atoms: 4 5 20 21 22\n",
      "[22:36:23] Can't kekulize mol.  Unkekulized atoms: 10 11 12 14 19 20 21\n",
      "[22:36:23] SMILES Parse Error: ring closure 3 duplicates bond between atom 8 and atom 9 for input: 'CC(C)(C)c1ccc([C@@H]23[C@@H]3C(=O)N(c4ccc(Br)cc4)C(=O)[C@H]3[C@H]3C=Cc4ccccc4N32)cc1'\n",
      "[22:36:23] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN(CC(=O)N(C)Cc2ccco2)C[C@H](C)[S@]1CC1'\n",
      "[22:36:23] Can't kekulize mol.  Unkekulized atoms: 6 7 9 10 25 28 29\n",
      "[22:36:23] SMILES Parse Error: extra close parentheses while parsing: Cc1noc(C)c1CCCN1c2cccc3ccn(C)c23)CC1\n",
      "[22:36:23] SMILES Parse Error: Failed parsing SMILES 'Cc1noc(C)c1CCCN1c2cccc3ccn(C)c23)CC1' for input: 'Cc1noc(C)c1CCCN1c2cccc3ccn(C)c23)CC1'\n",
      "[22:36:23] Can't kekulize mol.  Unkekulized atoms: 2 3 15 16 21\n",
      "[22:36:24] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[22:36:24] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CN(C(=O)COC(=O)C1[C@H]2C[C@@H](C1)[C@H]1C(=O)NC1=O)C1CCCCC1'\n",
      "[22:36:24] Can't kekulize mol.  Unkekulized atoms: 3 4 6\n",
      "[22:36:24] Can't kekulize mol.  Unkekulized atoms: 10 11 16\n",
      "[22:36:24] SMILES Parse Error: extra close parentheses while parsing: CN1CC[C@@H](NC(=O)CN(c2ccc(F)c(Cl)c2)S(C)(=O)=O)[C@H](C)c2ccccc2)C1\n",
      "[22:36:24] SMILES Parse Error: Failed parsing SMILES 'CN1CC[C@@H](NC(=O)CN(c2ccc(F)c(Cl)c2)S(C)(=O)=O)[C@H](C)c2ccccc2)C1' for input: 'CN1CC[C@@H](NC(=O)CN(c2ccc(F)c(Cl)c2)S(C)(=O)=O)[C@H](C)c2ccccc2)C1'\n",
      "[22:36:24] Can't kekulize mol.  Unkekulized atoms: 3 4 5 12 13 20 21\n",
      "[22:36:24] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 19 20 22\n",
      "[22:36:24] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN2C[C@@H]3OCCN(C(=O)[C@H]3C[C@@H]3C)[C@H]2C(C)C)cc1F'\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'NC(=O)[C@H]1CN(C(=O)[C@@H]Cc2ccccc2C(F)(F)F)C2CC1'\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'CCn1cc(NC(=O)[C@@H]2CS[C@H]3c4ccc(OC)c(OC)c4C[C@H]24)c2ccccc21'\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1COC[C@@]2(C)C[C@H]1NC(=O)CNS(C)(=O)=O'\n",
      "[22:36:25] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 10 11 30 31\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'CN1CC[C@@H](N[C@H]2CCC[C@H]2CCN(C)C2=O)C1'\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'CCN1C[C@H]2C3(CCN(C(=O)c4cccnc4)CC3)CC[C@@]2(C(=O)N3CCCC2)C1'\n",
      "[22:36:25] SMILES Parse Error: syntax error while parsing: COc1ccc(N((C)=C/c2nc3ccc(C)cc3[nH]c2=O)c2ccccc12\n",
      "[22:36:25] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(N((C)=C/c2nc3ccc(C)cc3[nH]c2=O)c2ccccc12' for input: 'COc1ccc(N((C)=C/c2nc3ccc(C)cc3[nH]c2=O)c2ccccc12'\n",
      "[22:36:25] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 22 23 24\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'O=c1c(Cl)c(Cl)sn1Clc1=N\\C(=S)Nc1ccccc1'\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'O=c1c2cnc3ncnn2c2ccnn1CCCn1ccnc1'\n",
      "[22:36:25] SMILES Parse Error: unclosed ring for input: 'C[C@]12c3c3ccccc4[nH]c(=O)n2[C@@H]2C(=O)O1'\n",
      "[22:36:25] Can't kekulize mol.  Unkekulized atoms: 4 5 6 14 15\n",
      "[22:36:26] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 9 10 11 12\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1cc(C)c(C)c(COc2ccc(/C=C3/SC(Nc4ccc(Cl)cc4)=Nc3ccccc3)cc2)c1'\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1OC(C)(C)N2[C@@H]3CC=CC[C@H]4[C@H]21'\n",
      "[22:36:26] Can't kekulize mol.  Unkekulized atoms: 18 19 21 22 23 24 25\n",
      "[22:36:26] Can't kekulize mol.  Unkekulized atoms: 15 17 18\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'CNC(=O)/C(O)=C/C(=O)Nc1ccc(C(c2(C)C)c2ccccc2)cc1'\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc(N2CCCN2C2=O)c1)OCC(F)(F)F'\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'O=C(CCNC(=O)[C@@]1(N2CCC(CO)CCO)CCC1)NCc1cccc(F)c1'\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'CC[C@]1(O)CCC(=O)[C@@H]2C3CC1C4'\n",
      "[22:36:26] Can't kekulize mol.  Unkekulized atoms: 1 2 9 10 11\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'O=C(NC1CCCC1)c1cc(CN2CCC[C@@]2(COCc3ccccc3)C2)[nH]n1'\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Cc2nn3c([C@@H]4COc4ccccc5O4)nnc3s2)cc1'\n",
      "[22:36:26] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1cncc(N2C[C@@H]3COC[C@@H](C2)N(C)C4)c1'\n",
      "[22:36:27] SMILES Parse Error: unclosed ring for input: 'CN(C)C(=O)c1c(NC(=O)N2CC2(CCCC2)Cc2ccccc2)cccn1C(C)C'\n",
      "[22:36:27] Can't kekulize mol.  Unkekulized atoms: 5 6 11 12 13\n",
      "[22:36:27] Can't kekulize mol.  Unkekulized atoms: 2 14 15 16 17\n",
      "[22:36:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)N[C@@H]2[C@@H]3CC[C@@H]2CN(C3CC2)C3=O)cc1OC'\n",
      "[22:36:27] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@H]2C(C(=O)OC)=CN=c3s/c(=C/c4cc(Br)c(I)c(I)c3)c(=O)n32)cc1'\n",
      "[22:36:27] Explicit valence for atom # 8 C, 5, is greater than permitted\n",
      "[22:36:27] non-ring atom 17 marked aromatic\n",
      "[22:36:27] Can't kekulize mol.  Unkekulized atoms: 2 14 15 16 17 18 19\n",
      "[22:36:27] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@@H]3C[C@H]([C@@H]4[C@@H]5CC[C@H](O)[C@H](O)[C@H]34)[C@@H]2[C@H]2[C@@H]3CC[C@@H]([C@H]12)[C@H]1[C@@H]2CC[C@H](C2)[C@H]31'\n",
      "[22:36:27] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)cc(N(CC(=O)NC[C@@H](C)n2CC(C)C)S(C)(=O)=O)c1'\n",
      "[22:36:27] SMILES Parse Error: ring closure 3 duplicates bond between atom 13 and atom 18 for input: 'COCc1nc([C@@H]2COCCn2nc23ccccc23)ncc1C(=O)O'\n",
      "[22:36:27] Can't kekulize mol.  Unkekulized atoms: 18 19 20 21 29\n",
      "[22:36:27] SMILES Parse Error: unclosed ring for input: 'CO[C@H]1[C@H]2[C@@H]3[C@@H]4OC[C@@H](O)[C@H](OC)[C@H]4[C@H]3[C@H]3[C@@H](O)CC[C@]4(C)[C@H]2C(=O)[C@@]12C'\n",
      "[22:36:28] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[22:36:28] SMILES Parse Error: unclosed ring for input: 'COc1cc(CN2C[C@@H](CC(=O)O)[C@H](c3ccsc3)C2)cc(OC)c2OC'\n",
      "[22:36:28] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 25\n",
      "[22:36:28] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 24 25 26 27 29 31 32\n",
      "[22:36:28] SMILES Parse Error: unclosed ring for input: 'CC1(C)[C@H]2CC[C@]1(C)[C@H](n1c(=O)c3c(C)c(C)c(C)cc3c3C)C(=O)N12'\n",
      "[22:36:28] Can't kekulize mol.  Unkekulized atoms: 12 13 14 28 29 30 31\n",
      "[22:36:28] SMILES Parse Error: unclosed ring for input: 'O=C(N[C@@H]1CCOC2(CCS(=O)(=O)N3CCOCC3)C1)C1(c2ccc(Cl)cc2)CC1'\n",
      "[22:36:28] Can't kekulize mol.  Unkekulized atoms: 8 9 10 26 28\n",
      "[22:36:28] SMILES Parse Error: unclosed ring for input: 'O=C(NCCSc1cnn[nH]1)C1CCN(C(=O)[C@H]2Cc3ccccc3C=O)CC1'\n",
      "[22:36:28] SMILES Parse Error: extra close parentheses while parsing: CC(C)[C@H]1CC1(F)F)NC(=O)N1C[C@@H](O)C12CC3CC(CC(C3)C1)C2\n",
      "[22:36:28] SMILES Parse Error: Failed parsing SMILES 'CC(C)[C@H]1CC1(F)F)NC(=O)N1C[C@@H](O)C12CC3CC(CC(C3)C1)C2' for input: 'CC(C)[C@H]1CC1(F)F)NC(=O)N1C[C@@H](O)C12CC3CC(CC(C3)C1)C2'\n",
      "[22:36:28] SMILES Parse Error: unclosed ring for input: 'C[C@H](O)C12CC3CC(CC(C1)C1)C2'\n",
      "[22:36:28] SMILES Parse Error: unclosed ring for input: 'CC(C)CCC[C@@H]1[C@H]2C[C@H](O)[C@H]2CN(C(=O)c3ccc(=O)n(C)n3)C(=O)N12'\n",
      "[22:36:29] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 10 13 15\n",
      "[22:36:29] Can't kekulize mol.  Unkekulized atoms: 4 21 22 23 24 25 26 27 28 29 30\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'CCc1ccc(NC(=O)CSc2nnc(-c3cnc4ccccc3c3)n2-c2ccc(C)cc2)cc1'\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2Oc3c(c(Br)cc31)[C@H]2C(=O)N(c1ccc(Cl)cc1)C2=O'\n",
      "[22:36:29] Can't kekulize mol.  Unkekulized atoms: 15 16 17 19 20 21 22 24 25\n",
      "[22:36:29] SMILES Parse Error: syntax error while parsing: COc1ccc(S(=O)(=O)Nc2nc3ccc4cccc(4)c3n2)cc1\n",
      "[22:36:29] SMILES Parse Error: Failed parsing SMILES 'COc1ccc(S(=O)(=O)Nc2nc3ccc4cccc(4)c3n2)cc1' for input: 'COc1ccc(S(=O)(=O)Nc2nc3ccc4cccc(4)c3n2)cc1'\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'Cn1nc2c(c1[C@H](c1ccccc1)CCC1)N1CCC(C(=O)O)CC1'\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2nnc3sc(NC(=O)[C@@H]4CC(=O)N(c4ccc(Br)cc4)C3=O)sc2C)cc1'\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'Cn1cnc2cc(C(=O)N3CC4(C[C@H](COc4cccn5CC(C)C)C4)CS3)ccc21'\n",
      "[22:36:29] SMILES Parse Error: extra close parentheses while parsing: CCNC(=O)N1C[C@@H]2Cc3c(cnn3C)C2)[C@H]1Cc1ccccc1\n",
      "[22:36:29] SMILES Parse Error: Failed parsing SMILES 'CCNC(=O)N1C[C@@H]2Cc3c(cnn3C)C2)[C@H]1Cc1ccccc1' for input: 'CCNC(=O)N1C[C@@H]2Cc3c(cnn3C)C2)[C@H]1Cc1ccccc1'\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)[C@@H]1[C@H]2C[C@H]2NC(=O)[C@@H]1O2'\n",
      "[22:36:29] SMILES Parse Error: unclosed ring for input: 'Cc1nc(NC(=O)c2sc3nc4n(c(=O)c3c2C)CC[C@H](C)CC5)c(=O)[nH]c1=O'\n",
      "[22:36:29] Can't kekulize mol.  Unkekulized atoms: 2 3 5 17 18\n",
      "[22:36:29] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 16\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 22 23 25\n",
      "[22:36:30] SMILES Parse Error: unclosed ring for input: 'O=C(NC[C@@H]1CC(=O)N(CCc2ccccc2)C1)c1ccccc1OC2CCNCC1'\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18\n",
      "[22:36:30] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@@H]1SC(c2ccccc2)=C(C)[C@H]2C1=C1=CC=CC2'\n",
      "[22:36:30] SMILES Parse Error: unclosed ring for input: 'Cc1cc(N2CC[C@H](O)[C@]2(CO)CC2)ccn1'\n",
      "[22:36:30] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)c(NC(=O)[C@H]2CN2CCN2C)c1'\n",
      "[22:36:30] SMILES Parse Error: extra close parentheses while parsing: CCC[C@H](C(=O)NC[C@@H]1CCCN1C1CC1)c1ccccc1)c1ccccc1\n",
      "[22:36:30] SMILES Parse Error: Failed parsing SMILES 'CCC[C@H](C(=O)NC[C@@H]1CCCN1C1CC1)c1ccccc1)c1ccccc1' for input: 'CCC[C@H](C(=O)NC[C@@H]1CCCN1C1CC1)c1ccccc1)c1ccccc1'\n",
      "[22:36:30] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CN(C)[C@@H](C)C(=O)Nc1ccc3c(c1)OCO2'\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 2 3 4 16 17 29 30\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 19\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 19 30\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 4 5 6 22 23 24 25\n",
      "[22:36:30] SMILES Parse Error: unclosed ring for input: 'COc1ccc(/C=C2/C[C@@H]3[C@H]4CCc5ccccc5[C@H]42)c2cc1OC'\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 5 6 17 18 20 23 24\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 16\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 16 17 18\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 22\n",
      "[22:36:30] Can't kekulize mol.  Unkekulized atoms: 15 16 18 19 20 21 22\n",
      "[22:36:31] Can't kekulize mol.  Unkekulized atoms: 4 5 6 20 21\n",
      "[22:36:31] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 14 15 20 21 22\n",
      "[22:36:31] SMILES Parse Error: extra close parentheses while parsing: CC(=O)[C@H]1CC2CCC(C(=O)Nc3cc(C)ccc3OC)CC2)C1\n",
      "[22:36:31] SMILES Parse Error: Failed parsing SMILES 'CC(=O)[C@H]1CC2CCC(C(=O)Nc3cc(C)ccc3OC)CC2)C1' for input: 'CC(=O)[C@H]1CC2CCC(C(=O)Nc3cc(C)ccc3OC)CC2)C1'\n",
      "[22:36:31] Can't kekulize mol.  Unkekulized atoms: 2 16 17 18 19 20 21\n",
      "[22:36:31] SMILES Parse Error: unclosed ring for input: 'Cc1nc(-c2cccc(C(=O)N3C[C@H]4CC[C@@H]3CS3(=O)=O)c2)cs1'\n",
      "[22:36:31] SMILES Parse Error: unclosed ring for input: 'O=C(Cc1ccccc1)Nc1nc(-c2ccc3[nH]c(C(F)(F)F)cc2s2)cs1'\n",
      "[22:36:31] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 19 20 21\n",
      "[22:36:31] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@@H]3C[C@@H]([C@@H]4[C@@H]5CC[C@H](O5)[C@@H]34)[C@H]2[C@H]2[C@@H]3C=C[C@@H]4[C@@H]51[C@@H]1[C@H]5[C@H]43'\n",
      "[22:36:32] SMILES Parse Error: unclosed ring for input: 'Cc1cnc(C)c(N[C@H]2CCOC2(C2CC2)C2)n1'\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 12 13 15 16 17 18 19\n",
      "[22:36:32] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1OCc1nnn(C1CCN(C2CCCC2)CC2)n1'\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6\n",
      "[22:36:32] SMILES Parse Error: extra close parentheses while parsing: Cc1nnc(SCC(=O)Nc2ccc(C(F)(F)F)cc2)s1)c1ccccc1\n",
      "[22:36:32] SMILES Parse Error: Failed parsing SMILES 'Cc1nnc(SCC(=O)Nc2ccc(C(F)(F)F)cc2)s1)c1ccccc1' for input: 'Cc1nnc(SCC(=O)Nc2ccc(C(F)(F)F)cc2)s1)c1ccccc1'\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 2 3 5 19 20 21 23 24 25\n",
      "[22:36:32] SMILES Parse Error: unclosed ring for input: 'CCc1c(C)nc2c(-c3ccccc3)c3c(C)nnn2c1O'\n",
      "[22:36:32] SMILES Parse Error: extra close parentheses while parsing: CCCC[C@H](N)C(=O)N1CC[C@@H]2C[C@@H]3C2)[C@@H](OCC2CC2)C1\n",
      "[22:36:32] SMILES Parse Error: Failed parsing SMILES 'CCCC[C@H](N)C(=O)N1CC[C@@H]2C[C@@H]3C2)[C@@H](OCC2CC2)C1' for input: 'CCCC[C@H](N)C(=O)N1CC[C@@H]2C[C@@H]3C2)[C@@H](OCC2CC2)C1'\n",
      "[22:36:32] Explicit valence for atom # 11 C, 5, is greater than permitted\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 16\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 5 6 7\n",
      "[22:36:32] SMILES Parse Error: unclosed ring for input: 'O=C(NC1CC1)[C@H]1C[C@H]2c1cccc(Cl)c1F'\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 9 10 17 18 19\n",
      "[22:36:32] Can't kekulize mol.  Unkekulized atoms: 1 2 17\n",
      "[22:36:32] SMILES Parse Error: unclosed ring for input: 'Cc1nc([C@H]2CCCN2C)cc(N2C3CC3(CC2)C[C@]3(C)C(=O)O)n1'\n",
      "[22:36:32] SMILES Parse Error: extra close parentheses while parsing: CO[C@]1(COc2cccc(C)c2)CC)CN(Cc2ccccc2)C[C@@H]1C\n",
      "[22:36:32] SMILES Parse Error: Failed parsing SMILES 'CO[C@]1(COc2cccc(C)c2)CC)CN(Cc2ccccc2)C[C@@H]1C' for input: 'CO[C@]1(COc2cccc(C)c2)CC)CN(Cc2ccccc2)C[C@@H]1C'\n",
      "[22:36:33] Can't kekulize mol.  Unkekulized atoms: 7 8 9 30 31\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)-c1cc(C(=O)NS(=O)(=O)c3c(C)n(C)c(=O)n4C)ccc1n2C'\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'Cn1c(C(=O)N2CCC(c3nnc4n3CCCCC3)CC2)cc2cc(F)ccc21'\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'NS(=O)(=O)c1ccc2c(c1)C(=O)Nc1ccc(NC(=O)COc3ccccc3Cl)cc11'\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'O=C(O)c1ccc([C@@H]2Nc3c(Cl)ccc(Cl)c3[C@@H]3[C@H](O)C2(Cl)Cl)cc1'\n",
      "[22:36:33] Can't kekulize mol.  Unkekulized atoms: 10 11 12 30 31\n",
      "[22:36:33] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 25\n",
      "[22:36:33] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10\n",
      "[22:36:33] Can't kekulize mol.  Unkekulized atoms: 1 2 3 10 22\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'COC(=O)[C@@H]1[C@@H]2[C@@H](NC(=O)c3ccccc3)[C@H]4CC[C@@H]2O[C@H]12'\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'Cc1cncc2c1Br(CCN(Cc1ccccn1)CCN(C3CCCC1)C2)OC2'\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)[C@@H]2Nc3ccccc4O[C@H]22)cc1OC'\n",
      "[22:36:33] Can't kekulize mol.  Unkekulized atoms: 11 18 19 20 21 22 23\n",
      "[22:36:33] SMILES Parse Error: unclosed ring for input: 'NC1=NS(=O)(=O)c2ccccc212'\n",
      "[22:36:34] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 25\n",
      "[22:36:34] SMILES Parse Error: unclosed ring for input: 'COCCN1C(=O)[C@H]2CC[C@H]1CN(CC1=Nc3cc(O)ccc3O[C@@H]3C1)C2'\n",
      "[22:36:34] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[22:36:34] SMILES Parse Error: extra close parentheses while parsing: Cc1ncn(-c2ccc(NC(=O)[C@@H]3CCCN(S(C)(=O)=O)c3)cc2)c1)[C@@H]1CCS(=O)(=O)C1\n",
      "[22:36:34] SMILES Parse Error: Failed parsing SMILES 'Cc1ncn(-c2ccc(NC(=O)[C@@H]3CCCN(S(C)(=O)=O)c3)cc2)c1)[C@@H]1CCS(=O)(=O)C1' for input: 'Cc1ncn(-c2ccc(NC(=O)[C@@H]3CCCN(S(C)(=O)=O)c3)cc2)c1)[C@@H]1CCS(=O)(=O)C1'\n",
      "[22:36:34] SMILES Parse Error: unclosed ring for input: 'CCCn1c(=O)c2c(nc3n2[C@@H](CO)[C@@H](c2ccc(OC)c(OC)c2)N2)C(=O)N[C@H]1c1ccc(OC)cc1'\n",
      "[22:36:34] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[22:36:34] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)CCC(C2(CCNCC3CC3(C3)C3CC3)CC2)CC1'\n",
      "[22:36:34] SMILES Parse Error: unclosed ring for input: 'CC(=O)O[C@H]1CC[C@@]2(C)[C@@H](CC[C@@H]3[C@@H]2CC(=O)[C@]2(C)[C@@H]4[C@H](OC(C)=O)[C@@]2(O)C(=O)O[C@@H]4[C@H]4C[C@@H](OC(C)=O)[C@H]5[C@H]42)[C@@H]21'\n",
      "[22:36:34] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 23\n",
      "[22:36:34] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1ccccc1)C1=C(c2ccccc2)Sc2nc2ccccc2N1'\n",
      "[22:36:34] SMILES Parse Error: extra close parentheses while parsing: O[C@@]1(C2CCC3)CC2)CC=CC1\n",
      "[22:36:34] SMILES Parse Error: Failed parsing SMILES 'O[C@@]1(C2CCC3)CC2)CC=CC1' for input: 'O[C@@]1(C2CCC3)CC2)CC=CC1'\n",
      "[22:36:34] SMILES Parse Error: unclosed ring for input: 'COc1ccc(Cl)c(CN[C@@H]2CCc3nc[nH]c3C[C@H]23)c1'\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 13 14 26 27\n",
      "[22:36:35] Explicit valence for atom # 1 O, 3, is greater than permitted\n",
      "[22:36:35] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CNC(=O)[C@@H](C)O[C@H](c2ncc(C(C)(C)C)oc2C)n2)cc1'\n",
      "[22:36:35] SMILES Parse Error: unclosed ring for input: 'O=C(OC[C@@H]1C[C@]12CCOCC1)c1cc(Cl)c2c(c1)OCCO2'\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 3 4 5 10 11 24 25\n",
      "[22:36:35] SMILES Parse Error: unclosed ring for input: 'COCCN1C2C[C@H]2C[C@@H]1CCN2Cc1cc(C)c2c(c1)OCCO2'\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 1 2 3 23 25\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 7 8 10 17 18 19 20\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 5 7 8 9 17 18 22\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 25 26\n",
      "[22:36:35] SMILES Parse Error: unclosed ring for input: 'CC1(C)CN(C(=O)[C@@H]2C[C@]3(CO2CCN(Cc4ccsc4)C3)CCO2)C[C@H](C)O1'\n",
      "[22:36:35] SMILES Parse Error: unclosed ring for input: 'O=C(CC1=C[C@H]2C[C@@H](CN1CC1)C2)Nc1ccccc1'\n",
      "[22:36:35] Can't kekulize mol.  Unkekulized atoms: 13 16 17\n",
      "[22:36:35] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1CN1C[C@@H]2C[C@@H](c3ccsc3)OC2=N2)c1\n",
      "[22:36:35] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1CN1C[C@@H]2C[C@@H](c3ccsc3)OC2=N2)c1' for input: 'COc1ccccc1CN1C[C@@H]2C[C@@H](c3ccsc3)OC2=N2)c1'\n",
      "[22:36:35] SMILES Parse Error: extra close parentheses while parsing: OCc1ccc(F)cc1CN1CCN([C@@H]2C[C@H]3C=O)CC2)CC1\n",
      "[22:36:35] SMILES Parse Error: Failed parsing SMILES 'OCc1ccc(F)cc1CN1CCN([C@@H]2C[C@H]3C=O)CC2)CC1' for input: 'OCc1ccc(F)cc1CN1CCN([C@@H]2C[C@H]3C=O)CC2)CC1'\n",
      "[22:36:35] SMILES Parse Error: unclosed ring for input: 'CC12CC3CC(CC(C1)C1)C2'\n",
      "[22:36:36] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[22:36:36] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 28 29\n",
      "[22:36:36] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@H]2N=C3NCN(S(=O)(=O)c4ccccc4)C(=O)[C@@H]3[C@H]32)c1'\n",
      "[22:36:36] Can't kekulize mol.  Unkekulized atoms: 2 3 4 24 27\n",
      "[22:36:36] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 18 29 30 31\n",
      "[22:36:36] SMILES Parse Error: unclosed ring for input: 'Cc1cnc(NC(=O)c2cc(C3CC3)on2)c(C(=O)NC2CC3)c1'\n",
      "[22:36:36] SMILES Parse Error: extra close parentheses while parsing: CN(CCC(=O)NC1CC1)c1ncnc2ccccc12)C(C)C\n",
      "[22:36:36] SMILES Parse Error: Failed parsing SMILES 'CN(CCC(=O)NC1CC1)c1ncnc2ccccc12)C(C)C' for input: 'CN(CCC(=O)NC1CC1)c1ncnc2ccccc12)C(C)C'\n",
      "[22:36:36] SMILES Parse Error: unclosed ring for input: 'CCCS(=O)(=O)N1CC[C@]2(C(=O)Nc3ccccc33)CN(c2cc(C)ccn2)C1'\n",
      "[22:36:36] Explicit valence for atom # 15 O, 3, is greater than permitted\n",
      "[22:36:36] SMILES Parse Error: unclosed ring for input: 'CC(=O)c1ccccc1NC(=O)CCN1CCCN(C)CC'\n",
      "[22:36:36] SMILES Parse Error: extra close parentheses while parsing: C[C@H](Sc1nc2sc3c(c2c(=O)n1C[C@@H]1O)CCC3)C(=O)NCC2)c1ccccc1\n",
      "[22:36:36] SMILES Parse Error: Failed parsing SMILES 'C[C@H](Sc1nc2sc3c(c2c(=O)n1C[C@@H]1O)CCC3)C(=O)NCC2)c1ccccc1' for input: 'C[C@H](Sc1nc2sc3c(c2c(=O)n1C[C@@H]1O)CCC3)C(=O)NCC2)c1ccccc1'\n",
      "[22:36:37] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1[C@H](C)N[C@H](C(=O)NC1CC1)c1ccccc1)c1ccccc1\n",
      "[22:36:37] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1[C@H](C)N[C@H](C(=O)NC1CC1)c1ccccc1)c1ccccc1' for input: 'COc1ccccc1[C@H](C)N[C@H](C(=O)NC1CC1)c1ccccc1)c1ccccc1'\n",
      "[22:36:37] SMILES Parse Error: extra open parentheses for input: 'CN(c1nc(NCC(=O)N2CCC[C@H]2c2nc3ccccc3n2C)cc1Cl'\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'Cc1nc(C)c(-c2cnc3n2[C@@H]2CN(Cc2ccc(F)cc2)CC3)c1C'\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'COCCN1C[C@@]23C=[C@H][C@H](O2)[C@@H](C(=O)NCc2cccnc2)[C@H](c2ccc(F)cc2)[C@@H]1C'\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'COc1cc2nc3c(cc2cc1NC(=O)c1cccnc1O)CC[C@](C)(O)[C@@H]2C'\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cccc([C@H]2C[C@H]3CC[C@H]2C[C@H]3C2(F)F)n1)c1ccco1'\n",
      "[22:36:37] SMILES Parse Error: ring closure 2 duplicates bond between atom 11 and atom 14 for input: 'CC(C)(C)OC(=O)N1CCC(C2(CN)[C@@H]2c2ccccc2Cl)CC1'\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 12 17 18\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 17\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 1 2 3 10 18\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 26 27\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'Cc1nc([C@H]2CCCN2C)cc(N2C3CC3(CC2)CO3)c1'\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1CN(CC(=O)N2N=c3cc[nH]cc3CC2)[C@@H]2CCCO1'\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 3 5 6\n",
      "[22:36:37] SMILES Parse Error: unclosed ring for input: 'Cn1c(N2C[C@H]3CN(C4=O)CC3)C(C2CC2)CO1'\n",
      "[22:36:37] Can't kekulize mol.  Unkekulized atoms: 12 13 14 16 26\n",
      "[22:36:38] SMILES Parse Error: unclosed ring for input: 'O=c1c(Cn2ccnn2)noc1NC(=O)N[C@H]1C2CC3CC1CC2'\n",
      "[22:36:38] SMILES Parse Error: unclosed ring for input: 'O=C1N[C@@H]2[C@@H]3COC[C@H]4N1C(=O)N(Cc1ccccc1)C3'\n",
      "[22:36:38] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16\n",
      "[22:36:38] SMILES Parse Error: extra close parentheses while parsing: CCCCn1nnnc1COC(=O)[C@@H]2CC(=O)N(c3ccc(F)cc3)C2)c1\n",
      "[22:36:38] SMILES Parse Error: Failed parsing SMILES 'CCCCn1nnnc1COC(=O)[C@@H]2CC(=O)N(c3ccc(F)cc3)C2)c1' for input: 'CCCCn1nnnc1COC(=O)[C@@H]2CC(=O)N(c3ccc(F)cc3)C2)c1'\n",
      "[22:36:38] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@@H]2C3=C(N=c4Nc4cc(C)ccc4O)nnn3C2=O)c1'\n",
      "[22:36:38] SMILES Parse Error: unclosed ring for input: 'CC(C)(Oc1cccc(Cl)c1)c1nc(-c2ccc(Cn3cccn2)cc2)no1'\n",
      "[22:36:38] SMILES Parse Error: unclosed ring for input: 'O=C(CCn1c(N2CCN(c3ncccn3)CC2)nc2c1c(=O)n(C)c(=O)n2C)n2Cc1ccccc1'\n",
      "[22:36:39] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(OCCNC(=O)Cc2cccs1)c1ccccn1'\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 11 12 15\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 1 2 3 33 35\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 2 24 26 27 28 29 30 31 32\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 19\n",
      "[22:36:39] SMILES Parse Error: unclosed ring for input: 'Cc1nonc1OCCNC(=O)Nc1cc2n3c(n1)CCN(C)C3'\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 3 4 24 25 26 27 28 29 30\n",
      "[22:36:39] SMILES Parse Error: unclosed ring for input: 'CC1(C)C[C@@H](O)[C@]2(C)CC[C@]3(C)C(=CC[C@@H]4[C@@]5(C)C(=O)C[C@H]([C@H](C)C(C)C)[C@@]5(C)CC[C@@H]42)C2'\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 7 8 10\n",
      "[22:36:39] SMILES Parse Error: extra close parentheses while parsing: O=C(NCC1(O)C(F)(F)F)c(=O)[nH]c1=O)c1ccc(F)c(Cl)c1\n",
      "[22:36:39] SMILES Parse Error: Failed parsing SMILES 'O=C(NCC1(O)C(F)(F)F)c(=O)[nH]c1=O)c1ccc(F)c(Cl)c1' for input: 'O=C(NCC1(O)C(F)(F)F)c(=O)[nH]c1=O)c1ccc(F)c(Cl)c1'\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 24\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 9 10 11\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 17 18 19\n",
      "[22:36:39] SMILES Parse Error: unclosed ring for input: 'CCCc1cc(NC(=O)N2CCC[C@H]3[C@@H]2CCCN32)ccc1F'\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 1 2 3 14 15 16 17\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 4 5 22\n",
      "[22:36:39] SMILES Parse Error: syntax error while parsing: COc1c[nH]c(C==)c1CC(=O)Nc1ccc(CC(=O)N(C)C)cc1\n",
      "[22:36:39] SMILES Parse Error: Failed parsing SMILES 'COc1c[nH]c(C==)c1CC(=O)Nc1ccc(CC(=O)N(C)C)cc1' for input: 'COc1c[nH]c(C==)c1CC(=O)Nc1ccc(CC(=O)N(C)C)cc1'\n",
      "[22:36:39] Can't kekulize mol.  Unkekulized atoms: 4 5 24\n",
      "[22:36:40] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[22:36:40] SMILES Parse Error: ring closure 2 duplicates bond between atom 10 and atom 11 for input: 'O[C@@H]1CN(c2cc[nH]n2)C[C@H]2c2cnccc21'\n",
      "[22:36:40] Explicit valence for atom # 18 O, 3, is greater than permitted\n",
      "[22:36:40] Can't kekulize mol.  Unkekulized atoms: 1 2 3 28 30\n",
      "[22:36:40] SMILES Parse Error: extra open parentheses for input: 'c1ccc(-n2cc(C(C(F)F)n3cc(C(F)(F)F)nc3C2CC2)nn1'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)[C@@H](O)CCc1cccnc1'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'COc1ccc(S(=O)(=O)N2CC(=O)N3CCCc4cc(OC)ccc42)cc1'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)C(=O)C[C@@H]2[C@H]3CCC[C@H]3CC(=O)CC[C@]12C'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'COCc1cc(C)nc2sc(-c3nc(Cc4cc(OC)c5OC)c(OC)c(OC)c4)nn3c2=O'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'COc1ccc([C@@H]2NC3=NCN(CCCCO)CN3c3nnc4n2C)cc1'\n",
      "[22:36:40] Can't kekulize mol.  Unkekulized atoms: 1 2 3 18 19 20 22 23 24\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'Cc1nnc([C@@H]2CN(C(=O)[C@H]2CC(=O)N(C)C2)Cc2cccnc2NC(=O)CC(C)C)o1'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'COc1ccc(OC)c(C(=O)NC[C@@H]2CC3c4ccccc4C3c4ccccc4n32)c1'\n",
      "[22:36:40] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 18\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'O=c1c(CNc2ccc3nc(-c4ccco4)nc3n2CCCC3)noc1-c1ccc2c(c1)OCO2'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C(=O)N2CCc3ccccc3C2)c2c([C@@H]1CCCN(C(C)=O)C1)CC2'\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'N=C1NN(c2ccccc2)C2(c3ccncc3C(=O)O)C1CCN1'\n",
      "[22:36:40] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23 24 25 26 27\n",
      "[22:36:40] SMILES Parse Error: unclosed ring for input: 'O=c1c2c3n(c(Cc4ccccc4)c3n1CC1)CCCC3'\n",
      "[22:36:40] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[22:36:40] SMILES Parse Error: ring closure 1 duplicates bond between atom 14 and atom 19 for input: 'O=C(N1CCC(O)(c2nn[nH]n2)CC1)C1(CC2CC2)C1'\n",
      "[22:36:41] Explicit valence for atom # 3 Cl, 2, is greater than permitted\n",
      "[22:36:41] Can't kekulize mol.  Unkekulized atoms: 3 27 28 29 30 31 32\n",
      "[22:36:41] SMILES Parse Error: extra close parentheses while parsing: CCC[C@@]12C=C[C@]3(CCN(C)C3=O)C1)CC(=O)N2\n",
      "[22:36:41] SMILES Parse Error: Failed parsing SMILES 'CCC[C@@]12C=C[C@]3(CCN(C)C3=O)C1)CC(=O)N2' for input: 'CCC[C@@]12C=C[C@]3(CCN(C)C3=O)C1)CC(=O)N2'\n",
      "[22:36:41] SMILES Parse Error: unclosed ring for input: 'CC(=O)[C@@H]1CC[C@@]2(C)[C@@H](CC[C@H]3[C@H]2CC[C@@]2(C)[C@H]3CC(=O)C[C@@H]22)C1'\n",
      "[22:36:41] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 22\n",
      "[22:36:41] Can't kekulize mol.  Unkekulized atoms: 2 3 17 18 19 20 21 22 23\n",
      "[22:36:41] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20\n",
      "[22:36:41] SMILES Parse Error: unclosed ring for input: 'COc1cccc([C@@H]2SCC(O)=Nc3n2c(=O)n(Cc2ccco2)c(=O)n2C)c1'\n",
      "[22:36:41] Can't kekulize mol.  Unkekulized atoms: 3 5 6\n",
      "[22:36:41] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2c(C)on3c(=O)c4c(cc2O)CCN4C(C)=O)cc2c1OC'\n",
      "[22:36:41] SMILES Parse Error: unclosed ring for input: 'Cn1ncc2c1CN(S(C)(=O)=O)1[C@@H](C1CC1)C2'\n",
      "[22:36:41] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(S(=O)(=O)N2CCn3cccc(C(=O)N[C@@H]3CCS(=O)(=O)C3)c2C)cc1'\n",
      "[22:36:42] Can't kekulize mol.  Unkekulized atoms: 4 5 13\n",
      "[22:36:42] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 20 21 22\n",
      "[22:36:42] Explicit valence for atom # 6 Cl, 2, is greater than permitted\n",
      "[22:36:42] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 15\n",
      "[22:36:42] Can't kekulize mol.  Unkekulized atoms: 9 10 12 17 18 24 25\n",
      "[22:36:42] Can't kekulize mol.  Unkekulized atoms: 16 18 19\n",
      "[22:36:42] SMILES Parse Error: unclosed ring for input: 'Cc1nn(-c2ccccc2)c2c1[C@@H](c1cccnc1)N1N=N[C@@H](c3ccc(Cl)cc3)C1'\n",
      "[22:36:42] Can't kekulize mol.  Unkekulized atoms: 3 4 5\n",
      "[22:36:42] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2C[C@H]3CCO[C@]3(COCC3)C2)c(F)c1'\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'CC(C)C(=O)N1CC[C@H](NC(=O)c2cccc(N3NC(=O)[C@@H]4CC=CC[C@H]3C3=O)c2)C1'\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'COc1cc(-c2nnc3n2-c4ccccc4C2)cc(OC)c1OC'\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'CC(C)(C)OC(=O)N1CC[C@@]2(CCn3ncnc3[C@H]2CCNC2=O)C1'\n",
      "[22:36:43] SMILES Parse Error: extra close parentheses while parsing: O=C(C1CCC1)N1CC[C@H]2OCCN(C(=O)C3CC3)C2)C1\n",
      "[22:36:43] SMILES Parse Error: Failed parsing SMILES 'O=C(C1CCC1)N1CC[C@H]2OCCN(C(=O)C3CC3)C2)C1' for input: 'O=C(C1CCC1)N1CC[C@H]2OCCN(C(=O)C3CC3)C2)C1'\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 6 7 9 25 26\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'Cc1cs/c(=N\\C(=O)c3ccccc3)c(=O)oc2c1C'\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2sc3nc4c(ccc4c2O)CCCC4)cc1'\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 2\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'CCN(CC)Cc1cnc2c(C(N)=O)cccc2c1N2CCO[C@H](C)C1'\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)NC(=S)Nc2sc3c(c2C(N)=O)C[C@H](C)CCC4)c(OC)c1'\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 2\n",
      "[22:36:43] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(-c2noc(CN3CCC(n(c4ccncc4)CC3)CC2)cn1'\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'O=C1[C@@H]2[C@@H]3[C@H]4[C@@H]2[C@H]2[C@H]1[C@@H]2[C@@H]3C(=O)[C@@H]3[C@H]2[C@@H]2[C@@H]5[C@@H]431'\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 18 19\n",
      "[22:36:43] SMILES Parse Error: unclosed ring for input: 'Cc1nc([C@H](C)n2cnc3sc4c(c3c2=O)CC[C@H](NCc2c(F)cccc2F)C3)no1'\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 18 19 20\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 22 26 27\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 2 4 5 7 26\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 3 4 13 14 16\n",
      "[22:36:43] Can't kekulize mol.  Unkekulized atoms: 12 13 16 17 20\n",
      "[22:36:44] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CCCS(=O)(=O)NC1CCCCC1)Nc1cccnc1'\n",
      "[22:36:44] SMILES Parse Error: extra close parentheses while parsing: Cc1ncsc1CN1C[C@@H]2[C@@H](C1)c1c(-c3ccc(OCc4ccccc4)cc3)c[nH]c2cc1O)CCC2\n",
      "[22:36:44] SMILES Parse Error: Failed parsing SMILES 'Cc1ncsc1CN1C[C@@H]2[C@@H](C1)c1c(-c3ccc(OCc4ccccc4)cc3)c[nH]c2cc1O)CCC2' for input: 'Cc1ncsc1CN1C[C@@H]2[C@@H](C1)c1c(-c3ccc(OCc4ccccc4)cc3)c[nH]c2cc1O)CCC2'\n",
      "[22:36:44] SMILES Parse Error: extra open parentheses for input: 'CCOC(=O)CNC(=O)CN(C(=O)[C@@H](C)Sc1nccs1'\n",
      "[22:36:44] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 8\n",
      "[22:36:44] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c(C(c2ccc(F)cc2)c2nnnn2C2CCCC2)N(C)C'\n",
      "[22:36:44] SMILES Parse Error: unclosed ring for input: 'CC(C)Oc1ccc(C(=O)N2CCN=c2cccc(Cl)c2)cc1'\n",
      "[22:36:44] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 12 13\n",
      "[22:36:44] Can't kekulize mol.  Unkekulized atoms: 5 6 22\n",
      "[22:36:44] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23 24 25 26 27\n",
      "[22:36:44] Explicit valence for atom # 4 C, 5, is greater than permitted\n",
      "[22:36:44] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 17 18 19 20\n",
      "[22:36:44] SMILES Parse Error: unclosed ring for input: 'CCn1nc(C)c2c1c(=O)n(Cc1ccc2c(c1)OCO3)n(C)c1=O'\n",
      "[22:36:44] Can't kekulize mol.  Unkekulized atoms: 11 12 13 14 15 16 17\n",
      "[22:36:44] SMILES Parse Error: unclosed ring for input: 'CCOC(=O)C[C@@H](NC(=O)Cn1nnc(-c2ccc(Cl)cc2)n2)c1ccc(OC)cc1'\n",
      "[22:36:45] Can't kekulize mol.  Unkekulized atoms: 1 2 3 17 18\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'O=C([C@@H]1CCCN(c2nccn3nc(Cc4ccccc3F)cn23)C1)N1CCCCCC1'\n",
      "[22:36:45] SMILES Parse Error: syntax error while parsing: Cc1nn(-c2ccccc2)c([C@H]2CCCN2C(=O)c2ccnc(2)c2)c1C\n",
      "[22:36:45] SMILES Parse Error: Failed parsing SMILES 'Cc1nn(-c2ccccc2)c([C@H]2CCCN2C(=O)c2ccnc(2)c2)c1C' for input: 'Cc1nn(-c2ccccc2)c([C@H]2CCCN2C(=O)c2ccnc(2)c2)c1C'\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'C[C@]12CC3CC(C(=O)Nc4sccc4C(N)=O)CC[C@@]1(C)C[C@]12C'\n",
      "[22:36:45] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 25\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'NC(=O)N1CCC(NC(=O)NC[C@@H]2Cc3cc(F)cc(O)c2O2)CC1'\n",
      "[22:36:45] Can't kekulize mol.  Unkekulized atoms: 4 5 6\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'Cc1nc(-c2ncc[nH]2)cc([C@@H]2CCCN2c2snc3c1CCCC3)n1'\n",
      "[22:36:45] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[22:36:45] Can't kekulize mol.  Unkekulized atoms: 11 13 14\n",
      "[22:36:45] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1[C@@H]1[C@H]2NC(=O)N(CC(=O)Nc3ccccc3)C(=O)[C@@H]2O2)c(OC)c1C\n",
      "[22:36:45] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1[C@@H]1[C@H]2NC(=O)N(CC(=O)Nc3ccccc3)C(=O)[C@@H]2O2)c(OC)c1C' for input: 'COc1ccccc1[C@@H]1[C@H]2NC(=O)N(CC(=O)Nc3ccccc3)C(=O)[C@@H]2O2)c(OC)c1C'\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'Cc1occc1C(=O)N(C)Cc1nnc2n1CCC[C@H]1CSC2'\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CCC[C@](I)(C(=O)O)CC(=O)[C@H]12'\n",
      "[22:36:45] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 20 21 22\n",
      "[22:36:45] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(CNc2cc(C)cc3oc(=O)nc22)c1'\n",
      "[22:36:46] SMILES Parse Error: unclosed ring for input: 'O=C(CSc1nc(CC(=O)N2CCc3ccccc32)c2ccccc2[nH])Nc1ccccc1Cl'\n",
      "[22:36:46] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1[C@@H](NC(=O)C2CCOCC2)CCN1C(=O)c1cc(=O)c2c(F)cccc2F'\n",
      "[22:36:46] Can't kekulize mol.  Unkekulized atoms: 1 2 4\n",
      "[22:36:46] SMILES Parse Error: extra close parentheses while parsing: O=C(c1ccc2ccccc2n1)N1CCC2(CC1)OCCC[C@H]2O)c1ccc(Cl)cc1\n",
      "[22:36:46] SMILES Parse Error: Failed parsing SMILES 'O=C(c1ccc2ccccc2n1)N1CCC2(CC1)OCCC[C@H]2O)c1ccc(Cl)cc1' for input: 'O=C(c1ccc2ccccc2n1)N1CCC2(CC1)OCCC[C@H]2O)c1ccc(Cl)cc1'\n",
      "[22:36:46] SMILES Parse Error: extra open parentheses for input: 'COc1cccc(-c2noc([C@@H]3CCCN(Cc3cccc4c3OCN4C3)C2)n1'\n",
      "[22:36:46] SMILES Parse Error: unclosed ring for input: 'Fc1ccccc1-c1nn2c(CSc3ncccn3)nc3ccccc23'\n",
      "[22:36:47] Can't kekulize mol.  Unkekulized atoms: 8 9 10 26 29\n",
      "[22:36:47] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "[22:36:47] SMILES Parse Error: unclosed ring for input: 'Cc1sc(NC(=O)CN2C[C@H]3C[C@H]2[C@@H]2C(=O)NC[C@@H]2C(F)(F)F)nc1C1CC1'\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import glob\n",
    "sys.path.append('../src/model/')\n",
    "\n",
    "for dataset in dataset_list:\n",
    "\tmodel_dir = f'{PRETRAIN_RESULTS}/{dataset}_results/vanilalstm'\n",
    "\tsample_num_list = [10000, 41743] # 41743 : numer of SMILES before randomization\n",
    "\n",
    "\tfor sample_num in sample_num_list:\n",
    "\t\toutfd   = f'{PRETRAIN_RESULTS}/{dataset}_results/sampling_{sample_num}'\n",
    "\t\tappname = 'generative_models.apps.Sampling.SamplingApp'\n",
    "\t\tvocab   = f'{model_dir}/model/vocabulary.pickle'\n",
    "\t\tm_str \t= glob.glob(f'{model_dir}/model/best_model_structure_epoch*.pickle')[0]\n",
    "\t\tm_state = glob.glob(f'{model_dir}/model/best_model_epoch*.pth')[0]\n",
    "\t\tn \t\t= sample_num\n",
    "\t\trseed \t= 42\n",
    "\t\tparams = [\t'--model=lstm',\n",
    "\t\t\t\t\tf'--model-structure={m_str}',\n",
    "\t\t\t\t\tf'--model-state={m_state}',\n",
    "\t\t\t\t\tf'--vocab={vocab}',\n",
    "\t\t\t\t\tf'--n={n}',\n",
    "\t\t\t\t\t'--allow-override',\n",
    "\t\t\t\t\tf'--outdir={outfd}',\n",
    "\t\t\t\t\tf'--random-seed={rseed}',\n",
    "\t\t\t\t\tf'--write-excel-file'\n",
    "\t\t]\n",
    "\n",
    "\t\trun(appname, *params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finetune the LSTM model\n",
    "Finetuning the trained model using a small number of compounds is a routine by loading the trained model and training them using `finetune.py` code. The cli tool is `apps.FineTune.py`. \n",
    "\n",
    "To switch between **filtered** and **unfiltered** fine-tuning data, modify the variable `FINETUNE_FILTER` in `src/paths.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning data status: filtered\n"
     ]
    }
   ],
   "source": [
    "from src.paths import FINETUNE_FILTER\n",
    "print(f'Fine-tuning data status: {FINETUNE_FILTER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logging\n",
      "Starting: Finetuner, Namespace(num_workers=1, outdir='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/finetune/filtered/pubchem_filtered_ac_results/CHEMBL4005_finetune', case='', override_folder=1, tensorboard_prefix='tensor_board', data='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/data/finetune/filtered/filtered_CHEMBL4005_train_rdsmi3.tsv', smi_colname='rd3_smiles', vocab='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/pretrain/pubchem_filtered_ac_results/vanilalstm/model/vocabulary.pickle', random_seed=42, debug=None, sampling_epoch=10000, model='lstm', write_xlsx=None, save_snapshot_models='', batch_size=16, epochs=100, validation_ratio=0.1, lr=0.0001, exclude_pad_loss=1, early_stopping_patience=0, load_model=None, model_structure='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/pretrain/pubchem_filtered_ac_results/vanilalstm/model/best_model_structure_epoch28.pickle', model_state='/home/abe/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/results/pretrain/pubchem_filtered_ac_results/vanilalstm/model/best_model_epoch28.pth', standardize_smiles=None, save_epoch_models=None, use_cpus=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CCOc1ccc(NC(=O)C2=C(CCNC(=O)C2CC2)N(C)C)cc1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 9 23\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc(C#N)c(N3CCC3(CC3)OCCO4)c2C)cc1OC'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'N#Cc1cc2c(nc1SCc1cnn3n1Cc1ccccc1)CCCC2'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CN(C)C(=O)N1CCN(Cc2nc3sc(-c4ccccc4)c(=O)[nH]2)CC1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 17\n",
      "[01:02:55] SMILES Parse Error: ring closure 3 duplicates bond between atom 14 and atom 15 for input: 'Clc1ccc(C23CCN(Cc3nnnn3C3CCCCC3)CC2)cc1'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'C[C@H]1CN([C@@H](C)CO)C(=O)Cc2cc(NC(=O)Nc3ccccc3)ccc2O[C@H](C)CN1CCCC1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 20 21 23 24\n",
      "[01:02:55] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 13\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CCN1C[C@@H]2C(c3ccccc3)C3(c4ccccc4)C(=O)N1[C@H]2c3ccccc3OC[C@@H]1[C@@H]2c1ccccc1'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1nnc(/C=C/s1ccc(C)cc1)c1ccc(F)cc1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 3 14 16\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CC(C)C(=O)Nc1cccc(C(=O)N2CCCCC22)c1'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(OCC(=O)N2C3CCCC2CC2C)c1'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'OC1CCN(c2c3c(nc4cc(-c5cccc(F)c4)nn24)CCCC3)CC1'\n",
      "[01:02:55] SMILES Parse Error: ring closure 4 duplicates bond between atom 15 and atom 16 for input: 'Cc1ccc(-n2c(SCC(=O)Nc3ccc4c4c(cccc35)CC4)nc(O)cc2=O)cc1'\n",
      "[01:02:55] SMILES Parse Error: ring closure 1 duplicates bond between atom 2 and atom 3 for input: 'O=C(C1C1CCCCN1C(c1cccs1)N1CCOCC1)N1CCN(c2cccc(C(F)(F)F)c2)CC1'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'c1ccc(CSc2nccn2Cc3ccccc3)cc2'\n",
      "[01:02:55] non-ring atom 21 marked aromatic\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'O=S1(=O)N(CCN2CC=C(c3c[nH]c4cc(F)ccc34)CC2)c2cccc3c2S1(=O)=O'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CC1(C)CC2(CC(c3ccccc3)O3)Oc3ccccc3C2(C)C'\n",
      "[01:02:55] SMILES Parse Error: extra close parentheses while parsing: O=C1c2nn2cc(-c3ccccc3)nc2s1)N1CCOCC1\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'O=C1c2nn2cc(-c3ccccc3)nc2s1)N1CCOCC1' for input: 'O=C1c2nn2cc(-c3ccccc3)nc2s1)N1CCOCC1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 18 19 25\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1nc2oc3c(=O)n(-c4cccc(Cl)c4)c(N4CCN(C(=O)c5ccco5)CC4)n3c2c2c1C'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1cc2cc3c(cc2n1C(c1cccc(Cl)c1)N(C)CC3)OCO3'\n",
      "[01:02:55] SMILES Parse Error: syntax error while parsing: Cc1ccc(\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(' for input: 'Cc1ccc('\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 3 4 5 7 8 9 10 11 12\n",
      "[01:02:55] SMILES Parse Error: extra close parentheses while parsing: Cc1ccc2ncc(S(=O)(=O)c3ccccc3)c(N3CCC[C@H]4C(=O)Nc4ccc(F)cc4)C23)c1\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc2ncc(S(=O)(=O)c3ccccc3)c(N3CCC[C@H]4C(=O)Nc4ccc(F)cc4)C23)c1' for input: 'Cc1ccc2ncc(S(=O)(=O)c3ccccc3)c(N3CCC[C@H]4C(=O)Nc4ccc(F)cc4)C23)c1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 16 17 19 20 21 22 23 24 25\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 21 22 23 24 25 26 29\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1SCc1csc3ncnc12'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11 17 18 19 20\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)c2c(c1)NC(=O)C2(C)Cc1ccccc1O2'\n",
      "[01:02:55] SMILES Parse Error: syntax error while parsing: O=c1nc2cccc3c2=c(//)C(=O)CCC1CCCC1\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'O=c1nc2cccc3c2=c(//)C(=O)CCC1CCCC1' for input: 'O=c1nc2cccc3c2=c(//)C(=O)CCC1CCCC1'\n",
      "[01:02:55] Explicit valence for atom # 13 Cl, 2, is greater than permitted\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 1 2 11 12 13 18 19\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CCN1CCN(CC1CCN(C(=O)c3ccc(C(C)(C)C)cc3)CC2)CC1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17 25 26\n",
      "[01:02:55] SMILES Parse Error: extra open parentheses for input: 'OC(c1cccc(Cl)c1cc1nc(-c2cc3ccccc3o2)cs1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 7 8 10\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)CSc2nc3c(c(-c4cccs4)c2C#N)CN(C)C)cc1OC'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 26\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 0 1 16 17 18\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 20 21 22 23 25 26\n",
      "[01:02:55] Explicit valence for atom # 20 O, 3, is greater than permitted\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'CC(=O)Nc1ccc(N2CCc3c(C)c(C#N)c4nc5ccccc4n4c32)cc1'\n",
      "[01:02:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(C(CNC(=O)CCc2nnc(CCCc3ccccc3)o2)cc2'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 3 4 16 17 18\n",
      "[01:02:55] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(CNC(=O)CSCc2cccc(Cl)c2'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 5 9 10 11 12 13 14\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 5 6 14 15 16 17 18 19\n",
      "[01:02:55] Explicit valence for atom # 11 N, 4, is greater than permitted\n",
      "[01:02:55] SMILES Parse Error: syntax error while parsing: CCOc1ccc(C2=()OC(C(=O)O)NC2(c3ccccc3)CC(C)(C)N2)cc1\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'CCOc1ccc(C2=()OC(C(=O)O)NC2(c3ccccc3)CC(C)(C)N2)cc1' for input: 'CCOc1ccc(C2=()OC(C(=O)O)NC2(c3ccccc3)CC(C)(C)N2)cc1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1ccc2c(c1)Cn1nc1nc1ccccc11'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2csc3nc(-c4c(Oc4ccccc5)cs4)noc23)cc1'\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'COc1ccc2c(C(=O)N3CCC4(CC3)NC(C)(C)CC3)cccc2c1'\n",
      "[01:02:55] SMILES Parse Error: syntax error while parsing: COc1cc(OC)cc(C(=O)Nc2c(\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'COc1cc(OC)cc(C(=O)Nc2c(' for input: 'COc1cc(OC)cc(C(=O)Nc2c('\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 4 6 7 8 21 22\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4\n",
      "[01:02:55] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(OCC(O)CN2C3CCCC2)c(C)c1'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 5 6 14 15 16\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 9 10 12 20 21 22\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 19 20 22\n",
      "[01:02:55] SMILES Parse Error: extra close parentheses while parsing: CCCN(C)C[C@@H]1Oc2ncc(-c3nc(-c4ccc(Cl)cc4)cs3)cs2)c(O)c1\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'CCCN(C)C[C@@H]1Oc2ncc(-c3nc(-c4ccc(Cl)cc4)cs3)cs2)c(O)c1' for input: 'CCCN(C)C[C@@H]1Oc2ncc(-c3nc(-c4ccc(Cl)cc4)cs3)cs2)c(O)c1'\n",
      "[01:02:55] SMILES Parse Error: extra close parentheses while parsing: FC(F)(F)C1=nNc2ccccn2)c2ccccc21\n",
      "[01:02:55] SMILES Parse Error: Failed parsing SMILES 'FC(F)(F)C1=nNc2ccccn2)c2ccccc21' for input: 'FC(F)(F)C1=nNc2ccccn2)c2ccccc21'\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8\n",
      "[01:02:55] Can't kekulize mol.  Unkekulized atoms: 2 3 14 15 16 17 18 19 20\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 12 16\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'c1cc2c(cc1-c1c[nH]c3ncnn12)OCCO2'\n",
      "[01:02:56] SMILES Parse Error: extra open parentheses for input: 'CC1CCN(C(C(=O)Nc2cc(C(C)(C)C)nn2Cc2ccccc2)CC1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 9 10 23\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Clc1ccc(CN2C3=NCCCN3c3cc(C2CC3)nc23)cc1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 16 17\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CCN(CC)c1cc2nc3c(c(=O)[nH]2)CCCC3'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 7 8 9 18 27\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: O=C(O)CCC(=O)Nc1ccccc1)c1ccc2c(c1)OCO2\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CCC(=O)Nc1ccccc1)c1ccc2c(c1)OCO2' for input: 'O=C(O)CCC(=O)Nc1ccccc1)c1ccc2c(c1)OCO2'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: CSCCC1NC(=O)c2ccc(Cl)cc2)C1O\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'CSCCC1NC(=O)c2ccc(Cl)cc2)C1O' for input: 'CSCCC1NC(=O)c2ccc(Cl)cc2)C1O'\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: Cc1nc2sc3c(=O)n(-c4cccc(Cl)c4)c(Nc4cccc(Oc4cc)cc4)c3c2)cn1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'Cc1nc2sc3c(=O)n(-c4cccc(Cl)c4)c(Nc4cccc(Oc4cc)cc4)c3c2)cn1' for input: 'Cc1nc2sc3c(=O)n(-c4cccc(Cl)c4)c(Nc4cccc(Oc4cc)cc4)c3c2)cn1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 0 1 2 4 19 20\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 6 7 8 22 23\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 3 4 15\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2C[C@@H](CO)O[C@@H](n3cnc4c(N4CCCC5)ncnc43)C2)cc1'\n",
      "[01:02:56] Explicit valence for atom # 5 C, 5, is greater than permitted\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 6 23 25 26 27\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccc(CN2CN(Cc3ccc(C)cc3)C(=O)N(n3c4ccccc4)CC2(C)C)cc1'\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: Cc1ccccc1N1C(=O)CN(C(C)=O)C2CC(=O)NCc2cccs2)C1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'Cc1ccccc1N1C(=O)CN(C(C)=O)C2CC(=O)NCc2cccs2)C1' for input: 'Cc1ccccc1N1C(=O)CN(C(C)=O)C2CC(=O)NCc2cccs2)C1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'C[C@@H]1[C@@H]2N(CC3CCCCC3)C(=O)[C@@]26CCCN12'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Oc1cc2c(cc1-c1ccc3c(c1)OCCO3)N1CCC2(CC1)OCCO2'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CC(CO2CCC1)N(Cc1ccco1)C(=O)Nc1ccc(Cl)c(Cl)c1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 6 7 8 22 23\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CSc2ncc3ccccc2n2)cc1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 19\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COCn1ncc2c(C(=O)Nc3ccccc3SCC)cccc2c12'\n",
      "[01:02:56] SMILES Parse Error: ring closure 4 duplicates bond between atom 13 and atom 14 for input: 'Cc1nc2ccc(NC(=O)N3CCn4c4ccccc4S3)cc2s1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(cc1OC)CCc1c-2c(cc1C)CCC3'\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: COCCN1CC2CN(Cc3ccc(F)cc3)C(=O)C2(C)[C@H](c3ccccc3)N3CCCCC23)C1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'COCCN1CC2CN(Cc3ccc(F)cc3)C(=O)C2(C)[C@H](c3ccccc3)N3CCCCC23)C1' for input: 'COCCN1CC2CN(Cc3ccc(F)cc3)C(=O)C2(C)[C@H](c3ccccc3)N3CCCCC23)C1'\n",
      "[01:02:56] SMILES Parse Error: syntax error while parsing: CC(=O)Nc1ccc(S==)C(CC(C)C)N2CCN(C)CC2)cc1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'CC(=O)Nc1ccc(S==)C(CC(C)C)N2CCN(C)CC2)cc1' for input: 'CC(=O)Nc1ccc(S==)C(CC(C)C)N2CCN(C)CC2)cc1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 10 12 13 14 15\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 23 25\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 16 17 18 19 22\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1ncn(-c2ccc(Nc3ccc(Nc4ncc4ccccc4n3)cc3)nc2)n1'\n",
      "[01:02:56] SMILES Parse Error: extra open parentheses for input: 'CC(C(=O)N1N=C2/C(=C/c3ccco3)CCCC2C1c1cccs1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 17 18 19 24 25\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccc(NC(=O)C2C3C3CCC3C3)c(OC)c1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 3 14 15 17 18 19 20 21 22\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1CCN1COCC1CN(C(=O)C1CC1)CC1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CCN1CCN(C2CCN(Cc3c(C)n(C)c3ccccc34)CC2)CC1'\n",
      "[01:02:56] SMILES Parse Error: syntax error while parsing: Cc1ccc(C#=)Nn2c(SCC(=O)N3CCCC3)nc(O)cc2c1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'Cc1ccc(C#=)Nn2c(SCC(=O)N3CCCC3)nc(O)cc2c1' for input: 'Cc1ccc(C#=)Nn2c(SCC(=O)N3CCCC3)nc(O)cc2c1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CC(C)CC(=O)Nc1nc2c(nn12)C(C)(C)CC2'\n",
      "[01:02:56] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1nnc(N(C(=O)CSc2nc3ccccc3s2)s2)c1C#N'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 4 5 6 7 14\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(CN2CCSc3ccc(C(=O)NC4CCN(Cc5ccc6ccccc5c5)cc43)cc2)cc1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'c1cnc2c(-c3cnc4c(Nc5ccc5[nH]ccc5c4)nccn34)cccc2c1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 10 12 13 14 15 16 17\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 3 18 19 20 21 22 23 24\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 14 15 22\n",
      "[01:02:56] non-ring atom 14 marked aromatic\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'O=c1oc(N2CC=O)nc2c1cnn2-c1ccccc1'\n",
      "[01:02:56] SMILES Parse Error: syntax error while parsing: Cc1cccc(-n2sc3c(-2CC(C)(C)CC(C)(C)C4)c(C(=O)NCCC(C)C)n2c1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'Cc1cccc(-n2sc3c(-2CC(C)(C)CC(C)(C)C4)c(C(=O)NCCC(C)C)n2c1' for input: 'Cc1cccc(-n2sc3c(-2CC(C)(C)CC(C)(C)C4)c(C(=O)NCCC(C)C)n2c1'\n",
      "[01:02:56] SMILES Parse Error: extra open parentheses for input: 'CC(NC(=O)CSc1nc2c(c(C)nn1-c1ccccc1)CN(C)C2'\n",
      "[01:02:56] SMILES Parse Error: ring closure 2 duplicates bond between atom 18 and atom 19 for input: 'Cc1cc(C)c(-n2c(O)c(C2CCCCC3)c(=O)n2-c2ccccc2)n1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 4 16 18\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'C[C@]12CCC3C(CC[C@H]4[C@H](CC2O)CC[C@]34C)C1CC[C@@H]2O'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 20 21 22 29 30 31 32 33 34\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 11\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 4 6 13\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: Cc1cc2nc3n(c2cc1C)CC1CC(C(=O)NC2CCS(=O)(=O)C2)S3)c1OC\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'Cc1cc2nc3n(c2cc1C)CC1CC(C(=O)NC2CCS(=O)(=O)C2)S3)c1OC' for input: 'Cc1cc2nc3n(c2cc1C)CC1CC(C(=O)NC2CCS(=O)(=O)C2)S3)c1OC'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16 17 18 19 21\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 17 18 19\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 29\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 8 9 11 12 13 14 15 16 17 18 19\n",
      "[01:02:56] SMILES Parse Error: ring closure 1 duplicates bond between atom 20 and atom 21 for input: 'O=C(CCC1CCCC1c1)Nc1cc(C(F)(F)F)ccc1N1CCOCC1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 22\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CCCNC(=O)c1ccc(CN2CC[C@]3(C)C(=O)NC2CC(=O)O)cc1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 19 20 21 22 23 24\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 4 25 26\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CSc1ncccc1C(=O)Nc1nc(SC(F)(F)F)sc1c1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 10 11 12 13 20\n",
      "[01:02:56] SMILES Parse Error: ring closure 2 duplicates bond between atom 8 and atom 9 for input: 'O=C(C1CN(CCCn2c2ccccc2)CC1)Nc1ccc(F)cc1'\n",
      "[01:02:56] SMILES Parse Error: ring closure 1 duplicates bond between atom 9 and atom 10 for input: 'Cc1oc2ccccc1c1-c1ccc2c(c1)[C@@H]1CN(C)CCC1=C2'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CCCSc1nnc2c(n1)OC(c1csc3ncnc(=O)c1C)N2CCO'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 7 8 9 17 18 19 20 21 22\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 15 16\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1cc2cc3c(cc2nc1N2CCCC1)OCCO3'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 17\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15\n",
      "[01:02:56] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1NC(=O)N(C1CC1CC1)C(=O)c1ccc(S(C)(=O)=O)cc1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 6 7 17 18 19 20 21 22 29\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(NC(=O)c2ccc3c(c2)nnn2C)cc1C'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Clc1cccc2sccc1-c1nnc2sc(COc3ccncc3)nn12'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 6 7 8 9 12\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CCSc1nc(SCC(=O)O)c2s3c(n2c(=O)[nH]c(=O)[nH]c23)C1CCCO1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 7 8 9 10 24\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 8 9 10 11 12 13 14 21 23\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 22 23 24\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'O=C(NCc1ccc2nsnc1c1)NC1CCCCC1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CC12CSC3(CC(=O)Nc2ccccc1)OCC(=O)N2'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CN(C)CCNC(=O)c1cc2sc3c(c2c1CN(Cc1ccccc1)CC1)CCC3'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1cccc(NC(=O)NC2CC3CCC(C2)N23)c1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'OC(c1ccc2(F)cccc1Nc1nc2ccccc2[nH]1)c1ccc2c(c1)OCO2'\n",
      "[01:02:56] SMILES Parse Error: extra close parentheses while parsing: Cc1cc(C2=NOC(C)(c3nnc4n4CCCCC4)C3)n2-c2ccccc2)no1\n",
      "[01:02:56] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C2=NOC(C)(c3nnc4n4CCCCC4)C3)n2-c2ccccc2)no1' for input: 'Cc1cc(C2=NOC(C)(c3nnc4n4CCCCC4)C3)n2-c2ccccc2)no1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 4 5 7 8 9 16\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CN1CCCC(N(C)CC(=O)Nc2ccccc2)c1ccccc1Cl'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 9 10 11 12\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 10 11\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 15 16 24\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 12 13 17 18 25 28 29\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2oc3cc4c(cc3c2-c2ccc(O)cc2)OCO3)cc1'\n",
      "[01:02:56] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(C(=O)Nc2ccc(C(=O)NCC(c3cccs3)cc2)cc1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2nc(C#N)c(N3CC4CCCC3C3)o2)cc1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Fc1cccc(-c2cc3nc4c(c(N5CCC5(CC5)OCCO6)n3n2)CCC4)c1'\n",
      "[01:02:56] SMILES Parse Error: extra open parentheses for input: 'CCN(C(=O)CCSc1nc(C)cc(C)c1C#N'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COCc1cc(/C=C\\\\CCCC2)nc(-c2ccccc2)n1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2 3 4 6 20 21\n",
      "[01:02:56] Explicit valence for atom # 11 C, 5, is greater than permitted\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 20 21 23 24\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)n(CCN2CCC(n3ncc4cc(-c5ccccc5)nn3)CC2)n1'\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 17\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 2\n",
      "[01:02:56] Explicit valence for atom # 2 O, 3, is greater than permitted\n",
      "[01:02:56] Can't kekulize mol.  Unkekulized atoms: 4 26 27\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'CCn1c(SCC(=O)N2CCCc3cc(OC)c(OC)cc3)nnc1-c1cccs1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Clc1cccc2ccc(CN3CCn4cncnc42)cn12'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'COc1ccc(-c2cc3nc(-c4ccccc4)cc(N4CC5CCC(C)C5)nc3n2)cc1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1cc(C)n(CCc2nc(-c3cc4c(c(=c3[nH])c3C(=O)NCc4ccco4)no3)cn2)c1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Fc1ccc(-c2c3c(nc4sc5c(=O)[nH]c(C)nc5c23)CCC3)cc1'\n",
      "[01:02:56] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(C23C(=O)NC(C)(C)C2(C)C(=O)Nc2ccc3c(c2)OCCO3)cc1'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'CCN(CC)c1nc2c3c(nc4c(c3c1C#N)CC(=O)N4c1ccccc1)CCC3'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 15 16 17 18 19\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'c1ccc(C2CC3C(c4cccnc4)=NN22)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 21 22\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'COc1ccc(C(=O)Nc2ccc3c(c2)C(=O)N3CCCCC3)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 3\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'Clc1cccc(Oc2ccccc2CNC2=NCCN23)c1'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'CN1C2=Nc3ccccc3OC23CC1c1ccccc1Cl'\n",
      "[01:02:57] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(C2=Nn3c(n2c(=O)c(=O)n(-c4ccc(C)cc4)c(=O)n3C2)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 3 5 21\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 1 2 14\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'CN(C)c1cccc(Nc2ccnc3c(-c4cccc(O)c3)cnn23)c1'\n",
      "[01:02:57] SMILES Parse Error: extra close parentheses while parsing: COc1ccc2nc3c(c(=O)[nH]2)C(C)(C)CC3)cc1\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'COc1ccc2nc3c(c(=O)[nH]2)C(C)(C)CC3)cc1' for input: 'COc1ccc2nc3c(c(=O)[nH]2)C(C)(C)CC3)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 6 7 9\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'Cc1cccc2c1c1Sc1ccc(Cl)cc1CC2N1CCNCC1'\n",
      "[01:02:57] SMILES Parse Error: extra close parentheses while parsing: c1ccc(OCc2nn3sc(-c4cccs4)n3)n2-c2ccccc2)cc1\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'c1ccc(OCc2nn3sc(-c4cccs4)n3)n2-c2ccccc2)cc1' for input: 'c1ccc(OCc2nn3sc(-c4cccs4)n3)n2-c2ccccc2)cc1'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'Cc1cccn2c(CN(C)Cc3nc4cc(C)ccc4C)c(C(=O)N3CCOCC3)nc12'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 23\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(-c2cc3nc(-c4ccccc4)cc(N4CCC5(CC5)OCCO5)n3n2)cc1'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'c1ccc2c(c1)Oc1ccc3ccccc3c2'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'N#Cc1ccc(-c2c3c(nc4ccccc24)CCNC2)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 9 10 11 12 13 14 16\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'Fc1cccc(CSc2nc(NCC2CCCO3)c3ccccc3n2)c1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 12 13 14\n",
      "[01:02:57] SMILES Parse Error: extra close parentheses while parsing: CC1CC2C(C)(C)N(C(=O)c3ccc(Cl)cc3)c3cc(F)ccc3C2(C)C)c1\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'CC1CC2C(C)(C)N(C(=O)c3ccc(Cl)cc3)c3cc(F)ccc3C2(C)C)c1' for input: 'CC1CC2C(C)(C)N(C(=O)c3ccc(Cl)cc3)c3cc(F)ccc3C2(C)C)c1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 11 12\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'COc1cc2c(cc1OC)C1Cc3c(cnn1-c3cccc(Cl)c3)C1=O2'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7 8 9 22 23\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'CNC(=O)COc1ccc2oc3c(c3c1nCCCC4)CCCC3'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'O=C(Nc1cnc2cnc(-c3cccs3)c(-c3cccs3)n1)c1cccs1'\n",
      "[01:02:57] SMILES Parse Error: extra close parentheses while parsing: O=c1c2nc(-c3ccc(Cl)cc3)n(Cc3ccccc3)n2)c2ccccc2n1\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'O=c1c2nc(-c3ccc(Cl)cc3)n(Cc3ccccc3)n2)c2ccccc2n1' for input: 'O=c1c2nc(-c3ccc(Cl)cc3)n(Cc3ccccc3)n2)c2ccccc2n1'\n",
      "[01:02:57] SMILES Parse Error: extra close parentheses while parsing: CCC(=O)N1N=C(c2c(F)cccc2Cl)C1=O)C1CCCCC1\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'CCC(=O)N1N=C(c2c(F)cccc2Cl)C1=O)C1CCCCC1' for input: 'CCC(=O)N1N=C(c2c(F)cccc2Cl)C1=O)C1CCCCC1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 10 11 12 23 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling smiles at most 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1C1(CCn2c(=O)oc2ccccc2)CCO1'\n",
      "[01:02:57] SMILES Parse Error: extra open parentheses for input: 'Cc1ccc(N(C2=NC3CCCCC3=NC2(C(=O)NCc2ccco2)C2)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 6 7 17 18 19 20 21 22 23\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'COc1ccccc1N1CCN(CCC(Oc2ccc(C)cc2)CC2CCO)CC1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 6 7 8\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 15 16 17\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 12 13 24\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 3 4 5 6 7\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 4 5 6 11 12 13 14 15 16\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'O=C(CN1CCCC1)N1c2ccccc2C1(c1ccccc1)CCCC1'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'CC(C)N1CC2C3CC1C(C)(CO)N4C(=O)c1cc2ccccc2o1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 6 7 9 10 18 19 22\n",
      "[01:02:57] SMILES Parse Error: extra open parentheses for input: 'COc1ccc(C(CNC(=O)c2cc3cc(C)ccc3n2C)cc1'\n",
      "[01:02:57] SMILES Parse Error: extra close parentheses while parsing: COc1ccccc1NC(=O)CS(=O)(=O)c1ccccc1)CC\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'COc1ccccc1NC(=O)CS(=O)(=O)c1ccccc1)CC' for input: 'COc1ccccc1NC(=O)CS(=O)(=O)c1ccccc1)CC'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'COc1ccc(S(=O)(=O)n2c3ccc(OC)cc3sc3n2)cc1'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 2 3 4 5 23 24 26\n",
      "[01:02:57] SMILES Parse Error: syntax error while parsing: Cc1cc(C)n(-c2cccc(Cl)c2C#=O)N1CCN(C)CC1\n",
      "[01:02:57] SMILES Parse Error: Failed parsing SMILES 'Cc1cc(C)n(-c2cccc(Cl)c2C#=O)N1CCN(C)CC1' for input: 'Cc1cc(C)n(-c2cccc(Cl)c2C#=O)N1CCN(C)CC1'\n",
      "[01:02:57] SMILES Parse Error: unclosed ring for input: 'Cc1ccc(SCCN2CCc3(C)CCC(=O)N34CCCC3)cc2'\n",
      "[01:02:57] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 19 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, training: 9 batches, size 16*1\n",
      "Epoch 1 -- Batch 1/ 9, training loss 1.9817802906036377\n",
      "Epoch 1 -- Batch 2/ 9, training loss 1.890822172164917\n",
      "Epoch 1 -- Batch 3/ 9, training loss 1.9888304471969604\n",
      "Epoch 1 -- Batch 4/ 9, training loss 1.689042091369629\n",
      "Epoch 1 -- Batch 5/ 9, training loss 1.3676780462265015\n",
      "Epoch 1 -- Batch 6/ 9, training loss 1.5268007516860962\n",
      "Epoch 1 -- Batch 7/ 9, training loss 1.4545291662216187\n",
      "Epoch 1 -- Batch 8/ 9, training loss 1.271896243095398\n",
      "Epoch 1 -- Batch 9/ 9, training loss 1.192453384399414\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     25\u001b[0m sampling_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     27\u001b[0m params \u001b[38;5;241m=\u001b[39m [\t\n\u001b[1;32m     28\u001b[0m \t\t\t\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--data=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m \t\t\t\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--smi-colname=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmicol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \t\t\t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--use-cpus\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# force working on CPUs\u001b[39;00m\n\u001b[1;32m     43\u001b[0m ]\n\u001b[0;32m---> 45\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, *argv)\u001b[0m\n\u001b[1;32m     21\u001b[0m argv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(argv)\n\u001b[1;32m     22\u001b[0m app_cls\u001b[38;5;241m=\u001b[39mimportstr(\u001b[38;5;241m*\u001b[39mapp\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 23\u001b[0m \u001b[43mapp_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/notebooks/../src/model/generative_models/apps/Finetune.py:13\u001b[0m, in \u001b[0;36mFinetunerApp.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     12\u001b[0m \ttuner \u001b[38;5;241m=\u001b[39m Finetuner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margv)\n\u001b[0;32m---> 13\u001b[0m \t\u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/notebooks/../src/model/generative_models/generative_models/finetune.py:143\u001b[0m, in \u001b[0;36mFinetuner.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m tr_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_average_loss(tr_metrics, \u001b[38;5;241m0\u001b[39m, nsample_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# sampling \u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m sampling_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_smiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfd_smiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_dl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg_smi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m nsampling \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    144\u001b[0m epoch_metrics \t \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_avg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: tr_avg_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m'\u001b[39m: nsampling} \n\u001b[1;32m    145\u001b[0m epoch_metrics\u001b[38;5;241m.\u001b[39mupdate(sampling_metrics)\n",
      "File \u001b[0;32m~/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/notebooks/../src/model/generative_models/generative_models/finetune.py:198\u001b[0m, in \u001b[0;36mFinetuner.sampling_smiles\u001b[0;34m(self, n, epoch_id, fd_smiles, training_smiles)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampling_smiles\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, epoch_id, fd_smiles, training_smiles):\n\u001b[1;32m    197\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m \t\t\tsampled_smiles \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m \t\t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPT sampling is unimplemented, here.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/notebooks/../src/model/generative_models/generative_models/sampling.py:42\u001b[0m, in \u001b[0;36mLSTMsampler.probability_sampling\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m sampled_tokens[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenids\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length):\n\u001b[0;32m---> 42\u001b[0m \tout, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \tprobs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(out\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (n, ntokens)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \tnext_tokenids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/alpha/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/alpha/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Paper/Pretraining-Assesment-for-LSTM-Molecular-Generation/notebooks/../src/model/generative_models/generative_models/lstm.py:50\u001b[0m, in \u001b[0;36mVanilaLSTM.forward\u001b[0;34m(self, input, input_state, return_state)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[1;32m     49\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m out, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm:\n\u001b[1;32m     52\u001b[0m \tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/alpha/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/alpha/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alpha/lib/python3.9/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "sys.path.append('../src/model/')\n",
    "from src.paths import ensure_dirs, FINETUNE_DATA, FINETUNE_RESULTS, FINETUNE_FILTER\n",
    "ensure_dirs()\n",
    "\n",
    "f_data_list = ['CHEMBL4005', 'CHEMBL1908389', 'CHEMBL284', 'CHEMBL214', 'CHEMBL253']\n",
    "\n",
    "for dataset in dataset_list:\n",
    "\tfor f_data in f_data_list:\n",
    "\t\tmodel_dir = f'{PRETRAIN_RESULTS}/{dataset}_results/vanilalstm'\n",
    "\t\toutfd     = f'{FINETUNE_RESULTS}/{dataset}_results/{f_data}_finetune'\n",
    "\t\tappname   = 'generative_models.apps.Finetune.FinetunerApp'\n",
    "\t\tvocab     = f'{model_dir}/model/vocabulary.pickle'\n",
    "\t\tm_str \t  = glob.glob(f'{model_dir}/model/best_model_structure_epoch*.pickle')[0]\n",
    "\t\tm_state   = glob.glob(f'{model_dir}/model/best_model_epoch*.pth')[0]\n",
    "\t\tsave_epoch_models = False\n",
    "\n",
    "\t\tdata      = f'{FINETUNE_DATA}/{FINETUNE_FILTER}-{f_data}_train_rdsmi3.tsv'\n",
    "\t\tsmicol    = 'rd3_smiles'\n",
    "\t\tepochs\t  = 100\n",
    "\t\trseed \t  = 42\n",
    "\t\tbatchsize = 16\n",
    "\t\tlr \t\t  = 1e-4 # small learning rate\n",
    "\t\tsampling_epoch = 10000\n",
    "\n",
    "\t\tparams = [\t\n",
    "\t\t\t\t\tf'--data={data}',\n",
    "\t\t\t\t\tf'--smi-colname={smicol}',\n",
    "\t\t\t\t\tf'--sampling-epoch={sampling_epoch}',\n",
    "\t\t\t\t\t'--model=lstm',\n",
    "\t\t\t\t\tf'--model-structure={m_str}',\n",
    "\t\t\t\t\tf'--model-state={m_state}',\n",
    "\t\t\t\t\tf'--vocab={vocab}',\n",
    "\t\t\t\t\tf'--epochs={epochs}',\n",
    "\t\t\t\t\t'--override-folder=1',\n",
    "\t\t\t\t\tf'--outdir={outfd}',\n",
    "\t\t\t\t\tf'--random-seed={rseed}',\n",
    "\t\t\t\t\tf'--batch-size={batchsize}',\n",
    "\t\t\t\t\tf'--lr={lr}',\n",
    "\t\t\t\t\tf'--exclude-pad-loss=1',\n",
    "\t\t\t\t\t'--use-cpus' # force working on CPUs\n",
    "\t\t]\n",
    "\n",
    "\t\trun(appname, *params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smi-lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
